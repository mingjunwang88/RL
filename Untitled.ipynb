{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fbe396",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.distributions.normal import Normal\n",
    "\n",
    "dist = Normal(0,1)\n",
    "for i in range(10):\n",
    "    a = dist.sample()\n",
    "    prob = dist.log_prob(a)\n",
    "    \n",
    "    print(a, prob)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "061edf45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6)\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([1,2,3])\n",
    "print(torch.prod(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0e15ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.5\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as t\n",
    "print (t.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb276c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-04 15:53:50,948\tINFO services.py:1272 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8266\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 15:53:53 (running for 00:00:00.22)<br>Memory usage on this node: 11.5/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">    lr</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">0.0001</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=83435)\u001b[0m 2021-11-04 15:53:57,974\tINFO trainer.py:753 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=83435)\u001b[0m 2021-11-04 15:53:57,974\tINFO ppo.py:167 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(pid=83435)\u001b[0m 2021-11-04 15:53:57,974\tINFO trainer.py:772 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=83425)\u001b[0m 2021-11-04 15:54:02,294\tWARNING deprecation.py:39 -- DeprecationWarning: `SampleBatch['is_training']` has been deprecated. Use `SampleBatch.is_training` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=83435)\u001b[0m 2021-11-04 15:54:03,603\tWARNING deprecation.py:39 -- DeprecationWarning: `SampleBatch['is_training']` has been deprecated. Use `SampleBatch.is_training` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 15:54:04 (running for 00:00:11.51)<br>Memory usage on this node: 12.4/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=83435)\u001b[0m 2021-11-04 15:54:04,859\tWARNING trainer_template.py:186 -- `execution_plan` functions should accept `trainer`, `workers`, and `config` as args!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 15:54:05 (running for 00:00:12.52)<br>Memory usage on this node: 12.4/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=83435)\u001b[0m 2021-11-04 15:54:08,327\tWARNING deprecation.py:39 -- DeprecationWarning: `slice` has been deprecated. Use `SampleBatch[start:stop]` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 15:54:10 (running for 00:00:17.52)<br>Memory usage on this node: 12.4/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_15-54-11\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -770.8213603838424\n",
      "  episode_reward_mean: -1233.3890338688866\n",
      "  episode_reward_min: -1685.554320738407\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 20\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.4127031564712524\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007051610387861729\n",
      "          model: {}\n",
      "          policy_loss: 0.005410493817180395\n",
      "          total_loss: 149657.421875\n",
      "          vf_explained_var: -0.000414686364820227\n",
      "          vf_loss: 149657.421875\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.299999999999999\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12208014235146222\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17958036486593737\n",
      "    mean_inference_ms: 1.2663414572430274\n",
      "    mean_raw_obs_processing_ms: 0.10702742272052453\n",
      "  time_since_restore: 6.464773654937744\n",
      "  time_this_iter_s: 6.464773654937744\n",
      "  time_total_s: 6.464773654937744\n",
      "  timers:\n",
      "    learn_throughput: 1328.839\n",
      "    learn_time_ms: 3010.145\n",
      "    load_throughput: 6127544.193\n",
      "    load_time_ms: 0.653\n",
      "    sample_throughput: 1154.059\n",
      "    sample_time_ms: 3466.028\n",
      "    update_time_ms: 3.479\n",
      "  timestamp: 1636041251\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 15:54:16 (running for 00:00:23.03)<br>Memory usage on this node: 12.4/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         6.46477</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\">-1233.39</td><td style=\"text-align: right;\">            -770.821</td><td style=\"text-align: right;\">            -1685.55</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_15-54-17\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -767.5826266724762\n",
      "  episode_reward_mean: -1158.8270073465517\n",
      "  episode_reward_min: -1776.2899276425132\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 40\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.4227598905563354\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00813443586230278\n",
      "          model: {}\n",
      "          policy_loss: 0.0029238152783364058\n",
      "          total_loss: 117275.2890625\n",
      "          vf_explained_var: 0.006032002158463001\n",
      "          vf_loss: 117275.28125\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 8000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.1875\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12250489929891577\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1804862397027928\n",
      "    mean_inference_ms: 1.2655140214537393\n",
      "    mean_raw_obs_processing_ms: 0.10675609288924677\n",
      "  time_since_restore: 12.725764036178589\n",
      "  time_this_iter_s: 6.260990381240845\n",
      "  time_total_s: 12.725764036178589\n",
      "  timers:\n",
      "    learn_throughput: 1381.309\n",
      "    learn_time_ms: 2895.804\n",
      "    load_throughput: 5922067.067\n",
      "    load_time_ms: 0.675\n",
      "    sample_throughput: 800.032\n",
      "    sample_time_ms: 4999.798\n",
      "    update_time_ms: 3.266\n",
      "  timestamp: 1636041257\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 2\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 15:54:21 (running for 00:00:28.42)<br>Memory usage on this node: 12.4/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         12.7258</td><td style=\"text-align: right;\">8000</td><td style=\"text-align: right;\">-1158.83</td><td style=\"text-align: right;\">            -767.583</td><td style=\"text-align: right;\">            -1776.29</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_15-54-24\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -767.5826266724762\n",
      "  episode_reward_mean: -1155.7162360270543\n",
      "  episode_reward_min: -1787.4131629778672\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 60\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.4479613304138184\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006515010725706816\n",
      "          model: {}\n",
      "          policy_loss: 0.006047605536878109\n",
      "          total_loss: 119726.84375\n",
      "          vf_explained_var: 0.015949124470353127\n",
      "          vf_loss: 119726.8359375\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.412500000000001\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12278732449833415\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.181061435193131\n",
      "    mean_inference_ms: 1.2654726351249532\n",
      "    mean_raw_obs_processing_ms: 0.10676239255244648\n",
      "  time_since_restore: 19.048197746276855\n",
      "  time_this_iter_s: 6.322433710098267\n",
      "  time_total_s: 19.048197746276855\n",
      "  timers:\n",
      "    learn_throughput: 1393.502\n",
      "    learn_time_ms: 2870.465\n",
      "    load_throughput: 6035693.488\n",
      "    load_time_ms: 0.663\n",
      "    sample_throughput: 730.449\n",
      "    sample_time_ms: 5476.085\n",
      "    update_time_ms: 3.177\n",
      "  timestamp: 1636041264\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 15:54:27 (running for 00:00:33.84)<br>Memory usage on this node: 12.4/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         19.0482</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">-1155.72</td><td style=\"text-align: right;\">            -767.583</td><td style=\"text-align: right;\">            -1787.41</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_15-54-30\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -767.5826266724762\n",
      "  episode_reward_mean: -1136.9252639363463\n",
      "  episode_reward_min: -1787.4131629778672\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 80\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.4305073022842407\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.004209517035633326\n",
      "          model: {}\n",
      "          policy_loss: 0.010362128727138042\n",
      "          total_loss: 95586.2890625\n",
      "          vf_explained_var: 0.025313878431916237\n",
      "          vf_loss: 95586.28125\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 16000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.375\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12298213701878365\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.18146421135117474\n",
      "    mean_inference_ms: 1.2658199470163964\n",
      "    mean_raw_obs_processing_ms: 0.10680336387351361\n",
      "  time_since_restore: 25.359060525894165\n",
      "  time_this_iter_s: 6.31086277961731\n",
      "  time_total_s: 25.359060525894165\n",
      "  timers:\n",
      "    learn_throughput: 1402.926\n",
      "    learn_time_ms: 2851.183\n",
      "    load_throughput: 5964172.058\n",
      "    load_time_ms: 0.671\n",
      "    sample_throughput: 700.477\n",
      "    sample_time_ms: 5710.392\n",
      "    update_time_ms: 3.233\n",
      "  timestamp: 1636041270\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 4\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 15:54:32 (running for 00:00:39.24)<br>Memory usage on this node: 12.4/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         25.3591</td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\">-1136.93</td><td style=\"text-align: right;\">            -767.583</td><td style=\"text-align: right;\">            -1787.41</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 20000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_15-54-36\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -757.2518460428563\n",
      "  episode_reward_mean: -1119.6704019875592\n",
      "  episode_reward_min: -1787.4131629778672\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 100\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000149011612\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.421486496925354\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006300297565758228\n",
      "          model: {}\n",
      "          policy_loss: 0.006227793637663126\n",
      "          total_loss: 87398.921875\n",
      "          vf_explained_var: 0.004430216271430254\n",
      "          vf_loss: 87398.9140625\n",
      "    num_agent_steps_sampled: 20000\n",
      "    num_agent_steps_trained: 20000\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.942857142857141\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12302564636292929\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.18157716377368735\n",
      "    mean_inference_ms: 1.264679779336017\n",
      "    mean_raw_obs_processing_ms: 0.10672614672617904\n",
      "  time_since_restore: 31.487109899520874\n",
      "  time_this_iter_s: 6.128049373626709\n",
      "  time_total_s: 31.487109899520874\n",
      "  timers:\n",
      "    learn_throughput: 1411.8\n",
      "    learn_time_ms: 2833.262\n",
      "    load_throughput: 5990151.385\n",
      "    load_time_ms: 0.668\n",
      "    sample_throughput: 686.976\n",
      "    sample_time_ms: 5822.617\n",
      "    update_time_ms: 3.25\n",
      "  timestamp: 1636041276\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 15:54:37 (running for 00:00:44.43)<br>Memory usage on this node: 12.4/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         31.4871</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">-1119.67</td><td style=\"text-align: right;\">            -757.252</td><td style=\"text-align: right;\">            -1787.41</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 15:54:42 (running for 00:00:49.44)<br>Memory usage on this node: 12.4/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         31.4871</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">-1119.67</td><td style=\"text-align: right;\">            -757.252</td><td style=\"text-align: right;\">            -1787.41</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_15-54-43\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -757.2518460428563\n",
      "  episode_reward_mean: -1100.5273573790996\n",
      "  episode_reward_min: -1787.4131629778672\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 120\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000149011612\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.5032885074615479\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013459468260407448\n",
      "          model: {}\n",
      "          policy_loss: 0.0039429920725524426\n",
      "          total_loss: 98090.4609375\n",
      "          vf_explained_var: 0.0032736738212406635\n",
      "          vf_loss: 98090.4609375\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_steps_sampled: 24000\n",
      "    num_steps_trained: 24000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.3875\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12320920624404902\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.18204125086498088\n",
      "    mean_inference_ms: 1.2629326767757891\n",
      "    mean_raw_obs_processing_ms: 0.10656908777790466\n",
      "  time_since_restore: 37.69061899185181\n",
      "  time_this_iter_s: 6.203509092330933\n",
      "  time_total_s: 37.69061899185181\n",
      "  timers:\n",
      "    learn_throughput: 1416.739\n",
      "    learn_time_ms: 2823.385\n",
      "    load_throughput: 5781923.952\n",
      "    load_time_ms: 0.692\n",
      "    sample_throughput: 678.705\n",
      "    sample_time_ms: 5893.577\n",
      "    update_time_ms: 3.356\n",
      "  timestamp: 1636041283\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 6\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 15:54:48 (running for 00:00:54.69)<br>Memory usage on this node: 12.4/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         37.6906</td><td style=\"text-align: right;\">24000</td><td style=\"text-align: right;\">-1100.53</td><td style=\"text-align: right;\">            -757.252</td><td style=\"text-align: right;\">            -1787.41</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 28000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_15-54-49\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -757.2518460428563\n",
      "  episode_reward_mean: -1099.4036181214299\n",
      "  episode_reward_min: -1787.4131629778672\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 140\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000149011612\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.4534506797790527\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006126642692834139\n",
      "          model: {}\n",
      "          policy_loss: 0.007523188367486\n",
      "          total_loss: 76938.2890625\n",
      "          vf_explained_var: 0.001953613245859742\n",
      "          vf_loss: 76938.28125\n",
      "    num_agent_steps_sampled: 28000\n",
      "    num_agent_steps_trained: 28000\n",
      "    num_steps_sampled: 28000\n",
      "    num_steps_trained: 28000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.462499999999999\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12324303716394988\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.18220087359090292\n",
      "    mean_inference_ms: 1.261716779315228\n",
      "    mean_raw_obs_processing_ms: 0.10653319752682057\n",
      "  time_since_restore: 43.958197593688965\n",
      "  time_this_iter_s: 6.267578601837158\n",
      "  time_total_s: 43.958197593688965\n",
      "  timers:\n",
      "    learn_throughput: 1421.541\n",
      "    learn_time_ms: 2813.847\n",
      "    load_throughput: 5845428.899\n",
      "    load_time_ms: 0.684\n",
      "    sample_throughput: 671.471\n",
      "    sample_time_ms: 5957.068\n",
      "    update_time_ms: 3.338\n",
      "  timestamp: 1636041289\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 7\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 15:54:53 (running for 00:01:00.00)<br>Memory usage on this node: 12.4/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         43.9582</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\"> -1099.4</td><td style=\"text-align: right;\">            -757.252</td><td style=\"text-align: right;\">            -1787.41</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 32000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_15-54-55\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -757.2518460428563\n",
      "  episode_reward_mean: -1094.6727943780347\n",
      "  episode_reward_min: -1715.8605021896678\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 160\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000149011612\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.4736645221710205\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008610421791672707\n",
      "          model: {}\n",
      "          policy_loss: 0.007029910571873188\n",
      "          total_loss: 78753.890625\n",
      "          vf_explained_var: 0.0037973555736243725\n",
      "          vf_loss: 78753.890625\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_steps_sampled: 32000\n",
      "    num_steps_trained: 32000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.249999999999998\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12316260808954596\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.18211841448568464\n",
      "    mean_inference_ms: 1.2598876788910345\n",
      "    mean_raw_obs_processing_ms: 0.10638002091209596\n",
      "  time_since_restore: 50.135677099227905\n",
      "  time_this_iter_s: 6.17747950553894\n",
      "  time_total_s: 50.135677099227905\n",
      "  timers:\n",
      "    learn_throughput: 1423.346\n",
      "    learn_time_ms: 2810.28\n",
      "    load_throughput: 5872576.154\n",
      "    load_time_ms: 0.681\n",
      "    sample_throughput: 668.1\n",
      "    sample_time_ms: 5987.129\n",
      "    update_time_ms: 3.299\n",
      "  timestamp: 1636041295\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 32000\n",
      "  training_iteration: 8\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 15:54:58 (running for 00:01:05.23)<br>Memory usage on this node: 12.4/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         50.1357</td><td style=\"text-align: right;\">32000</td><td style=\"text-align: right;\">-1094.67</td><td style=\"text-align: right;\">            -757.252</td><td style=\"text-align: right;\">            -1715.86</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 36000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_15-55-01\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -757.2518460428563\n",
      "  episode_reward_mean: -1095.0904339551237\n",
      "  episode_reward_min: -1719.8064214585092\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 180\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000149011612\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.4467343091964722\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006945260334759951\n",
      "          model: {}\n",
      "          policy_loss: 0.009994235821068287\n",
      "          total_loss: 68868.875\n",
      "          vf_explained_var: 0.002913225907832384\n",
      "          vf_loss: 68868.8671875\n",
      "    num_agent_steps_sampled: 36000\n",
      "    num_agent_steps_trained: 36000\n",
      "    num_steps_sampled: 36000\n",
      "    num_steps_trained: 36000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.3375\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12301395764770928\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1819048863644116\n",
      "    mean_inference_ms: 1.2573989599894158\n",
      "    mean_raw_obs_processing_ms: 0.10617476014859323\n",
      "  time_since_restore: 56.35312533378601\n",
      "  time_this_iter_s: 6.2174482345581055\n",
      "  time_total_s: 56.35312533378601\n",
      "  timers:\n",
      "    learn_throughput: 1422.586\n",
      "    learn_time_ms: 2811.781\n",
      "    load_throughput: 5911402.106\n",
      "    load_time_ms: 0.677\n",
      "    sample_throughput: 665.128\n",
      "    sample_time_ms: 6013.881\n",
      "    update_time_ms: 3.329\n",
      "  timestamp: 1636041301\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 9\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 15:55:03 (running for 00:01:10.49)<br>Memory usage on this node: 12.4/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         56.3531</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\">-1095.09</td><td style=\"text-align: right;\">            -757.252</td><td style=\"text-align: right;\">            -1719.81</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 40000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_15-55-08\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -792.8455946354574\n",
      "  episode_reward_mean: -1082.568886957764\n",
      "  episode_reward_min: -1719.8064214585092\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 200\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000149011612\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3609117269515991\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01605849713087082\n",
      "          model: {}\n",
      "          policy_loss: 0.0009590712725184858\n",
      "          total_loss: 48152.5\n",
      "          vf_explained_var: 0.0027887201867997646\n",
      "          vf_loss: 48152.49609375\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_steps_sampled: 40000\n",
      "    num_steps_trained: 40000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.942857142857145\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12292401691647031\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.18180490572780875\n",
      "    mean_inference_ms: 1.2560927759724299\n",
      "    mean_raw_obs_processing_ms: 0.10605634915519328\n",
      "  time_since_restore: 62.592294692993164\n",
      "  time_this_iter_s: 6.239169359207153\n",
      "  time_total_s: 62.592294692993164\n",
      "  timers:\n",
      "    learn_throughput: 1423.811\n",
      "    learn_time_ms: 2809.363\n",
      "    load_throughput: 5940940.51\n",
      "    load_time_ms: 0.673\n",
      "    sample_throughput: 661.72\n",
      "    sample_time_ms: 6044.857\n",
      "    update_time_ms: 3.308\n",
      "  timestamp: 1636041308\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 10\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 15:55:09 (running for 00:01:15.79)<br>Memory usage on this node: 12.4/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         62.5923</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\">-1082.57</td><td style=\"text-align: right;\">            -792.846</td><td style=\"text-align: right;\">            -1719.81</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 15:55:14 (running for 00:01:20.79)<br>Memory usage on this node: 12.4/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         62.5923</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\">-1082.57</td><td style=\"text-align: right;\">            -792.846</td><td style=\"text-align: right;\">            -1719.81</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 44000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_15-55-14\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -792.8455946354574\n",
      "  episode_reward_mean: -1068.436303215115\n",
      "  episode_reward_min: -1719.8064214585092\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 220\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.10000000149011612\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.4899095296859741\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0216714758425951\n",
      "          model: {}\n",
      "          policy_loss: 0.002483868272975087\n",
      "          total_loss: 55809.7265625\n",
      "          vf_explained_var: 0.0034602016676217318\n",
      "          vf_loss: 55809.71875\n",
      "    num_agent_steps_sampled: 44000\n",
      "    num_agent_steps_trained: 44000\n",
      "    num_steps_sampled: 44000\n",
      "    num_steps_trained: 44000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.2375\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12288603403111993\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1817478394883891\n",
      "    mean_inference_ms: 1.2553262031160337\n",
      "    mean_raw_obs_processing_ms: 0.10598406322653446\n",
      "  time_since_restore: 68.89134216308594\n",
      "  time_this_iter_s: 6.299047470092773\n",
      "  time_total_s: 68.89134216308594\n",
      "  timers:\n",
      "    learn_throughput: 1433.247\n",
      "    learn_time_ms: 2790.866\n",
      "    load_throughput: 5554633.823\n",
      "    load_time_ms: 0.72\n",
      "    sample_throughput: 631.929\n",
      "    sample_time_ms: 6329.822\n",
      "    update_time_ms: 3.257\n",
      "  timestamp: 1636041314\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 44000\n",
      "  training_iteration: 11\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 15:55:19 (running for 00:01:26.14)<br>Memory usage on this node: 12.4/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         68.8913</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\">-1068.44</td><td style=\"text-align: right;\">            -792.846</td><td style=\"text-align: right;\">            -1719.81</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 48000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_15-55-20\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -751.1474756749641\n",
      "  episode_reward_mean: -1064.227634130607\n",
      "  episode_reward_min: -1719.8064214585092\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 240\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15000000596046448\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.4875906705856323\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014657913707196712\n",
      "          model: {}\n",
      "          policy_loss: 0.0026834427844733\n",
      "          total_loss: 49720.28515625\n",
      "          vf_explained_var: 0.0014368611155077815\n",
      "          vf_loss: 49720.28125\n",
      "    num_agent_steps_sampled: 48000\n",
      "    num_agent_steps_trained: 48000\n",
      "    num_steps_sampled: 48000\n",
      "    num_steps_trained: 48000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.1875\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12280240356187154\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.18158439341962584\n",
      "    mean_inference_ms: 1.2540249362047633\n",
      "    mean_raw_obs_processing_ms: 0.10586224943151169\n",
      "  time_since_restore: 75.06096744537354\n",
      "  time_this_iter_s: 6.169625282287598\n",
      "  time_total_s: 75.06096744537354\n",
      "  timers:\n",
      "    learn_throughput: 1433.126\n",
      "    learn_time_ms: 2791.102\n",
      "    load_throughput: 5582542.841\n",
      "    load_time_ms: 0.717\n",
      "    sample_throughput: 634.682\n",
      "    sample_time_ms: 6302.368\n",
      "    update_time_ms: 3.249\n",
      "  timestamp: 1636041320\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 48000\n",
      "  training_iteration: 12\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 15:55:24 (running for 00:01:31.36)<br>Memory usage on this node: 12.4/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">          75.061</td><td style=\"text-align: right;\">48000</td><td style=\"text-align: right;\">-1064.23</td><td style=\"text-align: right;\">            -751.147</td><td style=\"text-align: right;\">            -1719.81</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 52000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_15-55-27\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -751.1474756749641\n",
      "  episode_reward_mean: -1047.4727787987106\n",
      "  episode_reward_min: -1719.8064214585092\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 260\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15000000596046448\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.427693247795105\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012669989839196205\n",
      "          model: {}\n",
      "          policy_loss: 0.004738053772598505\n",
      "          total_loss: 44029.22265625\n",
      "          vf_explained_var: 0.00617385795339942\n",
      "          vf_loss: 44029.21484375\n",
      "    num_agent_steps_sampled: 52000\n",
      "    num_agent_steps_trained: 52000\n",
      "    num_steps_sampled: 52000\n",
      "    num_steps_trained: 52000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.9875\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12275983555011324\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.18151844665186567\n",
      "    mean_inference_ms: 1.2533300948127157\n",
      "    mean_raw_obs_processing_ms: 0.10581275785533849\n",
      "  time_since_restore: 81.34078979492188\n",
      "  time_this_iter_s: 6.27982234954834\n",
      "  time_total_s: 81.34078979492188\n",
      "  timers:\n",
      "    learn_throughput: 1433.917\n",
      "    learn_time_ms: 2789.561\n",
      "    load_throughput: 5581242.848\n",
      "    load_time_ms: 0.717\n",
      "    sample_throughput: 635.872\n",
      "    sample_time_ms: 6290.571\n",
      "    update_time_ms: 3.252\n",
      "  timestamp: 1636041327\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 52000\n",
      "  training_iteration: 13\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 15:55:30 (running for 00:01:36.69)<br>Memory usage on this node: 12.4/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         81.3408</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\">-1047.47</td><td style=\"text-align: right;\">            -751.147</td><td style=\"text-align: right;\">            -1719.81</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 56000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_15-55-33\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -727.6022296442321\n",
      "  episode_reward_mean: -1033.6019133867503\n",
      "  episode_reward_min: -1535.8239155484532\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 280\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15000000596046448\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3797181844711304\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0086321160197258\n",
      "          model: {}\n",
      "          policy_loss: 0.004904964007437229\n",
      "          total_loss: 39215.0234375\n",
      "          vf_explained_var: 0.0009267734712921083\n",
      "          vf_loss: 39215.01953125\n",
      "    num_agent_steps_sampled: 56000\n",
      "    num_agent_steps_trained: 56000\n",
      "    num_steps_sampled: 56000\n",
      "    num_steps_trained: 56000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.099999999999998\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12272503189862351\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.18146094120407041\n",
      "    mean_inference_ms: 1.2527727015537493\n",
      "    mean_raw_obs_processing_ms: 0.1057578114089609\n",
      "  time_since_restore: 87.52337718009949\n",
      "  time_this_iter_s: 6.182587385177612\n",
      "  time_total_s: 87.52337718009949\n",
      "  timers:\n",
      "    learn_throughput: 1434.306\n",
      "    learn_time_ms: 2788.805\n",
      "    load_throughput: 5617873.024\n",
      "    load_time_ms: 0.712\n",
      "    sample_throughput: 637.524\n",
      "    sample_time_ms: 6274.277\n",
      "    update_time_ms: 3.202\n",
      "  timestamp: 1636041333\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 56000\n",
      "  training_iteration: 14\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 15:55:35 (running for 00:01:41.92)<br>Memory usage on this node: 12.4/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">         87.5234</td><td style=\"text-align: right;\">56000</td><td style=\"text-align: right;\"> -1033.6</td><td style=\"text-align: right;\">            -727.602</td><td style=\"text-align: right;\">            -1535.82</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 60000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_15-55-39\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -727.6022296442321\n",
      "  episode_reward_mean: -1029.81075461937\n",
      "  episode_reward_min: -1535.8239155484532\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 300\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15000000596046448\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.276857852935791\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010514616034924984\n",
      "          model: {}\n",
      "          policy_loss: 0.007582388818264008\n",
      "          total_loss: 29895.146484375\n",
      "          vf_explained_var: 0.004608155228197575\n",
      "          vf_loss: 29895.140625\n",
      "    num_agent_steps_sampled: 60000\n",
      "    num_agent_steps_trained: 60000\n",
      "    num_steps_sampled: 60000\n",
      "    num_steps_trained: 60000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.3\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.122718987673126\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.18146006217461733\n",
      "    mean_inference_ms: 1.2525911072984357\n",
      "    mean_raw_obs_processing_ms: 0.1057378245422219\n",
      "  time_since_restore: 93.78138041496277\n",
      "  time_this_iter_s: 6.258003234863281\n",
      "  time_total_s: 93.78138041496277\n",
      "  timers:\n",
      "    learn_throughput: 1433.097\n",
      "    learn_time_ms: 2791.157\n",
      "    load_throughput: 5461511.117\n",
      "    load_time_ms: 0.732\n",
      "    sample_throughput: 637.168\n",
      "    sample_time_ms: 6277.775\n",
      "    update_time_ms: 3.159\n",
      "  timestamp: 1636041339\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 60000\n",
      "  training_iteration: 15\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 15:55:40 (running for 00:01:47.23)<br>Memory usage on this node: 12.4/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         93.7814</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\">-1029.81</td><td style=\"text-align: right;\">            -727.602</td><td style=\"text-align: right;\">            -1535.82</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 15:55:45 (running for 00:01:52.23)<br>Memory usage on this node: 12.4/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         93.7814</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\">-1029.81</td><td style=\"text-align: right;\">            -727.602</td><td style=\"text-align: right;\">            -1535.82</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 64000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_15-55-45\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -727.6022296442321\n",
      "  episode_reward_mean: -1024.7571930825375\n",
      "  episode_reward_min: -1535.8239155484532\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 320\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15000000596046448\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.362396001815796\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006726974621415138\n",
      "          model: {}\n",
      "          policy_loss: 0.007477083709090948\n",
      "          total_loss: 35168.71484375\n",
      "          vf_explained_var: 0.008231008425354958\n",
      "          vf_loss: 35168.70703125\n",
      "    num_agent_steps_sampled: 64000\n",
      "    num_agent_steps_trained: 64000\n",
      "    num_steps_sampled: 64000\n",
      "    num_steps_trained: 64000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.3\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1226938129440709\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1814291545765358\n",
      "    mean_inference_ms: 1.2523164041070263\n",
      "    mean_raw_obs_processing_ms: 0.10569326644766402\n",
      "  time_since_restore: 99.98922896385193\n",
      "  time_this_iter_s: 6.20784854888916\n",
      "  time_total_s: 99.98922896385193\n",
      "  timers:\n",
      "    learn_throughput: 1432.731\n",
      "    learn_time_ms: 2791.872\n",
      "    load_throughput: 5595389.541\n",
      "    load_time_ms: 0.715\n",
      "    sample_throughput: 637.013\n",
      "    sample_time_ms: 6279.308\n",
      "    update_time_ms: 3.072\n",
      "  timestamp: 1636041345\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 64000\n",
      "  training_iteration: 16\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 15:55:50 (running for 00:01:57.49)<br>Memory usage on this node: 12.4/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         99.9892</td><td style=\"text-align: right;\">64000</td><td style=\"text-align: right;\">-1024.76</td><td style=\"text-align: right;\">            -727.602</td><td style=\"text-align: right;\">            -1535.82</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 68000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_15-55-52\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -727.6022296442321\n",
      "  episode_reward_mean: -1009.9077424045777\n",
      "  episode_reward_min: -1509.7244337964069\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 340\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.15000000596046448\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.2585654258728027\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02206575684249401\n",
      "          model: {}\n",
      "          policy_loss: -0.002163634868338704\n",
      "          total_loss: 28313.9375\n",
      "          vf_explained_var: 0.0058028362691402435\n",
      "          vf_loss: 28313.939453125\n",
      "    num_agent_steps_sampled: 68000\n",
      "    num_agent_steps_trained: 68000\n",
      "    num_steps_sampled: 68000\n",
      "    num_steps_trained: 68000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.9125\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12270398012220034\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.18146580838776832\n",
      "    mean_inference_ms: 1.2524723071521202\n",
      "    mean_raw_obs_processing_ms: 0.10570056430880805\n",
      "  time_since_restore: 106.2834575176239\n",
      "  time_this_iter_s: 6.294228553771973\n",
      "  time_total_s: 106.2834575176239\n",
      "  timers:\n",
      "    learn_throughput: 1429.515\n",
      "    learn_time_ms: 2798.151\n",
      "    load_throughput: 5587376.694\n",
      "    load_time_ms: 0.716\n",
      "    sample_throughput: 637.378\n",
      "    sample_time_ms: 6275.71\n",
      "    update_time_ms: 3.116\n",
      "  timestamp: 1636041352\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 68000\n",
      "  training_iteration: 17\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 15:55:56 (running for 00:02:02.83)<br>Memory usage on this node: 12.4/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         106.283</td><td style=\"text-align: right;\">68000</td><td style=\"text-align: right;\">-1009.91</td><td style=\"text-align: right;\">            -727.602</td><td style=\"text-align: right;\">            -1509.72</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 72000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_15-55-58\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -648.5401716445583\n",
      "  episode_reward_mean: -1002.7738469839671\n",
      "  episode_reward_min: -1777.8305627875461\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 360\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.22499999403953552\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3359405994415283\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00855876225978136\n",
      "          model: {}\n",
      "          policy_loss: 0.006257031578570604\n",
      "          total_loss: 30826.841796875\n",
      "          vf_explained_var: 0.0060477894730865955\n",
      "          vf_loss: 30826.830078125\n",
      "    num_agent_steps_sampled: 72000\n",
      "    num_agent_steps_trained: 72000\n",
      "    num_steps_sampled: 72000\n",
      "    num_steps_trained: 72000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.2375\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12269266370361528\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.18145947133657456\n",
      "    mean_inference_ms: 1.2523229253006083\n",
      "    mean_raw_obs_processing_ms: 0.10568315605690151\n",
      "  time_since_restore: 112.47841548919678\n",
      "  time_this_iter_s: 6.194957971572876\n",
      "  time_total_s: 112.47841548919678\n",
      "  timers:\n",
      "    learn_throughput: 1429.435\n",
      "    learn_time_ms: 2798.308\n",
      "    load_throughput: 5599871.829\n",
      "    load_time_ms: 0.714\n",
      "    sample_throughput: 636.528\n",
      "    sample_time_ms: 6284.09\n",
      "    update_time_ms: 3.128\n",
      "  timestamp: 1636041358\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 72000\n",
      "  training_iteration: 18\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 15:56:01 (running for 00:02:08.07)<br>Memory usage on this node: 12.4/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">         112.478</td><td style=\"text-align: right;\">72000</td><td style=\"text-align: right;\">-1002.77</td><td style=\"text-align: right;\">             -648.54</td><td style=\"text-align: right;\">            -1777.83</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 76000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_15-56-04\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -648.5401716445583\n",
      "  episode_reward_mean: -997.7182733877139\n",
      "  episode_reward_min: -1777.8305627875461\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 380\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.22499999403953552\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3022661209106445\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02022824063897133\n",
      "          model: {}\n",
      "          policy_loss: -0.0029782403726130724\n",
      "          total_loss: 25837.892578125\n",
      "          vf_explained_var: 0.007843302562832832\n",
      "          vf_loss: 25837.890625\n",
      "    num_agent_steps_sampled: 76000\n",
      "    num_agent_steps_trained: 76000\n",
      "    num_steps_sampled: 76000\n",
      "    num_steps_trained: 76000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.771428571428572\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12269086661640138\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.18147636939136447\n",
      "    mean_inference_ms: 1.252285054636278\n",
      "    mean_raw_obs_processing_ms: 0.10568612022291785\n",
      "  time_since_restore: 118.67830300331116\n",
      "  time_this_iter_s: 6.19988751411438\n",
      "  time_total_s: 118.67830300331116\n",
      "  timers:\n",
      "    learn_throughput: 1430.97\n",
      "    learn_time_ms: 2795.306\n",
      "    load_throughput: 5274858.832\n",
      "    load_time_ms: 0.758\n",
      "    sample_throughput: 636.399\n",
      "    sample_time_ms: 6285.37\n",
      "    update_time_ms: 3.105\n",
      "  timestamp: 1636041364\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 76000\n",
      "  training_iteration: 19\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 15:56:06 (running for 00:02:13.32)<br>Memory usage on this node: 12.4/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         118.678</td><td style=\"text-align: right;\">76000</td><td style=\"text-align: right;\">-997.718</td><td style=\"text-align: right;\">             -648.54</td><td style=\"text-align: right;\">            -1777.83</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 80000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_15-56-10\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -648.5401716445583\n",
      "  episode_reward_mean: -1031.1542833775902\n",
      "  episode_reward_min: -1777.8305627875461\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 400\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3375000059604645\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.833743691444397\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006304909475147724\n",
      "          model: {}\n",
      "          policy_loss: 0.008205455727875233\n",
      "          total_loss: 40766.02734375\n",
      "          vf_explained_var: 0.0072433496825397015\n",
      "          vf_loss: 40766.01171875\n",
      "    num_agent_steps_sampled: 80000\n",
      "    num_agent_steps_trained: 80000\n",
      "    num_steps_sampled: 80000\n",
      "    num_steps_trained: 80000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.875\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12267502259746337\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.18144651570880765\n",
      "    mean_inference_ms: 1.251959129137914\n",
      "    mean_raw_obs_processing_ms: 0.10566580208126218\n",
      "  time_since_restore: 124.91289758682251\n",
      "  time_this_iter_s: 6.2345945835113525\n",
      "  time_total_s: 124.91289758682251\n",
      "  timers:\n",
      "    learn_throughput: 1429.585\n",
      "    learn_time_ms: 2798.015\n",
      "    load_throughput: 5230457.663\n",
      "    load_time_ms: 0.765\n",
      "    sample_throughput: 636.999\n",
      "    sample_time_ms: 6279.442\n",
      "    update_time_ms: 3.095\n",
      "  timestamp: 1636041370\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 80000\n",
      "  training_iteration: 20\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 15:56:11 (running for 00:02:18.60)<br>Memory usage on this node: 12.4/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         124.913</td><td style=\"text-align: right;\">80000</td><td style=\"text-align: right;\">-1031.15</td><td style=\"text-align: right;\">             -648.54</td><td style=\"text-align: right;\">            -1777.83</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 15:56:16 (running for 00:02:23.61)<br>Memory usage on this node: 12.5/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         124.913</td><td style=\"text-align: right;\">80000</td><td style=\"text-align: right;\">-1031.15</td><td style=\"text-align: right;\">             -648.54</td><td style=\"text-align: right;\">            -1777.83</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 84000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_15-56-17\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -648.5401716445583\n",
      "  episode_reward_mean: -1041.9754932922135\n",
      "  episode_reward_min: -1777.8305627875461\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 420\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3375000059604645\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.806684970855713\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014364790171384811\n",
      "          model: {}\n",
      "          policy_loss: 0.0005707670352421701\n",
      "          total_loss: 36272.84375\n",
      "          vf_explained_var: 0.013128592632710934\n",
      "          vf_loss: 36272.83984375\n",
      "    num_agent_steps_sampled: 84000\n",
      "    num_agent_steps_trained: 84000\n",
      "    num_steps_sampled: 84000\n",
      "    num_steps_trained: 84000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.75\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12266407492753922\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.18143150296977562\n",
      "    mean_inference_ms: 1.2516795445779398\n",
      "    mean_raw_obs_processing_ms: 0.10565622315511634\n",
      "  time_since_restore: 131.16945910453796\n",
      "  time_this_iter_s: 6.256561517715454\n",
      "  time_total_s: 131.16945910453796\n",
      "  timers:\n",
      "    learn_throughput: 1429.497\n",
      "    learn_time_ms: 2798.186\n",
      "    load_throughput: 5575122.454\n",
      "    load_time_ms: 0.717\n",
      "    sample_throughput: 637.227\n",
      "    sample_time_ms: 6277.198\n",
      "    update_time_ms: 3.09\n",
      "  timestamp: 1636041377\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 84000\n",
      "  training_iteration: 21\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 15:56:22 (running for 00:02:28.91)<br>Memory usage on this node: 12.4/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">         131.169</td><td style=\"text-align: right;\">84000</td><td style=\"text-align: right;\">-1041.98</td><td style=\"text-align: right;\">             -648.54</td><td style=\"text-align: right;\">            -1777.83</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 88000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_15-56-23\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -648.5401716445583\n",
      "  episode_reward_mean: -1035.1194113869535\n",
      "  episode_reward_min: -1777.8305627875461\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 440\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3375000059604645\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.1982890367507935\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010238676331937313\n",
      "          model: {}\n",
      "          policy_loss: 0.003458651015534997\n",
      "          total_loss: 16902.763671875\n",
      "          vf_explained_var: 0.04435088112950325\n",
      "          vf_loss: 16902.7578125\n",
      "    num_agent_steps_sampled: 88000\n",
      "    num_agent_steps_trained: 88000\n",
      "    num_steps_sampled: 88000\n",
      "    num_steps_trained: 88000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.325\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12266762080478599\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1814521443866855\n",
      "    mean_inference_ms: 1.2515852861374719\n",
      "    mean_raw_obs_processing_ms: 0.10564355993418709\n",
      "  time_since_restore: 137.4580466747284\n",
      "  time_this_iter_s: 6.28858757019043\n",
      "  time_total_s: 137.4580466747284\n",
      "  timers:\n",
      "    learn_throughput: 1429.574\n",
      "    learn_time_ms: 2798.037\n",
      "    load_throughput: 5594829.76\n",
      "    load_time_ms: 0.715\n",
      "    sample_throughput: 635.987\n",
      "    sample_time_ms: 6289.437\n",
      "    update_time_ms: 3.08\n",
      "  timestamp: 1636041383\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 88000\n",
      "  training_iteration: 22\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 15:56:27 (running for 00:02:34.25)<br>Memory usage on this node: 12.4/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    22</td><td style=\"text-align: right;\">         137.458</td><td style=\"text-align: right;\">88000</td><td style=\"text-align: right;\">-1035.12</td><td style=\"text-align: right;\">             -648.54</td><td style=\"text-align: right;\">            -1777.83</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 92000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_15-56-29\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -746.4742160938032\n",
      "  episode_reward_mean: -1029.7281524641646\n",
      "  episode_reward_min: -1759.1672778399948\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 460\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3375000059604645\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.383886456489563\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007362362928688526\n",
      "          model: {}\n",
      "          policy_loss: 0.006166340317577124\n",
      "          total_loss: 22430.064453125\n",
      "          vf_explained_var: 0.027694962918758392\n",
      "          vf_loss: 22430.056640625\n",
      "    num_agent_steps_sampled: 92000\n",
      "    num_agent_steps_trained: 92000\n",
      "    num_steps_sampled: 92000\n",
      "    num_steps_trained: 92000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.075\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12268988175145976\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.18150934290086768\n",
      "    mean_inference_ms: 1.2516843681437226\n",
      "    mean_raw_obs_processing_ms: 0.10564854445326546\n",
      "  time_since_restore: 143.7065761089325\n",
      "  time_this_iter_s: 6.248529434204102\n",
      "  time_total_s: 143.7065761089325\n",
      "  timers:\n",
      "    learn_throughput: 1430.481\n",
      "    learn_time_ms: 2796.263\n",
      "    load_throughput: 5602489.815\n",
      "    load_time_ms: 0.714\n",
      "    sample_throughput: 636.185\n",
      "    sample_time_ms: 6287.476\n",
      "    update_time_ms: 3.141\n",
      "  timestamp: 1636041389\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 92000\n",
      "  training_iteration: 23\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 15:56:32 (running for 00:02:39.55)<br>Memory usage on this node: 12.4/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    23</td><td style=\"text-align: right;\">         143.707</td><td style=\"text-align: right;\">92000</td><td style=\"text-align: right;\">-1029.73</td><td style=\"text-align: right;\">            -746.474</td><td style=\"text-align: right;\">            -1759.17</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 96000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_15-56-36\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -735.8610002714181\n",
      "  episode_reward_mean: -1024.9768947191874\n",
      "  episode_reward_min: -1759.1672778399948\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 480\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3375000059604645\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.4019899368286133\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007924254052340984\n",
      "          model: {}\n",
      "          policy_loss: 0.004641787149012089\n",
      "          total_loss: 20826.375\n",
      "          vf_explained_var: 0.08795925974845886\n",
      "          vf_loss: 20826.365234375\n",
      "    num_agent_steps_sampled: 96000\n",
      "    num_agent_steps_trained: 96000\n",
      "    num_steps_sampled: 96000\n",
      "    num_steps_trained: 96000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.1125\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12271493426187091\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1815600454648883\n",
      "    mean_inference_ms: 1.2518531604659209\n",
      "    mean_raw_obs_processing_ms: 0.10565035801399268\n",
      "  time_since_restore: 149.9257791042328\n",
      "  time_this_iter_s: 6.219202995300293\n",
      "  time_total_s: 149.9257791042328\n",
      "  timers:\n",
      "    learn_throughput: 1429.502\n",
      "    learn_time_ms: 2798.177\n",
      "    load_throughput: 5598563.753\n",
      "    load_time_ms: 0.714\n",
      "    sample_throughput: 636.159\n",
      "    sample_time_ms: 6287.738\n",
      "    update_time_ms: 3.165\n",
      "  timestamp: 1636041396\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 96000\n",
      "  training_iteration: 24\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 15:56:38 (running for 00:02:44.81)<br>Memory usage on this node: 12.4/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    24</td><td style=\"text-align: right;\">         149.926</td><td style=\"text-align: right;\">96000</td><td style=\"text-align: right;\">-1024.98</td><td style=\"text-align: right;\">            -735.861</td><td style=\"text-align: right;\">            -1759.17</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 100000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_15-56-42\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -632.5506000390571\n",
      "  episode_reward_mean: -998.5861782846684\n",
      "  episode_reward_min: -1643.9633791222614\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 500\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3375000059604645\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.4342938661575317\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00875193253159523\n",
      "          model: {}\n",
      "          policy_loss: 0.00253612850792706\n",
      "          total_loss: 19670.962890625\n",
      "          vf_explained_var: 0.08635923266410828\n",
      "          vf_loss: 19670.958984375\n",
      "    num_agent_steps_sampled: 100000\n",
      "    num_agent_steps_trained: 100000\n",
      "    num_steps_sampled: 100000\n",
      "    num_steps_trained: 100000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.9375\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12273924386185835\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1816145822053631\n",
      "    mean_inference_ms: 1.2520797833516217\n",
      "    mean_raw_obs_processing_ms: 0.10565230919688798\n",
      "  time_since_restore: 156.22009825706482\n",
      "  time_this_iter_s: 6.294319152832031\n",
      "  time_total_s: 156.22009825706482\n",
      "  timers:\n",
      "    learn_throughput: 1426.817\n",
      "    learn_time_ms: 2803.443\n",
      "    load_throughput: 5680452.345\n",
      "    load_time_ms: 0.704\n",
      "    sample_throughput: 636.117\n",
      "    sample_time_ms: 6288.155\n",
      "    update_time_ms: 3.161\n",
      "  timestamp: 1636041402\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 100000\n",
      "  training_iteration: 25\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 15:56:43 (running for 00:02:50.16)<br>Memory usage on this node: 12.4/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">          156.22</td><td style=\"text-align: right;\">100000</td><td style=\"text-align: right;\">-998.586</td><td style=\"text-align: right;\">            -632.551</td><td style=\"text-align: right;\">            -1643.96</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 15:56:48 (running for 00:02:55.16)<br>Memory usage on this node: 12.4/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">          156.22</td><td style=\"text-align: right;\">100000</td><td style=\"text-align: right;\">-998.586</td><td style=\"text-align: right;\">            -632.551</td><td style=\"text-align: right;\">            -1643.96</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 104000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_15-56-48\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -632.5506000390571\n",
      "  episode_reward_mean: -992.7157980475661\n",
      "  episode_reward_min: -1666.5406916383693\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 520\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3375000059604645\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.6770237684249878\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006770916283130646\n",
      "          model: {}\n",
      "          policy_loss: 0.005499046295881271\n",
      "          total_loss: 26345.189453125\n",
      "          vf_explained_var: 0.05942053720355034\n",
      "          vf_loss: 26345.18359375\n",
      "    num_agent_steps_sampled: 104000\n",
      "    num_agent_steps_trained: 104000\n",
      "    num_steps_sampled: 104000\n",
      "    num_steps_trained: 104000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.0125\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12277258277020285\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1816834075632111\n",
      "    mean_inference_ms: 1.2524301740851411\n",
      "    mean_raw_obs_processing_ms: 0.10567026130354033\n",
      "  time_since_restore: 162.52311658859253\n",
      "  time_this_iter_s: 6.30301833152771\n",
      "  time_total_s: 162.52311658859253\n",
      "  timers:\n",
      "    learn_throughput: 1425.874\n",
      "    learn_time_ms: 2805.297\n",
      "    load_throughput: 5619754.807\n",
      "    load_time_ms: 0.712\n",
      "    sample_throughput: 634.838\n",
      "    sample_time_ms: 6300.824\n",
      "    update_time_ms: 3.139\n",
      "  timestamp: 1636041408\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 104000\n",
      "  training_iteration: 26\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 15:56:53 (running for 00:03:00.51)<br>Memory usage on this node: 12.4/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    26</td><td style=\"text-align: right;\">         162.523</td><td style=\"text-align: right;\">104000</td><td style=\"text-align: right;\">-992.716</td><td style=\"text-align: right;\">            -632.551</td><td style=\"text-align: right;\">            -1666.54</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 108000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_15-56-55\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -632.5506000390571\n",
      "  episode_reward_mean: -984.9640740808692\n",
      "  episode_reward_min: -1666.5406916383693\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 540\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3375000059604645\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.2054545879364014\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014925151132047176\n",
      "          model: {}\n",
      "          policy_loss: 0.00020768911053892225\n",
      "          total_loss: 12750.4892578125\n",
      "          vf_explained_var: 0.1693200320005417\n",
      "          vf_loss: 12750.4853515625\n",
      "    num_agent_steps_sampled: 108000\n",
      "    num_agent_steps_trained: 108000\n",
      "    num_steps_sampled: 108000\n",
      "    num_steps_trained: 108000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.850000000000001\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12278410691368148\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1816999843384957\n",
      "    mean_inference_ms: 1.2524746120738852\n",
      "    mean_raw_obs_processing_ms: 0.10567872408514817\n",
      "  time_since_restore: 168.81236147880554\n",
      "  time_this_iter_s: 6.289244890213013\n",
      "  time_total_s: 168.81236147880554\n",
      "  timers:\n",
      "    learn_throughput: 1424.922\n",
      "    learn_time_ms: 2807.171\n",
      "    load_throughput: 5590728.115\n",
      "    load_time_ms: 0.715\n",
      "    sample_throughput: 634.875\n",
      "    sample_time_ms: 6300.456\n",
      "    update_time_ms: 3.072\n",
      "  timestamp: 1636041415\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 108000\n",
      "  training_iteration: 27\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 15:56:59 (running for 00:03:05.89)<br>Memory usage on this node: 12.4/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    27</td><td style=\"text-align: right;\">         168.812</td><td style=\"text-align: right;\">108000</td><td style=\"text-align: right;\">-984.964</td><td style=\"text-align: right;\">            -632.551</td><td style=\"text-align: right;\">            -1666.54</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 112000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_15-57-01\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -628.3642874070321\n",
      "  episode_reward_mean: -997.2569533729048\n",
      "  episode_reward_min: -1668.0146620148053\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 560\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3375000059604645\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.802782654762268\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02582271583378315\n",
      "          model: {}\n",
      "          policy_loss: -0.01131453551352024\n",
      "          total_loss: 24595.677734375\n",
      "          vf_explained_var: 0.15579450130462646\n",
      "          vf_loss: 24595.6796875\n",
      "    num_agent_steps_sampled: 112000\n",
      "    num_agent_steps_trained: 112000\n",
      "    num_steps_sampled: 112000\n",
      "    num_steps_trained: 112000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.775\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12278516821403652\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.18170596396703048\n",
      "    mean_inference_ms: 1.2525017883160146\n",
      "    mean_raw_obs_processing_ms: 0.10568560641728199\n",
      "  time_since_restore: 175.10778832435608\n",
      "  time_this_iter_s: 6.295426845550537\n",
      "  time_total_s: 175.10778832435608\n",
      "  timers:\n",
      "    learn_throughput: 1422.443\n",
      "    learn_time_ms: 2812.064\n",
      "    load_throughput: 5575493.005\n",
      "    load_time_ms: 0.717\n",
      "    sample_throughput: 633.763\n",
      "    sample_time_ms: 6311.504\n",
      "    update_time_ms: 3.187\n",
      "  timestamp: 1636041421\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 112000\n",
      "  training_iteration: 28\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 15:57:04 (running for 00:03:11.30)<br>Memory usage on this node: 12.4/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    28</td><td style=\"text-align: right;\">         175.108</td><td style=\"text-align: right;\">112000</td><td style=\"text-align: right;\">-997.257</td><td style=\"text-align: right;\">            -628.364</td><td style=\"text-align: right;\">            -1668.01</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 116000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_15-57-07\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -628.3642874070321\n",
      "  episode_reward_mean: -1012.5170606122118\n",
      "  episode_reward_min: -1671.7700991295428\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 580\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.8681410551071167\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011415147222578526\n",
      "          model: {}\n",
      "          policy_loss: 0.0038836614694446325\n",
      "          total_loss: 22077.6171875\n",
      "          vf_explained_var: 0.1585557907819748\n",
      "          vf_loss: 22077.609375\n",
      "    num_agent_steps_sampled: 116000\n",
      "    num_agent_steps_trained: 116000\n",
      "    num_steps_sampled: 116000\n",
      "    num_steps_trained: 116000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.914285714285715\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12278350984892399\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.18170634858469573\n",
      "    mean_inference_ms: 1.2524595705718067\n",
      "    mean_raw_obs_processing_ms: 0.10568817660482167\n",
      "  time_since_restore: 181.25673007965088\n",
      "  time_this_iter_s: 6.1489417552948\n",
      "  time_total_s: 181.25673007965088\n",
      "  timers:\n",
      "    learn_throughput: 1423.052\n",
      "    learn_time_ms: 2810.859\n",
      "    load_throughput: 5924158.192\n",
      "    load_time_ms: 0.675\n",
      "    sample_throughput: 633.048\n",
      "    sample_time_ms: 6318.633\n",
      "    update_time_ms: 3.131\n",
      "  timestamp: 1636041427\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 116000\n",
      "  training_iteration: 29\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 15:57:09 (running for 00:03:16.49)<br>Memory usage on this node: 12.4/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    29</td><td style=\"text-align: right;\">         181.257</td><td style=\"text-align: right;\">116000</td><td style=\"text-align: right;\">-1012.52</td><td style=\"text-align: right;\">            -628.364</td><td style=\"text-align: right;\">            -1671.77</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 120000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_15-57-14\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -626.0392350537508\n",
      "  episode_reward_mean: -984.1390139255413\n",
      "  episode_reward_min: -1671.7700991295428\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 600\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.0680174827575684\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008517337962985039\n",
      "          model: {}\n",
      "          policy_loss: 0.005706695839762688\n",
      "          total_loss: 11917.5810546875\n",
      "          vf_explained_var: 0.207038015127182\n",
      "          vf_loss: 11917.5703125\n",
      "    num_agent_steps_sampled: 120000\n",
      "    num_agent_steps_trained: 120000\n",
      "    num_steps_sampled: 120000\n",
      "    num_steps_trained: 120000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 30\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.1375\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12278596860309678\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.18171194964111245\n",
      "    mean_inference_ms: 1.2524528153385912\n",
      "    mean_raw_obs_processing_ms: 0.10569463868311019\n",
      "  time_since_restore: 187.52219128608704\n",
      "  time_this_iter_s: 6.265461206436157\n",
      "  time_total_s: 187.52219128608704\n",
      "  timers:\n",
      "    learn_throughput: 1423.943\n",
      "    learn_time_ms: 2809.102\n",
      "    load_throughput: 5980542.544\n",
      "    load_time_ms: 0.669\n",
      "    sample_throughput: 632.701\n",
      "    sample_time_ms: 6322.101\n",
      "    update_time_ms: 3.103\n",
      "  timestamp: 1636041434\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 120000\n",
      "  training_iteration: 30\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 15:57:15 (running for 00:03:21.81)<br>Memory usage on this node: 12.4/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    30</td><td style=\"text-align: right;\">         187.522</td><td style=\"text-align: right;\">120000</td><td style=\"text-align: right;\">-984.139</td><td style=\"text-align: right;\">            -626.039</td><td style=\"text-align: right;\">            -1671.77</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 15:57:20 (running for 00:03:26.82)<br>Memory usage on this node: 12.4/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    30</td><td style=\"text-align: right;\">         187.522</td><td style=\"text-align: right;\">120000</td><td style=\"text-align: right;\">-984.139</td><td style=\"text-align: right;\">            -626.039</td><td style=\"text-align: right;\">            -1671.77</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 124000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_15-57-20\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -626.0392350537508\n",
      "  episode_reward_mean: -954.0354898213039\n",
      "  episode_reward_min: -1729.7378286372887\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 620\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.1486515998840332\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007974979467689991\n",
      "          model: {}\n",
      "          policy_loss: 0.001834951457567513\n",
      "          total_loss: 14740.13671875\n",
      "          vf_explained_var: 0.18727149069309235\n",
      "          vf_loss: 14740.130859375\n",
      "    num_agent_steps_sampled: 124000\n",
      "    num_agent_steps_trained: 124000\n",
      "    num_steps_sampled: 124000\n",
      "    num_steps_trained: 124000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 31\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.7625\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12278432680053203\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.18170350905418098\n",
      "    mean_inference_ms: 1.252355417037806\n",
      "    mean_raw_obs_processing_ms: 0.10569132022762293\n",
      "  time_since_restore: 193.77522802352905\n",
      "  time_this_iter_s: 6.253036737442017\n",
      "  time_total_s: 193.77522802352905\n",
      "  timers:\n",
      "    learn_throughput: 1425.469\n",
      "    learn_time_ms: 2806.094\n",
      "    load_throughput: 5908302.578\n",
      "    load_time_ms: 0.677\n",
      "    sample_throughput: 632.586\n",
      "    sample_time_ms: 6323.249\n",
      "    update_time_ms: 3.131\n",
      "  timestamp: 1636041440\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 124000\n",
      "  training_iteration: 31\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 15:57:25 (running for 00:03:32.12)<br>Memory usage on this node: 12.4/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    31</td><td style=\"text-align: right;\">         193.775</td><td style=\"text-align: right;\">124000</td><td style=\"text-align: right;\">-954.035</td><td style=\"text-align: right;\">            -626.039</td><td style=\"text-align: right;\">            -1729.74</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 128000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_15-57-26\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -597.4742729737308\n",
      "  episode_reward_mean: -946.7097061531765\n",
      "  episode_reward_min: -1729.7378286372887\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 640\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.9821625351905823\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008275044150650501\n",
      "          model: {}\n",
      "          policy_loss: 0.005007862579077482\n",
      "          total_loss: 9658.8759765625\n",
      "          vf_explained_var: 0.2953573167324066\n",
      "          vf_loss: 9658.8662109375\n",
      "    num_agent_steps_sampled: 128000\n",
      "    num_agent_steps_trained: 128000\n",
      "    num_steps_sampled: 128000\n",
      "    num_steps_trained: 128000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 32\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.950000000000001\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12276967479896715\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.18167206326393526\n",
      "    mean_inference_ms: 1.252120203981729\n",
      "    mean_raw_obs_processing_ms: 0.10567369740836276\n",
      "  time_since_restore: 199.93972611427307\n",
      "  time_this_iter_s: 6.1644980907440186\n",
      "  time_total_s: 199.93972611427307\n",
      "  timers:\n",
      "    learn_throughput: 1424.522\n",
      "    learn_time_ms: 2807.96\n",
      "    load_throughput: 5814519.997\n",
      "    load_time_ms: 0.688\n",
      "    sample_throughput: 634.313\n",
      "    sample_time_ms: 6306.039\n",
      "    update_time_ms: 3.157\n",
      "  timestamp: 1636041446\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 128000\n",
      "  training_iteration: 32\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 15:57:30 (running for 00:03:37.33)<br>Memory usage on this node: 12.4/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">          199.94</td><td style=\"text-align: right;\">128000</td><td style=\"text-align: right;\"> -946.71</td><td style=\"text-align: right;\">            -597.474</td><td style=\"text-align: right;\">            -1729.74</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 132000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_15-57-32\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -597.4742729737308\n",
      "  episode_reward_mean: -955.9676322805198\n",
      "  episode_reward_min: -1729.7378286372887\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 660\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 2.079258680343628\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005198057275265455\n",
      "          model: {}\n",
      "          policy_loss: 0.005026886239647865\n",
      "          total_loss: 23074.0078125\n",
      "          vf_explained_var: 0.2258290946483612\n",
      "          vf_loss: 23074.0\n",
      "    num_agent_steps_sampled: 132000\n",
      "    num_agent_steps_trained: 132000\n",
      "    num_steps_sampled: 132000\n",
      "    num_steps_trained: 132000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 33\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.0875\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1227605527880383\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.18164445127991358\n",
      "    mean_inference_ms: 1.251910245081574\n",
      "    mean_raw_obs_processing_ms: 0.10565270277478782\n",
      "  time_since_restore: 206.1920611858368\n",
      "  time_this_iter_s: 6.252335071563721\n",
      "  time_total_s: 206.1920611858368\n",
      "  timers:\n",
      "    learn_throughput: 1423.461\n",
      "    learn_time_ms: 2810.052\n",
      "    load_throughput: 5800849.181\n",
      "    load_time_ms: 0.69\n",
      "    sample_throughput: 634.269\n",
      "    sample_time_ms: 6306.475\n",
      "    update_time_ms: 3.087\n",
      "  timestamp: 1636041452\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 132000\n",
      "  training_iteration: 33\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 15:57:36 (running for 00:03:42.63)<br>Memory usage on this node: 12.4/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    33</td><td style=\"text-align: right;\">         206.192</td><td style=\"text-align: right;\">132000</td><td style=\"text-align: right;\">-955.968</td><td style=\"text-align: right;\">            -597.474</td><td style=\"text-align: right;\">            -1729.74</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 136000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_15-57-39\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -597.4742729737308\n",
      "  episode_reward_mean: -941.9909203899518\n",
      "  episode_reward_min: -1729.7378286372887\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 680\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.5367172956466675\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006969737354665995\n",
      "          model: {}\n",
      "          policy_loss: 0.005950467195361853\n",
      "          total_loss: 16331.576171875\n",
      "          vf_explained_var: 0.21122883260250092\n",
      "          vf_loss: 16331.56640625\n",
      "    num_agent_steps_sampled: 136000\n",
      "    num_agent_steps_trained: 136000\n",
      "    num_steps_sampled: 136000\n",
      "    num_steps_trained: 136000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 34\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.1125\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12276218214486403\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.18164012627403645\n",
      "    mean_inference_ms: 1.2518308937727585\n",
      "    mean_raw_obs_processing_ms: 0.1056501137199004\n",
      "  time_since_restore: 212.43164801597595\n",
      "  time_this_iter_s: 6.23958683013916\n",
      "  time_total_s: 212.43164801597595\n",
      "  timers:\n",
      "    learn_throughput: 1423.616\n",
      "    learn_time_ms: 2809.747\n",
      "    load_throughput: 5815527.748\n",
      "    load_time_ms: 0.688\n",
      "    sample_throughput: 633.873\n",
      "    sample_time_ms: 6310.417\n",
      "    update_time_ms: 3.102\n",
      "  timestamp: 1636041459\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 136000\n",
      "  training_iteration: 34\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 15:57:41 (running for 00:03:47.95)<br>Memory usage on this node: 12.4/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    34</td><td style=\"text-align: right;\">         212.432</td><td style=\"text-align: right;\">136000</td><td style=\"text-align: right;\">-941.991</td><td style=\"text-align: right;\">            -597.474</td><td style=\"text-align: right;\">            -1729.74</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 140000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_15-57-45\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -597.4742729737308\n",
      "  episode_reward_mean: -943.9701833345873\n",
      "  episode_reward_min: -1729.7378286372887\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 700\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.1346230506896973\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007647916208952665\n",
      "          model: {}\n",
      "          policy_loss: 0.006974441464990377\n",
      "          total_loss: 9861.4296875\n",
      "          vf_explained_var: 0.35141515731811523\n",
      "          vf_loss: 9861.4189453125\n",
      "    num_agent_steps_sampled: 140000\n",
      "    num_agent_steps_trained: 140000\n",
      "    num_steps_sampled: 140000\n",
      "    num_steps_trained: 140000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 35\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.857142857142858\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12276131856572015\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.18162773600832538\n",
      "    mean_inference_ms: 1.2516868429728933\n",
      "    mean_raw_obs_processing_ms: 0.1056420727361245\n",
      "  time_since_restore: 218.6281921863556\n",
      "  time_this_iter_s: 6.196544170379639\n",
      "  time_total_s: 218.6281921863556\n",
      "  timers:\n",
      "    learn_throughput: 1425.601\n",
      "    learn_time_ms: 2805.835\n",
      "    load_throughput: 5887568.782\n",
      "    load_time_ms: 0.679\n",
      "    sample_throughput: 634.181\n",
      "    sample_time_ms: 6307.345\n",
      "    update_time_ms: 3.103\n",
      "  timestamp: 1636041465\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 140000\n",
      "  training_iteration: 35\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 15:57:46 (running for 00:03:53.19)<br>Memory usage on this node: 12.4/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    35</td><td style=\"text-align: right;\">         218.628</td><td style=\"text-align: right;\">140000</td><td style=\"text-align: right;\"> -943.97</td><td style=\"text-align: right;\">            -597.474</td><td style=\"text-align: right;\">            -1729.74</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 15:57:51 (running for 00:03:58.20)<br>Memory usage on this node: 12.4/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    35</td><td style=\"text-align: right;\">         218.628</td><td style=\"text-align: right;\">140000</td><td style=\"text-align: right;\"> -943.97</td><td style=\"text-align: right;\">            -597.474</td><td style=\"text-align: right;\">            -1729.74</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 144000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_15-57-51\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -597.4742729737308\n",
      "  episode_reward_mean: -942.9246321531792\n",
      "  episode_reward_min: -1717.9716168621514\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 720\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.2058616876602173\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015283046290278435\n",
      "          model: {}\n",
      "          policy_loss: -0.0009522798354737461\n",
      "          total_loss: 12091.95703125\n",
      "          vf_explained_var: 0.29149502515792847\n",
      "          vf_loss: 12091.9501953125\n",
      "    num_agent_steps_sampled: 144000\n",
      "    num_agent_steps_trained: 144000\n",
      "    num_steps_sampled: 144000\n",
      "    num_steps_trained: 144000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 36\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.087499999999999\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12275312373534895\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.18160242935497795\n",
      "    mean_inference_ms: 1.2514548164103942\n",
      "    mean_raw_obs_processing_ms: 0.10562453669152168\n",
      "  time_since_restore: 224.8197500705719\n",
      "  time_this_iter_s: 6.191557884216309\n",
      "  time_total_s: 224.8197500705719\n",
      "  timers:\n",
      "    learn_throughput: 1426.059\n",
      "    learn_time_ms: 2804.934\n",
      "    load_throughput: 5943676.622\n",
      "    load_time_ms: 0.673\n",
      "    sample_throughput: 635.588\n",
      "    sample_time_ms: 6293.384\n",
      "    update_time_ms: 3.127\n",
      "  timestamp: 1636041471\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 144000\n",
      "  training_iteration: 36\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 15:57:56 (running for 00:04:03.44)<br>Memory usage on this node: 12.4/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    36</td><td style=\"text-align: right;\">          224.82</td><td style=\"text-align: right;\">144000</td><td style=\"text-align: right;\">-942.925</td><td style=\"text-align: right;\">            -597.474</td><td style=\"text-align: right;\">            -1717.97</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 148000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_15-57-58\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -627.7954912102074\n",
      "  episode_reward_mean: -978.8827089017461\n",
      "  episode_reward_min: -1800.0030221752177\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 740\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.9644770622253418\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006391179747879505\n",
      "          model: {}\n",
      "          policy_loss: 0.004229105077683926\n",
      "          total_loss: 19625.123046875\n",
      "          vf_explained_var: 0.30409395694732666\n",
      "          vf_loss: 19625.115234375\n",
      "    num_agent_steps_sampled: 148000\n",
      "    num_agent_steps_trained: 148000\n",
      "    num_steps_sampled: 148000\n",
      "    num_steps_trained: 148000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 37\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.9625\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12275749340979844\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1816075531429683\n",
      "    mean_inference_ms: 1.251412181430049\n",
      "    mean_raw_obs_processing_ms: 0.10562227461683658\n",
      "  time_since_restore: 231.08775520324707\n",
      "  time_this_iter_s: 6.268005132675171\n",
      "  time_total_s: 231.08775520324707\n",
      "  timers:\n",
      "    learn_throughput: 1428.266\n",
      "    learn_time_ms: 2800.599\n",
      "    load_throughput: 5977772.394\n",
      "    load_time_ms: 0.669\n",
      "    sample_throughput: 635.463\n",
      "    sample_time_ms: 6294.627\n",
      "    update_time_ms: 3.14\n",
      "  timestamp: 1636041478\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 148000\n",
      "  training_iteration: 37\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 15:58:02 (running for 00:04:08.75)<br>Memory usage on this node: 12.4/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    37</td><td style=\"text-align: right;\">         231.088</td><td style=\"text-align: right;\">148000</td><td style=\"text-align: right;\">-978.883</td><td style=\"text-align: right;\">            -627.795</td><td style=\"text-align: right;\">               -1800</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 152000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_15-58-04\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -627.0562916980897\n",
      "  episode_reward_mean: -974.4037114962712\n",
      "  episode_reward_min: -1800.0030221752177\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 760\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 2.043311357498169\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008309606462717056\n",
      "          model: {}\n",
      "          policy_loss: 0.00283204042352736\n",
      "          total_loss: 17500.580078125\n",
      "          vf_explained_var: 0.3396511971950531\n",
      "          vf_loss: 17500.57421875\n",
      "    num_agent_steps_sampled: 152000\n",
      "    num_agent_steps_trained: 152000\n",
      "    num_steps_sampled: 152000\n",
      "    num_steps_trained: 152000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 38\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.1125\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12275529363355726\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1815970649123387\n",
      "    mean_inference_ms: 1.2512718352246786\n",
      "    mean_raw_obs_processing_ms: 0.10561268865628513\n",
      "  time_since_restore: 237.27258825302124\n",
      "  time_this_iter_s: 6.18483304977417\n",
      "  time_total_s: 237.27258825302124\n",
      "  timers:\n",
      "    learn_throughput: 1430.162\n",
      "    learn_time_ms: 2796.885\n",
      "    load_throughput: 5916637.043\n",
      "    load_time_ms: 0.676\n",
      "    sample_throughput: 637.089\n",
      "    sample_time_ms: 6278.56\n",
      "    update_time_ms: 3.012\n",
      "  timestamp: 1636041484\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 152000\n",
      "  training_iteration: 38\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 15:58:07 (running for 00:04:14.00)<br>Memory usage on this node: 12.4/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    38</td><td style=\"text-align: right;\">         237.273</td><td style=\"text-align: right;\">152000</td><td style=\"text-align: right;\">-974.404</td><td style=\"text-align: right;\">            -627.056</td><td style=\"text-align: right;\">               -1800</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 156000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_15-58-10\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -627.0562916980897\n",
      "  episode_reward_mean: -978.7372140569008\n",
      "  episode_reward_min: -1800.0030221752177\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 780\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.6322870254516602\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009775770828127861\n",
      "          model: {}\n",
      "          policy_loss: 0.004014169331640005\n",
      "          total_loss: 14500.8525390625\n",
      "          vf_explained_var: 0.3592738211154938\n",
      "          vf_loss: 14500.8427734375\n",
      "    num_agent_steps_sampled: 156000\n",
      "    num_agent_steps_trained: 156000\n",
      "    num_steps_sampled: 156000\n",
      "    num_steps_trained: 156000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 39\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.8125\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12275697409579851\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.18159070392283955\n",
      "    mean_inference_ms: 1.2511729634011626\n",
      "    mean_raw_obs_processing_ms: 0.10560602723706176\n",
      "  time_since_restore: 243.57595801353455\n",
      "  time_this_iter_s: 6.303369760513306\n",
      "  time_total_s: 243.57595801353455\n",
      "  timers:\n",
      "    learn_throughput: 1428.513\n",
      "    learn_time_ms: 2800.114\n",
      "    load_throughput: 5893359.562\n",
      "    load_time_ms: 0.679\n",
      "    sample_throughput: 636.83\n",
      "    sample_time_ms: 6281.113\n",
      "    update_time_ms: 4.02\n",
      "  timestamp: 1636041490\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 156000\n",
      "  training_iteration: 39\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 15:58:12 (running for 00:04:19.42)<br>Memory usage on this node: 12.4/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    39</td><td style=\"text-align: right;\">         243.576</td><td style=\"text-align: right;\">156000</td><td style=\"text-align: right;\">-978.737</td><td style=\"text-align: right;\">            -627.056</td><td style=\"text-align: right;\">               -1800</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 160000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_15-58-17\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -627.0562916980897\n",
      "  episode_reward_mean: -964.466232671757\n",
      "  episode_reward_min: -1800.0030221752177\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 800\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.7475897073745728\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008252277970314026\n",
      "          model: {}\n",
      "          policy_loss: 0.011531333439052105\n",
      "          total_loss: 9582.2431640625\n",
      "          vf_explained_var: 0.12200357019901276\n",
      "          vf_loss: 9582.2275390625\n",
      "    num_agent_steps_sampled: 160000\n",
      "    num_agent_steps_trained: 160000\n",
      "    num_steps_sampled: 160000\n",
      "    num_steps_trained: 160000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 40\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.700000000000001\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1227547576341012\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.18158612287311057\n",
      "    mean_inference_ms: 1.2510907598634278\n",
      "    mean_raw_obs_processing_ms: 0.10560397890866229\n",
      "  time_since_restore: 249.79218220710754\n",
      "  time_this_iter_s: 6.216224193572998\n",
      "  time_total_s: 249.79218220710754\n",
      "  timers:\n",
      "    learn_throughput: 1428.744\n",
      "    learn_time_ms: 2799.661\n",
      "    load_throughput: 5891290.119\n",
      "    load_time_ms: 0.679\n",
      "    sample_throughput: 636.081\n",
      "    sample_time_ms: 6288.51\n",
      "    update_time_ms: 4.032\n",
      "  timestamp: 1636041497\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 160000\n",
      "  training_iteration: 40\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 15:58:18 (running for 00:04:24.69)<br>Memory usage on this node: 12.5/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">         249.792</td><td style=\"text-align: right;\">160000</td><td style=\"text-align: right;\">-964.466</td><td style=\"text-align: right;\">            -627.056</td><td style=\"text-align: right;\">               -1800</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 15:58:23 (running for 00:04:29.70)<br>Memory usage on this node: 12.4/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">         249.792</td><td style=\"text-align: right;\">160000</td><td style=\"text-align: right;\">-964.466</td><td style=\"text-align: right;\">            -627.056</td><td style=\"text-align: right;\">               -1800</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 164000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_15-58-23\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -627.0562916980897\n",
      "  episode_reward_mean: -975.8354241886478\n",
      "  episode_reward_min: -1800.0030221752177\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 820\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.4479190111160278\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008822045288980007\n",
      "          model: {}\n",
      "          policy_loss: 0.0046172901056706905\n",
      "          total_loss: 12190.890625\n",
      "          vf_explained_var: 0.3778648376464844\n",
      "          vf_loss: 12190.8828125\n",
      "    num_agent_steps_sampled: 164000\n",
      "    num_agent_steps_trained: 164000\n",
      "    num_steps_sampled: 164000\n",
      "    num_steps_trained: 164000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 41\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.9625\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12276131094188422\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.18159800579633903\n",
      "    mean_inference_ms: 1.2510943676466513\n",
      "    mean_raw_obs_processing_ms: 0.10561162458908648\n",
      "  time_since_restore: 256.04939436912537\n",
      "  time_this_iter_s: 6.257212162017822\n",
      "  time_total_s: 256.04939436912537\n",
      "  timers:\n",
      "    learn_throughput: 1427.891\n",
      "    learn_time_ms: 2801.335\n",
      "    load_throughput: 5959511.225\n",
      "    load_time_ms: 0.671\n",
      "    sample_throughput: 636.248\n",
      "    sample_time_ms: 6286.852\n",
      "    update_time_ms: 4.002\n",
      "  timestamp: 1636041503\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 164000\n",
      "  training_iteration: 41\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 15:58:28 (running for 00:04:35.01)<br>Memory usage on this node: 12.4/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    41</td><td style=\"text-align: right;\">         256.049</td><td style=\"text-align: right;\">164000</td><td style=\"text-align: right;\">-975.835</td><td style=\"text-align: right;\">            -627.056</td><td style=\"text-align: right;\">               -1800</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 168000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_15-58-29\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -627.0562916980897\n",
      "  episode_reward_mean: -947.9412781399822\n",
      "  episode_reward_min: -1720.7890692977387\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 840\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3052242994308472\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011037701740860939\n",
      "          model: {}\n",
      "          policy_loss: -0.000732633750885725\n",
      "          total_loss: 11965.9248046875\n",
      "          vf_explained_var: 0.2490144670009613\n",
      "          vf_loss: 11965.9208984375\n",
      "    num_agent_steps_sampled: 168000\n",
      "    num_agent_steps_trained: 168000\n",
      "    num_steps_sampled: 168000\n",
      "    num_steps_trained: 168000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 42\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.1875\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12276256601309571\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1815927085047049\n",
      "    mean_inference_ms: 1.2510241876481785\n",
      "    mean_raw_obs_processing_ms: 0.10561372199327533\n",
      "  time_since_restore: 262.27548241615295\n",
      "  time_this_iter_s: 6.226088047027588\n",
      "  time_total_s: 262.27548241615295\n",
      "  timers:\n",
      "    learn_throughput: 1426.927\n",
      "    learn_time_ms: 2803.227\n",
      "    load_throughput: 6051295.221\n",
      "    load_time_ms: 0.661\n",
      "    sample_throughput: 635.53\n",
      "    sample_time_ms: 6293.96\n",
      "    update_time_ms: 3.965\n",
      "  timestamp: 1636041509\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 168000\n",
      "  training_iteration: 42\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 15:58:33 (running for 00:04:40.29)<br>Memory usage on this node: 12.4/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    42</td><td style=\"text-align: right;\">         262.275</td><td style=\"text-align: right;\">168000</td><td style=\"text-align: right;\">-947.941</td><td style=\"text-align: right;\">            -627.056</td><td style=\"text-align: right;\">            -1720.79</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 172000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_15-58-35\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -627.0666293642562\n",
      "  episode_reward_mean: -939.3423751253022\n",
      "  episode_reward_min: -1738.4317178564952\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 860\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.957980990409851\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007651533931493759\n",
      "          model: {}\n",
      "          policy_loss: 0.004602824803441763\n",
      "          total_loss: 15053.173828125\n",
      "          vf_explained_var: 0.28749629855155945\n",
      "          vf_loss: 15053.1669921875\n",
      "    num_agent_steps_sampled: 172000\n",
      "    num_agent_steps_trained: 172000\n",
      "    num_steps_sampled: 172000\n",
      "    num_steps_trained: 172000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 43\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.9125\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1227663123010332\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.18159208158585557\n",
      "    mean_inference_ms: 1.2509860785570435\n",
      "    mean_raw_obs_processing_ms: 0.10561866339717063\n",
      "  time_since_restore: 268.47811365127563\n",
      "  time_this_iter_s: 6.202631235122681\n",
      "  time_total_s: 268.47811365127563\n",
      "  timers:\n",
      "    learn_throughput: 1427.361\n",
      "    learn_time_ms: 2802.374\n",
      "    load_throughput: 6033667.554\n",
      "    load_time_ms: 0.663\n",
      "    sample_throughput: 635.748\n",
      "    sample_time_ms: 6291.802\n",
      "    update_time_ms: 3.949\n",
      "  timestamp: 1636041515\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 172000\n",
      "  training_iteration: 43\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 15:58:38 (running for 00:04:45.54)<br>Memory usage on this node: 12.4/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    43</td><td style=\"text-align: right;\">         268.478</td><td style=\"text-align: right;\">172000</td><td style=\"text-align: right;\">-939.342</td><td style=\"text-align: right;\">            -627.067</td><td style=\"text-align: right;\">            -1738.43</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 176000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_15-58-42\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -592.7030080129504\n",
      "  episode_reward_mean: -918.3003453753706\n",
      "  episode_reward_min: -1739.6574657747328\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 880\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3831243515014648\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008244235999882221\n",
      "          model: {}\n",
      "          policy_loss: 0.007513768505305052\n",
      "          total_loss: 10531.5732421875\n",
      "          vf_explained_var: 0.3003945052623749\n",
      "          vf_loss: 10531.5615234375\n",
      "    num_agent_steps_sampled: 176000\n",
      "    num_agent_steps_trained: 176000\n",
      "    num_steps_sampled: 176000\n",
      "    num_steps_trained: 176000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 44\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.875\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12276002921971721\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.18158007365815984\n",
      "    mean_inference_ms: 1.2508720257918446\n",
      "    mean_raw_obs_processing_ms: 0.10561516071823242\n",
      "  time_since_restore: 274.72996377944946\n",
      "  time_this_iter_s: 6.251850128173828\n",
      "  time_total_s: 274.72996377944946\n",
      "  timers:\n",
      "    learn_throughput: 1427.12\n",
      "    learn_time_ms: 2802.847\n",
      "    load_throughput: 6030414.435\n",
      "    load_time_ms: 0.663\n",
      "    sample_throughput: 635.743\n",
      "    sample_time_ms: 6291.854\n",
      "    update_time_ms: 3.913\n",
      "  timestamp: 1636041522\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 176000\n",
      "  training_iteration: 44\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 15:58:44 (running for 00:04:50.83)<br>Memory usage on this node: 12.5/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    44</td><td style=\"text-align: right;\">          274.73</td><td style=\"text-align: right;\">176000</td><td style=\"text-align: right;\">  -918.3</td><td style=\"text-align: right;\">            -592.703</td><td style=\"text-align: right;\">            -1739.66</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 180000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_15-58-48\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -592.7030080129504\n",
      "  episode_reward_mean: -946.7294660507984\n",
      "  episode_reward_min: -1744.6662834296408\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 900\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.658363938331604\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007826115004718304\n",
      "          model: {}\n",
      "          policy_loss: -0.0007571312598884106\n",
      "          total_loss: 11628.544921875\n",
      "          vf_explained_var: 0.3651624321937561\n",
      "          vf_loss: 11628.5419921875\n",
      "    num_agent_steps_sampled: 180000\n",
      "    num_agent_steps_trained: 180000\n",
      "    num_steps_sampled: 180000\n",
      "    num_steps_trained: 180000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 45\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.962499999999999\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12275563371664173\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.18157043841933912\n",
      "    mean_inference_ms: 1.250783874392676\n",
      "    mean_raw_obs_processing_ms: 0.10561090737689817\n",
      "  time_since_restore: 280.9548759460449\n",
      "  time_this_iter_s: 6.224912166595459\n",
      "  time_total_s: 280.9548759460449\n",
      "  timers:\n",
      "    learn_throughput: 1427.077\n",
      "    learn_time_ms: 2802.933\n",
      "    load_throughput: 6019595.996\n",
      "    load_time_ms: 0.664\n",
      "    sample_throughput: 635.759\n",
      "    sample_time_ms: 6291.691\n",
      "    update_time_ms: 3.903\n",
      "  timestamp: 1636041528\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 180000\n",
      "  training_iteration: 45\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 15:58:49 (running for 00:04:56.11)<br>Memory usage on this node: 12.5/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         280.955</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">-946.729</td><td style=\"text-align: right;\">            -592.703</td><td style=\"text-align: right;\">            -1744.67</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 15:58:54 (running for 00:05:01.12)<br>Memory usage on this node: 12.5/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         280.955</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">-946.729</td><td style=\"text-align: right;\">            -592.703</td><td style=\"text-align: right;\">            -1744.67</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 184000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_15-58-54\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -592.7030080129504\n",
      "  episode_reward_mean: -933.7235073754526\n",
      "  episode_reward_min: -1744.6662834296408\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 920\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.0216368436813354\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011592957191169262\n",
      "          model: {}\n",
      "          policy_loss: 0.010417203418910503\n",
      "          total_loss: 10151.7646484375\n",
      "          vf_explained_var: 0.23442688584327698\n",
      "          vf_loss: 10151.748046875\n",
      "    num_agent_steps_sampled: 184000\n",
      "    num_agent_steps_trained: 184000\n",
      "    num_steps_sampled: 184000\n",
      "    num_steps_trained: 184000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 46\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.0625\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12274773997105032\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.18155349163600235\n",
      "    mean_inference_ms: 1.2506372880263843\n",
      "    mean_raw_obs_processing_ms: 0.10560091347170833\n",
      "  time_since_restore: 287.14042711257935\n",
      "  time_this_iter_s: 6.185551166534424\n",
      "  time_total_s: 287.14042711257935\n",
      "  timers:\n",
      "    learn_throughput: 1427.629\n",
      "    learn_time_ms: 2801.848\n",
      "    load_throughput: 6015063.818\n",
      "    load_time_ms: 0.665\n",
      "    sample_throughput: 635.799\n",
      "    sample_time_ms: 6291.299\n",
      "    update_time_ms: 3.892\n",
      "  timestamp: 1636041534\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 184000\n",
      "  training_iteration: 46\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 15:58:59 (running for 00:05:06.43)<br>Memory usage on this node: 12.5/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    46</td><td style=\"text-align: right;\">          287.14</td><td style=\"text-align: right;\">184000</td><td style=\"text-align: right;\">-933.724</td><td style=\"text-align: right;\">            -592.703</td><td style=\"text-align: right;\">            -1744.67</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 188000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_15-59-00\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -592.7030080129504\n",
      "  episode_reward_mean: -917.564078073233\n",
      "  episode_reward_min: -1744.6662834296408\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 940\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.6961225867271423\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011036435142159462\n",
      "          model: {}\n",
      "          policy_loss: 0.004642168991267681\n",
      "          total_loss: 7250.1923828125\n",
      "          vf_explained_var: 0.3215334713459015\n",
      "          vf_loss: 7250.181640625\n",
      "    num_agent_steps_sampled: 188000\n",
      "    num_agent_steps_trained: 188000\n",
      "    num_steps_sampled: 188000\n",
      "    num_steps_trained: 188000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 47\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.814285714285713\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12274053512036258\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.18154050966021779\n",
      "    mean_inference_ms: 1.2505133736806102\n",
      "    mean_raw_obs_processing_ms: 0.10559004752855511\n",
      "  time_since_restore: 293.34049582481384\n",
      "  time_this_iter_s: 6.200068712234497\n",
      "  time_total_s: 293.34049582481384\n",
      "  timers:\n",
      "    learn_throughput: 1428.149\n",
      "    learn_time_ms: 2800.827\n",
      "    load_throughput: 5950000.355\n",
      "    load_time_ms: 0.672\n",
      "    sample_throughput: 635.554\n",
      "    sample_time_ms: 6293.724\n",
      "    update_time_ms: 3.865\n",
      "  timestamp: 1636041540\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 188000\n",
      "  training_iteration: 47\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 15:59:05 (running for 00:05:11.68)<br>Memory usage on this node: 12.5/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    47</td><td style=\"text-align: right;\">          293.34</td><td style=\"text-align: right;\">188000</td><td style=\"text-align: right;\">-917.564</td><td style=\"text-align: right;\">            -592.703</td><td style=\"text-align: right;\">            -1744.67</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 192000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_15-59-07\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -592.7030080129504\n",
      "  episode_reward_mean: -902.591492557825\n",
      "  episode_reward_min: -1763.6340451773274\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 960\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3427445888519287\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011398105882108212\n",
      "          model: {}\n",
      "          policy_loss: 0.001667896518483758\n",
      "          total_loss: 10976.1982421875\n",
      "          vf_explained_var: 0.32900938391685486\n",
      "          vf_loss: 10976.1904296875\n",
      "    num_agent_steps_sampled: 192000\n",
      "    num_agent_steps_trained: 192000\n",
      "    num_steps_sampled: 192000\n",
      "    num_steps_trained: 192000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 48\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.125\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12273367471973617\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1815304796844787\n",
      "    mean_inference_ms: 1.2504225794063009\n",
      "    mean_raw_obs_processing_ms: 0.10558172774492425\n",
      "  time_since_restore: 299.56460094451904\n",
      "  time_this_iter_s: 6.2241051197052\n",
      "  time_total_s: 299.56460094451904\n",
      "  timers:\n",
      "    learn_throughput: 1428.13\n",
      "    learn_time_ms: 2800.866\n",
      "    load_throughput: 5948945.465\n",
      "    load_time_ms: 0.672\n",
      "    sample_throughput: 635.256\n",
      "    sample_time_ms: 6296.672\n",
      "    update_time_ms: 3.837\n",
      "  timestamp: 1636041547\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 192000\n",
      "  training_iteration: 48\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 15:59:10 (running for 00:05:16.95)<br>Memory usage on this node: 12.5/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    48</td><td style=\"text-align: right;\">         299.565</td><td style=\"text-align: right;\">192000</td><td style=\"text-align: right;\">-902.591</td><td style=\"text-align: right;\">            -592.703</td><td style=\"text-align: right;\">            -1763.63</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 196000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_15-59-13\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -614.1275920393937\n",
      "  episode_reward_mean: -908.8959577511725\n",
      "  episode_reward_min: -1763.6340451773274\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 980\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.1786266565322876\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009079140610992908\n",
      "          model: {}\n",
      "          policy_loss: 0.0050588687881827354\n",
      "          total_loss: 8640.8505859375\n",
      "          vf_explained_var: 0.3566383421421051\n",
      "          vf_loss: 8640.8408203125\n",
      "    num_agent_steps_sampled: 196000\n",
      "    num_agent_steps_trained: 196000\n",
      "    num_steps_sampled: 196000\n",
      "    num_steps_trained: 196000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 49\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.05\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12272647851202902\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.18151539010154502\n",
      "    mean_inference_ms: 1.2502940405232155\n",
      "    mean_raw_obs_processing_ms: 0.1055698267988593\n",
      "  time_since_restore: 305.75606179237366\n",
      "  time_this_iter_s: 6.191460847854614\n",
      "  time_total_s: 305.75606179237366\n",
      "  timers:\n",
      "    learn_throughput: 1428.683\n",
      "    learn_time_ms: 2799.782\n",
      "    load_throughput: 5652891.27\n",
      "    load_time_ms: 0.708\n",
      "    sample_throughput: 636.296\n",
      "    sample_time_ms: 6286.386\n",
      "    update_time_ms: 2.829\n",
      "  timestamp: 1636041553\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 196000\n",
      "  training_iteration: 49\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 15:59:15 (running for 00:05:22.19)<br>Memory usage on this node: 12.5/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    49</td><td style=\"text-align: right;\">         305.756</td><td style=\"text-align: right;\">196000</td><td style=\"text-align: right;\">-908.896</td><td style=\"text-align: right;\">            -614.128</td><td style=\"text-align: right;\">            -1763.63</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 200000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_15-59-19\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -577.0193683447136\n",
      "  episode_reward_mean: -898.1627570714551\n",
      "  episode_reward_min: -1763.6340451773274\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1000\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3539878129959106\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008747303858399391\n",
      "          model: {}\n",
      "          policy_loss: 0.005422267131507397\n",
      "          total_loss: 10235.9921875\n",
      "          vf_explained_var: 0.36186617612838745\n",
      "          vf_loss: 10235.982421875\n",
      "    num_agent_steps_sampled: 200000\n",
      "    num_agent_steps_trained: 200000\n",
      "    num_steps_sampled: 200000\n",
      "    num_steps_trained: 200000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 50\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 18.3\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12272011874923684\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1814995328256753\n",
      "    mean_inference_ms: 1.2501839819547245\n",
      "    mean_raw_obs_processing_ms: 0.10555905729527792\n",
      "  time_since_restore: 312.08628368377686\n",
      "  time_this_iter_s: 6.330221891403198\n",
      "  time_total_s: 312.08628368377686\n",
      "  timers:\n",
      "    learn_throughput: 1424.22\n",
      "    learn_time_ms: 2808.555\n",
      "    load_throughput: 5597816.556\n",
      "    load_time_ms: 0.715\n",
      "    sample_throughput: 637.032\n",
      "    sample_time_ms: 6279.124\n",
      "    update_time_ms: 2.862\n",
      "  timestamp: 1636041559\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 200000\n",
      "  training_iteration: 50\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 15:59:20 (running for 00:05:27.58)<br>Memory usage on this node: 12.5/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         312.086</td><td style=\"text-align: right;\">200000</td><td style=\"text-align: right;\">-898.163</td><td style=\"text-align: right;\">            -577.019</td><td style=\"text-align: right;\">            -1763.63</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 15:59:25 (running for 00:05:32.58)<br>Memory usage on this node: 12.5/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         312.086</td><td style=\"text-align: right;\">200000</td><td style=\"text-align: right;\">-898.163</td><td style=\"text-align: right;\">            -577.019</td><td style=\"text-align: right;\">            -1763.63</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 204000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_15-59-26\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -577.0193683447136\n",
      "  episode_reward_mean: -895.8725697202607\n",
      "  episode_reward_min: -1763.6340451773274\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1020\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.2835686206817627\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00818727444857359\n",
      "          model: {}\n",
      "          policy_loss: 0.008944314904510975\n",
      "          total_loss: 10700.384765625\n",
      "          vf_explained_var: 0.23697568476200104\n",
      "          vf_loss: 10700.373046875\n",
      "    num_agent_steps_sampled: 204000\n",
      "    num_agent_steps_trained: 204000\n",
      "    num_steps_sampled: 204000\n",
      "    num_steps_trained: 204000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 51\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.175\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12272335547845026\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.18150488263394776\n",
      "    mean_inference_ms: 1.2501983834808117\n",
      "    mean_raw_obs_processing_ms: 0.10555935911389767\n",
      "  time_since_restore: 318.41878366470337\n",
      "  time_this_iter_s: 6.332499980926514\n",
      "  time_total_s: 318.41878366470337\n",
      "  timers:\n",
      "    learn_throughput: 1425.253\n",
      "    learn_time_ms: 2806.52\n",
      "    load_throughput: 5604361.304\n",
      "    load_time_ms: 0.714\n",
      "    sample_throughput: 635.204\n",
      "    sample_time_ms: 6297.19\n",
      "    update_time_ms: 2.863\n",
      "  timestamp: 1636041566\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 204000\n",
      "  training_iteration: 51\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 15:59:31 (running for 00:05:38.05)<br>Memory usage on this node: 12.5/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    51</td><td style=\"text-align: right;\">         318.419</td><td style=\"text-align: right;\">204000</td><td style=\"text-align: right;\">-895.873</td><td style=\"text-align: right;\">            -577.019</td><td style=\"text-align: right;\">            -1763.63</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 208000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_15-59-32\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -577.0193683447136\n",
      "  episode_reward_mean: -920.7134478060963\n",
      "  episode_reward_min: -1763.6340451773274\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1040\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.596805214881897\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009123262949287891\n",
      "          model: {}\n",
      "          policy_loss: 0.0021696893963962793\n",
      "          total_loss: 10746.4013671875\n",
      "          vf_explained_var: 0.3370745778083801\n",
      "          vf_loss: 10746.39453125\n",
      "    num_agent_steps_sampled: 208000\n",
      "    num_agent_steps_trained: 208000\n",
      "    num_steps_sampled: 208000\n",
      "    num_steps_trained: 208000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 52\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.857142857142858\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12272568082979514\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.18150853548300688\n",
      "    mean_inference_ms: 1.2502144497299696\n",
      "    mean_raw_obs_processing_ms: 0.10555964188625774\n",
      "  time_since_restore: 324.60160303115845\n",
      "  time_this_iter_s: 6.182819366455078\n",
      "  time_total_s: 324.60160303115845\n",
      "  timers:\n",
      "    learn_throughput: 1427.55\n",
      "    learn_time_ms: 2802.004\n",
      "    load_throughput: 5599124.282\n",
      "    load_time_ms: 0.714\n",
      "    sample_throughput: 634.55\n",
      "    sample_time_ms: 6303.678\n",
      "    update_time_ms: 2.869\n",
      "  timestamp: 1636041572\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 208000\n",
      "  training_iteration: 52\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 15:59:36 (running for 00:05:43.28)<br>Memory usage on this node: 12.5/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    52</td><td style=\"text-align: right;\">         324.602</td><td style=\"text-align: right;\">208000</td><td style=\"text-align: right;\">-920.713</td><td style=\"text-align: right;\">            -577.019</td><td style=\"text-align: right;\">            -1763.63</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 212000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_15-59-38\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -577.0193683447136\n",
      "  episode_reward_mean: -925.971937221571\n",
      "  episode_reward_min: -1739.7354772174585\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1060\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.7624789476394653\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013814731501042843\n",
      "          model: {}\n",
      "          policy_loss: 0.0011861019302159548\n",
      "          total_loss: 9963.1396484375\n",
      "          vf_explained_var: 0.26879292726516724\n",
      "          vf_loss: 9963.130859375\n",
      "    num_agent_steps_sampled: 212000\n",
      "    num_agent_steps_trained: 212000\n",
      "    num_steps_sampled: 212000\n",
      "    num_steps_trained: 212000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 53\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.1875\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12272523628484128\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.18150924903145413\n",
      "    mean_inference_ms: 1.2502042338228485\n",
      "    mean_raw_obs_processing_ms: 0.10555919819516096\n",
      "  time_since_restore: 330.7947406768799\n",
      "  time_this_iter_s: 6.1931376457214355\n",
      "  time_total_s: 330.7947406768799\n",
      "  timers:\n",
      "    learn_throughput: 1428.542\n",
      "    learn_time_ms: 2800.058\n",
      "    load_throughput: 5533565.091\n",
      "    load_time_ms: 0.723\n",
      "    sample_throughput: 634.92\n",
      "    sample_time_ms: 6300.002\n",
      "    update_time_ms: 2.873\n",
      "  timestamp: 1636041578\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 212000\n",
      "  training_iteration: 53\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 15:59:41 (running for 00:05:48.52)<br>Memory usage on this node: 12.5/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    53</td><td style=\"text-align: right;\">         330.795</td><td style=\"text-align: right;\">212000</td><td style=\"text-align: right;\">-925.972</td><td style=\"text-align: right;\">            -577.019</td><td style=\"text-align: right;\">            -1739.74</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 216000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_15-59-45\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -577.0193683447136\n",
      "  episode_reward_mean: -936.9087542703937\n",
      "  episode_reward_min: -1739.7354772174585\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1080\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 2.0415992736816406\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007133521605283022\n",
      "          model: {}\n",
      "          policy_loss: -0.0014085888396948576\n",
      "          total_loss: 11743.509765625\n",
      "          vf_explained_var: 0.39117905497550964\n",
      "          vf_loss: 11743.5078125\n",
      "    num_agent_steps_sampled: 216000\n",
      "    num_agent_steps_trained: 216000\n",
      "    num_steps_sampled: 216000\n",
      "    num_steps_trained: 216000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 54\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.274999999999999\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12272316528220667\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1815114064578462\n",
      "    mean_inference_ms: 1.2502033047043997\n",
      "    mean_raw_obs_processing_ms: 0.10556022233860961\n",
      "  time_since_restore: 337.0089750289917\n",
      "  time_this_iter_s: 6.214234352111816\n",
      "  time_total_s: 337.0089750289917\n",
      "  timers:\n",
      "    learn_throughput: 1427.879\n",
      "    learn_time_ms: 2801.357\n",
      "    load_throughput: 5529552.75\n",
      "    load_time_ms: 0.723\n",
      "    sample_throughput: 635.618\n",
      "    sample_time_ms: 6293.091\n",
      "    update_time_ms: 2.852\n",
      "  timestamp: 1636041585\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 216000\n",
      "  training_iteration: 54\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 15:59:47 (running for 00:05:53.78)<br>Memory usage on this node: 12.5/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    54</td><td style=\"text-align: right;\">         337.009</td><td style=\"text-align: right;\">216000</td><td style=\"text-align: right;\">-936.909</td><td style=\"text-align: right;\">            -577.019</td><td style=\"text-align: right;\">            -1739.74</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 220000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_15-59-51\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -518.0281871289559\n",
      "  episode_reward_mean: -919.7801419536851\n",
      "  episode_reward_min: -1714.7236631400929\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1100\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.2479946613311768\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008023301139473915\n",
      "          model: {}\n",
      "          policy_loss: 0.0027344105765223503\n",
      "          total_loss: 6909.6162109375\n",
      "          vf_explained_var: 0.39062950015068054\n",
      "          vf_loss: 6909.60986328125\n",
      "    num_agent_steps_sampled: 220000\n",
      "    num_agent_steps_trained: 220000\n",
      "    num_steps_sampled: 220000\n",
      "    num_steps_trained: 220000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 55\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.1375\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1227216882779248\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1815111567974671\n",
      "    mean_inference_ms: 1.250151937714927\n",
      "    mean_raw_obs_processing_ms: 0.1055582712326628\n",
      "  time_since_restore: 343.2018811702728\n",
      "  time_this_iter_s: 6.192906141281128\n",
      "  time_total_s: 343.2018811702728\n",
      "  timers:\n",
      "    learn_throughput: 1427.893\n",
      "    learn_time_ms: 2801.331\n",
      "    load_throughput: 5517732.027\n",
      "    load_time_ms: 0.725\n",
      "    sample_throughput: 635.793\n",
      "    sample_time_ms: 6291.353\n",
      "    update_time_ms: 2.89\n",
      "  timestamp: 1636041591\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 220000\n",
      "  training_iteration: 55\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 15:59:52 (running for 00:05:59.03)<br>Memory usage on this node: 12.5/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    55</td><td style=\"text-align: right;\">         343.202</td><td style=\"text-align: right;\">220000</td><td style=\"text-align: right;\"> -919.78</td><td style=\"text-align: right;\">            -518.028</td><td style=\"text-align: right;\">            -1714.72</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 15:59:57 (running for 00:06:04.03)<br>Memory usage on this node: 12.5/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    55</td><td style=\"text-align: right;\">         343.202</td><td style=\"text-align: right;\">220000</td><td style=\"text-align: right;\"> -919.78</td><td style=\"text-align: right;\">            -518.028</td><td style=\"text-align: right;\">            -1714.72</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 224000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_15-59-57\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -512.5613650232718\n",
      "  episode_reward_mean: -903.3866869492095\n",
      "  episode_reward_min: -1736.7532236788177\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1120\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.099147915840149\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008478272706270218\n",
      "          model: {}\n",
      "          policy_loss: 0.005142306908965111\n",
      "          total_loss: 8378.5703125\n",
      "          vf_explained_var: 0.27734997868537903\n",
      "          vf_loss: 8378.5615234375\n",
      "    num_agent_steps_sampled: 224000\n",
      "    num_agent_steps_trained: 224000\n",
      "    num_steps_sampled: 224000\n",
      "    num_steps_trained: 224000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 56\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.037500000000001\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12270820176093984\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1814860068099805\n",
      "    mean_inference_ms: 1.2499808280660476\n",
      "    mean_raw_obs_processing_ms: 0.10554440660469357\n",
      "  time_since_restore: 349.41687297821045\n",
      "  time_this_iter_s: 6.214991807937622\n",
      "  time_total_s: 349.41687297821045\n",
      "  timers:\n",
      "    learn_throughput: 1426.793\n",
      "    learn_time_ms: 2803.489\n",
      "    load_throughput: 5469701.692\n",
      "    load_time_ms: 0.731\n",
      "    sample_throughput: 635.625\n",
      "    sample_time_ms: 6293.023\n",
      "    update_time_ms: 2.907\n",
      "  timestamp: 1636041597\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 224000\n",
      "  training_iteration: 56\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:00:02 (running for 00:06:09.30)<br>Memory usage on this node: 12.5/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    56</td><td style=\"text-align: right;\">         349.417</td><td style=\"text-align: right;\">224000</td><td style=\"text-align: right;\">-903.387</td><td style=\"text-align: right;\">            -512.561</td><td style=\"text-align: right;\">            -1736.75</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 228000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-00-03\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -508.6739155235433\n",
      "  episode_reward_mean: -878.1254715803384\n",
      "  episode_reward_min: -1736.7532236788177\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1140\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.1994198560714722\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01159993652254343\n",
      "          model: {}\n",
      "          policy_loss: 0.016012176871299744\n",
      "          total_loss: 7672.55712890625\n",
      "          vf_explained_var: 0.3645663559436798\n",
      "          vf_loss: 7672.5341796875\n",
      "    num_agent_steps_sampled: 228000\n",
      "    num_agent_steps_trained: 228000\n",
      "    num_steps_sampled: 228000\n",
      "    num_steps_trained: 228000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 57\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.842857142857143\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12270100683949849\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.18146905331533725\n",
      "    mean_inference_ms: 1.24984748051209\n",
      "    mean_raw_obs_processing_ms: 0.10553439957101766\n",
      "  time_since_restore: 355.65170645713806\n",
      "  time_this_iter_s: 6.234833478927612\n",
      "  time_total_s: 355.65170645713806\n",
      "  timers:\n",
      "    learn_throughput: 1426.715\n",
      "    learn_time_ms: 2803.642\n",
      "    load_throughput: 5513742.605\n",
      "    load_time_ms: 0.725\n",
      "    sample_throughput: 635.923\n",
      "    sample_time_ms: 6290.074\n",
      "    update_time_ms: 2.897\n",
      "  timestamp: 1636041603\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 228000\n",
      "  training_iteration: 57\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:00:07 (running for 00:06:14.58)<br>Memory usage on this node: 12.5/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    57</td><td style=\"text-align: right;\">         355.652</td><td style=\"text-align: right;\">228000</td><td style=\"text-align: right;\">-878.125</td><td style=\"text-align: right;\">            -508.674</td><td style=\"text-align: right;\">            -1736.75</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 232000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-00-10\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -508.6739155235433\n",
      "  episode_reward_mean: -855.9780835963927\n",
      "  episode_reward_min: -1736.7532236788177\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1160\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.4037374258041382\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00857061892747879\n",
      "          model: {}\n",
      "          policy_loss: 0.0035545581486076117\n",
      "          total_loss: 9055.8974609375\n",
      "          vf_explained_var: 0.38531166315078735\n",
      "          vf_loss: 9055.890625\n",
      "    num_agent_steps_sampled: 232000\n",
      "    num_agent_steps_trained: 232000\n",
      "    num_steps_sampled: 232000\n",
      "    num_steps_trained: 232000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 58\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.412500000000001\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12269199901248763\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1814467505091484\n",
      "    mean_inference_ms: 1.2496907820526264\n",
      "    mean_raw_obs_processing_ms: 0.10552099448791052\n",
      "  time_since_restore: 361.84001088142395\n",
      "  time_this_iter_s: 6.188304424285889\n",
      "  time_total_s: 361.84001088142395\n",
      "  timers:\n",
      "    learn_throughput: 1426.767\n",
      "    learn_time_ms: 2803.54\n",
      "    load_throughput: 5543255.138\n",
      "    load_time_ms: 0.722\n",
      "    sample_throughput: 636.235\n",
      "    sample_time_ms: 6286.981\n",
      "    update_time_ms: 2.937\n",
      "  timestamp: 1636041610\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 232000\n",
      "  training_iteration: 58\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:00:13 (running for 00:06:19.82)<br>Memory usage on this node: 12.5/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    58</td><td style=\"text-align: right;\">          361.84</td><td style=\"text-align: right;\">232000</td><td style=\"text-align: right;\">-855.978</td><td style=\"text-align: right;\">            -508.674</td><td style=\"text-align: right;\">            -1736.75</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 236000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-00-16\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -508.6739155235433\n",
      "  episode_reward_mean: -833.9499131111711\n",
      "  episode_reward_min: -1736.7532236788177\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1180\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.2679386138916016\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008305798284709454\n",
      "          model: {}\n",
      "          policy_loss: 0.004113171715289354\n",
      "          total_loss: 8551.033203125\n",
      "          vf_explained_var: 0.41182827949523926\n",
      "          vf_loss: 8551.025390625\n",
      "    num_agent_steps_sampled: 236000\n",
      "    num_agent_steps_trained: 236000\n",
      "    num_steps_sampled: 236000\n",
      "    num_steps_trained: 236000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 59\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.0625\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12269374455038712\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.18144039671080947\n",
      "    mean_inference_ms: 1.2496268983147123\n",
      "    mean_raw_obs_processing_ms: 0.10551652512717069\n",
      "  time_since_restore: 368.1630630493164\n",
      "  time_this_iter_s: 6.323052167892456\n",
      "  time_total_s: 368.1630630493164\n",
      "  timers:\n",
      "    learn_throughput: 1425.929\n",
      "    learn_time_ms: 2805.188\n",
      "    load_throughput: 5861855.281\n",
      "    load_time_ms: 0.682\n",
      "    sample_throughput: 635.067\n",
      "    sample_time_ms: 6298.551\n",
      "    update_time_ms: 2.945\n",
      "  timestamp: 1636041616\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 236000\n",
      "  training_iteration: 59\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:00:18 (running for 00:06:25.19)<br>Memory usage on this node: 12.5/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    59</td><td style=\"text-align: right;\">         368.163</td><td style=\"text-align: right;\">236000</td><td style=\"text-align: right;\"> -833.95</td><td style=\"text-align: right;\">            -508.674</td><td style=\"text-align: right;\">            -1736.75</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 240000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-00-22\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -508.6739155235433\n",
      "  episode_reward_mean: -832.8464442602475\n",
      "  episode_reward_min: -1736.7532236788177\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1200\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.8790856003761292\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011771866120398045\n",
      "          model: {}\n",
      "          policy_loss: 0.005436439532786608\n",
      "          total_loss: 4493.68212890625\n",
      "          vf_explained_var: 0.5794723629951477\n",
      "          vf_loss: 4493.6708984375\n",
      "    num_agent_steps_sampled: 240000\n",
      "    num_agent_steps_trained: 240000\n",
      "    num_steps_sampled: 240000\n",
      "    num_steps_trained: 240000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 60\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.399999999999999\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1226922049619472\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.18143115247261826\n",
      "    mean_inference_ms: 1.2495559040277409\n",
      "    mean_raw_obs_processing_ms: 0.10551111036582592\n",
      "  time_since_restore: 374.341472864151\n",
      "  time_this_iter_s: 6.178409814834595\n",
      "  time_total_s: 374.341472864151\n",
      "  timers:\n",
      "    learn_throughput: 1429.453\n",
      "    learn_time_ms: 2798.272\n",
      "    load_throughput: 5912675.242\n",
      "    load_time_ms: 0.677\n",
      "    sample_throughput: 635.691\n",
      "    sample_time_ms: 6292.367\n",
      "    update_time_ms: 2.911\n",
      "  timestamp: 1636041622\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 240000\n",
      "  training_iteration: 60\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:00:23 (running for 00:06:30.43)<br>Memory usage on this node: 12.5/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    60</td><td style=\"text-align: right;\">         374.341</td><td style=\"text-align: right;\">240000</td><td style=\"text-align: right;\">-832.846</td><td style=\"text-align: right;\">            -508.674</td><td style=\"text-align: right;\">            -1736.75</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:00:28 (running for 00:06:35.43)<br>Memory usage on this node: 12.5/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    60</td><td style=\"text-align: right;\">         374.341</td><td style=\"text-align: right;\">240000</td><td style=\"text-align: right;\">-832.846</td><td style=\"text-align: right;\">            -508.674</td><td style=\"text-align: right;\">            -1736.75</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 244000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-00-29\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -508.6739155235433\n",
      "  episode_reward_mean: -846.578699013979\n",
      "  episode_reward_min: -1723.0710113760667\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1220\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3327938318252563\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008253045380115509\n",
      "          model: {}\n",
      "          policy_loss: 0.0020912166219204664\n",
      "          total_loss: 9009.3662109375\n",
      "          vf_explained_var: 0.3797861337661743\n",
      "          vf_loss: 9009.359375\n",
      "    num_agent_steps_sampled: 244000\n",
      "    num_agent_steps_trained: 244000\n",
      "    num_steps_sampled: 244000\n",
      "    num_steps_trained: 244000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 61\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.9\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12269405846577908\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.18142753352676666\n",
      "    mean_inference_ms: 1.2495199662822967\n",
      "    mean_raw_obs_processing_ms: 0.10550755012254025\n",
      "  time_since_restore: 380.7673931121826\n",
      "  time_this_iter_s: 6.425920248031616\n",
      "  time_total_s: 380.7673931121826\n",
      "  timers:\n",
      "    learn_throughput: 1418.695\n",
      "    learn_time_ms: 2819.492\n",
      "    load_throughput: 5868621.799\n",
      "    load_time_ms: 0.682\n",
      "    sample_throughput: 637.574\n",
      "    sample_time_ms: 6273.784\n",
      "    update_time_ms: 2.929\n",
      "  timestamp: 1636041629\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 244000\n",
      "  training_iteration: 61\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:00:34 (running for 00:06:40.91)<br>Memory usage on this node: 12.5/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    61</td><td style=\"text-align: right;\">         380.767</td><td style=\"text-align: right;\">244000</td><td style=\"text-align: right;\">-846.579</td><td style=\"text-align: right;\">            -508.674</td><td style=\"text-align: right;\">            -1723.07</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 248000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-00-35\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -517.5680713959526\n",
      "  episode_reward_mean: -847.9862027424672\n",
      "  episode_reward_min: -1723.0710113760667\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1240\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.2892513275146484\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00956752896308899\n",
      "          model: {}\n",
      "          policy_loss: -0.0006911610835231841\n",
      "          total_loss: 8975.2529296875\n",
      "          vf_explained_var: 0.413601279258728\n",
      "          vf_loss: 8975.248046875\n",
      "    num_agent_steps_sampled: 248000\n",
      "    num_agent_steps_trained: 248000\n",
      "    num_steps_sampled: 248000\n",
      "    num_steps_trained: 248000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 62\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.350000000000001\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12269363198893486\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1814225154049131\n",
      "    mean_inference_ms: 1.2494794685638784\n",
      "    mean_raw_obs_processing_ms: 0.1055047314487811\n",
      "  time_since_restore: 387.0250813961029\n",
      "  time_this_iter_s: 6.257688283920288\n",
      "  time_total_s: 387.0250813961029\n",
      "  timers:\n",
      "    learn_throughput: 1416.538\n",
      "    learn_time_ms: 2823.786\n",
      "    load_throughput: 5831902.113\n",
      "    load_time_ms: 0.686\n",
      "    sample_throughput: 636.054\n",
      "    sample_time_ms: 6288.77\n",
      "    update_time_ms: 2.937\n",
      "  timestamp: 1636041635\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 248000\n",
      "  training_iteration: 62\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:00:39 (running for 00:06:46.21)<br>Memory usage on this node: 12.5/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    62</td><td style=\"text-align: right;\">         387.025</td><td style=\"text-align: right;\">248000</td><td style=\"text-align: right;\">-847.986</td><td style=\"text-align: right;\">            -517.568</td><td style=\"text-align: right;\">            -1723.07</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 252000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-00-41\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -517.5680713959526\n",
      "  episode_reward_mean: -833.3009160205132\n",
      "  episode_reward_min: -1758.2308744625893\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1260\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.0692378282546997\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009127604775130749\n",
      "          model: {}\n",
      "          policy_loss: -0.002961579244583845\n",
      "          total_loss: 8246.0390625\n",
      "          vf_explained_var: 0.26446807384490967\n",
      "          vf_loss: 8246.037109375\n",
      "    num_agent_steps_sampled: 252000\n",
      "    num_agent_steps_trained: 252000\n",
      "    num_steps_sampled: 252000\n",
      "    num_steps_trained: 252000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 63\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.037500000000001\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12269682093765616\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.181425524872599\n",
      "    mean_inference_ms: 1.2494795487918033\n",
      "    mean_raw_obs_processing_ms: 0.10550470097391203\n",
      "  time_since_restore: 393.2438704967499\n",
      "  time_this_iter_s: 6.218789100646973\n",
      "  time_total_s: 393.2438704967499\n",
      "  timers:\n",
      "    learn_throughput: 1415.645\n",
      "    learn_time_ms: 2825.567\n",
      "    load_throughput: 5882614.306\n",
      "    load_time_ms: 0.68\n",
      "    sample_throughput: 635.502\n",
      "    sample_time_ms: 6294.234\n",
      "    update_time_ms: 2.979\n",
      "  timestamp: 1636041641\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 252000\n",
      "  training_iteration: 63\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:00:44 (running for 00:06:51.50)<br>Memory usage on this node: 12.5/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    63</td><td style=\"text-align: right;\">         393.244</td><td style=\"text-align: right;\">252000</td><td style=\"text-align: right;\">-833.301</td><td style=\"text-align: right;\">            -517.568</td><td style=\"text-align: right;\">            -1758.23</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 256000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-00-48\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -515.7811979944464\n",
      "  episode_reward_mean: -826.5277133312028\n",
      "  episode_reward_min: -1758.2308744625893\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1280\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.9083244204521179\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012274659238755703\n",
      "          model: {}\n",
      "          policy_loss: 0.0011807691771537066\n",
      "          total_loss: 5635.26708984375\n",
      "          vf_explained_var: 0.3803934156894684\n",
      "          vf_loss: 5635.26025390625\n",
      "    num_agent_steps_sampled: 256000\n",
      "    num_agent_steps_trained: 256000\n",
      "    num_steps_sampled: 256000\n",
      "    num_steps_trained: 256000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 64\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.028571428571428\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12269598761444693\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.18142062745625026\n",
      "    mean_inference_ms: 1.249427226320702\n",
      "    mean_raw_obs_processing_ms: 0.10549831108671215\n",
      "  time_since_restore: 399.4702820777893\n",
      "  time_this_iter_s: 6.226411581039429\n",
      "  time_total_s: 399.4702820777893\n",
      "  timers:\n",
      "    learn_throughput: 1416.791\n",
      "    learn_time_ms: 2823.282\n",
      "    load_throughput: 5888808.705\n",
      "    load_time_ms: 0.679\n",
      "    sample_throughput: 634.824\n",
      "    sample_time_ms: 6300.958\n",
      "    update_time_ms: 3.021\n",
      "  timestamp: 1636041648\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 256000\n",
      "  training_iteration: 64\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:00:50 (running for 00:06:56.78)<br>Memory usage on this node: 12.5/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">          399.47</td><td style=\"text-align: right;\">256000</td><td style=\"text-align: right;\">-826.528</td><td style=\"text-align: right;\">            -515.781</td><td style=\"text-align: right;\">            -1758.23</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 260000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-00-54\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -515.7811979944464\n",
      "  episode_reward_mean: -865.7804740236554\n",
      "  episode_reward_min: -1758.2308744625893\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1300\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 2.1692991256713867\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007978197187185287\n",
      "          model: {}\n",
      "          policy_loss: 0.0007799356244504452\n",
      "          total_loss: 13262.396484375\n",
      "          vf_explained_var: 0.3095655143260956\n",
      "          vf_loss: 13262.3916015625\n",
      "    num_agent_steps_sampled: 260000\n",
      "    num_agent_steps_trained: 260000\n",
      "    num_steps_sampled: 260000\n",
      "    num_steps_trained: 260000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 65\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.0875\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12269874970605191\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1814211995010136\n",
      "    mean_inference_ms: 1.2494070251301903\n",
      "    mean_raw_obs_processing_ms: 0.1054936770908798\n",
      "  time_since_restore: 405.68277049064636\n",
      "  time_this_iter_s: 6.212488412857056\n",
      "  time_total_s: 405.68277049064636\n",
      "  timers:\n",
      "    learn_throughput: 1417.813\n",
      "    learn_time_ms: 2821.247\n",
      "    load_throughput: 5924785.818\n",
      "    load_time_ms: 0.675\n",
      "    sample_throughput: 634.554\n",
      "    sample_time_ms: 6303.64\n",
      "    update_time_ms: 2.985\n",
      "  timestamp: 1636041654\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 260000\n",
      "  training_iteration: 65\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:00:55 (running for 00:07:02.04)<br>Memory usage on this node: 12.5/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    65</td><td style=\"text-align: right;\">         405.683</td><td style=\"text-align: right;\">260000</td><td style=\"text-align: right;\"> -865.78</td><td style=\"text-align: right;\">            -515.781</td><td style=\"text-align: right;\">            -1758.23</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:01:00 (running for 00:07:07.05)<br>Memory usage on this node: 12.5/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    65</td><td style=\"text-align: right;\">         405.683</td><td style=\"text-align: right;\">260000</td><td style=\"text-align: right;\"> -865.78</td><td style=\"text-align: right;\">            -515.781</td><td style=\"text-align: right;\">            -1758.23</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 264000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-01-00\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -507.98432922422296\n",
      "  episode_reward_mean: -842.4580290717194\n",
      "  episode_reward_min: -1758.2308744625893\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1320\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.6177846193313599\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011855687946081161\n",
      "          model: {}\n",
      "          policy_loss: 0.0032453963067382574\n",
      "          total_loss: 5591.01611328125\n",
      "          vf_explained_var: 0.31404340267181396\n",
      "          vf_loss: 5591.00634765625\n",
      "    num_agent_steps_sampled: 264000\n",
      "    num_agent_steps_trained: 264000\n",
      "    num_steps_sampled: 264000\n",
      "    num_steps_trained: 264000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 66\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.175\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1226966458830878\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.18141405989654594\n",
      "    mean_inference_ms: 1.2493400236237577\n",
      "    mean_raw_obs_processing_ms: 0.10548566613900387\n",
      "  time_since_restore: 411.85653948783875\n",
      "  time_this_iter_s: 6.173768997192383\n",
      "  time_total_s: 411.85653948783875\n",
      "  timers:\n",
      "    learn_throughput: 1418.739\n",
      "    learn_time_ms: 2819.404\n",
      "    load_throughput: 5985236.345\n",
      "    load_time_ms: 0.668\n",
      "    sample_throughput: 634.995\n",
      "    sample_time_ms: 6299.263\n",
      "    update_time_ms: 2.982\n",
      "  timestamp: 1636041660\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 264000\n",
      "  training_iteration: 66\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:01:05 (running for 00:07:12.27)<br>Memory usage on this node: 12.5/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    66</td><td style=\"text-align: right;\">         411.857</td><td style=\"text-align: right;\">264000</td><td style=\"text-align: right;\">-842.458</td><td style=\"text-align: right;\">            -507.984</td><td style=\"text-align: right;\">            -1758.23</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 268000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-01-06\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -507.98432922422296\n",
      "  episode_reward_mean: -867.7142576919435\n",
      "  episode_reward_min: -1758.2308744625893\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1340\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.8944100141525269\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007641233503818512\n",
      "          model: {}\n",
      "          policy_loss: 0.011294813826680183\n",
      "          total_loss: 9788.46875\n",
      "          vf_explained_var: 0.38424432277679443\n",
      "          vf_loss: 9788.4541015625\n",
      "    num_agent_steps_sampled: 268000\n",
      "    num_agent_steps_trained: 268000\n",
      "    num_steps_sampled: 268000\n",
      "    num_steps_trained: 268000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 67\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.975\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12270023188175752\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.18141474660933762\n",
      "    mean_inference_ms: 1.2493122573532844\n",
      "    mean_raw_obs_processing_ms: 0.10548120229590265\n",
      "  time_since_restore: 418.16839814186096\n",
      "  time_this_iter_s: 6.311858654022217\n",
      "  time_total_s: 418.16839814186096\n",
      "  timers:\n",
      "    learn_throughput: 1417.387\n",
      "    learn_time_ms: 2822.095\n",
      "    load_throughput: 5799445.539\n",
      "    load_time_ms: 0.69\n",
      "    sample_throughput: 634.728\n",
      "    sample_time_ms: 6301.914\n",
      "    update_time_ms: 2.986\n",
      "  timestamp: 1636041666\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 268000\n",
      "  training_iteration: 67\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:01:10 (running for 00:07:17.62)<br>Memory usage on this node: 12.5/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    67</td><td style=\"text-align: right;\">         418.168</td><td style=\"text-align: right;\">268000</td><td style=\"text-align: right;\">-867.714</td><td style=\"text-align: right;\">            -507.984</td><td style=\"text-align: right;\">            -1758.23</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 272000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-01-13\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -507.98432922422296\n",
      "  episode_reward_mean: -909.5289232049919\n",
      "  episode_reward_min: -1726.5569369712302\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1360\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 2.2574009895324707\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00856374204158783\n",
      "          model: {}\n",
      "          policy_loss: -0.0022496033925563097\n",
      "          total_loss: 10851.150390625\n",
      "          vf_explained_var: 0.3757186830043793\n",
      "          vf_loss: 10851.1484375\n",
      "    num_agent_steps_sampled: 272000\n",
      "    num_agent_steps_trained: 272000\n",
      "    num_steps_sampled: 272000\n",
      "    num_steps_trained: 272000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 68\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.2625\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1227027841025691\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1814136862449214\n",
      "    mean_inference_ms: 1.2492753015783595\n",
      "    mean_raw_obs_processing_ms: 0.10547732271534169\n",
      "  time_since_restore: 424.37476921081543\n",
      "  time_this_iter_s: 6.206371068954468\n",
      "  time_total_s: 424.37476921081543\n",
      "  timers:\n",
      "    learn_throughput: 1416.868\n",
      "    learn_time_ms: 2823.128\n",
      "    load_throughput: 5807877.592\n",
      "    load_time_ms: 0.689\n",
      "    sample_throughput: 634.448\n",
      "    sample_time_ms: 6304.695\n",
      "    update_time_ms: 2.984\n",
      "  timestamp: 1636041673\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 272000\n",
      "  training_iteration: 68\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:01:16 (running for 00:07:22.89)<br>Memory usage on this node: 12.5/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    68</td><td style=\"text-align: right;\">         424.375</td><td style=\"text-align: right;\">272000</td><td style=\"text-align: right;\">-909.529</td><td style=\"text-align: right;\">            -507.984</td><td style=\"text-align: right;\">            -1726.56</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 276000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-01-19\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -507.98432922422296\n",
      "  episode_reward_mean: -909.3361972567557\n",
      "  episode_reward_min: -1726.5569369712302\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1380\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.1268621683120728\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012525520287454128\n",
      "          model: {}\n",
      "          policy_loss: 0.004757501184940338\n",
      "          total_loss: 6039.66064453125\n",
      "          vf_explained_var: 0.5226353406906128\n",
      "          vf_loss: 6039.6494140625\n",
      "    num_agent_steps_sampled: 276000\n",
      "    num_agent_steps_trained: 276000\n",
      "    num_steps_sampled: 276000\n",
      "    num_steps_trained: 276000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 69\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.212499999999999\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12270339182647066\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.18140914338735925\n",
      "    mean_inference_ms: 1.2492354023995176\n",
      "    mean_raw_obs_processing_ms: 0.10547307049883639\n",
      "  time_since_restore: 430.60475754737854\n",
      "  time_this_iter_s: 6.22998833656311\n",
      "  time_total_s: 430.60475754737854\n",
      "  timers:\n",
      "    learn_throughput: 1417.774\n",
      "    learn_time_ms: 2821.325\n",
      "    load_throughput: 5798242.958\n",
      "    load_time_ms: 0.69\n",
      "    sample_throughput: 635.011\n",
      "    sample_time_ms: 6299.099\n",
      "    update_time_ms: 2.995\n",
      "  timestamp: 1636041679\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 276000\n",
      "  training_iteration: 69\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:01:21 (running for 00:07:28.16)<br>Memory usage on this node: 12.5/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    69</td><td style=\"text-align: right;\">         430.605</td><td style=\"text-align: right;\">276000</td><td style=\"text-align: right;\">-909.336</td><td style=\"text-align: right;\">            -507.984</td><td style=\"text-align: right;\">            -1726.56</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 280000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-01-25\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -507.98432922422296\n",
      "  episode_reward_mean: -871.8084771219749\n",
      "  episode_reward_min: -1683.0384897651345\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1400\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.861875057220459\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009354392066597939\n",
      "          model: {}\n",
      "          policy_loss: -0.001310271443799138\n",
      "          total_loss: 6835.90966796875\n",
      "          vf_explained_var: 0.40528014302253723\n",
      "          vf_loss: 6835.90625\n",
      "    num_agent_steps_sampled: 280000\n",
      "    num_agent_steps_trained: 280000\n",
      "    num_steps_sampled: 280000\n",
      "    num_steps_trained: 280000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 70\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.028571428571427\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12270052182850393\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.18139956008811617\n",
      "    mean_inference_ms: 1.2491613360225364\n",
      "    mean_raw_obs_processing_ms: 0.10546702273817798\n",
      "  time_since_restore: 436.77596139907837\n",
      "  time_this_iter_s: 6.171203851699829\n",
      "  time_total_s: 436.77596139907837\n",
      "  timers:\n",
      "    learn_throughput: 1418.676\n",
      "    learn_time_ms: 2819.531\n",
      "    load_throughput: 5698395.489\n",
      "    load_time_ms: 0.702\n",
      "    sample_throughput: 635.134\n",
      "    sample_time_ms: 6297.885\n",
      "    update_time_ms: 2.998\n",
      "  timestamp: 1636041685\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 280000\n",
      "  training_iteration: 70\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:01:26 (running for 00:07:33.39)<br>Memory usage on this node: 12.5/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    70</td><td style=\"text-align: right;\">         436.776</td><td style=\"text-align: right;\">280000</td><td style=\"text-align: right;\">-871.808</td><td style=\"text-align: right;\">            -507.984</td><td style=\"text-align: right;\">            -1683.04</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:01:31 (running for 00:07:38.39)<br>Memory usage on this node: 12.5/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    70</td><td style=\"text-align: right;\">         436.776</td><td style=\"text-align: right;\">280000</td><td style=\"text-align: right;\">-871.808</td><td style=\"text-align: right;\">            -507.984</td><td style=\"text-align: right;\">            -1683.04</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 284000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-01-31\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -511.17424958008235\n",
      "  episode_reward_mean: -899.9307764749522\n",
      "  episode_reward_min: -1759.9603810451522\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1420\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.6719505786895752\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010511835105717182\n",
      "          model: {}\n",
      "          policy_loss: -0.00777248153463006\n",
      "          total_loss: 8187.626953125\n",
      "          vf_explained_var: 0.4115780293941498\n",
      "          vf_loss: 8187.62890625\n",
      "    num_agent_steps_sampled: 284000\n",
      "    num_agent_steps_trained: 284000\n",
      "    num_steps_sampled: 284000\n",
      "    num_steps_trained: 284000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 71\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.3125\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12269915320512198\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.18139355030008808\n",
      "    mean_inference_ms: 1.249104934487374\n",
      "    mean_raw_obs_processing_ms: 0.10546303802206827\n",
      "  time_since_restore: 442.978138923645\n",
      "  time_this_iter_s: 6.20217752456665\n",
      "  time_total_s: 442.978138923645\n",
      "  timers:\n",
      "    learn_throughput: 1429.125\n",
      "    learn_time_ms: 2798.915\n",
      "    load_throughput: 5740510.504\n",
      "    load_time_ms: 0.697\n",
      "    sample_throughput: 635.469\n",
      "    sample_time_ms: 6294.566\n",
      "    update_time_ms: 3.068\n",
      "  timestamp: 1636041691\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 284000\n",
      "  training_iteration: 71\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:01:37 (running for 00:07:43.64)<br>Memory usage on this node: 12.5/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    71</td><td style=\"text-align: right;\">         442.978</td><td style=\"text-align: right;\">284000</td><td style=\"text-align: right;\">-899.931</td><td style=\"text-align: right;\">            -511.174</td><td style=\"text-align: right;\">            -1759.96</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 288000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-01-38\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -511.17424958008235\n",
      "  episode_reward_mean: -872.3067840224489\n",
      "  episode_reward_min: -1759.9603810451522\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1440\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.0946109294891357\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010948547162115574\n",
      "          model: {}\n",
      "          policy_loss: 0.002672588685527444\n",
      "          total_loss: 6155.6279296875\n",
      "          vf_explained_var: 0.4669550657272339\n",
      "          vf_loss: 6155.619140625\n",
      "    num_agent_steps_sampled: 288000\n",
      "    num_agent_steps_trained: 288000\n",
      "    num_steps_sampled: 288000\n",
      "    num_steps_trained: 288000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 72\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.9375\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1226920821074319\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.18137972999601346\n",
      "    mean_inference_ms: 1.2489986303769145\n",
      "    mean_raw_obs_processing_ms: 0.10545730502988664\n",
      "  time_since_restore: 449.25003576278687\n",
      "  time_this_iter_s: 6.271896839141846\n",
      "  time_total_s: 449.25003576278687\n",
      "  timers:\n",
      "    learn_throughput: 1429.077\n",
      "    learn_time_ms: 2799.01\n",
      "    load_throughput: 5689699.189\n",
      "    load_time_ms: 0.703\n",
      "    sample_throughput: 637.424\n",
      "    sample_time_ms: 6275.255\n",
      "    update_time_ms: 3.099\n",
      "  timestamp: 1636041698\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 288000\n",
      "  training_iteration: 72\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:01:42 (running for 00:07:48.96)<br>Memory usage on this node: 12.5/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    72</td><td style=\"text-align: right;\">          449.25</td><td style=\"text-align: right;\">288000</td><td style=\"text-align: right;\">-872.307</td><td style=\"text-align: right;\">            -511.174</td><td style=\"text-align: right;\">            -1759.96</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 292000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-01-44\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -511.17424958008235\n",
      "  episode_reward_mean: -829.4573949519554\n",
      "  episode_reward_min: -1759.9603810451522\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1460\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.6985909342765808\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010249114595353603\n",
      "          model: {}\n",
      "          policy_loss: 0.021160371601581573\n",
      "          total_loss: 6362.04931640625\n",
      "          vf_explained_var: 0.39190229773521423\n",
      "          vf_loss: 6362.0224609375\n",
      "    num_agent_steps_sampled: 292000\n",
      "    num_agent_steps_trained: 292000\n",
      "    num_steps_sampled: 292000\n",
      "    num_steps_trained: 292000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 73\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.4375\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12268390851191552\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1813610198370624\n",
      "    mean_inference_ms: 1.248875149622287\n",
      "    mean_raw_obs_processing_ms: 0.10544850269918715\n",
      "  time_since_restore: 455.44343280792236\n",
      "  time_this_iter_s: 6.193397045135498\n",
      "  time_total_s: 455.44343280792236\n",
      "  timers:\n",
      "    learn_throughput: 1428.924\n",
      "    learn_time_ms: 2799.31\n",
      "    load_throughput: 5727771.67\n",
      "    load_time_ms: 0.698\n",
      "    sample_throughput: 637.737\n",
      "    sample_time_ms: 6272.18\n",
      "    update_time_ms: 3.085\n",
      "  timestamp: 1636041704\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 292000\n",
      "  training_iteration: 73\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:01:47 (running for 00:07:54.24)<br>Memory usage on this node: 12.5/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    73</td><td style=\"text-align: right;\">         455.443</td><td style=\"text-align: right;\">292000</td><td style=\"text-align: right;\">-829.457</td><td style=\"text-align: right;\">            -511.174</td><td style=\"text-align: right;\">            -1759.96</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 296000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-01-50\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -514.5602863649767\n",
      "  episode_reward_mean: -840.0022749579653\n",
      "  episode_reward_min: -1778.1531565543035\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1480\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.7472994327545166\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007764923851937056\n",
      "          model: {}\n",
      "          policy_loss: -0.0023868621792644262\n",
      "          total_loss: 8695.18359375\n",
      "          vf_explained_var: 0.4458533525466919\n",
      "          vf_loss: 8695.181640625\n",
      "    num_agent_steps_sampled: 296000\n",
      "    num_agent_steps_trained: 296000\n",
      "    num_steps_sampled: 296000\n",
      "    num_steps_trained: 296000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 74\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.128571428571432\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12267545336316983\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.18134358036536952\n",
      "    mean_inference_ms: 1.2487405251036703\n",
      "    mean_raw_obs_processing_ms: 0.10544141718983929\n",
      "  time_since_restore: 461.6383650302887\n",
      "  time_this_iter_s: 6.194932222366333\n",
      "  time_total_s: 461.6383650302887\n",
      "  timers:\n",
      "    learn_throughput: 1429.249\n",
      "    learn_time_ms: 2798.672\n",
      "    load_throughput: 5680837.03\n",
      "    load_time_ms: 0.704\n",
      "    sample_throughput: 637.777\n",
      "    sample_time_ms: 6271.786\n",
      "    update_time_ms: 3.078\n",
      "  timestamp: 1636041710\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 296000\n",
      "  training_iteration: 74\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:01:52 (running for 00:07:59.48)<br>Memory usage on this node: 12.5/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    74</td><td style=\"text-align: right;\">         461.638</td><td style=\"text-align: right;\">296000</td><td style=\"text-align: right;\">-840.002</td><td style=\"text-align: right;\">             -514.56</td><td style=\"text-align: right;\">            -1778.15</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 300000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-01-57\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -514.5602863649767\n",
      "  episode_reward_mean: -847.1567088876161\n",
      "  episode_reward_min: -1778.1531565543035\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1500\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.2880499362945557\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010504855774343014\n",
      "          model: {}\n",
      "          policy_loss: -0.002846027258783579\n",
      "          total_loss: 7502.39892578125\n",
      "          vf_explained_var: 0.3417953848838806\n",
      "          vf_loss: 7502.39697265625\n",
      "    num_agent_steps_sampled: 300000\n",
      "    num_agent_steps_trained: 300000\n",
      "    num_steps_sampled: 300000\n",
      "    num_steps_trained: 300000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 75\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.3\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1226752235200503\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1813402914421921\n",
      "    mean_inference_ms: 1.2486897734256013\n",
      "    mean_raw_obs_processing_ms: 0.10544026782215869\n",
      "  time_since_restore: 467.86936688423157\n",
      "  time_this_iter_s: 6.231001853942871\n",
      "  time_total_s: 467.86936688423157\n",
      "  timers:\n",
      "    learn_throughput: 1429.356\n",
      "    learn_time_ms: 2798.463\n",
      "    load_throughput: 5686035.383\n",
      "    load_time_ms: 0.703\n",
      "    sample_throughput: 637.723\n",
      "    sample_time_ms: 6272.315\n",
      "    update_time_ms: 3.096\n",
      "  timestamp: 1636041717\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 300000\n",
      "  training_iteration: 75\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:01:58 (running for 00:08:04.76)<br>Memory usage on this node: 12.5/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    75</td><td style=\"text-align: right;\">         467.869</td><td style=\"text-align: right;\">300000</td><td style=\"text-align: right;\">-847.157</td><td style=\"text-align: right;\">             -514.56</td><td style=\"text-align: right;\">            -1778.15</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:02:03 (running for 00:08:09.76)<br>Memory usage on this node: 12.5/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    75</td><td style=\"text-align: right;\">         467.869</td><td style=\"text-align: right;\">300000</td><td style=\"text-align: right;\">-847.157</td><td style=\"text-align: right;\">             -514.56</td><td style=\"text-align: right;\">            -1778.15</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 304000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-02-03\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -514.5602863649767\n",
      "  episode_reward_mean: -824.6476546180826\n",
      "  episode_reward_min: -1778.1531565543035\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1520\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.1726266145706177\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008181693032383919\n",
      "          model: {}\n",
      "          policy_loss: -0.001208850764669478\n",
      "          total_loss: 6849.68603515625\n",
      "          vf_explained_var: 0.3668951988220215\n",
      "          vf_loss: 6849.6826171875\n",
      "    num_agent_steps_sampled: 304000\n",
      "    num_agent_steps_trained: 304000\n",
      "    num_steps_sampled: 304000\n",
      "    num_steps_trained: 304000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 76\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.1875\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12267481140132479\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.18133591894778506\n",
      "    mean_inference_ms: 1.2486262996947983\n",
      "    mean_raw_obs_processing_ms: 0.105438549890621\n",
      "  time_since_restore: 474.09013056755066\n",
      "  time_this_iter_s: 6.220763683319092\n",
      "  time_total_s: 474.09013056755066\n",
      "  timers:\n",
      "    learn_throughput: 1428.605\n",
      "    learn_time_ms: 2799.934\n",
      "    load_throughput: 5683146.235\n",
      "    load_time_ms: 0.704\n",
      "    sample_throughput: 637.44\n",
      "    sample_time_ms: 6275.105\n",
      "    update_time_ms: 3.067\n",
      "  timestamp: 1636041723\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 304000\n",
      "  training_iteration: 76\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:02:08 (running for 00:08:15.03)<br>Memory usage on this node: 12.5/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    76</td><td style=\"text-align: right;\">          474.09</td><td style=\"text-align: right;\">304000</td><td style=\"text-align: right;\">-824.648</td><td style=\"text-align: right;\">             -514.56</td><td style=\"text-align: right;\">            -1778.15</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 308000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-02-09\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -509.7296350073396\n",
      "  episode_reward_mean: -804.9466320527765\n",
      "  episode_reward_min: -1778.1531565543035\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1540\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.5671871304512024\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009930122643709183\n",
      "          model: {}\n",
      "          policy_loss: 0.004468770697712898\n",
      "          total_loss: 4691.22509765625\n",
      "          vf_explained_var: 0.47093403339385986\n",
      "          vf_loss: 4691.21630859375\n",
      "    num_agent_steps_sampled: 308000\n",
      "    num_agent_steps_trained: 308000\n",
      "    num_steps_sampled: 308000\n",
      "    num_steps_trained: 308000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 77\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.4125\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12267385394589386\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.18132972091535202\n",
      "    mean_inference_ms: 1.24855982568078\n",
      "    mean_raw_obs_processing_ms: 0.10543342937977929\n",
      "  time_since_restore: 480.3157820701599\n",
      "  time_this_iter_s: 6.225651502609253\n",
      "  time_total_s: 480.3157820701599\n",
      "  timers:\n",
      "    learn_throughput: 1429.961\n",
      "    learn_time_ms: 2797.279\n",
      "    load_throughput: 5832713.114\n",
      "    load_time_ms: 0.686\n",
      "    sample_throughput: 637.945\n",
      "    sample_time_ms: 6270.133\n",
      "    update_time_ms: 3.09\n",
      "  timestamp: 1636041729\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 308000\n",
      "  training_iteration: 77\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:02:13 (running for 00:08:20.30)<br>Memory usage on this node: 12.5/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    77</td><td style=\"text-align: right;\">         480.316</td><td style=\"text-align: right;\">308000</td><td style=\"text-align: right;\">-804.947</td><td style=\"text-align: right;\">             -509.73</td><td style=\"text-align: right;\">            -1778.15</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 312000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-02-15\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -509.7296350073396\n",
      "  episode_reward_mean: -815.0803335174038\n",
      "  episode_reward_min: -1778.1531565543035\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1560\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.4562246799468994\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008214619942009449\n",
      "          model: {}\n",
      "          policy_loss: 0.0005909574101679027\n",
      "          total_loss: 9020.646484375\n",
      "          vf_explained_var: 0.284362256526947\n",
      "          vf_loss: 9020.6416015625\n",
      "    num_agent_steps_sampled: 312000\n",
      "    num_agent_steps_trained: 312000\n",
      "    num_steps_sampled: 312000\n",
      "    num_steps_trained: 312000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 78\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.524999999999999\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.122674362935247\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.18133000880801833\n",
      "    mean_inference_ms: 1.2485160610677237\n",
      "    mean_raw_obs_processing_ms: 0.10543114896739855\n",
      "  time_since_restore: 486.5328438282013\n",
      "  time_this_iter_s: 6.217061758041382\n",
      "  time_total_s: 486.5328438282013\n",
      "  timers:\n",
      "    learn_throughput: 1430.354\n",
      "    learn_time_ms: 2796.511\n",
      "    load_throughput: 5755674.637\n",
      "    load_time_ms: 0.695\n",
      "    sample_throughput: 637.994\n",
      "    sample_time_ms: 6269.65\n",
      "    update_time_ms: 3.069\n",
      "  timestamp: 1636041735\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 312000\n",
      "  training_iteration: 78\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:02:18 (running for 00:08:25.57)<br>Memory usage on this node: 12.5/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    78</td><td style=\"text-align: right;\">         486.533</td><td style=\"text-align: right;\">312000</td><td style=\"text-align: right;\"> -815.08</td><td style=\"text-align: right;\">             -509.73</td><td style=\"text-align: right;\">            -1778.15</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 316000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-02-22\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -509.7296350073396\n",
      "  episode_reward_mean: -804.6367998020297\n",
      "  episode_reward_min: -1742.1508723480922\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1580\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.2692855596542358\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009239615872502327\n",
      "          model: {}\n",
      "          policy_loss: 0.0003260698285885155\n",
      "          total_loss: 5718.05712890625\n",
      "          vf_explained_var: 0.4617784917354584\n",
      "          vf_loss: 5718.05126953125\n",
      "    num_agent_steps_sampled: 316000\n",
      "    num_agent_steps_trained: 316000\n",
      "    num_steps_sampled: 316000\n",
      "    num_steps_trained: 316000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 79\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 17.975\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12267747323150659\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.18133319788881536\n",
      "    mean_inference_ms: 1.2485569391617692\n",
      "    mean_raw_obs_processing_ms: 0.10542910669675556\n",
      "  time_since_restore: 492.8114378452301\n",
      "  time_this_iter_s: 6.278594017028809\n",
      "  time_total_s: 492.8114378452301\n",
      "  timers:\n",
      "    learn_throughput: 1430.189\n",
      "    learn_time_ms: 2796.834\n",
      "    load_throughput: 5759231.06\n",
      "    load_time_ms: 0.695\n",
      "    sample_throughput: 637.713\n",
      "    sample_time_ms: 6272.409\n",
      "    update_time_ms: 3.084\n",
      "  timestamp: 1636041742\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 316000\n",
      "  training_iteration: 79\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:02:24 (running for 00:08:30.90)<br>Memory usage on this node: 12.5/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    79</td><td style=\"text-align: right;\">         492.811</td><td style=\"text-align: right;\">316000</td><td style=\"text-align: right;\">-804.637</td><td style=\"text-align: right;\">             -509.73</td><td style=\"text-align: right;\">            -1742.15</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 320000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-02-28\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -509.7296350073396\n",
      "  episode_reward_mean: -784.6846908113056\n",
      "  episode_reward_min: -1711.1817832950403\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1600\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.826066792011261\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00972211267799139\n",
      "          model: {}\n",
      "          policy_loss: 0.00388964731246233\n",
      "          total_loss: 5816.26220703125\n",
      "          vf_explained_var: 0.3116048276424408\n",
      "          vf_loss: 5816.25390625\n",
      "    num_agent_steps_sampled: 320000\n",
      "    num_agent_steps_trained: 320000\n",
      "    num_steps_sampled: 320000\n",
      "    num_steps_trained: 320000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 80\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.985714285714286\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12267490508393576\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1813287010774208\n",
      "    mean_inference_ms: 1.2485787755919866\n",
      "    mean_raw_obs_processing_ms: 0.10542525862040421\n",
      "  time_since_restore: 499.05648827552795\n",
      "  time_this_iter_s: 6.245050430297852\n",
      "  time_total_s: 499.05648827552795\n",
      "  timers:\n",
      "    learn_throughput: 1430.023\n",
      "    learn_time_ms: 2797.157\n",
      "    load_throughput: 5806872.491\n",
      "    load_time_ms: 0.689\n",
      "    sample_throughput: 636.904\n",
      "    sample_time_ms: 6280.385\n",
      "    update_time_ms: 3.087\n",
      "  timestamp: 1636041748\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 320000\n",
      "  training_iteration: 80\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:02:29 (running for 00:08:36.19)<br>Memory usage on this node: 12.5/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    80</td><td style=\"text-align: right;\">         499.056</td><td style=\"text-align: right;\">320000</td><td style=\"text-align: right;\">-784.685</td><td style=\"text-align: right;\">             -509.73</td><td style=\"text-align: right;\">            -1711.18</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:02:34 (running for 00:08:41.20)<br>Memory usage on this node: 12.5/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    80</td><td style=\"text-align: right;\">         499.056</td><td style=\"text-align: right;\">320000</td><td style=\"text-align: right;\">-784.685</td><td style=\"text-align: right;\">             -509.73</td><td style=\"text-align: right;\">            -1711.18</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 324000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-02-34\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -509.7296350073396\n",
      "  episode_reward_mean: -790.2455279885088\n",
      "  episode_reward_min: -1711.1817832950403\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1620\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3155267238616943\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0076967403292655945\n",
      "          model: {}\n",
      "          policy_loss: 0.0003972361155319959\n",
      "          total_loss: 6736.546875\n",
      "          vf_explained_var: 0.4354008436203003\n",
      "          vf_loss: 6736.5419921875\n",
      "    num_agent_steps_sampled: 324000\n",
      "    num_agent_steps_trained: 324000\n",
      "    num_steps_sampled: 324000\n",
      "    num_steps_trained: 324000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 81\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.337499999999999\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12267303695932988\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.181324849703082\n",
      "    mean_inference_ms: 1.2486114720556754\n",
      "    mean_raw_obs_processing_ms: 0.10542373467583094\n",
      "  time_since_restore: 505.23705792427063\n",
      "  time_this_iter_s: 6.180569648742676\n",
      "  time_total_s: 505.23705792427063\n",
      "  timers:\n",
      "    learn_throughput: 1430.999\n",
      "    learn_time_ms: 2795.249\n",
      "    load_throughput: 5751728.205\n",
      "    load_time_ms: 0.695\n",
      "    sample_throughput: 636.922\n",
      "    sample_time_ms: 6280.206\n",
      "    update_time_ms: 2.992\n",
      "  timestamp: 1636041754\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 324000\n",
      "  training_iteration: 81\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:02:39 (running for 00:08:46.43)<br>Memory usage on this node: 12.5/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    81</td><td style=\"text-align: right;\">         505.237</td><td style=\"text-align: right;\">324000</td><td style=\"text-align: right;\">-790.246</td><td style=\"text-align: right;\">             -509.73</td><td style=\"text-align: right;\">            -1711.18</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 328000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-02-41\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -510.11226393922055\n",
      "  episode_reward_mean: -807.025097938317\n",
      "  episode_reward_min: -1711.1817832950403\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1640\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3635729551315308\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010832165367901325\n",
      "          model: {}\n",
      "          policy_loss: 0.004228336736559868\n",
      "          total_loss: 6067.93212890625\n",
      "          vf_explained_var: 0.5399144291877747\n",
      "          vf_loss: 6067.92236328125\n",
      "    num_agent_steps_sampled: 328000\n",
      "    num_agent_steps_trained: 328000\n",
      "    num_steps_sampled: 328000\n",
      "    num_steps_trained: 328000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 82\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.7625\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12267323396204483\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.18132067215205228\n",
      "    mean_inference_ms: 1.2486811462057132\n",
      "    mean_raw_obs_processing_ms: 0.10542340597062275\n",
      "  time_since_restore: 511.45056223869324\n",
      "  time_this_iter_s: 6.213504314422607\n",
      "  time_total_s: 511.45056223869324\n",
      "  timers:\n",
      "    learn_throughput: 1433.416\n",
      "    learn_time_ms: 2790.536\n",
      "    load_throughput: 5619190.14\n",
      "    load_time_ms: 0.712\n",
      "    sample_throughput: 637.166\n",
      "    sample_time_ms: 6277.799\n",
      "    update_time_ms: 2.947\n",
      "  timestamp: 1636041761\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 328000\n",
      "  training_iteration: 82\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:02:45 (running for 00:08:51.69)<br>Memory usage on this node: 12.5/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    82</td><td style=\"text-align: right;\">         511.451</td><td style=\"text-align: right;\">328000</td><td style=\"text-align: right;\">-807.025</td><td style=\"text-align: right;\">            -510.112</td><td style=\"text-align: right;\">            -1711.18</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 332000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-02-47\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -550.3140660535349\n",
      "  episode_reward_mean: -793.9812001914227\n",
      "  episode_reward_min: -1526.5232272988383\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1660\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.831528902053833\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00991139281541109\n",
      "          model: {}\n",
      "          policy_loss: 0.007817935198545456\n",
      "          total_loss: 4219.73388671875\n",
      "          vf_explained_var: 0.5236289501190186\n",
      "          vf_loss: 4219.72119140625\n",
      "    num_agent_steps_sampled: 332000\n",
      "    num_agent_steps_trained: 332000\n",
      "    num_steps_sampled: 332000\n",
      "    num_steps_trained: 332000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 83\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.425\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12267115972015169\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1813111120520145\n",
      "    mean_inference_ms: 1.2487203894535182\n",
      "    mean_raw_obs_processing_ms: 0.10542045798285303\n",
      "  time_since_restore: 517.6063446998596\n",
      "  time_this_iter_s: 6.155782461166382\n",
      "  time_total_s: 517.6063446998596\n",
      "  timers:\n",
      "    learn_throughput: 1434.036\n",
      "    learn_time_ms: 2789.33\n",
      "    load_throughput: 5614676.885\n",
      "    load_time_ms: 0.712\n",
      "    sample_throughput: 637.902\n",
      "    sample_time_ms: 6270.558\n",
      "    update_time_ms: 2.94\n",
      "  timestamp: 1636041767\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 332000\n",
      "  training_iteration: 83\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:02:50 (running for 00:08:56.96)<br>Memory usage on this node: 12.5/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    83</td><td style=\"text-align: right;\">         517.606</td><td style=\"text-align: right;\">332000</td><td style=\"text-align: right;\">-793.981</td><td style=\"text-align: right;\">            -550.314</td><td style=\"text-align: right;\">            -1526.52</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 336000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-02-53\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -511.6318013896641\n",
      "  episode_reward_mean: -771.67898175256\n",
      "  episode_reward_min: -1526.5232272988383\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1680\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.8293871879577637\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00946069322526455\n",
      "          model: {}\n",
      "          policy_loss: 0.0052456362172961235\n",
      "          total_loss: 4793.3720703125\n",
      "          vf_explained_var: 0.35531750321388245\n",
      "          vf_loss: 4793.3623046875\n",
      "    num_agent_steps_sampled: 336000\n",
      "    num_agent_steps_trained: 336000\n",
      "    num_steps_sampled: 336000\n",
      "    num_steps_trained: 336000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 84\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.871428571428568\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12266518786869447\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1812955544206357\n",
      "    mean_inference_ms: 1.2486754001839047\n",
      "    mean_raw_obs_processing_ms: 0.10541460844288576\n",
      "  time_since_restore: 523.8273386955261\n",
      "  time_this_iter_s: 6.220993995666504\n",
      "  time_total_s: 523.8273386955261\n",
      "  timers:\n",
      "    learn_throughput: 1432.815\n",
      "    learn_time_ms: 2791.708\n",
      "    load_throughput: 5658420.236\n",
      "    load_time_ms: 0.707\n",
      "    sample_throughput: 637.687\n",
      "    sample_time_ms: 6272.671\n",
      "    update_time_ms: 2.916\n",
      "  timestamp: 1636041773\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 336000\n",
      "  training_iteration: 84\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:02:55 (running for 00:09:02.23)<br>Memory usage on this node: 12.5/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    84</td><td style=\"text-align: right;\">         523.827</td><td style=\"text-align: right;\">336000</td><td style=\"text-align: right;\">-771.679</td><td style=\"text-align: right;\">            -511.632</td><td style=\"text-align: right;\">            -1526.52</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 340000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-02-59\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -511.6318013896641\n",
      "  episode_reward_mean: -790.6528097955936\n",
      "  episode_reward_min: -1592.8373010911787\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1700\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.4893735647201538\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007863365113735199\n",
      "          model: {}\n",
      "          policy_loss: -0.0009944693883880973\n",
      "          total_loss: 6302.10009765625\n",
      "          vf_explained_var: 0.5023027062416077\n",
      "          vf_loss: 6302.0966796875\n",
      "    num_agent_steps_sampled: 340000\n",
      "    num_agent_steps_trained: 340000\n",
      "    num_steps_sampled: 340000\n",
      "    num_steps_trained: 340000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 85\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.1375\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12265690588776096\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1812759257196568\n",
      "    mean_inference_ms: 1.2485866891993664\n",
      "    mean_raw_obs_processing_ms: 0.10540706952845155\n",
      "  time_since_restore: 530.0145711898804\n",
      "  time_this_iter_s: 6.187232494354248\n",
      "  time_total_s: 530.0145711898804\n",
      "  timers:\n",
      "    learn_throughput: 1431.704\n",
      "    learn_time_ms: 2793.873\n",
      "    load_throughput: 5608482.985\n",
      "    load_time_ms: 0.713\n",
      "    sample_throughput: 638.076\n",
      "    sample_time_ms: 6268.85\n",
      "    update_time_ms: 2.914\n",
      "  timestamp: 1636041779\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 340000\n",
      "  training_iteration: 85\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:03:00 (running for 00:09:07.47)<br>Memory usage on this node: 12.5/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    85</td><td style=\"text-align: right;\">         530.015</td><td style=\"text-align: right;\">340000</td><td style=\"text-align: right;\">-790.653</td><td style=\"text-align: right;\">            -511.632</td><td style=\"text-align: right;\">            -1592.84</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:03:05 (running for 00:09:12.48)<br>Memory usage on this node: 12.5/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    85</td><td style=\"text-align: right;\">         530.015</td><td style=\"text-align: right;\">340000</td><td style=\"text-align: right;\">-790.653</td><td style=\"text-align: right;\">            -511.632</td><td style=\"text-align: right;\">            -1592.84</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 344000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-03-06\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -511.6318013896641\n",
      "  episode_reward_mean: -795.0160130966304\n",
      "  episode_reward_min: -1667.6595015095645\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1720\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.4724265336990356\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009566238150000572\n",
      "          model: {}\n",
      "          policy_loss: -0.001609042868949473\n",
      "          total_loss: 6809.08935546875\n",
      "          vf_explained_var: 0.471414715051651\n",
      "          vf_loss: 6809.08642578125\n",
      "    num_agent_steps_sampled: 344000\n",
      "    num_agent_steps_trained: 344000\n",
      "    num_steps_sampled: 344000\n",
      "    num_steps_trained: 344000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 86\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.0625\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12264935942751444\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1812603696922168\n",
      "    mean_inference_ms: 1.2485271968341483\n",
      "    mean_raw_obs_processing_ms: 0.10540002184250084\n",
      "  time_since_restore: 536.2211861610413\n",
      "  time_this_iter_s: 6.206614971160889\n",
      "  time_total_s: 536.2211861610413\n",
      "  timers:\n",
      "    learn_throughput: 1432.523\n",
      "    learn_time_ms: 2792.276\n",
      "    load_throughput: 5586818.515\n",
      "    load_time_ms: 0.716\n",
      "    sample_throughput: 637.83\n",
      "    sample_time_ms: 6271.266\n",
      "    update_time_ms: 2.948\n",
      "  timestamp: 1636041786\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 344000\n",
      "  training_iteration: 86\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:03:11 (running for 00:09:17.73)<br>Memory usage on this node: 12.5/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    86</td><td style=\"text-align: right;\">         536.221</td><td style=\"text-align: right;\">344000</td><td style=\"text-align: right;\">-795.016</td><td style=\"text-align: right;\">            -511.632</td><td style=\"text-align: right;\">            -1667.66</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 348000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-03-12\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -511.6318013896641\n",
      "  episode_reward_mean: -803.111292446769\n",
      "  episode_reward_min: -1753.311797225162\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1740\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.448095679283142\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011389295570552349\n",
      "          model: {}\n",
      "          policy_loss: -0.0002356080658501014\n",
      "          total_loss: 6751.46826171875\n",
      "          vf_explained_var: 0.4595611095428467\n",
      "          vf_loss: 6751.46240234375\n",
      "    num_agent_steps_sampled: 348000\n",
      "    num_agent_steps_trained: 348000\n",
      "    num_steps_sampled: 348000\n",
      "    num_steps_trained: 348000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 87\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.037500000000001\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12263930390683003\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.181243304921507\n",
      "    mean_inference_ms: 1.2484231470636251\n",
      "    mean_raw_obs_processing_ms: 0.10539124354068967\n",
      "  time_since_restore: 542.4754450321198\n",
      "  time_this_iter_s: 6.254258871078491\n",
      "  time_total_s: 542.4754450321198\n",
      "  timers:\n",
      "    learn_throughput: 1430.285\n",
      "    learn_time_ms: 2796.645\n",
      "    load_throughput: 5629749.337\n",
      "    load_time_ms: 0.711\n",
      "    sample_throughput: 638.124\n",
      "    sample_time_ms: 6268.375\n",
      "    update_time_ms: 3.019\n",
      "  timestamp: 1636041792\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 348000\n",
      "  training_iteration: 87\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:03:16 (running for 00:09:23.03)<br>Memory usage on this node: 12.5/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    87</td><td style=\"text-align: right;\">         542.475</td><td style=\"text-align: right;\">348000</td><td style=\"text-align: right;\">-803.111</td><td style=\"text-align: right;\">            -511.632</td><td style=\"text-align: right;\">            -1753.31</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 352000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-03-18\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -506.5310343389228\n",
      "  episode_reward_mean: -817.4958540432949\n",
      "  episode_reward_min: -1753.311797225162\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1760\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.4747259616851807\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006730747874826193\n",
      "          model: {}\n",
      "          policy_loss: -0.0009592658025212586\n",
      "          total_loss: 7668.1240234375\n",
      "          vf_explained_var: 0.36798518896102905\n",
      "          vf_loss: 7668.12158203125\n",
      "    num_agent_steps_sampled: 352000\n",
      "    num_agent_steps_trained: 352000\n",
      "    num_steps_sampled: 352000\n",
      "    num_steps_trained: 352000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 88\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.075\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12263457266755648\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1812354559518004\n",
      "    mean_inference_ms: 1.248371908316634\n",
      "    mean_raw_obs_processing_ms: 0.10538718284230984\n",
      "  time_since_restore: 548.6754240989685\n",
      "  time_this_iter_s: 6.199979066848755\n",
      "  time_total_s: 548.6754240989685\n",
      "  timers:\n",
      "    learn_throughput: 1431.711\n",
      "    learn_time_ms: 2793.86\n",
      "    load_throughput: 5722301.579\n",
      "    load_time_ms: 0.699\n",
      "    sample_throughput: 637.543\n",
      "    sample_time_ms: 6274.091\n",
      "    update_time_ms: 3.013\n",
      "  timestamp: 1636041798\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 352000\n",
      "  training_iteration: 88\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:03:21 (running for 00:09:28.28)<br>Memory usage on this node: 12.5/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    88</td><td style=\"text-align: right;\">         548.675</td><td style=\"text-align: right;\">352000</td><td style=\"text-align: right;\">-817.496</td><td style=\"text-align: right;\">            -506.531</td><td style=\"text-align: right;\">            -1753.31</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 356000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-03-24\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -506.5310343389228\n",
      "  episode_reward_mean: -843.2587528475543\n",
      "  episode_reward_min: -1765.005136419288\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1780\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.7331533432006836\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015063433907926083\n",
      "          model: {}\n",
      "          policy_loss: -0.0045975386165082455\n",
      "          total_loss: 8091.6533203125\n",
      "          vf_explained_var: 0.42990052700042725\n",
      "          vf_loss: 8091.64990234375\n",
      "    num_agent_steps_sampled: 356000\n",
      "    num_agent_steps_trained: 356000\n",
      "    num_steps_sampled: 356000\n",
      "    num_steps_trained: 356000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 89\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.128571428571428\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12262962970061944\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.18122525422804153\n",
      "    mean_inference_ms: 1.2483056535331283\n",
      "    mean_raw_obs_processing_ms: 0.10538159059871521\n",
      "  time_since_restore: 554.8600907325745\n",
      "  time_this_iter_s: 6.184666633605957\n",
      "  time_total_s: 554.8600907325745\n",
      "  timers:\n",
      "    learn_throughput: 1431.363\n",
      "    learn_time_ms: 2794.54\n",
      "    load_throughput: 5721716.118\n",
      "    load_time_ms: 0.699\n",
      "    sample_throughput: 638.814\n",
      "    sample_time_ms: 6261.603\n",
      "    update_time_ms: 2.976\n",
      "  timestamp: 1636041804\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 356000\n",
      "  training_iteration: 89\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:03:26 (running for 00:09:33.53)<br>Memory usage on this node: 12.5/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    89</td><td style=\"text-align: right;\">          554.86</td><td style=\"text-align: right;\">356000</td><td style=\"text-align: right;\">-843.259</td><td style=\"text-align: right;\">            -506.531</td><td style=\"text-align: right;\">            -1765.01</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 360000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-03-31\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -506.22549740123065\n",
      "  episode_reward_mean: -829.7470172619969\n",
      "  episode_reward_min: -1801.7488186016608\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1800\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.194505214691162\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007940039038658142\n",
      "          model: {}\n",
      "          policy_loss: 0.003729023737832904\n",
      "          total_loss: 6570.83203125\n",
      "          vf_explained_var: 0.4459124803543091\n",
      "          vf_loss: 6570.82373046875\n",
      "    num_agent_steps_sampled: 360000\n",
      "    num_agent_steps_trained: 360000\n",
      "    num_steps_sampled: 360000\n",
      "    num_steps_trained: 360000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 90\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.15\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12262756437398735\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.181217836960529\n",
      "    mean_inference_ms: 1.2482601843082304\n",
      "    mean_raw_obs_processing_ms: 0.10537579293818154\n",
      "  time_since_restore: 561.0683209896088\n",
      "  time_this_iter_s: 6.208230257034302\n",
      "  time_total_s: 561.0683209896088\n",
      "  timers:\n",
      "    learn_throughput: 1432.055\n",
      "    learn_time_ms: 2793.188\n",
      "    load_throughput: 5691436.325\n",
      "    load_time_ms: 0.703\n",
      "    sample_throughput: 638.909\n",
      "    sample_time_ms: 6260.673\n",
      "    update_time_ms: 2.973\n",
      "  timestamp: 1636041811\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 360000\n",
      "  training_iteration: 90\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:03:32 (running for 00:09:38.78)<br>Memory usage on this node: 12.5/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    90</td><td style=\"text-align: right;\">         561.068</td><td style=\"text-align: right;\">360000</td><td style=\"text-align: right;\">-829.747</td><td style=\"text-align: right;\">            -506.225</td><td style=\"text-align: right;\">            -1801.75</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:03:37 (running for 00:09:43.79)<br>Memory usage on this node: 12.5/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    90</td><td style=\"text-align: right;\">         561.068</td><td style=\"text-align: right;\">360000</td><td style=\"text-align: right;\">-829.747</td><td style=\"text-align: right;\">            -506.225</td><td style=\"text-align: right;\">            -1801.75</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 364000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-03-37\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -506.22549740123065\n",
      "  episode_reward_mean: -829.3334250966058\n",
      "  episode_reward_min: -1801.7488186016608\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1820\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.6732847690582275\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0069768899120390415\n",
      "          model: {}\n",
      "          policy_loss: 0.0187616515904665\n",
      "          total_loss: 9097.9111328125\n",
      "          vf_explained_var: 0.405182421207428\n",
      "          vf_loss: 9097.8876953125\n",
      "    num_agent_steps_sampled: 364000\n",
      "    num_agent_steps_trained: 364000\n",
      "    num_steps_sampled: 364000\n",
      "    num_steps_trained: 364000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 91\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.0125\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12262525594503139\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1812085086465081\n",
      "    mean_inference_ms: 1.2482198672168878\n",
      "    mean_raw_obs_processing_ms: 0.10537070446938868\n",
      "  time_since_restore: 567.2830259799957\n",
      "  time_this_iter_s: 6.214704990386963\n",
      "  time_total_s: 567.2830259799957\n",
      "  timers:\n",
      "    learn_throughput: 1431.809\n",
      "    learn_time_ms: 2793.669\n",
      "    load_throughput: 5703819.95\n",
      "    load_time_ms: 0.701\n",
      "    sample_throughput: 638.769\n",
      "    sample_time_ms: 6262.047\n",
      "    update_time_ms: 2.964\n",
      "  timestamp: 1636041817\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 364000\n",
      "  training_iteration: 91\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:03:42 (running for 00:09:49.06)<br>Memory usage on this node: 12.5/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    91</td><td style=\"text-align: right;\">         567.283</td><td style=\"text-align: right;\">364000</td><td style=\"text-align: right;\">-829.333</td><td style=\"text-align: right;\">            -506.225</td><td style=\"text-align: right;\">            -1801.75</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 368000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-03-43\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -506.22549740123065\n",
      "  episode_reward_mean: -837.6536557558896\n",
      "  episode_reward_min: -1801.7488186016608\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1840\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 2.104400634765625\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008588949218392372\n",
      "          model: {}\n",
      "          policy_loss: -0.001736476900987327\n",
      "          total_loss: 9740.5849609375\n",
      "          vf_explained_var: 0.5137724876403809\n",
      "          vf_loss: 9740.5830078125\n",
      "    num_agent_steps_sampled: 368000\n",
      "    num_agent_steps_trained: 368000\n",
      "    num_steps_sampled: 368000\n",
      "    num_steps_trained: 368000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 92\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.387500000000003\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12262164808056525\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.18119608691856828\n",
      "    mean_inference_ms: 1.248158923844964\n",
      "    mean_raw_obs_processing_ms: 0.1053630695045636\n",
      "  time_since_restore: 573.4612200260162\n",
      "  time_this_iter_s: 6.178194046020508\n",
      "  time_total_s: 573.4612200260162\n",
      "  timers:\n",
      "    learn_throughput: 1430.169\n",
      "    learn_time_ms: 2796.872\n",
      "    load_throughput: 5532835.142\n",
      "    load_time_ms: 0.723\n",
      "    sample_throughput: 639.453\n",
      "    sample_time_ms: 6255.346\n",
      "    update_time_ms: 3.0\n",
      "  timestamp: 1636041823\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 368000\n",
      "  training_iteration: 92\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:03:47 (running for 00:09:54.28)<br>Memory usage on this node: 12.5/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    92</td><td style=\"text-align: right;\">         573.461</td><td style=\"text-align: right;\">368000</td><td style=\"text-align: right;\">-837.654</td><td style=\"text-align: right;\">            -506.225</td><td style=\"text-align: right;\">            -1801.75</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 372000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-03-49\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -504.88599014397306\n",
      "  episode_reward_mean: -801.420790353171\n",
      "  episode_reward_min: -1801.7488186016608\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1860\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.5713827610015869\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009610204957425594\n",
      "          model: {}\n",
      "          policy_loss: 0.0038876417092978954\n",
      "          total_loss: 3608.8828125\n",
      "          vf_explained_var: 0.49087899923324585\n",
      "          vf_loss: 3608.874267578125\n",
      "    num_agent_steps_sampled: 372000\n",
      "    num_agent_steps_trained: 372000\n",
      "    num_steps_sampled: 372000\n",
      "    num_steps_trained: 372000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 93\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.328571428571431\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12261716305857265\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.18118120838059057\n",
      "    mean_inference_ms: 1.2480853970768948\n",
      "    mean_raw_obs_processing_ms: 0.10535481693796975\n",
      "  time_since_restore: 579.6887435913086\n",
      "  time_this_iter_s: 6.227523565292358\n",
      "  time_total_s: 579.6887435913086\n",
      "  timers:\n",
      "    learn_throughput: 1428.605\n",
      "    learn_time_ms: 2799.933\n",
      "    load_throughput: 5528459.485\n",
      "    load_time_ms: 0.724\n",
      "    sample_throughput: 638.655\n",
      "    sample_time_ms: 6263.16\n",
      "    update_time_ms: 2.975\n",
      "  timestamp: 1636041829\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 372000\n",
      "  training_iteration: 93\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:03:52 (running for 00:09:59.57)<br>Memory usage on this node: 12.5/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    93</td><td style=\"text-align: right;\">         579.689</td><td style=\"text-align: right;\">372000</td><td style=\"text-align: right;\">-801.421</td><td style=\"text-align: right;\">            -504.886</td><td style=\"text-align: right;\">            -1801.75</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 376000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-03-56\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -504.88599014397306\n",
      "  episode_reward_mean: -779.2861705273807\n",
      "  episode_reward_min: -1801.7488186016608\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1880\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.0277683734893799\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009839368052780628\n",
      "          model: {}\n",
      "          policy_loss: -0.002394702984020114\n",
      "          total_loss: 4318.35302734375\n",
      "          vf_explained_var: 0.5859199166297913\n",
      "          vf_loss: 4318.3505859375\n",
      "    num_agent_steps_sampled: 376000\n",
      "    num_agent_steps_trained: 376000\n",
      "    num_steps_sampled: 376000\n",
      "    num_steps_trained: 376000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 94\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.962499999999999\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12261719406786416\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.181174307446364\n",
      "    mean_inference_ms: 1.248243189703556\n",
      "    mean_raw_obs_processing_ms: 0.10535197652649952\n",
      "  time_since_restore: 586.0957317352295\n",
      "  time_this_iter_s: 6.406988143920898\n",
      "  time_total_s: 586.0957317352295\n",
      "  timers:\n",
      "    learn_throughput: 1429.447\n",
      "    learn_time_ms: 2798.284\n",
      "    load_throughput: 5485797.992\n",
      "    load_time_ms: 0.729\n",
      "    sample_throughput: 636.855\n",
      "    sample_time_ms: 6280.863\n",
      "    update_time_ms: 2.985\n",
      "  timestamp: 1636041836\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 376000\n",
      "  training_iteration: 94\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:03:58 (running for 00:10:05.02)<br>Memory usage on this node: 12.5/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    94</td><td style=\"text-align: right;\">         586.096</td><td style=\"text-align: right;\">376000</td><td style=\"text-align: right;\">-779.286</td><td style=\"text-align: right;\">            -504.886</td><td style=\"text-align: right;\">            -1801.75</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 380000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-04-02\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -504.2311325211194\n",
      "  episode_reward_mean: -769.5469845960424\n",
      "  episode_reward_min: -1769.3493049000137\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1900\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.9868334531784058\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008829410187900066\n",
      "          model: {}\n",
      "          policy_loss: 0.0025693897623568773\n",
      "          total_loss: 3955.974609375\n",
      "          vf_explained_var: 0.5132604241371155\n",
      "          vf_loss: 3955.967529296875\n",
      "    num_agent_steps_sampled: 380000\n",
      "    num_agent_steps_trained: 380000\n",
      "    num_steps_sampled: 380000\n",
      "    num_steps_trained: 380000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 95\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.0\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12261533873940589\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.18116348169639054\n",
      "    mean_inference_ms: 1.2483753091801275\n",
      "    mean_raw_obs_processing_ms: 0.10534835102724965\n",
      "  time_since_restore: 592.2936475276947\n",
      "  time_this_iter_s: 6.19791579246521\n",
      "  time_total_s: 592.2936475276947\n",
      "  timers:\n",
      "    learn_throughput: 1428.47\n",
      "    learn_time_ms: 2800.198\n",
      "    load_throughput: 5513017.876\n",
      "    load_time_ms: 0.726\n",
      "    sample_throughput: 637.12\n",
      "    sample_time_ms: 6278.253\n",
      "    update_time_ms: 2.979\n",
      "  timestamp: 1636041842\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 380000\n",
      "  training_iteration: 95\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:04:03 (running for 00:10:10.28)<br>Memory usage on this node: 12.5/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    95</td><td style=\"text-align: right;\">         592.294</td><td style=\"text-align: right;\">380000</td><td style=\"text-align: right;\">-769.547</td><td style=\"text-align: right;\">            -504.231</td><td style=\"text-align: right;\">            -1769.35</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:04:08 (running for 00:10:15.28)<br>Memory usage on this node: 12.5/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    95</td><td style=\"text-align: right;\">         592.294</td><td style=\"text-align: right;\">380000</td><td style=\"text-align: right;\">-769.547</td><td style=\"text-align: right;\">            -504.231</td><td style=\"text-align: right;\">            -1769.35</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 384000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-04-08\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -391.1020134396607\n",
      "  episode_reward_mean: -754.2407643370844\n",
      "  episode_reward_min: -1741.384643903601\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1920\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.2666311264038086\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010798967443406582\n",
      "          model: {}\n",
      "          policy_loss: 0.005701511166989803\n",
      "          total_loss: 4892.68603515625\n",
      "          vf_explained_var: 0.46091577410697937\n",
      "          vf_loss: 4892.67529296875\n",
      "    num_agent_steps_sampled: 384000\n",
      "    num_agent_steps_trained: 384000\n",
      "    num_steps_sampled: 384000\n",
      "    num_steps_trained: 384000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 96\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.2875\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1226141881805931\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1811542296457381\n",
      "    mean_inference_ms: 1.2484972651904516\n",
      "    mean_raw_obs_processing_ms: 0.1053423741455472\n",
      "  time_since_restore: 598.4957683086395\n",
      "  time_this_iter_s: 6.202120780944824\n",
      "  time_total_s: 598.4957683086395\n",
      "  timers:\n",
      "    learn_throughput: 1428.538\n",
      "    learn_time_ms: 2800.065\n",
      "    load_throughput: 5362531.484\n",
      "    load_time_ms: 0.746\n",
      "    sample_throughput: 636.916\n",
      "    sample_time_ms: 6280.266\n",
      "    update_time_ms: 2.971\n",
      "  timestamp: 1636041848\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 384000\n",
      "  training_iteration: 96\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:04:13 (running for 00:10:20.53)<br>Memory usage on this node: 12.5/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    96</td><td style=\"text-align: right;\">         598.496</td><td style=\"text-align: right;\">384000</td><td style=\"text-align: right;\">-754.241</td><td style=\"text-align: right;\">            -391.102</td><td style=\"text-align: right;\">            -1741.38</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 388000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-04-15\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -391.1020134396607\n",
      "  episode_reward_mean: -736.4344573699443\n",
      "  episode_reward_min: -1613.706706876631\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1940\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.5650243759155273\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009072133339941502\n",
      "          model: {}\n",
      "          policy_loss: 0.007351845968514681\n",
      "          total_loss: 5954.5\n",
      "          vf_explained_var: 0.43141964077949524\n",
      "          vf_loss: 5954.48876953125\n",
      "    num_agent_steps_sampled: 388000\n",
      "    num_agent_steps_trained: 388000\n",
      "    num_steps_sampled: 388000\n",
      "    num_steps_trained: 388000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 97\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 15.0625\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1226154777644134\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.18114842585506585\n",
      "    mean_inference_ms: 1.2486297097514585\n",
      "    mean_raw_obs_processing_ms: 0.10533947172489895\n",
      "  time_since_restore: 604.7054445743561\n",
      "  time_this_iter_s: 6.209676265716553\n",
      "  time_total_s: 604.7054445743561\n",
      "  timers:\n",
      "    learn_throughput: 1429.481\n",
      "    learn_time_ms: 2798.217\n",
      "    load_throughput: 5264100.907\n",
      "    load_time_ms: 0.76\n",
      "    sample_throughput: 637.172\n",
      "    sample_time_ms: 6277.741\n",
      "    update_time_ms: 2.872\n",
      "  timestamp: 1636041855\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 388000\n",
      "  training_iteration: 97\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:04:19 (running for 00:10:25.79)<br>Memory usage on this node: 12.5/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    97</td><td style=\"text-align: right;\">         604.705</td><td style=\"text-align: right;\">388000</td><td style=\"text-align: right;\">-736.434</td><td style=\"text-align: right;\">            -391.102</td><td style=\"text-align: right;\">            -1613.71</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 392000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-04-21\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -391.1020134396607\n",
      "  episode_reward_mean: -739.2644698063018\n",
      "  episode_reward_min: -1613.706706876631\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1960\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.6809171438217163\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010675206780433655\n",
      "          model: {}\n",
      "          policy_loss: -0.006679085083305836\n",
      "          total_loss: 4190.23779296875\n",
      "          vf_explained_var: 0.5392611026763916\n",
      "          vf_loss: 4190.2392578125\n",
      "    num_agent_steps_sampled: 392000\n",
      "    num_agent_steps_trained: 392000\n",
      "    num_steps_sampled: 392000\n",
      "    num_steps_trained: 392000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 98\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.8\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12261768318480275\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1811446767882571\n",
      "    mean_inference_ms: 1.2487709017482311\n",
      "    mean_raw_obs_processing_ms: 0.10533794030853669\n",
      "  time_since_restore: 610.9847364425659\n",
      "  time_this_iter_s: 6.279291868209839\n",
      "  time_total_s: 610.9847364425659\n",
      "  timers:\n",
      "    learn_throughput: 1427.245\n",
      "    learn_time_ms: 2802.602\n",
      "    load_throughput: 5271544.021\n",
      "    load_time_ms: 0.759\n",
      "    sample_throughput: 637.048\n",
      "    sample_time_ms: 6278.963\n",
      "    update_time_ms: 2.917\n",
      "  timestamp: 1636041861\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 392000\n",
      "  training_iteration: 98\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:04:24 (running for 00:10:31.12)<br>Memory usage on this node: 12.5/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    98</td><td style=\"text-align: right;\">         610.985</td><td style=\"text-align: right;\">392000</td><td style=\"text-align: right;\">-739.264</td><td style=\"text-align: right;\">            -391.102</td><td style=\"text-align: right;\">            -1613.71</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 396000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-04-27\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -391.1020134396607\n",
      "  episode_reward_mean: -731.7678044283567\n",
      "  episode_reward_min: -1717.2226829221063\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1980\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.0045138597488403\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009899970144033432\n",
      "          model: {}\n",
      "          policy_loss: -0.0014285518554970622\n",
      "          total_loss: 5040.14501953125\n",
      "          vf_explained_var: 0.5379491448402405\n",
      "          vf_loss: 5040.14208984375\n",
      "    num_agent_steps_sampled: 396000\n",
      "    num_agent_steps_trained: 396000\n",
      "    num_steps_sampled: 396000\n",
      "    num_steps_trained: 396000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 99\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.8875\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12261783978315828\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1811374992565461\n",
      "    mean_inference_ms: 1.2487080353556521\n",
      "    mean_raw_obs_processing_ms: 0.10533417858367476\n",
      "  time_since_restore: 617.1773071289062\n",
      "  time_this_iter_s: 6.192570686340332\n",
      "  time_total_s: 617.1773071289062\n",
      "  timers:\n",
      "    learn_throughput: 1429.069\n",
      "    learn_time_ms: 2799.024\n",
      "    load_throughput: 5224756.625\n",
      "    load_time_ms: 0.766\n",
      "    sample_throughput: 636.185\n",
      "    sample_time_ms: 6287.477\n",
      "    update_time_ms: 2.94\n",
      "  timestamp: 1636041867\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 396000\n",
      "  training_iteration: 99\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:04:29 (running for 00:10:36.40)<br>Memory usage on this node: 12.5/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         617.177</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\">-731.768</td><td style=\"text-align: right;\">            -391.102</td><td style=\"text-align: right;\">            -1717.22</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 400000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-04-33\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -391.1020134396607\n",
      "  episode_reward_mean: -748.4252801463147\n",
      "  episode_reward_min: -1717.2226829221063\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2000\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.4619208574295044\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010941741056740284\n",
      "          model: {}\n",
      "          policy_loss: -0.0008458955562673509\n",
      "          total_loss: 6884.24267578125\n",
      "          vf_explained_var: 0.3764880895614624\n",
      "          vf_loss: 6884.23779296875\n",
      "    num_agent_steps_sampled: 400000\n",
      "    num_agent_steps_trained: 400000\n",
      "    num_steps_sampled: 400000\n",
      "    num_steps_trained: 400000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 100\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.942857142857141\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12261685650948415\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.18112970168124376\n",
      "    mean_inference_ms: 1.248635976345645\n",
      "    mean_raw_obs_processing_ms: 0.10533058113294314\n",
      "  time_since_restore: 623.3315300941467\n",
      "  time_this_iter_s: 6.1542229652404785\n",
      "  time_total_s: 623.3315300941467\n",
      "  timers:\n",
      "    learn_throughput: 1428.857\n",
      "    learn_time_ms: 2799.44\n",
      "    load_throughput: 5289493.663\n",
      "    load_time_ms: 0.756\n",
      "    sample_throughput: 636.786\n",
      "    sample_time_ms: 6281.544\n",
      "    update_time_ms: 2.934\n",
      "  timestamp: 1636041873\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 400000\n",
      "  training_iteration: 100\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:04:34 (running for 00:10:41.60)<br>Memory usage on this node: 12.5/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">         623.332</td><td style=\"text-align: right;\">400000</td><td style=\"text-align: right;\">-748.425</td><td style=\"text-align: right;\">            -391.102</td><td style=\"text-align: right;\">            -1717.22</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:04:39 (running for 00:10:46.61)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">         623.332</td><td style=\"text-align: right;\">400000</td><td style=\"text-align: right;\">-748.425</td><td style=\"text-align: right;\">            -391.102</td><td style=\"text-align: right;\">            -1717.22</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 404000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-04-40\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -501.77918362914596\n",
      "  episode_reward_mean: -747.5153224365579\n",
      "  episode_reward_min: -1717.2226829221063\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2020\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.9733697772026062\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010965025052428246\n",
      "          model: {}\n",
      "          policy_loss: 0.0008780033094808459\n",
      "          total_loss: 3744.6064453125\n",
      "          vf_explained_var: 0.6254299879074097\n",
      "          vf_loss: 3744.600341796875\n",
      "    num_agent_steps_sampled: 404000\n",
      "    num_agent_steps_trained: 404000\n",
      "    num_steps_sampled: 404000\n",
      "    num_steps_trained: 404000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 101\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.425\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.5250000000000004\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12261363185605946\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.18111653984286097\n",
      "    mean_inference_ms: 1.2485363162883263\n",
      "    mean_raw_obs_processing_ms: 0.10532693032831056\n",
      "  time_since_restore: 629.4411356449127\n",
      "  time_this_iter_s: 6.109605550765991\n",
      "  time_total_s: 629.4411356449127\n",
      "  timers:\n",
      "    learn_throughput: 1431.758\n",
      "    learn_time_ms: 2793.769\n",
      "    load_throughput: 5299016.456\n",
      "    load_time_ms: 0.755\n",
      "    sample_throughput: 637.245\n",
      "    sample_time_ms: 6277.022\n",
      "    update_time_ms: 2.952\n",
      "  timestamp: 1636041880\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 404000\n",
      "  training_iteration: 101\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:04:45 (running for 00:10:51.77)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   101</td><td style=\"text-align: right;\">         629.441</td><td style=\"text-align: right;\">404000</td><td style=\"text-align: right;\">-747.515</td><td style=\"text-align: right;\">            -501.779</td><td style=\"text-align: right;\">            -1717.22</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 408000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-04-46\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -501.41599781200364\n",
      "  episode_reward_mean: -720.7127659094608\n",
      "  episode_reward_min: -1717.2226829221063\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2040\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.3763572871685028\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01043003611266613\n",
      "          model: {}\n",
      "          policy_loss: 0.004013002850115299\n",
      "          total_loss: 2903.76611328125\n",
      "          vf_explained_var: 0.5596248507499695\n",
      "          vf_loss: 2903.7568359375\n",
      "    num_agent_steps_sampled: 408000\n",
      "    num_agent_steps_trained: 408000\n",
      "    num_steps_sampled: 408000\n",
      "    num_steps_trained: 408000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 102\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.5125\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12260314458033479\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.18109398297177876\n",
      "    mean_inference_ms: 1.2483732535076013\n",
      "    mean_raw_obs_processing_ms: 0.10531620357731875\n",
      "  time_since_restore: 635.4433209896088\n",
      "  time_this_iter_s: 6.002185344696045\n",
      "  time_total_s: 635.4433209896088\n",
      "  timers:\n",
      "    learn_throughput: 1436.552\n",
      "    learn_time_ms: 2784.444\n",
      "    load_throughput: 5636937.137\n",
      "    load_time_ms: 0.71\n",
      "    sample_throughput: 638.668\n",
      "    sample_time_ms: 6263.033\n",
      "    update_time_ms: 2.916\n",
      "  timestamp: 1636041886\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 408000\n",
      "  training_iteration: 102\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:04:50 (running for 00:10:56.81)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   102</td><td style=\"text-align: right;\">         635.443</td><td style=\"text-align: right;\">408000</td><td style=\"text-align: right;\">-720.713</td><td style=\"text-align: right;\">            -501.416</td><td style=\"text-align: right;\">            -1717.22</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 412000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-04-52\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -478.2060748203183\n",
      "  episode_reward_mean: -727.1949322126798\n",
      "  episode_reward_min: -1717.2226829221063\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2060\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.9049235582351685\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010359513573348522\n",
      "          model: {}\n",
      "          policy_loss: 0.0034456553403288126\n",
      "          total_loss: 3391.721435546875\n",
      "          vf_explained_var: 0.5553258061408997\n",
      "          vf_loss: 3391.712890625\n",
      "    num_agent_steps_sampled: 412000\n",
      "    num_agent_steps_trained: 412000\n",
      "    num_steps_sampled: 412000\n",
      "    num_steps_trained: 412000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 103\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.342857142857143\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12258531228481814\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.18105670488624995\n",
      "    mean_inference_ms: 1.248100859512339\n",
      "    mean_raw_obs_processing_ms: 0.10529521251307311\n",
      "  time_since_restore: 641.3859283924103\n",
      "  time_this_iter_s: 5.942607402801514\n",
      "  time_total_s: 641.3859283924103\n",
      "  timers:\n",
      "    learn_throughput: 1443.385\n",
      "    learn_time_ms: 2771.264\n",
      "    load_throughput: 5648133.585\n",
      "    load_time_ms: 0.708\n",
      "    sample_throughput: 641.265\n",
      "    sample_time_ms: 6237.671\n",
      "    update_time_ms: 2.934\n",
      "  timestamp: 1636041892\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 412000\n",
      "  training_iteration: 103\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:04:56 (running for 00:11:02.80)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   103</td><td style=\"text-align: right;\">         641.386</td><td style=\"text-align: right;\">412000</td><td style=\"text-align: right;\">-727.195</td><td style=\"text-align: right;\">            -478.206</td><td style=\"text-align: right;\">            -1717.22</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 416000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-04-58\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -478.2060748203183\n",
      "  episode_reward_mean: -734.1269897070578\n",
      "  episode_reward_min: -1704.9283893651157\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2080\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.9377251267433167\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01041299756616354\n",
      "          model: {}\n",
      "          policy_loss: 0.009459300898015499\n",
      "          total_loss: 3806.020263671875\n",
      "          vf_explained_var: 0.5539267659187317\n",
      "          vf_loss: 3806.00537109375\n",
      "    num_agent_steps_sampled: 416000\n",
      "    num_agent_steps_trained: 416000\n",
      "    num_steps_sampled: 416000\n",
      "    num_steps_trained: 416000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 104\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.45\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12256159248176153\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1810078429262647\n",
      "    mean_inference_ms: 1.2477497274333487\n",
      "    mean_raw_obs_processing_ms: 0.10526739224135356\n",
      "  time_since_restore: 647.3823838233948\n",
      "  time_this_iter_s: 5.996455430984497\n",
      "  time_total_s: 647.3823838233948\n",
      "  timers:\n",
      "    learn_throughput: 1447.834\n",
      "    learn_time_ms: 2762.748\n",
      "    load_throughput: 5643763.582\n",
      "    load_time_ms: 0.709\n",
      "    sample_throughput: 646.117\n",
      "    sample_time_ms: 6190.829\n",
      "    update_time_ms: 2.926\n",
      "  timestamp: 1636041898\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 416000\n",
      "  training_iteration: 104\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:05:01 (running for 00:11:07.84)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   104</td><td style=\"text-align: right;\">         647.382</td><td style=\"text-align: right;\">416000</td><td style=\"text-align: right;\">-734.127</td><td style=\"text-align: right;\">            -478.206</td><td style=\"text-align: right;\">            -1704.93</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 420000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-05-04\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -393.50246017976133\n",
      "  episode_reward_mean: -751.9611632859263\n",
      "  episode_reward_min: -1751.6493118776373\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2100\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 2.307032346725464\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007855302654206753\n",
      "          model: {}\n",
      "          policy_loss: -0.005136922467499971\n",
      "          total_loss: 9435.826171875\n",
      "          vf_explained_var: 0.42195236682891846\n",
      "          vf_loss: 9435.828125\n",
      "    num_agent_steps_sampled: 420000\n",
      "    num_agent_steps_trained: 420000\n",
      "    num_steps_sampled: 420000\n",
      "    num_steps_trained: 420000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 105\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.728571428571428\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12253300646596989\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1809491595844517\n",
      "    mean_inference_ms: 1.2473326868379246\n",
      "    mean_raw_obs_processing_ms: 0.10523266936613393\n",
      "  time_since_restore: 653.3428840637207\n",
      "  time_this_iter_s: 5.960500240325928\n",
      "  time_total_s: 653.3428840637207\n",
      "  timers:\n",
      "    learn_throughput: 1453.435\n",
      "    learn_time_ms: 2752.1\n",
      "    load_throughput: 5641865.689\n",
      "    load_time_ms: 0.709\n",
      "    sample_throughput: 648.427\n",
      "    sample_time_ms: 6168.775\n",
      "    update_time_ms: 2.928\n",
      "  timestamp: 1636041904\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 420000\n",
      "  training_iteration: 105\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:05:06 (running for 00:11:12.85)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   105</td><td style=\"text-align: right;\">         653.343</td><td style=\"text-align: right;\">420000</td><td style=\"text-align: right;\">-751.961</td><td style=\"text-align: right;\">            -393.502</td><td style=\"text-align: right;\">            -1751.65</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 424000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-05-10\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -393.50246017976133\n",
      "  episode_reward_mean: -781.0845436898401\n",
      "  episode_reward_min: -1751.6493118776373\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2120\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.8261141777038574\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00997534953057766\n",
      "          model: {}\n",
      "          policy_loss: -0.0069779991172254086\n",
      "          total_loss: 6998.03955078125\n",
      "          vf_explained_var: 0.5455754995346069\n",
      "          vf_loss: 6998.0419921875\n",
      "    num_agent_steps_sampled: 424000\n",
      "    num_agent_steps_trained: 424000\n",
      "    num_steps_sampled: 424000\n",
      "    num_steps_trained: 424000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 106\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.45\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12249797141749974\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.18087985661530276\n",
      "    mean_inference_ms: 1.2468423936046202\n",
      "    mean_raw_obs_processing_ms: 0.10518962294680385\n",
      "  time_since_restore: 659.3362383842468\n",
      "  time_this_iter_s: 5.993354320526123\n",
      "  time_total_s: 659.3362383842468\n",
      "  timers:\n",
      "    learn_throughput: 1456.701\n",
      "    learn_time_ms: 2745.931\n",
      "    load_throughput: 5782056.796\n",
      "    load_time_ms: 0.692\n",
      "    sample_throughput: 651.166\n",
      "    sample_time_ms: 6142.828\n",
      "    update_time_ms: 2.913\n",
      "  timestamp: 1636041910\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 424000\n",
      "  training_iteration: 106\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:05:11 (running for 00:11:17.89)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   106</td><td style=\"text-align: right;\">         659.336</td><td style=\"text-align: right;\">424000</td><td style=\"text-align: right;\">-781.085</td><td style=\"text-align: right;\">            -393.502</td><td style=\"text-align: right;\">            -1751.65</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 428000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-05-16\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -393.50246017976133\n",
      "  episode_reward_mean: -791.345660940425\n",
      "  episode_reward_min: -1751.6493118776373\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2140\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.151781439781189\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008832938969135284\n",
      "          model: {}\n",
      "          policy_loss: 0.004536816384643316\n",
      "          total_loss: 5900.6337890625\n",
      "          vf_explained_var: 0.42076143622398376\n",
      "          vf_loss: 5900.62451171875\n",
      "    num_agent_steps_sampled: 428000\n",
      "    num_agent_steps_trained: 428000\n",
      "    num_steps_sampled: 428000\n",
      "    num_steps_trained: 428000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 107\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.514285714285714\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12246059225207272\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.18080594340509593\n",
      "    mean_inference_ms: 1.2463087419551295\n",
      "    mean_raw_obs_processing_ms: 0.10514473534602335\n",
      "  time_since_restore: 665.2708275318146\n",
      "  time_this_iter_s: 5.934589147567749\n",
      "  time_total_s: 665.2708275318146\n",
      "  timers:\n",
      "    learn_throughput: 1462.275\n",
      "    learn_time_ms: 2735.463\n",
      "    load_throughput: 5894601.925\n",
      "    load_time_ms: 0.679\n",
      "    sample_throughput: 653.659\n",
      "    sample_time_ms: 6119.395\n",
      "    update_time_ms: 2.928\n",
      "  timestamp: 1636041916\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 428000\n",
      "  training_iteration: 107\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:05:17 (running for 00:11:23.87)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   107</td><td style=\"text-align: right;\">         665.271</td><td style=\"text-align: right;\">428000</td><td style=\"text-align: right;\">-791.346</td><td style=\"text-align: right;\">            -393.502</td><td style=\"text-align: right;\">            -1751.65</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:05:22 (running for 00:11:28.88)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   107</td><td style=\"text-align: right;\">         665.271</td><td style=\"text-align: right;\">428000</td><td style=\"text-align: right;\">-791.346</td><td style=\"text-align: right;\">            -393.502</td><td style=\"text-align: right;\">            -1751.65</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 432000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-05-22\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -393.50246017976133\n",
      "  episode_reward_mean: -767.4824672353488\n",
      "  episode_reward_min: -1751.6493118776373\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2160\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.36278995871543884\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01083517074584961\n",
      "          model: {}\n",
      "          policy_loss: 0.003749692812561989\n",
      "          total_loss: 2552.856689453125\n",
      "          vf_explained_var: 0.564272403717041\n",
      "          vf_loss: 2552.847412109375\n",
      "    num_agent_steps_sampled: 432000\n",
      "    num_agent_steps_trained: 432000\n",
      "    num_steps_sampled: 432000\n",
      "    num_steps_trained: 432000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 108\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.337499999999999\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12242578251455737\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.18073934208105238\n",
      "    mean_inference_ms: 1.245837790204726\n",
      "    mean_raw_obs_processing_ms: 0.10510675312716071\n",
      "  time_since_restore: 671.3434643745422\n",
      "  time_this_iter_s: 6.072636842727661\n",
      "  time_total_s: 671.3434643745422\n",
      "  timers:\n",
      "    learn_throughput: 1468.568\n",
      "    learn_time_ms: 2723.742\n",
      "    load_throughput: 5911008.702\n",
      "    load_time_ms: 0.677\n",
      "    sample_throughput: 655.731\n",
      "    sample_time_ms: 6100.058\n",
      "    update_time_ms: 2.893\n",
      "  timestamp: 1636041922\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 432000\n",
      "  training_iteration: 108\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:05:27 (running for 00:11:34.00)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   108</td><td style=\"text-align: right;\">         671.343</td><td style=\"text-align: right;\">432000</td><td style=\"text-align: right;\">-767.482</td><td style=\"text-align: right;\">            -393.502</td><td style=\"text-align: right;\">            -1751.65</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 436000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-05-28\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -393.50246017976133\n",
      "  episode_reward_mean: -760.9831850694818\n",
      "  episode_reward_min: -1751.6493118776373\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2180\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.686985969543457\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010021818801760674\n",
      "          model: {}\n",
      "          policy_loss: 0.004545029252767563\n",
      "          total_loss: 3415.595703125\n",
      "          vf_explained_var: 0.5631862282752991\n",
      "          vf_loss: 3415.5859375\n",
      "    num_agent_steps_sampled: 436000\n",
      "    num_agent_steps_trained: 436000\n",
      "    num_steps_sampled: 436000\n",
      "    num_steps_trained: 436000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 109\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.271428571428572\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12238992612550545\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.180673615734914\n",
      "    mean_inference_ms: 1.245355454351792\n",
      "    mean_raw_obs_processing_ms: 0.10506850923591156\n",
      "  time_since_restore: 677.3098599910736\n",
      "  time_this_iter_s: 5.966395616531372\n",
      "  time_total_s: 677.3098599910736\n",
      "  timers:\n",
      "    learn_throughput: 1472.938\n",
      "    learn_time_ms: 2715.661\n",
      "    load_throughput: 5949367.376\n",
      "    load_time_ms: 0.672\n",
      "    sample_throughput: 658.591\n",
      "    sample_time_ms: 6073.573\n",
      "    update_time_ms: 2.872\n",
      "  timestamp: 1636041928\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 436000\n",
      "  training_iteration: 109\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:05:32 (running for 00:11:39.01)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   109</td><td style=\"text-align: right;\">          677.31</td><td style=\"text-align: right;\">436000</td><td style=\"text-align: right;\">-760.983</td><td style=\"text-align: right;\">            -393.502</td><td style=\"text-align: right;\">            -1751.65</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 440000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-05-34\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -464.40491717306224\n",
      "  episode_reward_mean: -723.8042783196386\n",
      "  episode_reward_min: -1803.248265217384\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2200\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.9756073355674744\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007893235422670841\n",
      "          model: {}\n",
      "          policy_loss: 0.0012560682371258736\n",
      "          total_loss: 5033.17626953125\n",
      "          vf_explained_var: 0.5202643871307373\n",
      "          vf_loss: 5033.1708984375\n",
      "    num_agent_steps_sampled: 440000\n",
      "    num_agent_steps_trained: 440000\n",
      "    num_steps_sampled: 440000\n",
      "    num_steps_trained: 440000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 110\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.5\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12235520320957885\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1806104178662584\n",
      "    mean_inference_ms: 1.244873071866309\n",
      "    mean_raw_obs_processing_ms: 0.10503146825544003\n",
      "  time_since_restore: 683.2587113380432\n",
      "  time_this_iter_s: 5.9488513469696045\n",
      "  time_total_s: 683.2587113380432\n",
      "  timers:\n",
      "    learn_throughput: 1477.724\n",
      "    learn_time_ms: 2706.865\n",
      "    load_throughput: 5978411.431\n",
      "    load_time_ms: 0.669\n",
      "    sample_throughput: 661.258\n",
      "    sample_time_ms: 6049.077\n",
      "    update_time_ms: 2.888\n",
      "  timestamp: 1636041934\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 440000\n",
      "  training_iteration: 110\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:05:38 (running for 00:11:45.01)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   110</td><td style=\"text-align: right;\">         683.259</td><td style=\"text-align: right;\">440000</td><td style=\"text-align: right;\">-723.804</td><td style=\"text-align: right;\">            -464.405</td><td style=\"text-align: right;\">            -1803.25</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 444000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-05-40\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -464.40491717306224\n",
      "  episode_reward_mean: -690.5323107805514\n",
      "  episode_reward_min: -1803.248265217384\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2220\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.0462595224380493\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011072606779634953\n",
      "          model: {}\n",
      "          policy_loss: -7.753564568702132e-05\n",
      "          total_loss: 3808.76123046875\n",
      "          vf_explained_var: 0.624644935131073\n",
      "          vf_loss: 3808.755859375\n",
      "    num_agent_steps_sampled: 444000\n",
      "    num_agent_steps_trained: 444000\n",
      "    num_steps_sampled: 444000\n",
      "    num_steps_trained: 444000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 111\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.471428571428572\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12232091708513369\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.18054840902767147\n",
      "    mean_inference_ms: 1.2443929118896708\n",
      "    mean_raw_obs_processing_ms: 0.10499568937663507\n",
      "  time_since_restore: 689.2045593261719\n",
      "  time_this_iter_s: 5.945847988128662\n",
      "  time_total_s: 689.2045593261719\n",
      "  timers:\n",
      "    learn_throughput: 1479.491\n",
      "    learn_time_ms: 2703.632\n",
      "    load_throughput: 5966293.03\n",
      "    load_time_ms: 0.67\n",
      "    sample_throughput: 663.695\n",
      "    sample_time_ms: 6026.866\n",
      "    update_time_ms: 2.888\n",
      "  timestamp: 1636041940\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 444000\n",
      "  training_iteration: 111\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:05:44 (running for 00:11:50.99)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   111</td><td style=\"text-align: right;\">         689.205</td><td style=\"text-align: right;\">444000</td><td style=\"text-align: right;\">-690.532</td><td style=\"text-align: right;\">            -464.405</td><td style=\"text-align: right;\">            -1803.25</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 448000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-05-46\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -464.40491717306224\n",
      "  episode_reward_mean: -684.5427412697215\n",
      "  episode_reward_min: -1803.248265217384\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2240\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.966335117816925\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01014886237680912\n",
      "          model: {}\n",
      "          policy_loss: 0.004990410525351763\n",
      "          total_loss: 4113.76123046875\n",
      "          vf_explained_var: 0.518007218837738\n",
      "          vf_loss: 4113.75146484375\n",
      "    num_agent_steps_sampled: 448000\n",
      "    num_agent_steps_trained: 448000\n",
      "    num_steps_sampled: 448000\n",
      "    num_steps_trained: 448000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 112\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.375\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12228921416353716\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.18049029192452995\n",
      "    mean_inference_ms: 1.2439428418052823\n",
      "    mean_raw_obs_processing_ms: 0.10496180904937341\n",
      "  time_since_restore: 695.1782238483429\n",
      "  time_this_iter_s: 5.9736645221710205\n",
      "  time_total_s: 695.1782238483429\n",
      "  timers:\n",
      "    learn_throughput: 1480.08\n",
      "    learn_time_ms: 2702.557\n",
      "    load_throughput: 5973728.325\n",
      "    load_time_ms: 0.67\n",
      "    sample_throughput: 664.187\n",
      "    sample_time_ms: 6022.399\n",
      "    update_time_ms: 2.887\n",
      "  timestamp: 1636041946\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 448000\n",
      "  training_iteration: 112\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:05:49 (running for 00:11:56.03)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   112</td><td style=\"text-align: right;\">         695.178</td><td style=\"text-align: right;\">448000</td><td style=\"text-align: right;\">-684.543</td><td style=\"text-align: right;\">            -464.405</td><td style=\"text-align: right;\">            -1803.25</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 452000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-05-52\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -467.84084128406414\n",
      "  episode_reward_mean: -733.6694657309398\n",
      "  episode_reward_min: -1803.248265217384\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2260\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.6181434392929077\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009683506563305855\n",
      "          model: {}\n",
      "          policy_loss: -0.005737952422350645\n",
      "          total_loss: 4910.583984375\n",
      "          vf_explained_var: 0.6714955568313599\n",
      "          vf_loss: 4910.58447265625\n",
      "    num_agent_steps_sampled: 452000\n",
      "    num_agent_steps_trained: 452000\n",
      "    num_steps_sampled: 452000\n",
      "    num_steps_trained: 452000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 113\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.385714285714288\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12225244924664129\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.18042362827856415\n",
      "    mean_inference_ms: 1.2434274871078848\n",
      "    mean_raw_obs_processing_ms: 0.10492122394353377\n",
      "  time_since_restore: 701.1152603626251\n",
      "  time_this_iter_s: 5.937036514282227\n",
      "  time_total_s: 701.1152603626251\n",
      "  timers:\n",
      "    learn_throughput: 1478.756\n",
      "    learn_time_ms: 2704.977\n",
      "    load_throughput: 5965232.356\n",
      "    load_time_ms: 0.671\n",
      "    sample_throughput: 664.65\n",
      "    sample_time_ms: 6018.203\n",
      "    update_time_ms: 2.872\n",
      "  timestamp: 1636041952\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 452000\n",
      "  training_iteration: 113\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:05:55 (running for 00:12:02.01)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   113</td><td style=\"text-align: right;\">         701.115</td><td style=\"text-align: right;\">452000</td><td style=\"text-align: right;\">-733.669</td><td style=\"text-align: right;\">            -467.841</td><td style=\"text-align: right;\">            -1803.25</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 456000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-05-58\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -389.1920269123624\n",
      "  episode_reward_mean: -743.2531943261088\n",
      "  episode_reward_min: -1803.248265217384\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2280\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.213296890258789\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010978435166180134\n",
      "          model: {}\n",
      "          policy_loss: 0.00805653352290392\n",
      "          total_loss: 3804.133544921875\n",
      "          vf_explained_var: 0.6423341631889343\n",
      "          vf_loss: 3804.119873046875\n",
      "    num_agent_steps_sampled: 456000\n",
      "    num_agent_steps_trained: 456000\n",
      "    num_steps_sampled: 456000\n",
      "    num_steps_trained: 456000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 114\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.287500000000001\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12221694541429667\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.18035917446042263\n",
      "    mean_inference_ms: 1.2429244579347476\n",
      "    mean_raw_obs_processing_ms: 0.10488167840770447\n",
      "  time_since_restore: 707.0847775936127\n",
      "  time_this_iter_s: 5.969517230987549\n",
      "  time_total_s: 707.0847775936127\n",
      "  timers:\n",
      "    learn_throughput: 1479.372\n",
      "    learn_time_ms: 2703.851\n",
      "    load_throughput: 5951055.619\n",
      "    load_time_ms: 0.672\n",
      "    sample_throughput: 664.544\n",
      "    sample_time_ms: 6019.165\n",
      "    update_time_ms: 2.876\n",
      "  timestamp: 1636041958\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 456000\n",
      "  training_iteration: 114\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:06:00 (running for 00:12:07.02)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   114</td><td style=\"text-align: right;\">         707.085</td><td style=\"text-align: right;\">456000</td><td style=\"text-align: right;\">-743.253</td><td style=\"text-align: right;\">            -389.192</td><td style=\"text-align: right;\">            -1803.25</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 460000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-06-04\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -384.88064159109405\n",
      "  episode_reward_mean: -737.1289207916118\n",
      "  episode_reward_min: -1619.4685625492186\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2300\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.7465237379074097\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013165557757019997\n",
      "          model: {}\n",
      "          policy_loss: -0.0026061320677399635\n",
      "          total_loss: 4457.46435546875\n",
      "          vf_explained_var: 0.46115800738334656\n",
      "          vf_loss: 4457.4609375\n",
      "    num_agent_steps_sampled: 460000\n",
      "    num_agent_steps_trained: 460000\n",
      "    num_steps_sampled: 460000\n",
      "    num_steps_trained: 460000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 115\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.4\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12218395285960948\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.18030019795209817\n",
      "    mean_inference_ms: 1.242463923800421\n",
      "    mean_raw_obs_processing_ms: 0.10484573126893426\n",
      "  time_since_restore: 713.0908334255219\n",
      "  time_this_iter_s: 6.00605583190918\n",
      "  time_total_s: 713.0908334255219\n",
      "  timers:\n",
      "    learn_throughput: 1479.462\n",
      "    learn_time_ms: 2703.686\n",
      "    load_throughput: 5930440.438\n",
      "    load_time_ms: 0.674\n",
      "    sample_throughput: 664.141\n",
      "    sample_time_ms: 6022.821\n",
      "    update_time_ms: 2.891\n",
      "  timestamp: 1636041964\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 460000\n",
      "  training_iteration: 115\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:06:05 (running for 00:12:12.10)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   115</td><td style=\"text-align: right;\">         713.091</td><td style=\"text-align: right;\">460000</td><td style=\"text-align: right;\">-737.129</td><td style=\"text-align: right;\">            -384.881</td><td style=\"text-align: right;\">            -1619.47</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 464000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-06-10\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -384.88064159109405\n",
      "  episode_reward_mean: -744.9766686733481\n",
      "  episode_reward_min: -1791.3081163458999\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2320\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.698477029800415\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008861462585628033\n",
      "          model: {}\n",
      "          policy_loss: -0.00298434984870255\n",
      "          total_loss: 6146.1396484375\n",
      "          vf_explained_var: 0.47566378116607666\n",
      "          vf_loss: 6146.13818359375\n",
      "    num_agent_steps_sampled: 464000\n",
      "    num_agent_steps_trained: 464000\n",
      "    num_steps_sampled: 464000\n",
      "    num_steps_trained: 464000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 116\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.328571428571427\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12215349501049257\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1802435542055788\n",
      "    mean_inference_ms: 1.242014278637716\n",
      "    mean_raw_obs_processing_ms: 0.10481139188736581\n",
      "  time_since_restore: 719.0352923870087\n",
      "  time_this_iter_s: 5.944458961486816\n",
      "  time_total_s: 719.0352923870087\n",
      "  timers:\n",
      "    learn_throughput: 1481.283\n",
      "    learn_time_ms: 2700.361\n",
      "    load_throughput: 5964596.132\n",
      "    load_time_ms: 0.671\n",
      "    sample_throughput: 664.099\n",
      "    sample_time_ms: 6023.198\n",
      "    update_time_ms: 2.896\n",
      "  timestamp: 1636041970\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 464000\n",
      "  training_iteration: 116\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:06:11 (running for 00:12:18.09)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   116</td><td style=\"text-align: right;\">         719.035</td><td style=\"text-align: right;\">464000</td><td style=\"text-align: right;\">-744.977</td><td style=\"text-align: right;\">            -384.881</td><td style=\"text-align: right;\">            -1791.31</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 468000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-06-16\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -384.88064159109405\n",
      "  episode_reward_mean: -775.0284374701118\n",
      "  episode_reward_min: -1791.3081163458999\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2340\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.985092282295227\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00941034872084856\n",
      "          model: {}\n",
      "          policy_loss: -0.0023569620680063963\n",
      "          total_loss: 7973.8046875\n",
      "          vf_explained_var: 0.5529855489730835\n",
      "          vf_loss: 7973.80224609375\n",
      "    num_agent_steps_sampled: 468000\n",
      "    num_agent_steps_trained: 468000\n",
      "    num_steps_sampled: 468000\n",
      "    num_steps_trained: 468000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 117\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.2875\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1221227672919282\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.18018663063293303\n",
      "    mean_inference_ms: 1.2415611152537054\n",
      "    mean_raw_obs_processing_ms: 0.10477667219564035\n",
      "  time_since_restore: 724.9819014072418\n",
      "  time_this_iter_s: 5.946609020233154\n",
      "  time_total_s: 724.9819014072418\n",
      "  timers:\n",
      "    learn_throughput: 1481.578\n",
      "    learn_time_ms: 2699.825\n",
      "    load_throughput: 5911841.855\n",
      "    load_time_ms: 0.677\n",
      "    sample_throughput: 664.269\n",
      "    sample_time_ms: 6021.654\n",
      "    update_time_ms: 2.884\n",
      "  timestamp: 1636041976\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 468000\n",
      "  training_iteration: 117\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:06:17 (running for 00:12:24.09)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   117</td><td style=\"text-align: right;\">         724.982</td><td style=\"text-align: right;\">468000</td><td style=\"text-align: right;\">-775.028</td><td style=\"text-align: right;\">            -384.881</td><td style=\"text-align: right;\">            -1791.31</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 472000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-06-22\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -384.88064159109405\n",
      "  episode_reward_mean: -758.607129071707\n",
      "  episode_reward_min: -1791.3081163458999\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2360\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.4375845193862915\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008668249472975731\n",
      "          model: {}\n",
      "          policy_loss: -0.003959743306040764\n",
      "          total_loss: 4101.4091796875\n",
      "          vf_explained_var: 0.5495014786720276\n",
      "          vf_loss: 4101.40869140625\n",
      "    num_agent_steps_sampled: 472000\n",
      "    num_agent_steps_trained: 472000\n",
      "    num_steps_sampled: 472000\n",
      "    num_steps_trained: 472000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 118\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.328571428571427\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1220921837190014\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.18012969568854\n",
      "    mean_inference_ms: 1.241100759332668\n",
      "    mean_raw_obs_processing_ms: 0.10474194259037305\n",
      "  time_since_restore: 730.8904747962952\n",
      "  time_this_iter_s: 5.908573389053345\n",
      "  time_total_s: 730.8904747962952\n",
      "  timers:\n",
      "    learn_throughput: 1482.107\n",
      "    learn_time_ms: 2698.86\n",
      "    load_throughput: 5832713.114\n",
      "    load_time_ms: 0.686\n",
      "    sample_throughput: 666.061\n",
      "    sample_time_ms: 6005.454\n",
      "    update_time_ms: 2.88\n",
      "  timestamp: 1636041982\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 472000\n",
      "  training_iteration: 118\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:06:23 (running for 00:12:30.04)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   118</td><td style=\"text-align: right;\">          730.89</td><td style=\"text-align: right;\">472000</td><td style=\"text-align: right;\">-758.607</td><td style=\"text-align: right;\">            -384.881</td><td style=\"text-align: right;\">            -1791.31</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 476000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-06-28\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -384.88064159109405\n",
      "  episode_reward_mean: -739.2172317683134\n",
      "  episode_reward_min: -1791.3081163458999\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2380\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.663058876991272\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009011420421302319\n",
      "          model: {}\n",
      "          policy_loss: 0.0074122194200754166\n",
      "          total_loss: 2878.186767578125\n",
      "          vf_explained_var: 0.5933454036712646\n",
      "          vf_loss: 2878.1748046875\n",
      "    num_agent_steps_sampled: 476000\n",
      "    num_agent_steps_trained: 476000\n",
      "    num_steps_sampled: 476000\n",
      "    num_steps_trained: 476000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 119\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.487499999999999\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12206003724423355\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.18006991182775742\n",
      "    mean_inference_ms: 1.2406174565548584\n",
      "    mean_raw_obs_processing_ms: 0.10470545493025372\n",
      "  time_since_restore: 736.8116335868835\n",
      "  time_this_iter_s: 5.921158790588379\n",
      "  time_total_s: 736.8116335868835\n",
      "  timers:\n",
      "    learn_throughput: 1481.516\n",
      "    learn_time_ms: 2699.937\n",
      "    load_throughput: 5862674.634\n",
      "    load_time_ms: 0.682\n",
      "    sample_throughput: 666.794\n",
      "    sample_time_ms: 5998.857\n",
      "    update_time_ms: 2.894\n",
      "  timestamp: 1636041988\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 476000\n",
      "  training_iteration: 119\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:06:28 (running for 00:12:35.05)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   119</td><td style=\"text-align: right;\">         736.812</td><td style=\"text-align: right;\">476000</td><td style=\"text-align: right;\">-739.217</td><td style=\"text-align: right;\">            -384.881</td><td style=\"text-align: right;\">            -1791.31</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:06:33 (running for 00:12:40.06)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   119</td><td style=\"text-align: right;\">         736.812</td><td style=\"text-align: right;\">476000</td><td style=\"text-align: right;\">-739.217</td><td style=\"text-align: right;\">            -384.881</td><td style=\"text-align: right;\">            -1791.31</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 480000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-06-34\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -414.9678959797924\n",
      "  episode_reward_mean: -723.2608394330989\n",
      "  episode_reward_min: -1802.8980459024099\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2400\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.7628545165061951\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00791582278907299\n",
      "          model: {}\n",
      "          policy_loss: -0.00015074848488438874\n",
      "          total_loss: 5139.21435546875\n",
      "          vf_explained_var: 0.4191201627254486\n",
      "          vf_loss: 5139.2109375\n",
      "    num_agent_steps_sampled: 480000\n",
      "    num_agent_steps_trained: 480000\n",
      "    num_steps_sampled: 480000\n",
      "    num_steps_trained: 480000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 120\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.299999999999999\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12202494067032842\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.18000571418102979\n",
      "    mean_inference_ms: 1.2401102913215662\n",
      "    mean_raw_obs_processing_ms: 0.10466622077249432\n",
      "  time_since_restore: 742.7577908039093\n",
      "  time_this_iter_s: 5.946157217025757\n",
      "  time_total_s: 742.7577908039093\n",
      "  timers:\n",
      "    learn_throughput: 1481.768\n",
      "    learn_time_ms: 2699.478\n",
      "    load_throughput: 5857148.443\n",
      "    load_time_ms: 0.683\n",
      "    sample_throughput: 666.133\n",
      "    sample_time_ms: 6004.809\n",
      "    update_time_ms: 2.874\n",
      "  timestamp: 1636041994\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 480000\n",
      "  training_iteration: 120\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:06:39 (running for 00:12:46.05)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   120</td><td style=\"text-align: right;\">         742.758</td><td style=\"text-align: right;\">480000</td><td style=\"text-align: right;\">-723.261</td><td style=\"text-align: right;\">            -414.968</td><td style=\"text-align: right;\">             -1802.9</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 484000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-06-40\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -414.18069780317387\n",
      "  episode_reward_mean: -710.193730751027\n",
      "  episode_reward_min: -1802.8980459024099\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2420\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.0376017093658447\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011764628812670708\n",
      "          model: {}\n",
      "          policy_loss: -0.0027828009333461523\n",
      "          total_loss: 4923.54296875\n",
      "          vf_explained_var: 0.5668929815292358\n",
      "          vf_loss: 4923.53955078125\n",
      "    num_agent_steps_sampled: 484000\n",
      "    num_agent_steps_trained: 484000\n",
      "    num_steps_sampled: 484000\n",
      "    num_steps_trained: 484000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 121\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.3625\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12198827892898904\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17994056318428212\n",
      "    mean_inference_ms: 1.2395998710217162\n",
      "    mean_raw_obs_processing_ms: 0.10462593740002964\n",
      "  time_since_restore: 748.7443585395813\n",
      "  time_this_iter_s: 5.986567735671997\n",
      "  time_total_s: 748.7443585395813\n",
      "  timers:\n",
      "    learn_throughput: 1479.008\n",
      "    learn_time_ms: 2704.516\n",
      "    load_throughput: 5907054.433\n",
      "    load_time_ms: 0.677\n",
      "    sample_throughput: 666.274\n",
      "    sample_time_ms: 6003.538\n",
      "    update_time_ms: 2.852\n",
      "  timestamp: 1636042000\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 484000\n",
      "  training_iteration: 121\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:06:44 (running for 00:12:51.08)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   121</td><td style=\"text-align: right;\">         748.744</td><td style=\"text-align: right;\">484000</td><td style=\"text-align: right;\">-710.194</td><td style=\"text-align: right;\">            -414.181</td><td style=\"text-align: right;\">             -1802.9</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 488000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-06-46\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -391.1804353980358\n",
      "  episode_reward_mean: -681.5732087894197\n",
      "  episode_reward_min: -1802.8980459024099\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2440\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.193259358406067\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01126620452851057\n",
      "          model: {}\n",
      "          policy_loss: 0.0003523976483847946\n",
      "          total_loss: 5184.99267578125\n",
      "          vf_explained_var: 0.6000141501426697\n",
      "          vf_loss: 5184.98583984375\n",
      "    num_agent_steps_sampled: 488000\n",
      "    num_agent_steps_trained: 488000\n",
      "    num_steps_sampled: 488000\n",
      "    num_steps_trained: 488000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 122\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.042857142857143\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12195307914878951\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17987737948395288\n",
      "    mean_inference_ms: 1.2391016406526658\n",
      "    mean_raw_obs_processing_ms: 0.10458688671463152\n",
      "  time_since_restore: 754.6912462711334\n",
      "  time_this_iter_s: 5.946887731552124\n",
      "  time_total_s: 754.6912462711334\n",
      "  timers:\n",
      "    learn_throughput: 1479.5\n",
      "    learn_time_ms: 2703.617\n",
      "    load_throughput: 5873964.008\n",
      "    load_time_ms: 0.681\n",
      "    sample_throughput: 666.019\n",
      "    sample_time_ms: 6005.832\n",
      "    update_time_ms: 2.858\n",
      "  timestamp: 1636042006\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 488000\n",
      "  training_iteration: 122\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:06:50 (running for 00:12:57.07)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   122</td><td style=\"text-align: right;\">         754.691</td><td style=\"text-align: right;\">488000</td><td style=\"text-align: right;\">-681.573</td><td style=\"text-align: right;\">             -391.18</td><td style=\"text-align: right;\">             -1802.9</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 492000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-06-52\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -391.1804353980358\n",
      "  episode_reward_mean: -686.6903236668569\n",
      "  episode_reward_min: -1802.8980459024099\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2460\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.5259581804275513\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011069659143686295\n",
      "          model: {}\n",
      "          policy_loss: -0.0059317718259990215\n",
      "          total_loss: 4513.66357421875\n",
      "          vf_explained_var: 0.5819502472877502\n",
      "          vf_loss: 4513.6640625\n",
      "    num_agent_steps_sampled: 492000\n",
      "    num_agent_steps_trained: 492000\n",
      "    num_steps_sampled: 492000\n",
      "    num_steps_trained: 492000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 123\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.225\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12192100637299447\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17981898868923638\n",
      "    mean_inference_ms: 1.2386328274519318\n",
      "    mean_raw_obs_processing_ms: 0.10455039265472266\n",
      "  time_since_restore: 760.6547555923462\n",
      "  time_this_iter_s: 5.9635093212127686\n",
      "  time_total_s: 760.6547555923462\n",
      "  timers:\n",
      "    learn_throughput: 1480.113\n",
      "    learn_time_ms: 2702.497\n",
      "    load_throughput: 5850204.338\n",
      "    load_time_ms: 0.684\n",
      "    sample_throughput: 665.716\n",
      "    sample_time_ms: 6008.57\n",
      "    update_time_ms: 2.868\n",
      "  timestamp: 1636042012\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 492000\n",
      "  training_iteration: 123\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:06:55 (running for 00:13:02.08)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   123</td><td style=\"text-align: right;\">         760.655</td><td style=\"text-align: right;\">492000</td><td style=\"text-align: right;\"> -686.69</td><td style=\"text-align: right;\">             -391.18</td><td style=\"text-align: right;\">             -1802.9</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 496000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-06-58\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -379.40663469963266\n",
      "  episode_reward_mean: -688.6451010397869\n",
      "  episode_reward_min: -1802.8980459024099\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2480\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.7828954458236694\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01796557754278183\n",
      "          model: {}\n",
      "          policy_loss: -0.004492566455155611\n",
      "          total_loss: 2910.290283203125\n",
      "          vf_explained_var: 0.6100260615348816\n",
      "          vf_loss: 2910.285400390625\n",
      "    num_agent_steps_sampled: 496000\n",
      "    num_agent_steps_trained: 496000\n",
      "    num_steps_sampled: 496000\n",
      "    num_steps_trained: 496000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 124\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.271428571428572\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12188870448623515\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17976169911293227\n",
      "    mean_inference_ms: 1.2381734957154311\n",
      "    mean_raw_obs_processing_ms: 0.10451423542101738\n",
      "  time_since_restore: 766.5788028240204\n",
      "  time_this_iter_s: 5.924047231674194\n",
      "  time_total_s: 766.5788028240204\n",
      "  timers:\n",
      "    learn_throughput: 1479.638\n",
      "    learn_time_ms: 2703.364\n",
      "    load_throughput: 5914551.223\n",
      "    load_time_ms: 0.676\n",
      "    sample_throughput: 666.446\n",
      "    sample_time_ms: 6001.986\n",
      "    update_time_ms: 2.881\n",
      "  timestamp: 1636042018\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 496000\n",
      "  training_iteration: 124\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:07:01 (running for 00:13:08.05)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   124</td><td style=\"text-align: right;\">         766.579</td><td style=\"text-align: right;\">496000</td><td style=\"text-align: right;\">-688.645</td><td style=\"text-align: right;\">            -379.407</td><td style=\"text-align: right;\">             -1802.9</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 500000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-07-04\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -379.40663469963266\n",
      "  episode_reward_mean: -699.9107055224605\n",
      "  episode_reward_min: -1766.7231659872093\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2500\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.5173574686050415\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014102076180279255\n",
      "          model: {}\n",
      "          policy_loss: 0.0008595466497354209\n",
      "          total_loss: 2423.757568359375\n",
      "          vf_explained_var: 0.6779319047927856\n",
      "          vf_loss: 2423.749755859375\n",
      "    num_agent_steps_sampled: 500000\n",
      "    num_agent_steps_trained: 500000\n",
      "    num_steps_sampled: 500000\n",
      "    num_steps_trained: 500000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 125\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.6125\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12185812803776447\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17970581956049678\n",
      "    mean_inference_ms: 1.2377309377517596\n",
      "    mean_raw_obs_processing_ms: 0.10447937651249645\n",
      "  time_since_restore: 772.5767183303833\n",
      "  time_this_iter_s: 5.997915506362915\n",
      "  time_total_s: 772.5767183303833\n",
      "  timers:\n",
      "    learn_throughput: 1479.655\n",
      "    learn_time_ms: 2703.333\n",
      "    load_throughput: 5951266.716\n",
      "    load_time_ms: 0.672\n",
      "    sample_throughput: 666.429\n",
      "    sample_time_ms: 6002.143\n",
      "    update_time_ms: 2.852\n",
      "  timestamp: 1636042024\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 500000\n",
      "  training_iteration: 125\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:07:06 (running for 00:13:13.10)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   125</td><td style=\"text-align: right;\">         772.577</td><td style=\"text-align: right;\">500000</td><td style=\"text-align: right;\">-699.911</td><td style=\"text-align: right;\">            -379.407</td><td style=\"text-align: right;\">            -1766.72</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 504000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-07-10\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -379.40663469963266\n",
      "  episode_reward_mean: -676.4051952349222\n",
      "  episode_reward_min: -1766.7231659872093\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2520\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.390875905752182\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010470597073435783\n",
      "          model: {}\n",
      "          policy_loss: 0.0036799036897718906\n",
      "          total_loss: 3553.338623046875\n",
      "          vf_explained_var: 0.4738277196884155\n",
      "          vf_loss: 3553.32958984375\n",
      "    num_agent_steps_sampled: 504000\n",
      "    num_agent_steps_trained: 504000\n",
      "    num_steps_sampled: 504000\n",
      "    num_steps_trained: 504000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 126\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.214285714285714\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1218289055980864\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1796515813378064\n",
      "    mean_inference_ms: 1.2372949272717033\n",
      "    mean_raw_obs_processing_ms: 0.10444536202345649\n",
      "  time_since_restore: 778.5194108486176\n",
      "  time_this_iter_s: 5.942692518234253\n",
      "  time_total_s: 778.5194108486176\n",
      "  timers:\n",
      "    learn_throughput: 1479.071\n",
      "    learn_time_ms: 2704.4\n",
      "    load_throughput: 5989937.52\n",
      "    load_time_ms: 0.668\n",
      "    sample_throughput: 666.79\n",
      "    sample_time_ms: 5998.89\n",
      "    update_time_ms: 2.863\n",
      "  timestamp: 1636042030\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 504000\n",
      "  training_iteration: 126\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:07:12 (running for 00:13:19.09)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   126</td><td style=\"text-align: right;\">         778.519</td><td style=\"text-align: right;\">504000</td><td style=\"text-align: right;\">-676.405</td><td style=\"text-align: right;\">            -379.407</td><td style=\"text-align: right;\">            -1766.72</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 508000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-07-16\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -379.40663469963266\n",
      "  episode_reward_mean: -670.0340114696037\n",
      "  episode_reward_min: -1684.118470379057\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2540\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.6267231702804565\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016161954030394554\n",
      "          model: {}\n",
      "          policy_loss: 0.002247511874884367\n",
      "          total_loss: 3507.571044921875\n",
      "          vf_explained_var: 0.4772081673145294\n",
      "          vf_loss: 3507.560791015625\n",
      "    num_agent_steps_sampled: 508000\n",
      "    num_agent_steps_trained: 508000\n",
      "    num_steps_sampled: 508000\n",
      "    num_steps_trained: 508000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 127\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.325\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12179869951439315\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17959559609769948\n",
      "    mean_inference_ms: 1.236857045469223\n",
      "    mean_raw_obs_processing_ms: 0.10441078862321472\n",
      "  time_since_restore: 784.4658091068268\n",
      "  time_this_iter_s: 5.9463982582092285\n",
      "  time_total_s: 784.4658091068268\n",
      "  timers:\n",
      "    learn_throughput: 1478.716\n",
      "    learn_time_ms: 2705.049\n",
      "    load_throughput: 6041706.939\n",
      "    load_time_ms: 0.662\n",
      "    sample_throughput: 666.751\n",
      "    sample_time_ms: 5999.243\n",
      "    update_time_ms: 2.862\n",
      "  timestamp: 1636042036\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 508000\n",
      "  training_iteration: 127\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:07:18 (running for 00:13:25.08)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   127</td><td style=\"text-align: right;\">         784.466</td><td style=\"text-align: right;\">508000</td><td style=\"text-align: right;\">-670.034</td><td style=\"text-align: right;\">            -379.407</td><td style=\"text-align: right;\">            -1684.12</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 512000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-07-22\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -379.40663469963266\n",
      "  episode_reward_mean: -679.7959405921164\n",
      "  episode_reward_min: -1719.9423844078499\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2560\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 2.164454936981201\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008425779640674591\n",
      "          model: {}\n",
      "          policy_loss: 0.00994475930929184\n",
      "          total_loss: 7154.30224609375\n",
      "          vf_explained_var: 0.5172536373138428\n",
      "          vf_loss: 7154.2880859375\n",
      "    num_agent_steps_sampled: 512000\n",
      "    num_agent_steps_trained: 512000\n",
      "    num_steps_sampled: 512000\n",
      "    num_steps_trained: 512000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 128\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.328571428571427\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12176950717715947\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17954254574176376\n",
      "    mean_inference_ms: 1.236449204696368\n",
      "    mean_raw_obs_processing_ms: 0.10437773862136612\n",
      "  time_since_restore: 790.4679088592529\n",
      "  time_this_iter_s: 6.0020997524261475\n",
      "  time_total_s: 790.4679088592529\n",
      "  timers:\n",
      "    learn_throughput: 1478.355\n",
      "    learn_time_ms: 2705.711\n",
      "    load_throughput: 6113031.882\n",
      "    load_time_ms: 0.654\n",
      "    sample_throughput: 665.684\n",
      "    sample_time_ms: 6008.854\n",
      "    update_time_ms: 2.854\n",
      "  timestamp: 1636042042\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 512000\n",
      "  training_iteration: 128\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:07:23 (running for 00:13:30.13)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   128</td><td style=\"text-align: right;\">         790.468</td><td style=\"text-align: right;\">512000</td><td style=\"text-align: right;\">-679.796</td><td style=\"text-align: right;\">            -379.407</td><td style=\"text-align: right;\">            -1719.94</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 516000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-07-28\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -380.67618681934385\n",
      "  episode_reward_mean: -680.9888391957564\n",
      "  episode_reward_min: -1719.9423844078499\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2580\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.8955234289169312\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011800622567534447\n",
      "          model: {}\n",
      "          policy_loss: -0.004998873453587294\n",
      "          total_loss: 3644.279296875\n",
      "          vf_explained_var: 0.4881087839603424\n",
      "          vf_loss: 3644.278564453125\n",
      "    num_agent_steps_sampled: 516000\n",
      "    num_agent_steps_trained: 516000\n",
      "    num_steps_sampled: 516000\n",
      "    num_steps_trained: 516000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 129\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.4\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12174105941220302\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17948985762433542\n",
      "    mean_inference_ms: 1.2360544041532118\n",
      "    mean_raw_obs_processing_ms: 0.1043456901080466\n",
      "  time_since_restore: 796.408839225769\n",
      "  time_this_iter_s: 5.940930366516113\n",
      "  time_total_s: 796.408839225769\n",
      "  timers:\n",
      "    learn_throughput: 1478.677\n",
      "    learn_time_ms: 2705.122\n",
      "    load_throughput: 6014416.921\n",
      "    load_time_ms: 0.665\n",
      "    sample_throughput: 665.303\n",
      "    sample_time_ms: 6012.303\n",
      "    update_time_ms: 2.842\n",
      "  timestamp: 1636042048\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 516000\n",
      "  training_iteration: 129\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:07:29 (running for 00:13:36.12)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   129</td><td style=\"text-align: right;\">         796.409</td><td style=\"text-align: right;\">516000</td><td style=\"text-align: right;\">-680.989</td><td style=\"text-align: right;\">            -380.676</td><td style=\"text-align: right;\">            -1719.94</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 520000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-07-34\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -380.67618681934385\n",
      "  episode_reward_mean: -720.200669464452\n",
      "  episode_reward_min: -1780.7637023768025\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2600\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.7119098901748657\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010170985944569111\n",
      "          model: {}\n",
      "          policy_loss: 0.021168062463402748\n",
      "          total_loss: 7951.83349609375\n",
      "          vf_explained_var: 0.3965548574924469\n",
      "          vf_loss: 7951.8076171875\n",
      "    num_agent_steps_sampled: 520000\n",
      "    num_agent_steps_trained: 520000\n",
      "    num_steps_sampled: 520000\n",
      "    num_steps_trained: 520000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 130\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.585714285714285\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1217106473237358\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17943454350378304\n",
      "    mean_inference_ms: 1.2356372013143788\n",
      "    mean_raw_obs_processing_ms: 0.1043117092891974\n",
      "  time_since_restore: 802.3303711414337\n",
      "  time_this_iter_s: 5.921531915664673\n",
      "  time_total_s: 802.3303711414337\n",
      "  timers:\n",
      "    learn_throughput: 1477.941\n",
      "    learn_time_ms: 2706.468\n",
      "    load_throughput: 6013985.733\n",
      "    load_time_ms: 0.665\n",
      "    sample_throughput: 666.331\n",
      "    sample_time_ms: 6003.023\n",
      "    update_time_ms: 2.846\n",
      "  timestamp: 1636042054\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 520000\n",
      "  training_iteration: 130\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:07:35 (running for 00:13:42.09)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   130</td><td style=\"text-align: right;\">          802.33</td><td style=\"text-align: right;\">520000</td><td style=\"text-align: right;\">-720.201</td><td style=\"text-align: right;\">            -380.676</td><td style=\"text-align: right;\">            -1780.76</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 524000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-07-40\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -380.67618681934385\n",
      "  episode_reward_mean: -750.8145067008742\n",
      "  episode_reward_min: -1780.7637023768025\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2620\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.1677836179733276\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012095967307686806\n",
      "          model: {}\n",
      "          policy_loss: -0.004729662090539932\n",
      "          total_loss: 6050.48828125\n",
      "          vf_explained_var: 0.5460556149482727\n",
      "          vf_loss: 6050.4873046875\n",
      "    num_agent_steps_sampled: 524000\n",
      "    num_agent_steps_trained: 524000\n",
      "    num_steps_sampled: 524000\n",
      "    num_steps_trained: 524000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 131\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.625\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12168004566142215\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17937913833751926\n",
      "    mean_inference_ms: 1.235222357327498\n",
      "    mean_raw_obs_processing_ms: 0.1042779244292679\n",
      "  time_since_restore: 808.2877078056335\n",
      "  time_this_iter_s: 5.957336664199829\n",
      "  time_total_s: 808.2877078056335\n",
      "  timers:\n",
      "    learn_throughput: 1479.48\n",
      "    learn_time_ms: 2703.653\n",
      "    load_throughput: 5972239.784\n",
      "    load_time_ms: 0.67\n",
      "    sample_throughput: 666.209\n",
      "    sample_time_ms: 6004.118\n",
      "    update_time_ms: 2.869\n",
      "  timestamp: 1636042060\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 524000\n",
      "  training_iteration: 131\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:07:40 (running for 00:13:47.09)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   131</td><td style=\"text-align: right;\">         808.288</td><td style=\"text-align: right;\">524000</td><td style=\"text-align: right;\">-750.815</td><td style=\"text-align: right;\">            -380.676</td><td style=\"text-align: right;\">            -1780.76</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:07:45 (running for 00:13:52.10)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   131</td><td style=\"text-align: right;\">         808.288</td><td style=\"text-align: right;\">524000</td><td style=\"text-align: right;\">-750.815</td><td style=\"text-align: right;\">            -380.676</td><td style=\"text-align: right;\">            -1780.76</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 528000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-07-46\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -380.67618681934385\n",
      "  episode_reward_mean: -731.0701317540162\n",
      "  episode_reward_min: -1780.7637023768025\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2640\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.2088000625371933\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015117397531867027\n",
      "          model: {}\n",
      "          policy_loss: -0.001121380366384983\n",
      "          total_loss: 2976.736572265625\n",
      "          vf_explained_var: 0.5588045716285706\n",
      "          vf_loss: 2976.730224609375\n",
      "    num_agent_steps_sampled: 528000\n",
      "    num_agent_steps_trained: 528000\n",
      "    num_steps_sampled: 528000\n",
      "    num_steps_trained: 528000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 132\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.571428571428571\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12165011607897394\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.179325319882705\n",
      "    mean_inference_ms: 1.234823889929574\n",
      "    mean_raw_obs_processing_ms: 0.10424487493973893\n",
      "  time_since_restore: 814.2471218109131\n",
      "  time_this_iter_s: 5.959414005279541\n",
      "  time_total_s: 814.2471218109131\n",
      "  timers:\n",
      "    learn_throughput: 1478.449\n",
      "    learn_time_ms: 2705.538\n",
      "    load_throughput: 5988868.423\n",
      "    load_time_ms: 0.668\n",
      "    sample_throughput: 666.576\n",
      "    sample_time_ms: 6000.815\n",
      "    update_time_ms: 2.869\n",
      "  timestamp: 1636042066\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 528000\n",
      "  training_iteration: 132\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:07:50 (running for 00:13:57.10)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   132</td><td style=\"text-align: right;\">         814.247</td><td style=\"text-align: right;\">528000</td><td style=\"text-align: right;\"> -731.07</td><td style=\"text-align: right;\">            -380.676</td><td style=\"text-align: right;\">            -1780.76</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 532000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-07-52\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -380.8966577005769\n",
      "  episode_reward_mean: -699.7631656427174\n",
      "  episode_reward_min: -1780.7637023768025\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2660\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.6346307992935181\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012594169937074184\n",
      "          model: {}\n",
      "          policy_loss: -0.00010056008613901213\n",
      "          total_loss: 2534.461669921875\n",
      "          vf_explained_var: 0.6471812725067139\n",
      "          vf_loss: 2534.455078125\n",
      "    num_agent_steps_sampled: 532000\n",
      "    num_agent_steps_trained: 532000\n",
      "    num_steps_sampled: 532000\n",
      "    num_steps_trained: 532000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 133\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.475000000000001\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12161946960068602\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1792700893372443\n",
      "    mean_inference_ms: 1.2344104405722711\n",
      "    mean_raw_obs_processing_ms: 0.10421135745557798\n",
      "  time_since_restore: 820.2246854305267\n",
      "  time_this_iter_s: 5.9775636196136475\n",
      "  time_total_s: 820.2246854305267\n",
      "  timers:\n",
      "    learn_throughput: 1477.256\n",
      "    learn_time_ms: 2707.723\n",
      "    load_throughput: 5899784.084\n",
      "    load_time_ms: 0.678\n",
      "    sample_throughput: 666.42\n",
      "    sample_time_ms: 6002.219\n",
      "    update_time_ms: 2.86\n",
      "  timestamp: 1636042072\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 532000\n",
      "  training_iteration: 133\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:07:55 (running for 00:14:02.13)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   133</td><td style=\"text-align: right;\">         820.225</td><td style=\"text-align: right;\">532000</td><td style=\"text-align: right;\">-699.763</td><td style=\"text-align: right;\">            -380.897</td><td style=\"text-align: right;\">            -1780.76</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 536000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-07-58\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -375.6872978950534\n",
      "  episode_reward_mean: -709.4916058476535\n",
      "  episode_reward_min: -1780.7637023768025\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2680\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.4549957513809204\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011822915636003017\n",
      "          model: {}\n",
      "          policy_loss: -0.007907186634838581\n",
      "          total_loss: 6423.79052734375\n",
      "          vf_explained_var: 0.5097949504852295\n",
      "          vf_loss: 6423.7919921875\n",
      "    num_agent_steps_sampled: 536000\n",
      "    num_agent_steps_trained: 536000\n",
      "    num_steps_sampled: 536000\n",
      "    num_steps_trained: 536000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 134\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.371428571428572\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12159215811974736\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17922119544130222\n",
      "    mean_inference_ms: 1.234033286202989\n",
      "    mean_raw_obs_processing_ms: 0.10418081900175258\n",
      "  time_since_restore: 826.1945989131927\n",
      "  time_this_iter_s: 5.969913482666016\n",
      "  time_total_s: 826.1945989131927\n",
      "  timers:\n",
      "    learn_throughput: 1477.894\n",
      "    learn_time_ms: 2706.553\n",
      "    load_throughput: 5902897.755\n",
      "    load_time_ms: 0.678\n",
      "    sample_throughput: 665.509\n",
      "    sample_time_ms: 6010.439\n",
      "    update_time_ms: 2.835\n",
      "  timestamp: 1636042078\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 536000\n",
      "  training_iteration: 134\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:08:00 (running for 00:14:07.15)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   134</td><td style=\"text-align: right;\">         826.195</td><td style=\"text-align: right;\">536000</td><td style=\"text-align: right;\">-709.492</td><td style=\"text-align: right;\">            -375.687</td><td style=\"text-align: right;\">            -1780.76</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 540000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-08-04\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -375.53110860053346\n",
      "  episode_reward_mean: -654.235414395961\n",
      "  episode_reward_min: -1745.4569259063642\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2700\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.2799064517021179\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013976676389575005\n",
      "          model: {}\n",
      "          policy_loss: 0.0010952568845823407\n",
      "          total_loss: 2869.413818359375\n",
      "          vf_explained_var: 0.592505156993866\n",
      "          vf_loss: 2869.40576171875\n",
      "    num_agent_steps_sampled: 540000\n",
      "    num_agent_steps_trained: 540000\n",
      "    num_steps_sampled: 540000\n",
      "    num_steps_trained: 540000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 135\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.8625\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12156453388550387\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17917139838106638\n",
      "    mean_inference_ms: 1.233659827810979\n",
      "    mean_raw_obs_processing_ms: 0.10415033207740448\n",
      "  time_since_restore: 832.1118493080139\n",
      "  time_this_iter_s: 5.917250394821167\n",
      "  time_total_s: 832.1118493080139\n",
      "  timers:\n",
      "    learn_throughput: 1477.732\n",
      "    learn_time_ms: 2706.852\n",
      "    load_throughput: 5887362.178\n",
      "    load_time_ms: 0.679\n",
      "    sample_throughput: 666.569\n",
      "    sample_time_ms: 6000.876\n",
      "    update_time_ms: 2.843\n",
      "  timestamp: 1636042084\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 540000\n",
      "  training_iteration: 135\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:08:06 (running for 00:14:13.11)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   135</td><td style=\"text-align: right;\">         832.112</td><td style=\"text-align: right;\">540000</td><td style=\"text-align: right;\">-654.235</td><td style=\"text-align: right;\">            -375.531</td><td style=\"text-align: right;\">            -1745.46</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 544000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-08-10\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -375.53110860053346\n",
      "  episode_reward_mean: -624.5061347288216\n",
      "  episode_reward_min: -1737.7523202035948\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2720\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.6796014308929443\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010859268717467785\n",
      "          model: {}\n",
      "          policy_loss: -0.010308203287422657\n",
      "          total_loss: 3912.870361328125\n",
      "          vf_explained_var: 0.48740360140800476\n",
      "          vf_loss: 3912.875244140625\n",
      "    num_agent_steps_sampled: 544000\n",
      "    num_agent_steps_trained: 544000\n",
      "    num_steps_sampled: 544000\n",
      "    num_steps_trained: 544000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 136\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.242857142857142\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12153776816548305\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17912388748983205\n",
      "    mean_inference_ms: 1.2332961142849266\n",
      "    mean_raw_obs_processing_ms: 0.10412089946204159\n",
      "  time_since_restore: 838.0484125614166\n",
      "  time_this_iter_s: 5.93656325340271\n",
      "  time_total_s: 838.0484125614166\n",
      "  timers:\n",
      "    learn_throughput: 1477.937\n",
      "    learn_time_ms: 2706.476\n",
      "    load_throughput: 5830888.68\n",
      "    load_time_ms: 0.686\n",
      "    sample_throughput: 666.594\n",
      "    sample_time_ms: 6000.656\n",
      "    update_time_ms: 2.808\n",
      "  timestamp: 1636042090\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 544000\n",
      "  training_iteration: 136\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:08:12 (running for 00:14:19.09)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   136</td><td style=\"text-align: right;\">         838.048</td><td style=\"text-align: right;\">544000</td><td style=\"text-align: right;\">-624.506</td><td style=\"text-align: right;\">            -375.531</td><td style=\"text-align: right;\">            -1737.75</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 548000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-08-16\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -375.53110860053346\n",
      "  episode_reward_mean: -612.9824748216513\n",
      "  episode_reward_min: -1737.7523202035948\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2740\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.03086426481604576\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012089761905372143\n",
      "          model: {}\n",
      "          policy_loss: 0.0018553859554231167\n",
      "          total_loss: 2106.802001953125\n",
      "          vf_explained_var: 0.4944494068622589\n",
      "          vf_loss: 2106.794189453125\n",
      "    num_agent_steps_sampled: 548000\n",
      "    num_agent_steps_trained: 548000\n",
      "    num_steps_sampled: 548000\n",
      "    num_steps_trained: 548000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 137\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.8125\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12151242557908239\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17907863755795397\n",
      "    mean_inference_ms: 1.2329477090091765\n",
      "    mean_raw_obs_processing_ms: 0.10409299848296535\n",
      "  time_since_restore: 844.0491428375244\n",
      "  time_this_iter_s: 6.000730276107788\n",
      "  time_total_s: 844.0491428375244\n",
      "  timers:\n",
      "    learn_throughput: 1478.128\n",
      "    learn_time_ms: 2706.126\n",
      "    load_throughput: 5830483.406\n",
      "    load_time_ms: 0.686\n",
      "    sample_throughput: 666.021\n",
      "    sample_time_ms: 6005.82\n",
      "    update_time_ms: 2.806\n",
      "  timestamp: 1636042096\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 548000\n",
      "  training_iteration: 137\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:08:17 (running for 00:14:24.14)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   137</td><td style=\"text-align: right;\">         844.049</td><td style=\"text-align: right;\">548000</td><td style=\"text-align: right;\">-612.982</td><td style=\"text-align: right;\">            -375.531</td><td style=\"text-align: right;\">            -1737.75</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:08:22 (running for 00:14:29.14)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   137</td><td style=\"text-align: right;\">         844.049</td><td style=\"text-align: right;\">548000</td><td style=\"text-align: right;\">-612.982</td><td style=\"text-align: right;\">            -375.531</td><td style=\"text-align: right;\">            -1737.75</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 552000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-08-22\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -375.53110860053346\n",
      "  episode_reward_mean: -609.5866793561253\n",
      "  episode_reward_min: -1800.4663387429562\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2760\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.9549773335456848\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008771288208663464\n",
      "          model: {}\n",
      "          policy_loss: 0.03752231225371361\n",
      "          total_loss: 3500.853515625\n",
      "          vf_explained_var: 0.5547012090682983\n",
      "          vf_loss: 3500.8115234375\n",
      "    num_agent_steps_sampled: 552000\n",
      "    num_agent_steps_trained: 552000\n",
      "    num_steps_sampled: 552000\n",
      "    num_steps_trained: 552000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 138\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.1875\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12148921921319453\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17903759206660433\n",
      "    mean_inference_ms: 1.2326478336846394\n",
      "    mean_raw_obs_processing_ms: 0.1040682834934594\n",
      "  time_since_restore: 850.0939486026764\n",
      "  time_this_iter_s: 6.0448057651519775\n",
      "  time_total_s: 850.0939486026764\n",
      "  timers:\n",
      "    learn_throughput: 1477.208\n",
      "    learn_time_ms: 2707.811\n",
      "    load_throughput: 5781459.044\n",
      "    load_time_ms: 0.692\n",
      "    sample_throughput: 665.803\n",
      "    sample_time_ms: 6007.782\n",
      "    update_time_ms: 2.811\n",
      "  timestamp: 1636042102\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 552000\n",
      "  training_iteration: 138\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:08:27 (running for 00:14:34.27)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   138</td><td style=\"text-align: right;\">         850.094</td><td style=\"text-align: right;\">552000</td><td style=\"text-align: right;\">-609.587</td><td style=\"text-align: right;\">            -375.531</td><td style=\"text-align: right;\">            -1800.47</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 556000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-08-28\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -375.53110860053346\n",
      "  episode_reward_mean: -574.372418590399\n",
      "  episode_reward_min: -1800.4663387429562\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2780\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.4187568128108978\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00823180004954338\n",
      "          model: {}\n",
      "          policy_loss: 0.002860726322978735\n",
      "          total_loss: 4364.46435546875\n",
      "          vf_explained_var: 0.5220679044723511\n",
      "          vf_loss: 4364.45703125\n",
      "    num_agent_steps_sampled: 556000\n",
      "    num_agent_steps_trained: 556000\n",
      "    num_steps_sampled: 556000\n",
      "    num_steps_trained: 556000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 139\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.442857142857141\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12146435842036075\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1789929511116603\n",
      "    mean_inference_ms: 1.2323350982413566\n",
      "    mean_raw_obs_processing_ms: 0.1040427136059291\n",
      "  time_since_restore: 856.0323021411896\n",
      "  time_this_iter_s: 5.938353538513184\n",
      "  time_total_s: 856.0323021411896\n",
      "  timers:\n",
      "    learn_throughput: 1477.868\n",
      "    learn_time_ms: 2706.602\n",
      "    load_throughput: 5841038.889\n",
      "    load_time_ms: 0.685\n",
      "    sample_throughput: 665.09\n",
      "    sample_time_ms: 6014.228\n",
      "    update_time_ms: 2.802\n",
      "  timestamp: 1636042108\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 556000\n",
      "  training_iteration: 139\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:08:33 (running for 00:14:40.26)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   139</td><td style=\"text-align: right;\">         856.032</td><td style=\"text-align: right;\">556000</td><td style=\"text-align: right;\">-574.372</td><td style=\"text-align: right;\">            -375.531</td><td style=\"text-align: right;\">            -1800.47</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 560000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-08-34\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -376.9698417112102\n",
      "  episode_reward_mean: -580.156128985418\n",
      "  episode_reward_min: -1800.4663387429562\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2800\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.4259651303291321\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012269658036530018\n",
      "          model: {}\n",
      "          policy_loss: 0.011284449137747288\n",
      "          total_loss: 2618.189208984375\n",
      "          vf_explained_var: 0.5571712255477905\n",
      "          vf_loss: 2618.17138671875\n",
      "    num_agent_steps_sampled: 560000\n",
      "    num_agent_steps_trained: 560000\n",
      "    num_steps_sampled: 560000\n",
      "    num_steps_trained: 560000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 140\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.525\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12144316259964548\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17895398330833168\n",
      "    mean_inference_ms: 1.2320517216115232\n",
      "    mean_raw_obs_processing_ms: 0.10402033997603352\n",
      "  time_since_restore: 862.0089931488037\n",
      "  time_this_iter_s: 5.976691007614136\n",
      "  time_total_s: 862.0089931488037\n",
      "  timers:\n",
      "    learn_throughput: 1478.268\n",
      "    learn_time_ms: 2705.87\n",
      "    load_throughput: 5806671.512\n",
      "    load_time_ms: 0.689\n",
      "    sample_throughput: 664.521\n",
      "    sample_time_ms: 6019.37\n",
      "    update_time_ms: 2.802\n",
      "  timestamp: 1636042114\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 560000\n",
      "  training_iteration: 140\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:08:38 (running for 00:14:45.28)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   140</td><td style=\"text-align: right;\">         862.009</td><td style=\"text-align: right;\">560000</td><td style=\"text-align: right;\">-580.156</td><td style=\"text-align: right;\">             -376.97</td><td style=\"text-align: right;\">            -1800.47</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 564000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-08-40\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -375.09400570807657\n",
      "  episode_reward_mean: -590.5793683583737\n",
      "  episode_reward_min: -1800.4663387429562\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2820\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.9459800124168396\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012458854354918003\n",
      "          model: {}\n",
      "          policy_loss: -0.010423288680613041\n",
      "          total_loss: 4017.92236328125\n",
      "          vf_explained_var: 0.6096966862678528\n",
      "          vf_loss: 4017.926513671875\n",
      "    num_agent_steps_sampled: 564000\n",
      "    num_agent_steps_trained: 564000\n",
      "    num_steps_sampled: 564000\n",
      "    num_steps_trained: 564000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 141\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.27142857142857\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12142404735918699\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1789173558505056\n",
      "    mean_inference_ms: 1.231778130159524\n",
      "    mean_raw_obs_processing_ms: 0.10399951862598904\n",
      "  time_since_restore: 867.9689259529114\n",
      "  time_this_iter_s: 5.959932804107666\n",
      "  time_total_s: 867.9689259529114\n",
      "  timers:\n",
      "    learn_throughput: 1478.986\n",
      "    learn_time_ms: 2704.555\n",
      "    load_throughput: 5812102.82\n",
      "    load_time_ms: 0.688\n",
      "    sample_throughput: 664.408\n",
      "    sample_time_ms: 6020.4\n",
      "    update_time_ms: 2.806\n",
      "  timestamp: 1636042120\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 564000\n",
      "  training_iteration: 141\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:08:43 (running for 00:14:50.29)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   141</td><td style=\"text-align: right;\">         867.969</td><td style=\"text-align: right;\">564000</td><td style=\"text-align: right;\">-590.579</td><td style=\"text-align: right;\">            -375.094</td><td style=\"text-align: right;\">            -1800.47</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 568000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-08-46\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -375.09400570807657\n",
      "  episode_reward_mean: -601.3209439779222\n",
      "  episode_reward_min: -1800.4663387429562\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2840\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.5037743449211121\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009542105719447136\n",
      "          model: {}\n",
      "          policy_loss: -0.006779286079108715\n",
      "          total_loss: 3969.028564453125\n",
      "          vf_explained_var: 0.5400193333625793\n",
      "          vf_loss: 3969.0302734375\n",
      "    num_agent_steps_sampled: 568000\n",
      "    num_agent_steps_trained: 568000\n",
      "    num_steps_sampled: 568000\n",
      "    num_steps_trained: 568000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 142\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.4875\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12140632397431288\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17888340300692002\n",
      "    mean_inference_ms: 1.2315070691539534\n",
      "    mean_raw_obs_processing_ms: 0.10397922544261257\n",
      "  time_since_restore: 873.9435007572174\n",
      "  time_this_iter_s: 5.97457480430603\n",
      "  time_total_s: 873.9435007572174\n",
      "  timers:\n",
      "    learn_throughput: 1478.804\n",
      "    learn_time_ms: 2704.888\n",
      "    load_throughput: 5852449.158\n",
      "    load_time_ms: 0.683\n",
      "    sample_throughput: 664.441\n",
      "    sample_time_ms: 6020.095\n",
      "    update_time_ms: 2.801\n",
      "  timestamp: 1636042126\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 568000\n",
      "  training_iteration: 142\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:08:48 (running for 00:14:55.31)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   142</td><td style=\"text-align: right;\">         873.944</td><td style=\"text-align: right;\">568000</td><td style=\"text-align: right;\">-601.321</td><td style=\"text-align: right;\">            -375.094</td><td style=\"text-align: right;\">            -1800.47</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 572000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-08-52\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -374.97432043016806\n",
      "  episode_reward_mean: -572.0187293916442\n",
      "  episode_reward_min: -1725.9435842935982\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2860\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: -0.0067306989803910255\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012899336405098438\n",
      "          model: {}\n",
      "          policy_loss: 0.003997407387942076\n",
      "          total_loss: 1955.8795166015625\n",
      "          vf_explained_var: 0.5903186798095703\n",
      "          vf_loss: 1955.869140625\n",
      "    num_agent_steps_sampled: 572000\n",
      "    num_agent_steps_trained: 572000\n",
      "    num_steps_sampled: 572000\n",
      "    num_steps_trained: 572000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 143\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.471428571428572\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12138647729047976\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.178845335190963\n",
      "    mean_inference_ms: 1.231191671554953\n",
      "    mean_raw_obs_processing_ms: 0.10395634305493093\n",
      "  time_since_restore: 879.8699622154236\n",
      "  time_this_iter_s: 5.926461458206177\n",
      "  time_total_s: 879.8699622154236\n",
      "  timers:\n",
      "    learn_throughput: 1480.854\n",
      "    learn_time_ms: 2701.145\n",
      "    load_throughput: 5975217.608\n",
      "    load_time_ms: 0.669\n",
      "    sample_throughput: 664.566\n",
      "    sample_time_ms: 6018.964\n",
      "    update_time_ms: 2.812\n",
      "  timestamp: 1636042132\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 572000\n",
      "  training_iteration: 143\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:08:54 (running for 00:15:01.28)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   143</td><td style=\"text-align: right;\">          879.87</td><td style=\"text-align: right;\">572000</td><td style=\"text-align: right;\">-572.019</td><td style=\"text-align: right;\">            -374.974</td><td style=\"text-align: right;\">            -1725.94</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 576000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-08-58\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -372.84941103152227\n",
      "  episode_reward_mean: -577.0664390202251\n",
      "  episode_reward_min: -1715.0588228872907\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2880\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.3158721327781677\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012779547832906246\n",
      "          model: {}\n",
      "          policy_loss: 0.005107646808028221\n",
      "          total_loss: 1879.5994873046875\n",
      "          vf_explained_var: 0.6464466452598572\n",
      "          vf_loss: 1879.587890625\n",
      "    num_agent_steps_sampled: 576000\n",
      "    num_agent_steps_trained: 576000\n",
      "    num_steps_sampled: 576000\n",
      "    num_steps_trained: 576000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 144\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.5375\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12136472551020361\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17880422534755333\n",
      "    mean_inference_ms: 1.2308504070687323\n",
      "    mean_raw_obs_processing_ms: 0.1039313483979347\n",
      "  time_since_restore: 885.7789127826691\n",
      "  time_this_iter_s: 5.908950567245483\n",
      "  time_total_s: 885.7789127826691\n",
      "  timers:\n",
      "    learn_throughput: 1480.078\n",
      "    learn_time_ms: 2702.56\n",
      "    load_throughput: 5898332.161\n",
      "    load_time_ms: 0.678\n",
      "    sample_throughput: 665.824\n",
      "    sample_time_ms: 6007.595\n",
      "    update_time_ms: 2.821\n",
      "  timestamp: 1636042138\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 576000\n",
      "  training_iteration: 144\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:09:00 (running for 00:15:07.24)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   144</td><td style=\"text-align: right;\">         885.779</td><td style=\"text-align: right;\">576000</td><td style=\"text-align: right;\">-577.066</td><td style=\"text-align: right;\">            -372.849</td><td style=\"text-align: right;\">            -1715.06</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 580000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-09-04\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -369.906117079452\n",
      "  episode_reward_mean: -563.2772049473472\n",
      "  episode_reward_min: -1715.0588228872907\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2900\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: -0.021063746884465218\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010605229996144772\n",
      "          model: {}\n",
      "          policy_loss: -0.004384380765259266\n",
      "          total_loss: 2536.572509765625\n",
      "          vf_explained_var: 0.4952017366886139\n",
      "          vf_loss: 2536.57177734375\n",
      "    num_agent_steps_sampled: 580000\n",
      "    num_agent_steps_trained: 580000\n",
      "    num_steps_sampled: 580000\n",
      "    num_steps_trained: 580000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 145\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.814285714285715\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12134127806751539\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17876173731119566\n",
      "    mean_inference_ms: 1.2304970328390596\n",
      "    mean_raw_obs_processing_ms: 0.10390548830798695\n",
      "  time_since_restore: 891.703106880188\n",
      "  time_this_iter_s: 5.924194097518921\n",
      "  time_total_s: 891.703106880188\n",
      "  timers:\n",
      "    learn_throughput: 1480.2\n",
      "    learn_time_ms: 2702.338\n",
      "    load_throughput: 5928763.87\n",
      "    load_time_ms: 0.675\n",
      "    sample_throughput: 665.563\n",
      "    sample_time_ms: 6009.947\n",
      "    update_time_ms: 2.831\n",
      "  timestamp: 1636042144\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 580000\n",
      "  training_iteration: 145\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:09:06 (running for 00:15:13.21)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   145</td><td style=\"text-align: right;\">         891.703</td><td style=\"text-align: right;\">580000</td><td style=\"text-align: right;\">-563.277</td><td style=\"text-align: right;\">            -369.906</td><td style=\"text-align: right;\">            -1715.06</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 584000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-09-10\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -369.906117079452\n",
      "  episode_reward_mean: -556.411253728662\n",
      "  episode_reward_min: -1715.0588228872907\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2920\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.745078980922699\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008289000950753689\n",
      "          model: {}\n",
      "          policy_loss: 0.0006025058100931346\n",
      "          total_loss: 4115.41796875\n",
      "          vf_explained_var: 0.5328227281570435\n",
      "          vf_loss: 4115.4130859375\n",
      "    num_agent_steps_sampled: 584000\n",
      "    num_agent_steps_trained: 584000\n",
      "    num_steps_sampled: 584000\n",
      "    num_steps_trained: 584000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 146\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.350000000000001\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12131485058905284\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17871642330946358\n",
      "    mean_inference_ms: 1.2301265438477251\n",
      "    mean_raw_obs_processing_ms: 0.10387776543441653\n",
      "  time_since_restore: 897.6411845684052\n",
      "  time_this_iter_s: 5.938077688217163\n",
      "  time_total_s: 897.6411845684052\n",
      "  timers:\n",
      "    learn_throughput: 1477.79\n",
      "    learn_time_ms: 2706.745\n",
      "    load_throughput: 5969477.317\n",
      "    load_time_ms: 0.67\n",
      "    sample_throughput: 666.036\n",
      "    sample_time_ms: 6005.678\n",
      "    update_time_ms: 2.864\n",
      "  timestamp: 1636042150\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 584000\n",
      "  training_iteration: 146\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:09:12 (running for 00:15:19.19)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   146</td><td style=\"text-align: right;\">         897.641</td><td style=\"text-align: right;\">584000</td><td style=\"text-align: right;\">-556.411</td><td style=\"text-align: right;\">            -369.906</td><td style=\"text-align: right;\">            -1715.06</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 588000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-09-16\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -369.906117079452\n",
      "  episode_reward_mean: -552.9115327762064\n",
      "  episode_reward_min: -1706.9065882693092\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2940\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.049553632736206055\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013096000999212265\n",
      "          model: {}\n",
      "          policy_loss: 0.002128335414454341\n",
      "          total_loss: 1977.0010986328125\n",
      "          vf_explained_var: 0.6222333312034607\n",
      "          vf_loss: 1976.9921875\n",
      "    num_agent_steps_sampled: 588000\n",
      "    num_agent_steps_trained: 588000\n",
      "    num_steps_sampled: 588000\n",
      "    num_steps_trained: 588000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 147\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.442857142857145\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12128837674114683\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17867033724856893\n",
      "    mean_inference_ms: 1.2297662229156616\n",
      "    mean_raw_obs_processing_ms: 0.10384985619460776\n",
      "  time_since_restore: 903.6254563331604\n",
      "  time_this_iter_s: 5.984271764755249\n",
      "  time_total_s: 903.6254563331604\n",
      "  timers:\n",
      "    learn_throughput: 1478.139\n",
      "    learn_time_ms: 2706.106\n",
      "    load_throughput: 5914759.739\n",
      "    load_time_ms: 0.676\n",
      "    sample_throughput: 665.647\n",
      "    sample_time_ms: 6009.195\n",
      "    update_time_ms: 2.877\n",
      "  timestamp: 1636042156\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 588000\n",
      "  training_iteration: 147\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:09:17 (running for 00:15:24.23)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   147</td><td style=\"text-align: right;\">         903.625</td><td style=\"text-align: right;\">588000</td><td style=\"text-align: right;\">-552.912</td><td style=\"text-align: right;\">            -369.906</td><td style=\"text-align: right;\">            -1706.91</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 592000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-09-22\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -369.906117079452\n",
      "  episode_reward_mean: -563.7836077957618\n",
      "  episode_reward_min: -1706.9065882693092\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2960\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.12426259368658066\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011010998860001564\n",
      "          model: {}\n",
      "          policy_loss: -0.002043910324573517\n",
      "          total_loss: 1547.8233642578125\n",
      "          vf_explained_var: 0.6925186514854431\n",
      "          vf_loss: 1547.819580078125\n",
      "    num_agent_steps_sampled: 592000\n",
      "    num_agent_steps_trained: 592000\n",
      "    num_steps_sampled: 592000\n",
      "    num_steps_trained: 592000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 148\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.4625\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12126203724904622\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17862448570433925\n",
      "    mean_inference_ms: 1.2294057498334692\n",
      "    mean_raw_obs_processing_ms: 0.10382183132386956\n",
      "  time_since_restore: 909.5595953464508\n",
      "  time_this_iter_s: 5.934139013290405\n",
      "  time_total_s: 909.5595953464508\n",
      "  timers:\n",
      "    learn_throughput: 1478.538\n",
      "    learn_time_ms: 2705.375\n",
      "    load_throughput: 5962476.366\n",
      "    load_time_ms: 0.671\n",
      "    sample_throughput: 666.863\n",
      "    sample_time_ms: 5998.236\n",
      "    update_time_ms: 2.878\n",
      "  timestamp: 1636042162\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 592000\n",
      "  training_iteration: 148\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:09:23 (running for 00:15:30.21)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   148</td><td style=\"text-align: right;\">          909.56</td><td style=\"text-align: right;\">592000</td><td style=\"text-align: right;\">-563.784</td><td style=\"text-align: right;\">            -369.906</td><td style=\"text-align: right;\">            -1706.91</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 596000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-09-28\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -369.906117079452\n",
      "  episode_reward_mean: -565.5116383257923\n",
      "  episode_reward_min: -1706.9065882693092\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2980\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.2751763164997101\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012515665031969547\n",
      "          model: {}\n",
      "          policy_loss: -0.006086378823965788\n",
      "          total_loss: 2210.64013671875\n",
      "          vf_explained_var: 0.6136903762817383\n",
      "          vf_loss: 2210.639892578125\n",
      "    num_agent_steps_sampled: 596000\n",
      "    num_agent_steps_trained: 596000\n",
      "    num_steps_sampled: 596000\n",
      "    num_steps_trained: 596000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 149\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.614285714285714\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12123778701972263\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17858308491412397\n",
      "    mean_inference_ms: 1.2290782608064914\n",
      "    mean_raw_obs_processing_ms: 0.10379616798004576\n",
      "  time_since_restore: 915.5082204341888\n",
      "  time_this_iter_s: 5.948625087738037\n",
      "  time_total_s: 915.5082204341888\n",
      "  timers:\n",
      "    learn_throughput: 1477.874\n",
      "    learn_time_ms: 2706.59\n",
      "    load_throughput: 5939888.83\n",
      "    load_time_ms: 0.673\n",
      "    sample_throughput: 667.4\n",
      "    sample_time_ms: 5993.404\n",
      "    update_time_ms: 2.877\n",
      "  timestamp: 1636042168\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 596000\n",
      "  training_iteration: 149\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:09:29 (running for 00:15:36.20)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   149</td><td style=\"text-align: right;\">         915.508</td><td style=\"text-align: right;\">596000</td><td style=\"text-align: right;\">-565.512</td><td style=\"text-align: right;\">            -369.906</td><td style=\"text-align: right;\">            -1706.91</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 600000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-09-34\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -294.51952876658476\n",
      "  episode_reward_mean: -562.8045481934872\n",
      "  episode_reward_min: -1706.9065882693092\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3000\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.2892426550388336\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01143947709351778\n",
      "          model: {}\n",
      "          policy_loss: 0.002965278457850218\n",
      "          total_loss: 2180.327880859375\n",
      "          vf_explained_var: 0.5617430210113525\n",
      "          vf_loss: 2180.3193359375\n",
      "    num_agent_steps_sampled: 600000\n",
      "    num_agent_steps_trained: 600000\n",
      "    num_steps_sampled: 600000\n",
      "    num_steps_trained: 600000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 150\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.512500000000001\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12121338138712742\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1785417781225071\n",
      "    mean_inference_ms: 1.2287644594617917\n",
      "    mean_raw_obs_processing_ms: 0.10377064986192445\n",
      "  time_since_restore: 921.4613673686981\n",
      "  time_this_iter_s: 5.953146934509277\n",
      "  time_total_s: 921.4613673686981\n",
      "  timers:\n",
      "    learn_throughput: 1477.468\n",
      "    learn_time_ms: 2707.334\n",
      "    load_throughput: 5968203.194\n",
      "    load_time_ms: 0.67\n",
      "    sample_throughput: 667.583\n",
      "    sample_time_ms: 5991.766\n",
      "    update_time_ms: 2.87\n",
      "  timestamp: 1636042174\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 600000\n",
      "  training_iteration: 150\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:09:35 (running for 00:15:42.20)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   150</td><td style=\"text-align: right;\">         921.461</td><td style=\"text-align: right;\">600000</td><td style=\"text-align: right;\">-562.805</td><td style=\"text-align: right;\">             -294.52</td><td style=\"text-align: right;\">            -1706.91</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 604000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-09-40\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -294.51952876658476\n",
      "  episode_reward_mean: -567.334406443887\n",
      "  episode_reward_min: -1788.8897339592131\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3020\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.8762439489364624\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00945308618247509\n",
      "          model: {}\n",
      "          policy_loss: -0.0016931487480178475\n",
      "          total_loss: 4923.80908203125\n",
      "          vf_explained_var: 0.5666671395301819\n",
      "          vf_loss: 4923.80615234375\n",
      "    num_agent_steps_sampled: 604000\n",
      "    num_agent_steps_trained: 604000\n",
      "    num_steps_sampled: 604000\n",
      "    num_steps_trained: 604000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 151\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.285714285714286\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12119203962661879\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17850344614151453\n",
      "    mean_inference_ms: 1.22847518099256\n",
      "    mean_raw_obs_processing_ms: 0.10374723379508737\n",
      "  time_since_restore: 927.3981239795685\n",
      "  time_this_iter_s: 5.936756610870361\n",
      "  time_total_s: 927.3981239795685\n",
      "  timers:\n",
      "    learn_throughput: 1477.845\n",
      "    learn_time_ms: 2706.643\n",
      "    load_throughput: 5942834.473\n",
      "    load_time_ms: 0.673\n",
      "    sample_throughput: 667.687\n",
      "    sample_time_ms: 5990.828\n",
      "    update_time_ms: 2.865\n",
      "  timestamp: 1636042180\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 604000\n",
      "  training_iteration: 151\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:09:41 (running for 00:15:48.18)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   151</td><td style=\"text-align: right;\">         927.398</td><td style=\"text-align: right;\">604000</td><td style=\"text-align: right;\">-567.334</td><td style=\"text-align: right;\">             -294.52</td><td style=\"text-align: right;\">            -1788.89</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 608000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-09-46\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -294.51952876658476\n",
      "  episode_reward_mean: -571.5698872134816\n",
      "  episode_reward_min: -1788.8897339592131\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3040\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.3813478648662567\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01029842346906662\n",
      "          model: {}\n",
      "          policy_loss: 0.005781788844615221\n",
      "          total_loss: 1987.6348876953125\n",
      "          vf_explained_var: 0.6712332963943481\n",
      "          vf_loss: 1987.6236572265625\n",
      "    num_agent_steps_sampled: 608000\n",
      "    num_agent_steps_trained: 608000\n",
      "    num_steps_sampled: 608000\n",
      "    num_steps_trained: 608000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 152\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.375\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12116877531147996\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17846341148422526\n",
      "    mean_inference_ms: 1.2281627572626161\n",
      "    mean_raw_obs_processing_ms: 0.10372362924873606\n",
      "  time_since_restore: 933.3474593162537\n",
      "  time_this_iter_s: 5.949335336685181\n",
      "  time_total_s: 933.3474593162537\n",
      "  timers:\n",
      "    learn_throughput: 1477.885\n",
      "    learn_time_ms: 2706.571\n",
      "    load_throughput: 5945783.039\n",
      "    load_time_ms: 0.673\n",
      "    sample_throughput: 668.046\n",
      "    sample_time_ms: 5987.614\n",
      "    update_time_ms: 2.862\n",
      "  timestamp: 1636042186\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 608000\n",
      "  training_iteration: 152\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:09:47 (running for 00:15:54.18)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   152</td><td style=\"text-align: right;\">         933.347</td><td style=\"text-align: right;\">608000</td><td style=\"text-align: right;\"> -571.57</td><td style=\"text-align: right;\">             -294.52</td><td style=\"text-align: right;\">            -1788.89</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 612000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-09-52\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -294.51952876658476\n",
      "  episode_reward_mean: -591.3935825660137\n",
      "  episode_reward_min: -1788.8897339592131\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3060\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.003530740737915\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00949792843312025\n",
      "          model: {}\n",
      "          policy_loss: -0.004312610719352961\n",
      "          total_loss: 2913.012939453125\n",
      "          vf_explained_var: 0.6743087768554688\n",
      "          vf_loss: 2913.012451171875\n",
      "    num_agent_steps_sampled: 612000\n",
      "    num_agent_steps_trained: 612000\n",
      "    num_steps_sampled: 612000\n",
      "    num_steps_trained: 612000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 153\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.614285714285714\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12114789723624313\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17842726432851155\n",
      "    mean_inference_ms: 1.2278835311026703\n",
      "    mean_raw_obs_processing_ms: 0.10370206214737218\n",
      "  time_since_restore: 939.3367478847504\n",
      "  time_this_iter_s: 5.989288568496704\n",
      "  time_total_s: 939.3367478847504\n",
      "  timers:\n",
      "    learn_throughput: 1477.783\n",
      "    learn_time_ms: 2706.758\n",
      "    load_throughput: 5937156.204\n",
      "    load_time_ms: 0.674\n",
      "    sample_throughput: 667.373\n",
      "    sample_time_ms: 5993.645\n",
      "    update_time_ms: 2.852\n",
      "  timestamp: 1636042192\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 612000\n",
      "  training_iteration: 153\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:09:52 (running for 00:15:59.22)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   153</td><td style=\"text-align: right;\">         939.337</td><td style=\"text-align: right;\">612000</td><td style=\"text-align: right;\">-591.394</td><td style=\"text-align: right;\">             -294.52</td><td style=\"text-align: right;\">            -1788.89</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:09:57 (running for 00:16:04.23)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   153</td><td style=\"text-align: right;\">         939.337</td><td style=\"text-align: right;\">612000</td><td style=\"text-align: right;\">-591.394</td><td style=\"text-align: right;\">             -294.52</td><td style=\"text-align: right;\">            -1788.89</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 616000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-09-58\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -294.51952876658476\n",
      "  episode_reward_mean: -592.0114478066351\n",
      "  episode_reward_min: -1788.8897339592131\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3080\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.5241445302963257\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013129464350640774\n",
      "          model: {}\n",
      "          policy_loss: -0.005245361477136612\n",
      "          total_loss: 3072.646240234375\n",
      "          vf_explained_var: 0.5615007281303406\n",
      "          vf_loss: 3072.644775390625\n",
      "    num_agent_steps_sampled: 616000\n",
      "    num_agent_steps_trained: 616000\n",
      "    num_steps_sampled: 616000\n",
      "    num_steps_trained: 616000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 154\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.5\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12112866442964554\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17839303732104658\n",
      "    mean_inference_ms: 1.2276151809702924\n",
      "    mean_raw_obs_processing_ms: 0.10368181762017135\n",
      "  time_since_restore: 945.3262121677399\n",
      "  time_this_iter_s: 5.989464282989502\n",
      "  time_total_s: 945.3262121677399\n",
      "  timers:\n",
      "    learn_throughput: 1477.352\n",
      "    learn_time_ms: 2707.547\n",
      "    load_throughput: 6012046.155\n",
      "    load_time_ms: 0.665\n",
      "    sample_throughput: 666.557\n",
      "    sample_time_ms: 6000.99\n",
      "    update_time_ms: 2.858\n",
      "  timestamp: 1636042198\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 616000\n",
      "  training_iteration: 154\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:10:02 (running for 00:16:09.26)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   154</td><td style=\"text-align: right;\">         945.326</td><td style=\"text-align: right;\">616000</td><td style=\"text-align: right;\">-592.011</td><td style=\"text-align: right;\">             -294.52</td><td style=\"text-align: right;\">            -1788.89</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 620000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-10-04\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -257.4209431394449\n",
      "  episode_reward_mean: -611.40304361829\n",
      "  episode_reward_min: -1788.8897339592131\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3100\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.8357245922088623\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013479670509696007\n",
      "          model: {}\n",
      "          policy_loss: -0.001109492382965982\n",
      "          total_loss: 5815.08349609375\n",
      "          vf_explained_var: 0.520637571811676\n",
      "          vf_loss: 5815.078125\n",
      "    num_agent_steps_sampled: 620000\n",
      "    num_agent_steps_trained: 620000\n",
      "    num_steps_sampled: 620000\n",
      "    num_steps_trained: 620000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 155\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.72857142857143\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12111103777897834\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1783612647536912\n",
      "    mean_inference_ms: 1.2273493296500348\n",
      "    mean_raw_obs_processing_ms: 0.1036626800355578\n",
      "  time_since_restore: 951.2716307640076\n",
      "  time_this_iter_s: 5.9454185962677\n",
      "  time_total_s: 951.2716307640076\n",
      "  timers:\n",
      "    learn_throughput: 1478.196\n",
      "    learn_time_ms: 2706.002\n",
      "    load_throughput: 5973728.325\n",
      "    load_time_ms: 0.67\n",
      "    sample_throughput: 666.064\n",
      "    sample_time_ms: 6005.425\n",
      "    update_time_ms: 2.84\n",
      "  timestamp: 1636042204\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 620000\n",
      "  training_iteration: 155\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:10:08 (running for 00:16:15.25)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   155</td><td style=\"text-align: right;\">         951.272</td><td style=\"text-align: right;\">620000</td><td style=\"text-align: right;\">-611.403</td><td style=\"text-align: right;\">            -257.421</td><td style=\"text-align: right;\">            -1788.89</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 624000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-10-10\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -257.4209431394449\n",
      "  episode_reward_mean: -614.41335224655\n",
      "  episode_reward_min: -1688.3903053471538\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3120\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.6314091682434082\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01638644002377987\n",
      "          model: {}\n",
      "          policy_loss: -0.008430439047515392\n",
      "          total_loss: 4303.5966796875\n",
      "          vf_explained_var: 0.5106530785560608\n",
      "          vf_loss: 4303.5966796875\n",
      "    num_agent_steps_sampled: 624000\n",
      "    num_agent_steps_trained: 624000\n",
      "    num_steps_sampled: 624000\n",
      "    num_steps_trained: 624000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 156\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.85\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12109295208493606\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17832965800405912\n",
      "    mean_inference_ms: 1.2270889695323186\n",
      "    mean_raw_obs_processing_ms: 0.10364417146478211\n",
      "  time_since_restore: 957.2164762020111\n",
      "  time_this_iter_s: 5.94484543800354\n",
      "  time_total_s: 957.2164762020111\n",
      "  timers:\n",
      "    learn_throughput: 1479.795\n",
      "    learn_time_ms: 2703.077\n",
      "    load_throughput: 5919350.81\n",
      "    load_time_ms: 0.676\n",
      "    sample_throughput: 665.851\n",
      "    sample_time_ms: 6007.346\n",
      "    update_time_ms: 2.822\n",
      "  timestamp: 1636042210\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 624000\n",
      "  training_iteration: 156\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:10:14 (running for 00:16:21.24)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   156</td><td style=\"text-align: right;\">         957.216</td><td style=\"text-align: right;\">624000</td><td style=\"text-align: right;\">-614.413</td><td style=\"text-align: right;\">            -257.421</td><td style=\"text-align: right;\">            -1688.39</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 628000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-10-16\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -257.4209431394449\n",
      "  episode_reward_mean: -615.3698126387477\n",
      "  episode_reward_min: -1783.2808065614943\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3140\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.4174577295780182\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009360113181173801\n",
      "          model: {}\n",
      "          policy_loss: -0.007204585243016481\n",
      "          total_loss: 3999.29150390625\n",
      "          vf_explained_var: 0.572089433670044\n",
      "          vf_loss: 3999.294189453125\n",
      "    num_agent_steps_sampled: 628000\n",
      "    num_agent_steps_trained: 628000\n",
      "    num_steps_sampled: 628000\n",
      "    num_steps_trained: 628000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 157\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.585714285714285\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12107599145510196\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17829884172656754\n",
      "    mean_inference_ms: 1.2268489357561172\n",
      "    mean_raw_obs_processing_ms: 0.10362631559185154\n",
      "  time_since_restore: 963.1705832481384\n",
      "  time_this_iter_s: 5.954107046127319\n",
      "  time_total_s: 963.1705832481384\n",
      "  timers:\n",
      "    learn_throughput: 1479.435\n",
      "    learn_time_ms: 2703.735\n",
      "    load_throughput: 5968627.842\n",
      "    load_time_ms: 0.67\n",
      "    sample_throughput: 666.573\n",
      "    sample_time_ms: 6000.845\n",
      "    update_time_ms: 2.815\n",
      "  timestamp: 1636042216\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 628000\n",
      "  training_iteration: 157\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:10:20 (running for 00:16:27.24)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   157</td><td style=\"text-align: right;\">         963.171</td><td style=\"text-align: right;\">628000</td><td style=\"text-align: right;\"> -615.37</td><td style=\"text-align: right;\">            -257.421</td><td style=\"text-align: right;\">            -1783.28</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 632000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-10-22\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -257.4209431394449\n",
      "  episode_reward_mean: -598.6702284394393\n",
      "  episode_reward_min: -1783.2808065614943\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3160\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.537517249584198\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01325052697211504\n",
      "          model: {}\n",
      "          policy_loss: -0.00610013073310256\n",
      "          total_loss: 2303.57421875\n",
      "          vf_explained_var: 0.6010124087333679\n",
      "          vf_loss: 2303.57373046875\n",
      "    num_agent_steps_sampled: 632000\n",
      "    num_agent_steps_trained: 632000\n",
      "    num_steps_sampled: 632000\n",
      "    num_steps_trained: 632000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 158\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.55\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12105660158987891\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17826459062873978\n",
      "    mean_inference_ms: 1.2265798003197435\n",
      "    mean_raw_obs_processing_ms: 0.10360687778769849\n",
      "  time_since_restore: 969.108015537262\n",
      "  time_this_iter_s: 5.937432289123535\n",
      "  time_total_s: 969.108015537262\n",
      "  timers:\n",
      "    learn_throughput: 1478.877\n",
      "    learn_time_ms: 2704.755\n",
      "    load_throughput: 5968415.51\n",
      "    load_time_ms: 0.67\n",
      "    sample_throughput: 666.585\n",
      "    sample_time_ms: 6000.738\n",
      "    update_time_ms: 2.822\n",
      "  timestamp: 1636042222\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 632000\n",
      "  training_iteration: 158\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:10:26 (running for 00:16:33.22)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   158</td><td style=\"text-align: right;\">         969.108</td><td style=\"text-align: right;\">632000</td><td style=\"text-align: right;\"> -598.67</td><td style=\"text-align: right;\">            -257.421</td><td style=\"text-align: right;\">            -1783.28</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 636000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-10-28\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -257.4209431394449\n",
      "  episode_reward_mean: -598.6768742329386\n",
      "  episode_reward_min: -1783.2808065614943\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3180\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.5346727967262268\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012626443058252335\n",
      "          model: {}\n",
      "          policy_loss: -0.0010101639200001955\n",
      "          total_loss: 3186.92236328125\n",
      "          vf_explained_var: 0.6022329926490784\n",
      "          vf_loss: 3186.916748046875\n",
      "    num_agent_steps_sampled: 636000\n",
      "    num_agent_steps_trained: 636000\n",
      "    num_steps_sampled: 636000\n",
      "    num_steps_trained: 636000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 159\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.671428571428573\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1210356633196335\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17822804854627955\n",
      "    mean_inference_ms: 1.2262972907479166\n",
      "    mean_raw_obs_processing_ms: 0.10358661566708242\n",
      "  time_since_restore: 975.0216391086578\n",
      "  time_this_iter_s: 5.913623571395874\n",
      "  time_total_s: 975.0216391086578\n",
      "  timers:\n",
      "    learn_throughput: 1479.874\n",
      "    learn_time_ms: 2702.932\n",
      "    load_throughput: 6015279.481\n",
      "    load_time_ms: 0.665\n",
      "    sample_throughput: 666.651\n",
      "    sample_time_ms: 6000.143\n",
      "    update_time_ms: 2.834\n",
      "  timestamp: 1636042228\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 636000\n",
      "  training_iteration: 159\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:10:32 (running for 00:16:39.18)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   159</td><td style=\"text-align: right;\">         975.022</td><td style=\"text-align: right;\">636000</td><td style=\"text-align: right;\">-598.677</td><td style=\"text-align: right;\">            -257.421</td><td style=\"text-align: right;\">            -1783.28</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 640000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-10-34\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -269.17534243573834\n",
      "  episode_reward_mean: -607.9175358153011\n",
      "  episode_reward_min: -1783.2808065614943\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3200\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.1313406229019165\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008054813370108604\n",
      "          model: {}\n",
      "          policy_loss: -0.0021869561169296503\n",
      "          total_loss: 3940.1650390625\n",
      "          vf_explained_var: 0.5846738815307617\n",
      "          vf_loss: 3940.1630859375\n",
      "    num_agent_steps_sampled: 640000\n",
      "    num_agent_steps_trained: 640000\n",
      "    num_steps_sampled: 640000\n",
      "    num_steps_trained: 640000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 160\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.8875\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12101602928812882\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17819337903544266\n",
      "    mean_inference_ms: 1.2260290069884225\n",
      "    mean_raw_obs_processing_ms: 0.10356665649234326\n",
      "  time_since_restore: 980.9905891418457\n",
      "  time_this_iter_s: 5.968950033187866\n",
      "  time_total_s: 980.9905891418457\n",
      "  timers:\n",
      "    learn_throughput: 1480.127\n",
      "    learn_time_ms: 2702.471\n",
      "    load_throughput: 6019595.996\n",
      "    load_time_ms: 0.664\n",
      "    sample_throughput: 666.661\n",
      "    sample_time_ms: 6000.048\n",
      "    update_time_ms: 2.853\n",
      "  timestamp: 1636042234\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 640000\n",
      "  training_iteration: 160\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:10:37 (running for 00:16:44.20)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   160</td><td style=\"text-align: right;\">         980.991</td><td style=\"text-align: right;\">640000</td><td style=\"text-align: right;\">-607.918</td><td style=\"text-align: right;\">            -269.175</td><td style=\"text-align: right;\">            -1783.28</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 644000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-10-40\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -269.17534243573834\n",
      "  episode_reward_mean: -587.9667139930691\n",
      "  episode_reward_min: -1783.2808065614943\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3220\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.27214398980140686\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012452401220798492\n",
      "          model: {}\n",
      "          policy_loss: 0.005395464599132538\n",
      "          total_loss: 1506.3831787109375\n",
      "          vf_explained_var: 0.6911036372184753\n",
      "          vf_loss: 1506.3714599609375\n",
      "    num_agent_steps_sampled: 644000\n",
      "    num_agent_steps_trained: 644000\n",
      "    num_steps_sampled: 644000\n",
      "    num_steps_trained: 644000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 161\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.557142857142859\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12099754778935959\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17816002825660193\n",
      "    mean_inference_ms: 1.2257685035161785\n",
      "    mean_raw_obs_processing_ms: 0.10354706379526393\n",
      "  time_since_restore: 986.9475908279419\n",
      "  time_this_iter_s: 5.957001686096191\n",
      "  time_total_s: 986.9475908279419\n",
      "  timers:\n",
      "    learn_throughput: 1479.539\n",
      "    learn_time_ms: 2703.545\n",
      "    load_throughput: 6077380.28\n",
      "    load_time_ms: 0.658\n",
      "    sample_throughput: 666.598\n",
      "    sample_time_ms: 6000.621\n",
      "    update_time_ms: 2.852\n",
      "  timestamp: 1636042240\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 644000\n",
      "  training_iteration: 161\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:10:42 (running for 00:16:49.20)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   161</td><td style=\"text-align: right;\">         986.948</td><td style=\"text-align: right;\">644000</td><td style=\"text-align: right;\">-587.967</td><td style=\"text-align: right;\">            -269.175</td><td style=\"text-align: right;\">            -1783.28</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 648000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-10-46\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -269.17534243573834\n",
      "  episode_reward_mean: -573.0814182786893\n",
      "  episode_reward_min: -1644.4322117847692\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3240\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: -0.0022434350103139877\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01079515554010868\n",
      "          model: {}\n",
      "          policy_loss: 0.0024445459712296724\n",
      "          total_loss: 2592.498046875\n",
      "          vf_explained_var: 0.5529412627220154\n",
      "          vf_loss: 2592.490234375\n",
      "    num_agent_steps_sampled: 648000\n",
      "    num_agent_steps_trained: 648000\n",
      "    num_steps_sampled: 648000\n",
      "    num_steps_trained: 648000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 162\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.6625\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12097868552758671\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17812697597053798\n",
      "    mean_inference_ms: 1.2255075869069307\n",
      "    mean_raw_obs_processing_ms: 0.10352709634908579\n",
      "  time_since_restore: 992.9060142040253\n",
      "  time_this_iter_s: 5.958423376083374\n",
      "  time_total_s: 992.9060142040253\n",
      "  timers:\n",
      "    learn_throughput: 1479.407\n",
      "    learn_time_ms: 2703.786\n",
      "    load_throughput: 6080904.676\n",
      "    load_time_ms: 0.658\n",
      "    sample_throughput: 666.39\n",
      "    sample_time_ms: 6002.495\n",
      "    update_time_ms: 2.864\n",
      "  timestamp: 1636042246\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 648000\n",
      "  training_iteration: 162\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:10:47 (running for 00:16:54.21)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   162</td><td style=\"text-align: right;\">         992.906</td><td style=\"text-align: right;\">648000</td><td style=\"text-align: right;\">-573.081</td><td style=\"text-align: right;\">            -269.175</td><td style=\"text-align: right;\">            -1644.43</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 652000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-10-52\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -269.17534243573834\n",
      "  episode_reward_mean: -584.9228814918304\n",
      "  episode_reward_min: -1692.0274426699448\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3260\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.9955037236213684\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014253383502364159\n",
      "          model: {}\n",
      "          policy_loss: -0.012120638974010944\n",
      "          total_loss: 5178.5283203125\n",
      "          vf_explained_var: 0.5054327845573425\n",
      "          vf_loss: 5178.533203125\n",
      "    num_agent_steps_sampled: 652000\n",
      "    num_agent_steps_trained: 652000\n",
      "    num_steps_sampled: 652000\n",
      "    num_steps_trained: 652000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 163\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.485714285714286\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12096116317604391\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17809506947845627\n",
      "    mean_inference_ms: 1.225258315565641\n",
      "    mean_raw_obs_processing_ms: 0.10350795163886341\n",
      "  time_since_restore: 998.8429610729218\n",
      "  time_this_iter_s: 5.936946868896484\n",
      "  time_total_s: 998.8429610729218\n",
      "  timers:\n",
      "    learn_throughput: 1479.276\n",
      "    learn_time_ms: 2704.026\n",
      "    load_throughput: 6086419.735\n",
      "    load_time_ms: 0.657\n",
      "    sample_throughput: 666.958\n",
      "    sample_time_ms: 5997.381\n",
      "    update_time_ms: 2.88\n",
      "  timestamp: 1636042252\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 652000\n",
      "  training_iteration: 163\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:10:53 (running for 00:17:00.20)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   163</td><td style=\"text-align: right;\">         998.843</td><td style=\"text-align: right;\">652000</td><td style=\"text-align: right;\">-584.923</td><td style=\"text-align: right;\">            -269.175</td><td style=\"text-align: right;\">            -1692.03</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 656000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-10-58\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -368.75881563461763\n",
      "  episode_reward_mean: -591.66766164395\n",
      "  episode_reward_min: -1692.0274426699448\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3280\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.6091115474700928\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008906365372240543\n",
      "          model: {}\n",
      "          policy_loss: 0.0016914495499804616\n",
      "          total_loss: 2790.773193359375\n",
      "          vf_explained_var: 0.619691014289856\n",
      "          vf_loss: 2790.7666015625\n",
      "    num_agent_steps_sampled: 656000\n",
      "    num_agent_steps_trained: 656000\n",
      "    num_steps_sampled: 656000\n",
      "    num_steps_trained: 656000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 164\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.475\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12094544366136059\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17806583531898226\n",
      "    mean_inference_ms: 1.2250265727290743\n",
      "    mean_raw_obs_processing_ms: 0.10349003219888551\n",
      "  time_since_restore: 1004.8227014541626\n",
      "  time_this_iter_s: 5.979740381240845\n",
      "  time_total_s: 1004.8227014541626\n",
      "  timers:\n",
      "    learn_throughput: 1480.121\n",
      "    learn_time_ms: 2702.481\n",
      "    load_throughput: 6054570.913\n",
      "    load_time_ms: 0.661\n",
      "    sample_throughput: 666.855\n",
      "    sample_time_ms: 5998.306\n",
      "    update_time_ms: 2.87\n",
      "  timestamp: 1636042258\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 656000\n",
      "  training_iteration: 164\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:10:58 (running for 00:17:05.22)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   164</td><td style=\"text-align: right;\">         1004.82</td><td style=\"text-align: right;\">656000</td><td style=\"text-align: right;\">-591.668</td><td style=\"text-align: right;\">            -368.759</td><td style=\"text-align: right;\">            -1692.03</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:11:03 (running for 00:17:10.23)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   164</td><td style=\"text-align: right;\">         1004.82</td><td style=\"text-align: right;\">656000</td><td style=\"text-align: right;\">-591.668</td><td style=\"text-align: right;\">            -368.759</td><td style=\"text-align: right;\">            -1692.03</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 660000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-11-04\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -368.75881563461763\n",
      "  episode_reward_mean: -584.0419579175037\n",
      "  episode_reward_min: -1692.0274426699448\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3300\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.8002157807350159\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010471373796463013\n",
      "          model: {}\n",
      "          policy_loss: -0.00035625952295958996\n",
      "          total_loss: 2532.674560546875\n",
      "          vf_explained_var: 0.6995998620986938\n",
      "          vf_loss: 2532.66943359375\n",
      "    num_agent_steps_sampled: 660000\n",
      "    num_agent_steps_trained: 660000\n",
      "    num_steps_sampled: 660000\n",
      "    num_steps_trained: 660000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 165\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.799999999999999\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12093000384929031\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1780366380682502\n",
      "    mean_inference_ms: 1.2248004243210668\n",
      "    mean_raw_obs_processing_ms: 0.10347290672652526\n",
      "  time_since_restore: 1010.7850239276886\n",
      "  time_this_iter_s: 5.962322473526001\n",
      "  time_total_s: 1010.7850239276886\n",
      "  timers:\n",
      "    learn_throughput: 1479.605\n",
      "    learn_time_ms: 2703.425\n",
      "    load_throughput: 6089733.575\n",
      "    load_time_ms: 0.657\n",
      "    sample_throughput: 666.945\n",
      "    sample_time_ms: 5997.501\n",
      "    update_time_ms: 2.877\n",
      "  timestamp: 1636042264\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 660000\n",
      "  training_iteration: 165\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:11:08 (running for 00:17:15.23)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   165</td><td style=\"text-align: right;\">         1010.79</td><td style=\"text-align: right;\">660000</td><td style=\"text-align: right;\">-584.042</td><td style=\"text-align: right;\">            -368.759</td><td style=\"text-align: right;\">            -1692.03</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 664000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-11-10\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -254.80499286657442\n",
      "  episode_reward_mean: -585.3582760371995\n",
      "  episode_reward_min: -1692.0274426699448\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3320\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: -0.03128136694431305\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02190951257944107\n",
      "          model: {}\n",
      "          policy_loss: 0.0031356029212474823\n",
      "          total_loss: 3300.310546875\n",
      "          vf_explained_var: 0.39315253496170044\n",
      "          vf_loss: 3300.296142578125\n",
      "    num_agent_steps_sampled: 664000\n",
      "    num_agent_steps_trained: 664000\n",
      "    num_steps_sampled: 664000\n",
      "    num_steps_trained: 664000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 166\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.6625\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12091405133242279\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1780079252105884\n",
      "    mean_inference_ms: 1.2245707509461667\n",
      "    mean_raw_obs_processing_ms: 0.10345562578831057\n",
      "  time_since_restore: 1016.7598855495453\n",
      "  time_this_iter_s: 5.9748616218566895\n",
      "  time_total_s: 1016.7598855495453\n",
      "  timers:\n",
      "    learn_throughput: 1479.524\n",
      "    learn_time_ms: 2703.573\n",
      "    load_throughput: 6095486.121\n",
      "    load_time_ms: 0.656\n",
      "    sample_throughput: 666.513\n",
      "    sample_time_ms: 6001.381\n",
      "    update_time_ms: 2.871\n",
      "  timestamp: 1636042270\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 664000\n",
      "  training_iteration: 166\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:11:13 (running for 00:17:20.25)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   166</td><td style=\"text-align: right;\">         1016.76</td><td style=\"text-align: right;\">664000</td><td style=\"text-align: right;\">-585.358</td><td style=\"text-align: right;\">            -254.805</td><td style=\"text-align: right;\">            -1692.03</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 668000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-11-16\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -254.80499286657442\n",
      "  episode_reward_mean: -617.8691919669076\n",
      "  episode_reward_min: -1692.0274426699448\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3340\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.8535326719284058\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009701601229608059\n",
      "          model: {}\n",
      "          policy_loss: -0.0066779362969100475\n",
      "          total_loss: 4203.3369140625\n",
      "          vf_explained_var: 0.5971782207489014\n",
      "          vf_loss: 4203.33642578125\n",
      "    num_agent_steps_sampled: 668000\n",
      "    num_agent_steps_trained: 668000\n",
      "    num_steps_sampled: 668000\n",
      "    num_steps_trained: 668000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 167\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.799999999999999\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12089931818340281\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17798145370497\n",
      "    mean_inference_ms: 1.2243492362952162\n",
      "    mean_raw_obs_processing_ms: 0.10343922104084256\n",
      "  time_since_restore: 1022.7349410057068\n",
      "  time_this_iter_s: 5.975055456161499\n",
      "  time_total_s: 1022.7349410057068\n",
      "  timers:\n",
      "    learn_throughput: 1479.22\n",
      "    learn_time_ms: 2704.128\n",
      "    load_throughput: 6108357.97\n",
      "    load_time_ms: 0.655\n",
      "    sample_throughput: 666.341\n",
      "    sample_time_ms: 6002.933\n",
      "    update_time_ms: 2.879\n",
      "  timestamp: 1636042276\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 668000\n",
      "  training_iteration: 167\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:11:18 (running for 00:17:25.27)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   167</td><td style=\"text-align: right;\">         1022.73</td><td style=\"text-align: right;\">668000</td><td style=\"text-align: right;\">-617.869</td><td style=\"text-align: right;\">            -254.805</td><td style=\"text-align: right;\">            -1692.03</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 672000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-11-22\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -254.80499286657442\n",
      "  episode_reward_mean: -601.3505987487321\n",
      "  episode_reward_min: -1682.3951042468577\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3360\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.45359253883361816\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006253913044929504\n",
      "          model: {}\n",
      "          policy_loss: -0.00017405427934136242\n",
      "          total_loss: 3692.98583984375\n",
      "          vf_explained_var: 0.5796443223953247\n",
      "          vf_loss: 3692.981201171875\n",
      "    num_agent_steps_sampled: 672000\n",
      "    num_agent_steps_trained: 672000\n",
      "    num_steps_sampled: 672000\n",
      "    num_steps_trained: 672000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 168\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.2625\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12088743915543013\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17796056617053332\n",
      "    mean_inference_ms: 1.2241785549312132\n",
      "    mean_raw_obs_processing_ms: 0.10342556921912072\n",
      "  time_since_restore: 1028.758343219757\n",
      "  time_this_iter_s: 6.023402214050293\n",
      "  time_total_s: 1028.758343219757\n",
      "  timers:\n",
      "    learn_throughput: 1480.978\n",
      "    learn_time_ms: 2700.918\n",
      "    load_throughput: 6123965.542\n",
      "    load_time_ms: 0.653\n",
      "    sample_throughput: 664.964\n",
      "    sample_time_ms: 6015.363\n",
      "    update_time_ms: 2.875\n",
      "  timestamp: 1636042282\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 672000\n",
      "  training_iteration: 168\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:11:23 (running for 00:17:30.35)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   168</td><td style=\"text-align: right;\">         1028.76</td><td style=\"text-align: right;\">672000</td><td style=\"text-align: right;\">-601.351</td><td style=\"text-align: right;\">            -254.805</td><td style=\"text-align: right;\">             -1682.4</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 676000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-11-28\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -254.80499286657442\n",
      "  episode_reward_mean: -589.5425295860515\n",
      "  episode_reward_min: -1682.3951042468577\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3380\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.17491360008716583\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009525774978101254\n",
      "          model: {}\n",
      "          policy_loss: -0.0018605896038934588\n",
      "          total_loss: 1678.5567626953125\n",
      "          vf_explained_var: 0.6334872841835022\n",
      "          vf_loss: 1678.5513916015625\n",
      "    num_agent_steps_sampled: 676000\n",
      "    num_agent_steps_trained: 676000\n",
      "    num_steps_sampled: 676000\n",
      "    num_steps_trained: 676000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 169\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.428571428571429\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12087447177900523\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17793800675153215\n",
      "    mean_inference_ms: 1.2239922207524354\n",
      "    mean_raw_obs_processing_ms: 0.10341109286587863\n",
      "  time_since_restore: 1034.6764850616455\n",
      "  time_this_iter_s: 5.918141841888428\n",
      "  time_total_s: 1034.6764850616455\n",
      "  timers:\n",
      "    learn_throughput: 1480.642\n",
      "    learn_time_ms: 2701.531\n",
      "    load_throughput: 6095043.232\n",
      "    load_time_ms: 0.656\n",
      "    sample_throughput: 665.32\n",
      "    sample_time_ms: 6012.146\n",
      "    update_time_ms: 2.874\n",
      "  timestamp: 1636042288\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 676000\n",
      "  training_iteration: 169\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:11:29 (running for 00:17:36.31)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   169</td><td style=\"text-align: right;\">         1034.68</td><td style=\"text-align: right;\">676000</td><td style=\"text-align: right;\">-589.543</td><td style=\"text-align: right;\">            -254.805</td><td style=\"text-align: right;\">             -1682.4</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 680000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-11-34\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -254.80499286657442\n",
      "  episode_reward_mean: -567.8000391475657\n",
      "  episode_reward_min: -1682.3951042468577\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3400\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.2827690839767456\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006783659104257822\n",
      "          model: {}\n",
      "          policy_loss: 0.0015523612964898348\n",
      "          total_loss: 2860.331787109375\n",
      "          vf_explained_var: 0.6014871001243591\n",
      "          vf_loss: 2860.3251953125\n",
      "    num_agent_steps_sampled: 680000\n",
      "    num_agent_steps_trained: 680000\n",
      "    num_steps_sampled: 680000\n",
      "    num_steps_trained: 680000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 170\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.7875\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12086091181147836\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17791460813417168\n",
      "    mean_inference_ms: 1.2237987707978115\n",
      "    mean_raw_obs_processing_ms: 0.10339620311421394\n",
      "  time_since_restore: 1040.628727197647\n",
      "  time_this_iter_s: 5.952242136001587\n",
      "  time_total_s: 1040.628727197647\n",
      "  timers:\n",
      "    learn_throughput: 1481.469\n",
      "    learn_time_ms: 2700.022\n",
      "    load_throughput: 6096593.626\n",
      "    load_time_ms: 0.656\n",
      "    sample_throughput: 665.268\n",
      "    sample_time_ms: 6012.618\n",
      "    update_time_ms: 2.864\n",
      "  timestamp: 1636042294\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 680000\n",
      "  training_iteration: 170\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:11:34 (running for 00:17:41.31)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   170</td><td style=\"text-align: right;\">         1040.63</td><td style=\"text-align: right;\">680000</td><td style=\"text-align: right;\">  -567.8</td><td style=\"text-align: right;\">            -254.805</td><td style=\"text-align: right;\">             -1682.4</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:11:39 (running for 00:17:46.32)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   170</td><td style=\"text-align: right;\">         1040.63</td><td style=\"text-align: right;\">680000</td><td style=\"text-align: right;\">  -567.8</td><td style=\"text-align: right;\">            -254.805</td><td style=\"text-align: right;\">             -1682.4</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 684000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-11-40\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -252.00457161790138\n",
      "  episode_reward_mean: -569.8315016767809\n",
      "  episode_reward_min: -1682.3951042468577\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3420\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.5653454065322876\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010171270929276943\n",
      "          model: {}\n",
      "          policy_loss: -0.007200875785201788\n",
      "          total_loss: 2493.1337890625\n",
      "          vf_explained_var: 0.5516493916511536\n",
      "          vf_loss: 2493.13330078125\n",
      "    num_agent_steps_sampled: 684000\n",
      "    num_agent_steps_trained: 684000\n",
      "    num_steps_sampled: 684000\n",
      "    num_steps_trained: 684000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 171\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.528571428571428\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1208469104829285\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.177889749496123\n",
      "    mean_inference_ms: 1.2235993619930259\n",
      "    mean_raw_obs_processing_ms: 0.10338075662046309\n",
      "  time_since_restore: 1046.559100151062\n",
      "  time_this_iter_s: 5.930372953414917\n",
      "  time_total_s: 1046.559100151062\n",
      "  timers:\n",
      "    learn_throughput: 1482.544\n",
      "    learn_time_ms: 2698.065\n",
      "    load_throughput: 6100140.348\n",
      "    load_time_ms: 0.656\n",
      "    sample_throughput: 665.523\n",
      "    sample_time_ms: 6010.307\n",
      "    update_time_ms: 2.858\n",
      "  timestamp: 1636042300\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 684000\n",
      "  training_iteration: 171\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:11:45 (running for 00:17:52.29)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   171</td><td style=\"text-align: right;\">         1046.56</td><td style=\"text-align: right;\">684000</td><td style=\"text-align: right;\">-569.832</td><td style=\"text-align: right;\">            -252.005</td><td style=\"text-align: right;\">             -1682.4</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 688000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-11-46\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -252.00457161790138\n",
      "  episode_reward_mean: -542.2228020182465\n",
      "  episode_reward_min: -1682.3951042468577\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3440\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.19768165051937103\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010147913359105587\n",
      "          model: {}\n",
      "          policy_loss: -0.006458392832428217\n",
      "          total_loss: 2209.268310546875\n",
      "          vf_explained_var: 0.5266373157501221\n",
      "          vf_loss: 2209.26708984375\n",
      "    num_agent_steps_sampled: 688000\n",
      "    num_agent_steps_trained: 688000\n",
      "    num_steps_sampled: 688000\n",
      "    num_steps_trained: 688000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 172\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.575\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12083300563237966\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1778650246030924\n",
      "    mean_inference_ms: 1.2234057386277906\n",
      "    mean_raw_obs_processing_ms: 0.10336596682839871\n",
      "  time_since_restore: 1052.5295226573944\n",
      "  time_this_iter_s: 5.9704225063323975\n",
      "  time_total_s: 1052.5295226573944\n",
      "  timers:\n",
      "    learn_throughput: 1483.752\n",
      "    learn_time_ms: 2695.868\n",
      "    load_throughput: 6004300.336\n",
      "    load_time_ms: 0.666\n",
      "    sample_throughput: 665.355\n",
      "    sample_time_ms: 6011.826\n",
      "    update_time_ms: 2.839\n",
      "  timestamp: 1636042306\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 688000\n",
      "  training_iteration: 172\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:11:50 (running for 00:17:57.31)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   172</td><td style=\"text-align: right;\">         1052.53</td><td style=\"text-align: right;\">688000</td><td style=\"text-align: right;\">-542.223</td><td style=\"text-align: right;\">            -252.005</td><td style=\"text-align: right;\">             -1682.4</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 692000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-11-52\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -252.00457161790138\n",
      "  episode_reward_mean: -537.8814082246969\n",
      "  episode_reward_min: -1601.9574365461795\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3460\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.23721927404403687\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009711413644254208\n",
      "          model: {}\n",
      "          policy_loss: 0.001301221433095634\n",
      "          total_loss: 2308.31640625\n",
      "          vf_explained_var: 0.5213578939437866\n",
      "          vf_loss: 2308.307861328125\n",
      "    num_agent_steps_sampled: 692000\n",
      "    num_agent_steps_trained: 692000\n",
      "    num_steps_sampled: 692000\n",
      "    num_steps_trained: 692000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 173\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.785714285714286\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12081686904321698\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1778363868151243\n",
      "    mean_inference_ms: 1.223168410154583\n",
      "    mean_raw_obs_processing_ms: 0.10334872881422247\n",
      "  time_since_restore: 1058.4693541526794\n",
      "  time_this_iter_s: 5.939831495285034\n",
      "  time_total_s: 1058.4693541526794\n",
      "  timers:\n",
      "    learn_throughput: 1483.369\n",
      "    learn_time_ms: 2696.565\n",
      "    load_throughput: 6006449.95\n",
      "    load_time_ms: 0.666\n",
      "    sample_throughput: 665.656\n",
      "    sample_time_ms: 6009.11\n",
      "    update_time_ms: 2.82\n",
      "  timestamp: 1636042312\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 692000\n",
      "  training_iteration: 173\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:11:56 (running for 00:18:03.29)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   173</td><td style=\"text-align: right;\">         1058.47</td><td style=\"text-align: right;\">692000</td><td style=\"text-align: right;\">-537.881</td><td style=\"text-align: right;\">            -252.005</td><td style=\"text-align: right;\">            -1601.96</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 696000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-11-58\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -252.00457161790138\n",
      "  episode_reward_mean: -530.0362315138187\n",
      "  episode_reward_min: -1601.9574365461795\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3480\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: -0.026844430714845657\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00831444002687931\n",
      "          model: {}\n",
      "          policy_loss: 0.0033838890958577394\n",
      "          total_loss: 1449.3900146484375\n",
      "          vf_explained_var: 0.708617627620697\n",
      "          vf_loss: 1449.38037109375\n",
      "    num_agent_steps_sampled: 696000\n",
      "    num_agent_steps_trained: 696000\n",
      "    num_steps_sampled: 696000\n",
      "    num_steps_trained: 696000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 174\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.625\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12080261700703675\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17781117562819326\n",
      "    mean_inference_ms: 1.2229486285787188\n",
      "    mean_raw_obs_processing_ms: 0.10333290386833628\n",
      "  time_since_restore: 1064.444531917572\n",
      "  time_this_iter_s: 5.975177764892578\n",
      "  time_total_s: 1064.444531917572\n",
      "  timers:\n",
      "    learn_throughput: 1483.581\n",
      "    learn_time_ms: 2696.179\n",
      "    load_throughput: 6020027.988\n",
      "    load_time_ms: 0.664\n",
      "    sample_throughput: 665.626\n",
      "    sample_time_ms: 6009.38\n",
      "    update_time_ms: 2.816\n",
      "  timestamp: 1636042318\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 696000\n",
      "  training_iteration: 174\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:12:01 (running for 00:18:08.31)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   174</td><td style=\"text-align: right;\">         1064.44</td><td style=\"text-align: right;\">696000</td><td style=\"text-align: right;\">-530.036</td><td style=\"text-align: right;\">            -252.005</td><td style=\"text-align: right;\">            -1601.96</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 700000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-12-04\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -252.00457161790138\n",
      "  episode_reward_mean: -537.1079460551829\n",
      "  episode_reward_min: -1665.0504512465577\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3500\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.32257717847824097\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008313638158142567\n",
      "          model: {}\n",
      "          policy_loss: 0.01970001310110092\n",
      "          total_loss: 2781.85205078125\n",
      "          vf_explained_var: 0.5734790563583374\n",
      "          vf_loss: 2781.826416015625\n",
      "    num_agent_steps_sampled: 700000\n",
      "    num_agent_steps_trained: 700000\n",
      "    num_steps_sampled: 700000\n",
      "    num_steps_trained: 700000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 175\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.27142857142857\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12078888625908071\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17778639338204966\n",
      "    mean_inference_ms: 1.2227348952271315\n",
      "    mean_raw_obs_processing_ms: 0.10331700025675365\n",
      "  time_since_restore: 1070.4253132343292\n",
      "  time_this_iter_s: 5.980781316757202\n",
      "  time_total_s: 1070.4253132343292\n",
      "  timers:\n",
      "    learn_throughput: 1484.028\n",
      "    learn_time_ms: 2695.368\n",
      "    load_throughput: 6012477.064\n",
      "    load_time_ms: 0.665\n",
      "    sample_throughput: 665.378\n",
      "    sample_time_ms: 6011.621\n",
      "    update_time_ms: 2.804\n",
      "  timestamp: 1636042324\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 700000\n",
      "  training_iteration: 175\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:12:06 (running for 00:18:13.34)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   175</td><td style=\"text-align: right;\">         1070.43</td><td style=\"text-align: right;\">700000</td><td style=\"text-align: right;\">-537.108</td><td style=\"text-align: right;\">            -252.005</td><td style=\"text-align: right;\">            -1665.05</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 704000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-12-10\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -253.1389661084355\n",
      "  episode_reward_mean: -521.4335776475781\n",
      "  episode_reward_min: -1665.0504512465577\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3520\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.17359666526317596\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008613206446170807\n",
      "          model: {}\n",
      "          policy_loss: 0.0014949064934626222\n",
      "          total_loss: 2343.221923828125\n",
      "          vf_explained_var: 0.6282879114151001\n",
      "          vf_loss: 2343.2138671875\n",
      "    num_agent_steps_sampled: 704000\n",
      "    num_agent_steps_trained: 704000\n",
      "    num_steps_sampled: 704000\n",
      "    num_steps_trained: 704000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 176\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.425\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12077622678750544\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17776325053583839\n",
      "    mean_inference_ms: 1.222532052678247\n",
      "    mean_raw_obs_processing_ms: 0.10330292320685469\n",
      "  time_since_restore: 1076.3762278556824\n",
      "  time_this_iter_s: 5.950914621353149\n",
      "  time_total_s: 1076.3762278556824\n",
      "  timers:\n",
      "    learn_throughput: 1483.59\n",
      "    learn_time_ms: 2696.162\n",
      "    load_throughput: 6060476.104\n",
      "    load_time_ms: 0.66\n",
      "    sample_throughput: 665.813\n",
      "    sample_time_ms: 6007.693\n",
      "    update_time_ms: 2.829\n",
      "  timestamp: 1636042330\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 704000\n",
      "  training_iteration: 176\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:12:12 (running for 00:18:19.34)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   176</td><td style=\"text-align: right;\">         1076.38</td><td style=\"text-align: right;\">704000</td><td style=\"text-align: right;\">-521.434</td><td style=\"text-align: right;\">            -253.139</td><td style=\"text-align: right;\">            -1665.05</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 708000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-12-16\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -246.70799952950554\n",
      "  episode_reward_mean: -547.8800747797608\n",
      "  episode_reward_min: -1756.9991556065984\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3540\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3416823148727417\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007110418286174536\n",
      "          model: {}\n",
      "          policy_loss: -0.002079152502119541\n",
      "          total_loss: 4536.7822265625\n",
      "          vf_explained_var: 0.6357500553131104\n",
      "          vf_loss: 4536.7783203125\n",
      "    num_agent_steps_sampled: 708000\n",
      "    num_agent_steps_trained: 708000\n",
      "    num_steps_sampled: 708000\n",
      "    num_steps_trained: 708000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 177\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.375\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12076305161927081\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17773885368587997\n",
      "    mean_inference_ms: 1.2223215894017994\n",
      "    mean_raw_obs_processing_ms: 0.10328812887385892\n",
      "  time_since_restore: 1082.344689130783\n",
      "  time_this_iter_s: 5.968461275100708\n",
      "  time_total_s: 1082.344689130783\n",
      "  timers:\n",
      "    learn_throughput: 1484.898\n",
      "    learn_time_ms: 2693.788\n",
      "    load_throughput: 6067928.677\n",
      "    load_time_ms: 0.659\n",
      "    sample_throughput: 665.604\n",
      "    sample_time_ms: 6009.578\n",
      "    update_time_ms: 2.821\n",
      "  timestamp: 1636042336\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 708000\n",
      "  training_iteration: 177\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:12:17 (running for 00:18:24.44)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   177</td><td style=\"text-align: right;\">         1082.34</td><td style=\"text-align: right;\">708000</td><td style=\"text-align: right;\"> -547.88</td><td style=\"text-align: right;\">            -246.708</td><td style=\"text-align: right;\">               -1757</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 712000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-12-22\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -241.6192484567856\n",
      "  episode_reward_mean: -547.006386656613\n",
      "  episode_reward_min: -1756.9991556065984\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3560\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.27533015608787537\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012874008156359196\n",
      "          model: {}\n",
      "          policy_loss: -0.0038667735643684864\n",
      "          total_loss: 2770.6884765625\n",
      "          vf_explained_var: 0.4957048296928406\n",
      "          vf_loss: 2770.6826171875\n",
      "    num_agent_steps_sampled: 712000\n",
      "    num_agent_steps_trained: 712000\n",
      "    num_steps_sampled: 712000\n",
      "    num_steps_trained: 712000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 178\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.514285714285714\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12075022624983331\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1777141143064997\n",
      "    mean_inference_ms: 1.2221182532651087\n",
      "    mean_raw_obs_processing_ms: 0.10327395984026659\n",
      "  time_since_restore: 1088.3168590068817\n",
      "  time_this_iter_s: 5.972169876098633\n",
      "  time_total_s: 1088.3168590068817\n",
      "  timers:\n",
      "    learn_throughput: 1483.674\n",
      "    learn_time_ms: 2696.01\n",
      "    load_throughput: 6058506.428\n",
      "    load_time_ms: 0.66\n",
      "    sample_throughput: 665.653\n",
      "    sample_time_ms: 6009.135\n",
      "    update_time_ms: 2.833\n",
      "  timestamp: 1636042342\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 712000\n",
      "  training_iteration: 178\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:12:22 (running for 00:18:29.46)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   178</td><td style=\"text-align: right;\">         1088.32</td><td style=\"text-align: right;\">712000</td><td style=\"text-align: right;\">-547.006</td><td style=\"text-align: right;\">            -241.619</td><td style=\"text-align: right;\">               -1757</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:12:27 (running for 00:18:34.47)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   178</td><td style=\"text-align: right;\">         1088.32</td><td style=\"text-align: right;\">712000</td><td style=\"text-align: right;\">-547.006</td><td style=\"text-align: right;\">            -241.619</td><td style=\"text-align: right;\">               -1757</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 716000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-12-28\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -129.30024063465348\n",
      "  episode_reward_mean: -543.5406286582592\n",
      "  episode_reward_min: -1760.0863460514643\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3580\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.4782390296459198\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007978970184922218\n",
      "          model: {}\n",
      "          policy_loss: -0.006603800226002932\n",
      "          total_loss: 3420.816162109375\n",
      "          vf_explained_var: 0.5231772661209106\n",
      "          vf_loss: 3420.816650390625\n",
      "    num_agent_steps_sampled: 716000\n",
      "    num_agent_steps_trained: 716000\n",
      "    num_steps_sampled: 716000\n",
      "    num_steps_trained: 716000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 179\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.45\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12073528070219357\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1776860082516941\n",
      "    mean_inference_ms: 1.2218990825950922\n",
      "    mean_raw_obs_processing_ms: 0.1032585210865642\n",
      "  time_since_restore: 1094.2530348300934\n",
      "  time_this_iter_s: 5.93617582321167\n",
      "  time_total_s: 1094.2530348300934\n",
      "  timers:\n",
      "    learn_throughput: 1482.568\n",
      "    learn_time_ms: 2698.021\n",
      "    load_throughput: 6079802.863\n",
      "    load_time_ms: 0.658\n",
      "    sample_throughput: 665.467\n",
      "    sample_time_ms: 6010.812\n",
      "    update_time_ms: 2.85\n",
      "  timestamp: 1636042348\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 716000\n",
      "  training_iteration: 179\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:12:33 (running for 00:18:40.44)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   179</td><td style=\"text-align: right;\">         1094.25</td><td style=\"text-align: right;\">716000</td><td style=\"text-align: right;\">-543.541</td><td style=\"text-align: right;\">              -129.3</td><td style=\"text-align: right;\">            -1760.09</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 720000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-12-34\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.8555237473513831\n",
      "  episode_reward_mean: -506.2384852340151\n",
      "  episode_reward_min: -1760.0863460514643\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3600\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: -0.2689398229122162\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012919574044644833\n",
      "          model: {}\n",
      "          policy_loss: -0.011692686937749386\n",
      "          total_loss: 2702.926513671875\n",
      "          vf_explained_var: 0.36142203211784363\n",
      "          vf_loss: 2702.928466796875\n",
      "    num_agent_steps_sampled: 720000\n",
      "    num_agent_steps_trained: 720000\n",
      "    num_steps_sampled: 720000\n",
      "    num_steps_trained: 720000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 180\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.885714285714288\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12071822456644789\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1776556317181993\n",
      "    mean_inference_ms: 1.2216617139171222\n",
      "    mean_raw_obs_processing_ms: 0.10324219606577273\n",
      "  time_since_restore: 1100.1571006774902\n",
      "  time_this_iter_s: 5.904065847396851\n",
      "  time_total_s: 1100.1571006774902\n",
      "  timers:\n",
      "    learn_throughput: 1482.178\n",
      "    learn_time_ms: 2698.732\n",
      "    load_throughput: 6085978.162\n",
      "    load_time_ms: 0.657\n",
      "    sample_throughput: 665.825\n",
      "    sample_time_ms: 6007.585\n",
      "    update_time_ms: 2.85\n",
      "  timestamp: 1636042354\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 720000\n",
      "  training_iteration: 180\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:12:39 (running for 00:18:46.39)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   180</td><td style=\"text-align: right;\">         1100.16</td><td style=\"text-align: right;\">720000</td><td style=\"text-align: right;\">-506.238</td><td style=\"text-align: right;\">           -0.855524</td><td style=\"text-align: right;\">            -1760.09</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 724000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-12-40\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.8555237473513831\n",
      "  episode_reward_mean: -471.175223689569\n",
      "  episode_reward_min: -1760.0863460514643\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3620\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: -0.3498070538043976\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009438807144761086\n",
      "          model: {}\n",
      "          policy_loss: 0.002510195830836892\n",
      "          total_loss: 1873.7198486328125\n",
      "          vf_explained_var: 0.34730613231658936\n",
      "          vf_loss: 1873.7103271484375\n",
      "    num_agent_steps_sampled: 724000\n",
      "    num_agent_steps_trained: 724000\n",
      "    num_steps_sampled: 724000\n",
      "    num_steps_trained: 724000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 181\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 9.212499999999999\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12070168543823755\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17762566706524197\n",
      "    mean_inference_ms: 1.2214443927921268\n",
      "    mean_raw_obs_processing_ms: 0.10322556726478752\n",
      "  time_since_restore: 1106.1341862678528\n",
      "  time_this_iter_s: 5.977085590362549\n",
      "  time_total_s: 1106.1341862678528\n",
      "  timers:\n",
      "    learn_throughput: 1481.568\n",
      "    learn_time_ms: 2699.842\n",
      "    load_throughput: 6039532.021\n",
      "    load_time_ms: 0.662\n",
      "    sample_throughput: 665.357\n",
      "    sample_time_ms: 6011.808\n",
      "    update_time_ms: 2.863\n",
      "  timestamp: 1636042360\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 724000\n",
      "  training_iteration: 181\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:12:44 (running for 00:18:51.43)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   181</td><td style=\"text-align: right;\">         1106.13</td><td style=\"text-align: right;\">724000</td><td style=\"text-align: right;\">-471.175</td><td style=\"text-align: right;\">           -0.855524</td><td style=\"text-align: right;\">            -1760.09</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 728000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-12-46\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.8555237473513831\n",
      "  episode_reward_mean: -409.59104511356696\n",
      "  episode_reward_min: -1760.0863460514643\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3640\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: -0.37085697054862976\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014089055359363556\n",
      "          model: {}\n",
      "          policy_loss: -0.01223905012011528\n",
      "          total_loss: 2121.70361328125\n",
      "          vf_explained_var: 0.39421504735946655\n",
      "          vf_loss: 2121.705322265625\n",
      "    num_agent_steps_sampled: 728000\n",
      "    num_agent_steps_trained: 728000\n",
      "    num_steps_sampled: 728000\n",
      "    num_steps_trained: 728000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 182\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.428571428571429\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12068570244910731\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17759689423513528\n",
      "    mean_inference_ms: 1.221236604341596\n",
      "    mean_raw_obs_processing_ms: 0.1032097871251589\n",
      "  time_since_restore: 1112.1012794971466\n",
      "  time_this_iter_s: 5.967093229293823\n",
      "  time_total_s: 1112.1012794971466\n",
      "  timers:\n",
      "    learn_throughput: 1481.58\n",
      "    learn_time_ms: 2699.82\n",
      "    load_throughput: 6101249.545\n",
      "    load_time_ms: 0.656\n",
      "    sample_throughput: 665.15\n",
      "    sample_time_ms: 6013.681\n",
      "    update_time_ms: 2.871\n",
      "  timestamp: 1636042366\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 728000\n",
      "  training_iteration: 182\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:12:49 (running for 00:18:56.44)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   182</td><td style=\"text-align: right;\">          1112.1</td><td style=\"text-align: right;\">728000</td><td style=\"text-align: right;\">-409.591</td><td style=\"text-align: right;\">           -0.855524</td><td style=\"text-align: right;\">            -1760.09</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 732000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-12-52\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.8555237473513831\n",
      "  episode_reward_mean: -395.3585051485859\n",
      "  episode_reward_min: -1760.0863460514643\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3660\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.3383268713951111\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008069750852882862\n",
      "          model: {}\n",
      "          policy_loss: -0.010446510277688503\n",
      "          total_loss: 3638.688232421875\n",
      "          vf_explained_var: 0.46417611837387085\n",
      "          vf_loss: 3638.6923828125\n",
      "    num_agent_steps_sampled: 732000\n",
      "    num_agent_steps_trained: 732000\n",
      "    num_steps_sampled: 732000\n",
      "    num_steps_trained: 732000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 183\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.537500000000001\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12066906921312123\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1775670162260546\n",
      "    mean_inference_ms: 1.2210193628189394\n",
      "    mean_raw_obs_processing_ms: 0.10319385743685018\n",
      "  time_since_restore: 1118.0171346664429\n",
      "  time_this_iter_s: 5.915855169296265\n",
      "  time_total_s: 1118.0171346664429\n",
      "  timers:\n",
      "    learn_throughput: 1481.571\n",
      "    learn_time_ms: 2699.836\n",
      "    load_throughput: 6091281.269\n",
      "    load_time_ms: 0.657\n",
      "    sample_throughput: 665.41\n",
      "    sample_time_ms: 6011.33\n",
      "    update_time_ms: 2.875\n",
      "  timestamp: 1636042372\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 732000\n",
      "  training_iteration: 183\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:12:55 (running for 00:19:02.43)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   183</td><td style=\"text-align: right;\">         1118.02</td><td style=\"text-align: right;\">732000</td><td style=\"text-align: right;\">-395.359</td><td style=\"text-align: right;\">           -0.855524</td><td style=\"text-align: right;\">            -1760.09</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 736000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-12-58\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.8555237473513831\n",
      "  episode_reward_mean: -384.52100397609934\n",
      "  episode_reward_min: -1768.0846289288515\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3680\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.4438609182834625\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008810353465378284\n",
      "          model: {}\n",
      "          policy_loss: -0.0194478128105402\n",
      "          total_loss: 3100.072998046875\n",
      "          vf_explained_var: 0.5342960357666016\n",
      "          vf_loss: 3100.0859375\n",
      "    num_agent_steps_sampled: 736000\n",
      "    num_agent_steps_trained: 736000\n",
      "    num_steps_sampled: 736000\n",
      "    num_steps_trained: 736000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 184\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.371428571428572\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12065363686751038\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17753851698040585\n",
      "    mean_inference_ms: 1.2208175382638826\n",
      "    mean_raw_obs_processing_ms: 0.10317836024259033\n",
      "  time_since_restore: 1123.958325624466\n",
      "  time_this_iter_s: 5.941190958023071\n",
      "  time_total_s: 1123.958325624466\n",
      "  timers:\n",
      "    learn_throughput: 1481.471\n",
      "    learn_time_ms: 2700.02\n",
      "    load_throughput: 6094157.646\n",
      "    load_time_ms: 0.656\n",
      "    sample_throughput: 665.553\n",
      "    sample_time_ms: 6010.035\n",
      "    update_time_ms: 2.903\n",
      "  timestamp: 1636042378\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 736000\n",
      "  training_iteration: 184\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:13:01 (running for 00:19:08.42)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   184</td><td style=\"text-align: right;\">         1123.96</td><td style=\"text-align: right;\">736000</td><td style=\"text-align: right;\">-384.521</td><td style=\"text-align: right;\">           -0.855524</td><td style=\"text-align: right;\">            -1768.08</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 740000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-13-04\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.8585211716306329\n",
      "  episode_reward_mean: -365.1988453978397\n",
      "  episode_reward_min: -1768.0846289288515\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3700\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: -0.4045623540878296\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010796644724905491\n",
      "          model: {}\n",
      "          policy_loss: -0.015446013770997524\n",
      "          total_loss: 412.366943359375\n",
      "          vf_explained_var: 0.4167872965335846\n",
      "          vf_loss: 412.37420654296875\n",
      "    num_agent_steps_sampled: 740000\n",
      "    num_agent_steps_trained: 740000\n",
      "    num_steps_sampled: 740000\n",
      "    num_steps_trained: 740000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 185\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.5625\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12064242975027088\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17751488707411384\n",
      "    mean_inference_ms: 1.2206589024873562\n",
      "    mean_raw_obs_processing_ms: 0.10316588305657995\n",
      "  time_since_restore: 1129.992663383484\n",
      "  time_this_iter_s: 6.034337759017944\n",
      "  time_total_s: 1129.992663383484\n",
      "  timers:\n",
      "    learn_throughput: 1480.826\n",
      "    learn_time_ms: 2701.195\n",
      "    load_throughput: 6023486.159\n",
      "    load_time_ms: 0.664\n",
      "    sample_throughput: 665.036\n",
      "    sample_time_ms: 6014.709\n",
      "    update_time_ms: 2.917\n",
      "  timestamp: 1636042384\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 740000\n",
      "  training_iteration: 185\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:13:06 (running for 00:19:13.50)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   185</td><td style=\"text-align: right;\">         1129.99</td><td style=\"text-align: right;\">740000</td><td style=\"text-align: right;\">-365.199</td><td style=\"text-align: right;\">           -0.858521</td><td style=\"text-align: right;\">            -1768.08</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 744000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-13-10\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.8585211716306329\n",
      "  episode_reward_mean: -387.80982711764693\n",
      "  episode_reward_min: -1768.0846289288515\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3720\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.5858438014984131\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008788196370005608\n",
      "          model: {}\n",
      "          policy_loss: -0.016501285135746002\n",
      "          total_loss: 2798.126953125\n",
      "          vf_explained_var: 0.5572191476821899\n",
      "          vf_loss: 2798.13671875\n",
      "    num_agent_steps_sampled: 744000\n",
      "    num_agent_steps_trained: 744000\n",
      "    num_steps_sampled: 744000\n",
      "    num_steps_trained: 744000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 186\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.442857142857141\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12063085221640486\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17749117686538285\n",
      "    mean_inference_ms: 1.220488173429916\n",
      "    mean_raw_obs_processing_ms: 0.10315350605813689\n",
      "  time_since_restore: 1135.960212945938\n",
      "  time_this_iter_s: 5.967549562454224\n",
      "  time_total_s: 1135.960212945938\n",
      "  timers:\n",
      "    learn_throughput: 1481.327\n",
      "    learn_time_ms: 2700.282\n",
      "    load_throughput: 5978198.404\n",
      "    load_time_ms: 0.669\n",
      "    sample_throughput: 664.637\n",
      "    sample_time_ms: 6018.326\n",
      "    update_time_ms: 2.902\n",
      "  timestamp: 1636042390\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 744000\n",
      "  training_iteration: 186\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:13:11 (running for 00:19:18.51)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   186</td><td style=\"text-align: right;\">         1135.96</td><td style=\"text-align: right;\">744000</td><td style=\"text-align: right;\"> -387.81</td><td style=\"text-align: right;\">           -0.858521</td><td style=\"text-align: right;\">            -1768.08</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 748000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-13-16\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.8585211716306329\n",
      "  episode_reward_mean: -366.8019060365221\n",
      "  episode_reward_min: -1768.0846289288515\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3740\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: -0.43946391344070435\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009260025806725025\n",
      "          model: {}\n",
      "          policy_loss: -0.01933075487613678\n",
      "          total_loss: 339.575927734375\n",
      "          vf_explained_var: 0.39525678753852844\n",
      "          vf_loss: 339.5882263183594\n",
      "    num_agent_steps_sampled: 748000\n",
      "    num_agent_steps_trained: 748000\n",
      "    num_steps_sampled: 748000\n",
      "    num_steps_trained: 748000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 187\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.549999999999999\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12062004592483742\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17746787511500867\n",
      "    mean_inference_ms: 1.2203215587115306\n",
      "    mean_raw_obs_processing_ms: 0.10314110306023803\n",
      "  time_since_restore: 1141.9398574829102\n",
      "  time_this_iter_s: 5.979644536972046\n",
      "  time_total_s: 1141.9398574829102\n",
      "  timers:\n",
      "    learn_throughput: 1479.682\n",
      "    learn_time_ms: 2703.283\n",
      "    load_throughput: 5923949.013\n",
      "    load_time_ms: 0.675\n",
      "    sample_throughput: 664.853\n",
      "    sample_time_ms: 6016.37\n",
      "    update_time_ms: 2.913\n",
      "  timestamp: 1636042396\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 748000\n",
      "  training_iteration: 187\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:13:16 (running for 00:19:23.54)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   187</td><td style=\"text-align: right;\">         1141.94</td><td style=\"text-align: right;\">748000</td><td style=\"text-align: right;\">-366.802</td><td style=\"text-align: right;\">           -0.858521</td><td style=\"text-align: right;\">            -1768.08</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:13:21 (running for 00:19:28.55)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   187</td><td style=\"text-align: right;\">         1141.94</td><td style=\"text-align: right;\">748000</td><td style=\"text-align: right;\">-366.802</td><td style=\"text-align: right;\">           -0.858521</td><td style=\"text-align: right;\">            -1768.08</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 752000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-13-22\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.5171798012896391\n",
      "  episode_reward_mean: -358.31491710675107\n",
      "  episode_reward_min: -1768.0846289288515\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3760\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.449172705411911\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010187642648816109\n",
      "          model: {}\n",
      "          policy_loss: -0.016500525176525116\n",
      "          total_loss: 2426.0849609375\n",
      "          vf_explained_var: 0.3867560923099518\n",
      "          vf_loss: 2426.09375\n",
      "    num_agent_steps_sampled: 752000\n",
      "    num_agent_steps_trained: 752000\n",
      "    num_steps_sampled: 752000\n",
      "    num_steps_trained: 752000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 188\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.32857142857143\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12061090351244093\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17744811480835082\n",
      "    mean_inference_ms: 1.220179587090908\n",
      "    mean_raw_obs_processing_ms: 0.1031301126683213\n",
      "  time_since_restore: 1147.9122216701508\n",
      "  time_this_iter_s: 5.972364187240601\n",
      "  time_total_s: 1147.9122216701508\n",
      "  timers:\n",
      "    learn_throughput: 1480.602\n",
      "    learn_time_ms: 2701.603\n",
      "    load_throughput: 5848980.616\n",
      "    load_time_ms: 0.684\n",
      "    sample_throughput: 665.351\n",
      "    sample_time_ms: 6011.864\n",
      "    update_time_ms: 2.917\n",
      "  timestamp: 1636042402\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 752000\n",
      "  training_iteration: 188\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:13:26 (running for 00:19:33.56)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   188</td><td style=\"text-align: right;\">         1147.91</td><td style=\"text-align: right;\">752000</td><td style=\"text-align: right;\">-358.315</td><td style=\"text-align: right;\">            -0.51718</td><td style=\"text-align: right;\">            -1768.08</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 756000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-13-28\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.5171798012896391\n",
      "  episode_reward_mean: -309.6411316354163\n",
      "  episode_reward_min: -1523.9489606238965\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3780\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: -0.423109769821167\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012587574310600758\n",
      "          model: {}\n",
      "          policy_loss: -0.016756923869252205\n",
      "          total_loss: 386.25164794921875\n",
      "          vf_explained_var: 0.43099191784858704\n",
      "          vf_loss: 386.25885009765625\n",
      "    num_agent_steps_sampled: 756000\n",
      "    num_agent_steps_trained: 756000\n",
      "    num_steps_sampled: 756000\n",
      "    num_steps_trained: 756000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 189\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.5625\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1206031610028207\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1774318272858661\n",
      "    mean_inference_ms: 1.2200564961973785\n",
      "    mean_raw_obs_processing_ms: 0.1031213053674522\n",
      "  time_since_restore: 1153.8933036327362\n",
      "  time_this_iter_s: 5.981081962585449\n",
      "  time_total_s: 1153.8933036327362\n",
      "  timers:\n",
      "    learn_throughput: 1481.225\n",
      "    learn_time_ms: 2700.468\n",
      "    load_throughput: 5879315.952\n",
      "    load_time_ms: 0.68\n",
      "    sample_throughput: 664.902\n",
      "    sample_time_ms: 6015.926\n",
      "    update_time_ms: 2.919\n",
      "  timestamp: 1636042408\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 756000\n",
      "  training_iteration: 189\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:13:31 (running for 00:19:38.59)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   189</td><td style=\"text-align: right;\">         1153.89</td><td style=\"text-align: right;\">756000</td><td style=\"text-align: right;\">-309.641</td><td style=\"text-align: right;\">            -0.51718</td><td style=\"text-align: right;\">            -1523.95</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 760000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-13-34\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.5171798012896391\n",
      "  episode_reward_mean: -337.6104790876497\n",
      "  episode_reward_min: -1523.9489606238965\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3800\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.5041394829750061\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0056450217962265015\n",
      "          model: {}\n",
      "          policy_loss: -0.01457341481000185\n",
      "          total_loss: 1705.510986328125\n",
      "          vf_explained_var: 0.42624831199645996\n",
      "          vf_loss: 1705.5213623046875\n",
      "    num_agent_steps_sampled: 760000\n",
      "    num_agent_steps_trained: 760000\n",
      "    num_steps_sampled: 760000\n",
      "    num_steps_trained: 760000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 190\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.557142857142859\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12059286942377505\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17741304794273952\n",
      "    mean_inference_ms: 1.2199051020106664\n",
      "    mean_raw_obs_processing_ms: 0.10311054100249865\n",
      "  time_since_restore: 1159.8117666244507\n",
      "  time_this_iter_s: 5.9184629917144775\n",
      "  time_total_s: 1159.8117666244507\n",
      "  timers:\n",
      "    learn_throughput: 1481.356\n",
      "    learn_time_ms: 2700.229\n",
      "    load_throughput: 5833118.698\n",
      "    load_time_ms: 0.686\n",
      "    sample_throughput: 664.839\n",
      "    sample_time_ms: 6016.491\n",
      "    update_time_ms: 2.913\n",
      "  timestamp: 1636042414\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 760000\n",
      "  training_iteration: 190\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:13:37 (running for 00:19:44.56)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   190</td><td style=\"text-align: right;\">         1159.81</td><td style=\"text-align: right;\">760000</td><td style=\"text-align: right;\"> -337.61</td><td style=\"text-align: right;\">            -0.51718</td><td style=\"text-align: right;\">            -1523.95</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 764000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-13-40\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.5171798012896391\n",
      "  episode_reward_mean: -332.480103875415\n",
      "  episode_reward_min: -1788.5547307251418\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3820\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.5880022048950195\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005267218686640263\n",
      "          model: {}\n",
      "          policy_loss: -0.008299279026687145\n",
      "          total_loss: 3079.46337890625\n",
      "          vf_explained_var: 0.5542805790901184\n",
      "          vf_loss: 3079.46826171875\n",
      "    num_agent_steps_sampled: 764000\n",
      "    num_agent_steps_trained: 764000\n",
      "    num_steps_sampled: 764000\n",
      "    num_steps_trained: 764000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 191\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.3875\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1205824061467894\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17739425822901927\n",
      "    mean_inference_ms: 1.219753100178898\n",
      "    mean_raw_obs_processing_ms: 0.10309916277424287\n",
      "  time_since_restore: 1165.778297662735\n",
      "  time_this_iter_s: 5.966531038284302\n",
      "  time_total_s: 1165.778297662735\n",
      "  timers:\n",
      "    learn_throughput: 1480.553\n",
      "    learn_time_ms: 2701.693\n",
      "    load_throughput: 5814721.519\n",
      "    load_time_ms: 0.688\n",
      "    sample_throughput: 665.119\n",
      "    sample_time_ms: 6013.959\n",
      "    update_time_ms: 2.909\n",
      "  timestamp: 1636042420\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 764000\n",
      "  training_iteration: 191\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:13:42 (running for 00:19:49.57)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   191</td><td style=\"text-align: right;\">         1165.78</td><td style=\"text-align: right;\">764000</td><td style=\"text-align: right;\"> -332.48</td><td style=\"text-align: right;\">            -0.51718</td><td style=\"text-align: right;\">            -1788.55</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 768000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-13-46\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.5171798012896391\n",
      "  episode_reward_mean: -348.7931523333077\n",
      "  episode_reward_min: -1788.5547307251418\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3840\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.28146111965179443\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010257396847009659\n",
      "          model: {}\n",
      "          policy_loss: -0.01872139610350132\n",
      "          total_loss: 1702.3275146484375\n",
      "          vf_explained_var: 0.2031470239162445\n",
      "          vf_loss: 1702.3385009765625\n",
      "    num_agent_steps_sampled: 768000\n",
      "    num_agent_steps_trained: 768000\n",
      "    num_steps_sampled: 768000\n",
      "    num_steps_trained: 768000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 192\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.214285714285714\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12057175761926514\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17737565262479507\n",
      "    mean_inference_ms: 1.2196004489642942\n",
      "    mean_raw_obs_processing_ms: 0.10308832758481945\n",
      "  time_since_restore: 1171.7522614002228\n",
      "  time_this_iter_s: 5.973963737487793\n",
      "  time_total_s: 1171.7522614002228\n",
      "  timers:\n",
      "    learn_throughput: 1480.282\n",
      "    learn_time_ms: 2702.187\n",
      "    load_throughput: 5849796.374\n",
      "    load_time_ms: 0.684\n",
      "    sample_throughput: 665.048\n",
      "    sample_time_ms: 6014.606\n",
      "    update_time_ms: 2.904\n",
      "  timestamp: 1636042426\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 768000\n",
      "  training_iteration: 192\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:13:47 (running for 00:19:54.59)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   192</td><td style=\"text-align: right;\">         1171.75</td><td style=\"text-align: right;\">768000</td><td style=\"text-align: right;\">-348.793</td><td style=\"text-align: right;\">            -0.51718</td><td style=\"text-align: right;\">            -1788.55</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:13:52 (running for 00:19:59.60)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   192</td><td style=\"text-align: right;\">         1171.75</td><td style=\"text-align: right;\">768000</td><td style=\"text-align: right;\">-348.793</td><td style=\"text-align: right;\">            -0.51718</td><td style=\"text-align: right;\">            -1788.55</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 772000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-13-53\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.5514952802386007\n",
      "  episode_reward_mean: -321.2108354162708\n",
      "  episode_reward_min: -1788.5547307251418\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3860\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.7593749761581421\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: -0.03053579479455948\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.004794022534042597\n",
      "          model: {}\n",
      "          policy_loss: -0.012558088637888432\n",
      "          total_loss: 2015.0509033203125\n",
      "          vf_explained_var: 0.4924590587615967\n",
      "          vf_loss: 2015.0599365234375\n",
      "    num_agent_steps_sampled: 772000\n",
      "    num_agent_steps_trained: 772000\n",
      "    num_steps_sampled: 772000\n",
      "    num_steps_trained: 772000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 193\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 9.5875\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12055962733638402\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1773541416846353\n",
      "    mean_inference_ms: 1.2194272904477161\n",
      "    mean_raw_obs_processing_ms: 0.10307546261061581\n",
      "  time_since_restore: 1177.9094641208649\n",
      "  time_this_iter_s: 6.15720272064209\n",
      "  time_total_s: 1177.9094641208649\n",
      "  timers:\n",
      "    learn_throughput: 1467.352\n",
      "    learn_time_ms: 2725.999\n",
      "    load_throughput: 5852449.158\n",
      "    load_time_ms: 0.683\n",
      "    sample_throughput: 664.971\n",
      "    sample_time_ms: 6015.299\n",
      "    update_time_ms: 2.895\n",
      "  timestamp: 1636042433\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 772000\n",
      "  training_iteration: 193\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:13:58 (running for 00:20:04.80)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   193</td><td style=\"text-align: right;\">         1177.91</td><td style=\"text-align: right;\">772000</td><td style=\"text-align: right;\">-321.211</td><td style=\"text-align: right;\">           -0.551495</td><td style=\"text-align: right;\">            -1788.55</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 776000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-13-59\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.30737106852088936\n",
      "  episode_reward_mean: -315.30662153034433\n",
      "  episode_reward_min: -1788.5547307251418\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3880\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.37968748807907104\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: -0.5690144896507263\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.020812487229704857\n",
      "          model: {}\n",
      "          policy_loss: -0.017964184284210205\n",
      "          total_loss: 94.12489318847656\n",
      "          vf_explained_var: 0.31663942337036133\n",
      "          vf_loss: 94.13494873046875\n",
      "    num_agent_steps_sampled: 776000\n",
      "    num_agent_steps_trained: 776000\n",
      "    num_steps_sampled: 776000\n",
      "    num_steps_trained: 776000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 194\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.6125\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1205469138053078\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17733077283915832\n",
      "    mean_inference_ms: 1.2192393775885315\n",
      "    mean_raw_obs_processing_ms: 0.10306124636190543\n",
      "  time_since_restore: 1183.8710956573486\n",
      "  time_this_iter_s: 5.961631536483765\n",
      "  time_total_s: 1183.8710956573486\n",
      "  timers:\n",
      "    learn_throughput: 1466.104\n",
      "    learn_time_ms: 2728.32\n",
      "    load_throughput: 5854491.398\n",
      "    load_time_ms: 0.683\n",
      "    sample_throughput: 662.608\n",
      "    sample_time_ms: 6036.749\n",
      "    update_time_ms: 2.868\n",
      "  timestamp: 1636042439\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 776000\n",
      "  training_iteration: 194\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:14:03 (running for 00:20:09.81)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   194</td><td style=\"text-align: right;\">         1183.87</td><td style=\"text-align: right;\">776000</td><td style=\"text-align: right;\">-315.307</td><td style=\"text-align: right;\">           -0.307371</td><td style=\"text-align: right;\">            -1788.55</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 780000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-14-05\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.30737106852088936\n",
      "  episode_reward_mean: -280.5263495553171\n",
      "  episode_reward_min: -1788.5547307251418\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3900\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.569531261920929\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: -0.4057296812534332\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008419389836490154\n",
      "          model: {}\n",
      "          policy_loss: -0.020502969622612\n",
      "          total_loss: 259.35113525390625\n",
      "          vf_explained_var: 0.5289685726165771\n",
      "          vf_loss: 259.3668212890625\n",
      "    num_agent_steps_sampled: 780000\n",
      "    num_agent_steps_trained: 780000\n",
      "    num_steps_sampled: 780000\n",
      "    num_steps_trained: 780000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 195\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.471428571428572\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12053462258433643\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17730764819131345\n",
      "    mean_inference_ms: 1.2190614936769293\n",
      "    mean_raw_obs_processing_ms: 0.10304774464102326\n",
      "  time_since_restore: 1189.8482184410095\n",
      "  time_this_iter_s: 5.977122783660889\n",
      "  time_total_s: 1189.8482184410095\n",
      "  timers:\n",
      "    learn_throughput: 1465.734\n",
      "    learn_time_ms: 2729.008\n",
      "    load_throughput: 5926878.864\n",
      "    load_time_ms: 0.675\n",
      "    sample_throughput: 663.089\n",
      "    sample_time_ms: 6032.369\n",
      "    update_time_ms: 2.854\n",
      "  timestamp: 1636042445\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 780000\n",
      "  training_iteration: 195\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:14:08 (running for 00:20:14.83)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   195</td><td style=\"text-align: right;\">         1189.85</td><td style=\"text-align: right;\">780000</td><td style=\"text-align: right;\">-280.526</td><td style=\"text-align: right;\">           -0.307371</td><td style=\"text-align: right;\">            -1788.55</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 784000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-14-11\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.30737106852088936\n",
      "  episode_reward_mean: -262.5177486276751\n",
      "  episode_reward_min: -1630.4710112950074\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3920\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.569531261920929\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.05115864425897598\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014549093320965767\n",
      "          model: {}\n",
      "          policy_loss: -0.019646646454930305\n",
      "          total_loss: 1934.039306640625\n",
      "          vf_explained_var: 0.4910949468612671\n",
      "          vf_loss: 1934.050537109375\n",
      "    num_agent_steps_sampled: 784000\n",
      "    num_agent_steps_trained: 784000\n",
      "    num_steps_sampled: 784000\n",
      "    num_steps_trained: 784000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 196\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.375\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12052242963562992\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17728500090853638\n",
      "    mean_inference_ms: 1.2188867933825582\n",
      "    mean_raw_obs_processing_ms: 0.10303472648331968\n",
      "  time_since_restore: 1195.8418428897858\n",
      "  time_this_iter_s: 5.993624448776245\n",
      "  time_total_s: 1195.8418428897858\n",
      "  timers:\n",
      "    learn_throughput: 1464.457\n",
      "    learn_time_ms: 2731.388\n",
      "    load_throughput: 5927088.25\n",
      "    load_time_ms: 0.675\n",
      "    sample_throughput: 662.967\n",
      "    sample_time_ms: 6033.485\n",
      "    update_time_ms: 2.853\n",
      "  timestamp: 1636042451\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 784000\n",
      "  training_iteration: 196\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:14:13 (running for 00:20:19.87)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   196</td><td style=\"text-align: right;\">         1195.84</td><td style=\"text-align: right;\">784000</td><td style=\"text-align: right;\">-262.518</td><td style=\"text-align: right;\">           -0.307371</td><td style=\"text-align: right;\">            -1630.47</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 788000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-14-17\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.30737106852088936\n",
      "  episode_reward_mean: -248.42152043947274\n",
      "  episode_reward_min: -1630.4710112950074\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3940\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.569531261920929\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: -0.3755348026752472\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010596582666039467\n",
      "          model: {}\n",
      "          policy_loss: -0.020132621750235558\n",
      "          total_loss: 224.9572296142578\n",
      "          vf_explained_var: 0.5381783246994019\n",
      "          vf_loss: 224.9713134765625\n",
      "    num_agent_steps_sampled: 788000\n",
      "    num_agent_steps_trained: 788000\n",
      "    num_steps_sampled: 788000\n",
      "    num_steps_trained: 788000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 197\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 9.028571428571428\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12051048533862879\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17726244195380558\n",
      "    mean_inference_ms: 1.2187132076853204\n",
      "    mean_raw_obs_processing_ms: 0.10302192151458912\n",
      "  time_since_restore: 1201.8249032497406\n",
      "  time_this_iter_s: 5.983060359954834\n",
      "  time_total_s: 1201.8249032497406\n",
      "  timers:\n",
      "    learn_throughput: 1464.297\n",
      "    learn_time_ms: 2731.687\n",
      "    load_throughput: 5939258.001\n",
      "    load_time_ms: 0.673\n",
      "    sample_throughput: 662.699\n",
      "    sample_time_ms: 6035.921\n",
      "    update_time_ms: 2.849\n",
      "  timestamp: 1636042457\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 788000\n",
      "  training_iteration: 197\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:14:18 (running for 00:20:24.90)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   197</td><td style=\"text-align: right;\">         1201.82</td><td style=\"text-align: right;\">788000</td><td style=\"text-align: right;\">-248.422</td><td style=\"text-align: right;\">           -0.307371</td><td style=\"text-align: right;\">            -1630.47</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:14:23 (running for 00:20:29.91)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   197</td><td style=\"text-align: right;\">         1201.82</td><td style=\"text-align: right;\">788000</td><td style=\"text-align: right;\">-248.422</td><td style=\"text-align: right;\">           -0.307371</td><td style=\"text-align: right;\">            -1630.47</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 792000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-14-23\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.30737106852088936\n",
      "  episode_reward_mean: -251.04936087850754\n",
      "  episode_reward_min: -1259.2105619577403\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3960\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.569531261920929\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: -0.4396160840988159\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01684807613492012\n",
      "          model: {}\n",
      "          policy_loss: -0.017521804198622704\n",
      "          total_loss: 1757.4351806640625\n",
      "          vf_explained_var: 0.44041064381599426\n",
      "          vf_loss: 1757.4432373046875\n",
      "    num_agent_steps_sampled: 792000\n",
      "    num_agent_steps_trained: 792000\n",
      "    num_steps_sampled: 792000\n",
      "    num_steps_trained: 792000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 198\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.149999999999999\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12049971395505957\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17724328260267053\n",
      "    mean_inference_ms: 1.2185792844527397\n",
      "    mean_raw_obs_processing_ms: 0.10301206192677767\n",
      "  time_since_restore: 1207.8494610786438\n",
      "  time_this_iter_s: 6.024557828903198\n",
      "  time_total_s: 1207.8494610786438\n",
      "  timers:\n",
      "    learn_throughput: 1463.657\n",
      "    learn_time_ms: 2732.88\n",
      "    load_throughput: 6023486.159\n",
      "    load_time_ms: 0.664\n",
      "    sample_throughput: 662.213\n",
      "    sample_time_ms: 6040.353\n",
      "    update_time_ms: 2.827\n",
      "  timestamp: 1636042463\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 792000\n",
      "  training_iteration: 198\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:14:28 (running for 00:20:34.98)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   198</td><td style=\"text-align: right;\">         1207.85</td><td style=\"text-align: right;\">792000</td><td style=\"text-align: right;\">-251.049</td><td style=\"text-align: right;\">           -0.307371</td><td style=\"text-align: right;\">            -1259.21</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 796000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-14-29\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.4989274716139189\n",
      "  episode_reward_mean: -300.77228327288196\n",
      "  episode_reward_min: -1720.3676368048762\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3980\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.569531261920929\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.5694711804389954\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007297131232917309\n",
      "          model: {}\n",
      "          policy_loss: -0.016747161746025085\n",
      "          total_loss: 3982.46826171875\n",
      "          vf_explained_var: 0.5595439076423645\n",
      "          vf_loss: 3982.48095703125\n",
      "    num_agent_steps_sampled: 796000\n",
      "    num_agent_steps_trained: 796000\n",
      "    num_steps_sampled: 796000\n",
      "    num_steps_trained: 796000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 199\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.414285714285715\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12048930468481006\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17722486890999847\n",
      "    mean_inference_ms: 1.218453835950064\n",
      "    mean_raw_obs_processing_ms: 0.10300274040241876\n",
      "  time_since_restore: 1213.8130340576172\n",
      "  time_this_iter_s: 5.963572978973389\n",
      "  time_total_s: 1213.8130340576172\n",
      "  timers:\n",
      "    learn_throughput: 1463.287\n",
      "    learn_time_ms: 2733.571\n",
      "    load_throughput: 5953378.517\n",
      "    load_time_ms: 0.672\n",
      "    sample_throughput: 662.373\n",
      "    sample_time_ms: 6038.896\n",
      "    update_time_ms: 2.805\n",
      "  timestamp: 1636042469\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 796000\n",
      "  training_iteration: 199\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:14:33 (running for 00:20:39.99)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   199</td><td style=\"text-align: right;\">         1213.81</td><td style=\"text-align: right;\">796000</td><td style=\"text-align: right;\">-300.772</td><td style=\"text-align: right;\">           -0.498927</td><td style=\"text-align: right;\">            -1720.37</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 800000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-14-35\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.4989274716139189\n",
      "  episode_reward_mean: -314.01177352636705\n",
      "  episode_reward_min: -1720.3676368048762\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4000\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.569531261920929\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: -0.06947541236877441\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0078806197270751\n",
      "          model: {}\n",
      "          policy_loss: -0.017811749130487442\n",
      "          total_loss: 924.1304321289062\n",
      "          vf_explained_var: 0.3762972056865692\n",
      "          vf_loss: 924.1437377929688\n",
      "    num_agent_steps_sampled: 800000\n",
      "    num_agent_steps_trained: 800000\n",
      "    num_steps_sampled: 800000\n",
      "    num_steps_trained: 800000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 200\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.6\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12048068644923751\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1772090644905847\n",
      "    mean_inference_ms: 1.218349280579071\n",
      "    mean_raw_obs_processing_ms: 0.10299449329851154\n",
      "  time_since_restore: 1219.8309309482574\n",
      "  time_this_iter_s: 6.017896890640259\n",
      "  time_total_s: 1219.8309309482574\n",
      "  timers:\n",
      "    learn_throughput: 1463.044\n",
      "    learn_time_ms: 2734.026\n",
      "    load_throughput: 5947680.091\n",
      "    load_time_ms: 0.673\n",
      "    sample_throughput: 661.251\n",
      "    sample_time_ms: 6049.144\n",
      "    update_time_ms: 2.803\n",
      "  timestamp: 1636042475\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 800000\n",
      "  training_iteration: 200\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:14:38 (running for 00:20:45.05)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   200</td><td style=\"text-align: right;\">         1219.83</td><td style=\"text-align: right;\">800000</td><td style=\"text-align: right;\">-314.012</td><td style=\"text-align: right;\">           -0.498927</td><td style=\"text-align: right;\">            -1720.37</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 804000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-14-41\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.5881375089308005\n",
      "  episode_reward_mean: -312.60257703491106\n",
      "  episode_reward_min: -1720.3676368048762\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4020\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.569531261920929\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: -0.13108596205711365\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006468311417847872\n",
      "          model: {}\n",
      "          policy_loss: -0.01222146861255169\n",
      "          total_loss: 1422.4595947265625\n",
      "          vf_explained_var: 0.38299745321273804\n",
      "          vf_loss: 1422.468017578125\n",
      "    num_agent_steps_sampled: 804000\n",
      "    num_agent_steps_trained: 804000\n",
      "    num_steps_sampled: 804000\n",
      "    num_steps_trained: 804000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 201\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.1875\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12047139966079147\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17719228863133696\n",
      "    mean_inference_ms: 1.218233391372924\n",
      "    mean_raw_obs_processing_ms: 0.10298586816550931\n",
      "  time_since_restore: 1225.7648057937622\n",
      "  time_this_iter_s: 5.933874845504761\n",
      "  time_total_s: 1225.7648057937622\n",
      "  timers:\n",
      "    learn_throughput: 1463.821\n",
      "    learn_time_ms: 2732.574\n",
      "    load_throughput: 6009892.535\n",
      "    load_time_ms: 0.666\n",
      "    sample_throughput: 661.419\n",
      "    sample_time_ms: 6047.602\n",
      "    update_time_ms: 2.797\n",
      "  timestamp: 1636042481\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 804000\n",
      "  training_iteration: 201\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:14:43 (running for 00:20:50.08)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   201</td><td style=\"text-align: right;\">         1225.76</td><td style=\"text-align: right;\">804000</td><td style=\"text-align: right;\">-312.603</td><td style=\"text-align: right;\">           -0.588138</td><td style=\"text-align: right;\">            -1720.37</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 808000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-14-47\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.5881375089308005\n",
      "  episode_reward_mean: -306.53079596625975\n",
      "  episode_reward_min: -1720.3676368048762\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4040\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.569531261920929\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: -0.47744467854499817\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011445840820670128\n",
      "          model: {}\n",
      "          policy_loss: -0.025709131732583046\n",
      "          total_loss: 138.8002471923828\n",
      "          vf_explained_var: 0.29489684104919434\n",
      "          vf_loss: 138.81942749023438\n",
      "    num_agent_steps_sampled: 808000\n",
      "    num_agent_steps_trained: 808000\n",
      "    num_steps_sampled: 808000\n",
      "    num_steps_trained: 808000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 202\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.214285714285714\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1204619188355153\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17717635809934307\n",
      "    mean_inference_ms: 1.2181175210194126\n",
      "    mean_raw_obs_processing_ms: 0.10297696980455168\n",
      "  time_since_restore: 1231.7135288715363\n",
      "  time_this_iter_s: 5.948723077774048\n",
      "  time_total_s: 1231.7135288715363\n",
      "  timers:\n",
      "    learn_throughput: 1464.176\n",
      "    learn_time_ms: 2731.911\n",
      "    load_throughput: 6004085.46\n",
      "    load_time_ms: 0.666\n",
      "    sample_throughput: 661.309\n",
      "    sample_time_ms: 6048.61\n",
      "    update_time_ms: 2.796\n",
      "  timestamp: 1636042487\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 808000\n",
      "  training_iteration: 202\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:14:49 (running for 00:20:56.07)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   202</td><td style=\"text-align: right;\">         1231.71</td><td style=\"text-align: right;\">808000</td><td style=\"text-align: right;\">-306.531</td><td style=\"text-align: right;\">           -0.588138</td><td style=\"text-align: right;\">            -1720.37</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 812000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-14-53\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.5881375089308005\n",
      "  episode_reward_mean: -305.297569224272\n",
      "  episode_reward_min: -1720.3676368048762\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4060\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.569531261920929\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: -0.3334539532661438\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008109872229397297\n",
      "          model: {}\n",
      "          policy_loss: -0.016716238111257553\n",
      "          total_loss: 566.8737182617188\n",
      "          vf_explained_var: 0.46890854835510254\n",
      "          vf_loss: 566.8858032226562\n",
      "    num_agent_steps_sampled: 812000\n",
      "    num_agent_steps_trained: 812000\n",
      "    num_steps_sampled: 812000\n",
      "    num_steps_trained: 812000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 203\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.575\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12045090936817622\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17715706180635252\n",
      "    mean_inference_ms: 1.2179641779337758\n",
      "    mean_raw_obs_processing_ms: 0.10296588088552174\n",
      "  time_since_restore: 1237.6494543552399\n",
      "  time_this_iter_s: 5.935925483703613\n",
      "  time_total_s: 1237.6494543552399\n",
      "  timers:\n",
      "    learn_throughput: 1476.203\n",
      "    learn_time_ms: 2709.654\n",
      "    load_throughput: 5977985.391\n",
      "    load_time_ms: 0.669\n",
      "    sample_throughput: 661.368\n",
      "    sample_time_ms: 6048.069\n",
      "    update_time_ms: 2.826\n",
      "  timestamp: 1636042493\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 812000\n",
      "  training_iteration: 203\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:14:55 (running for 00:21:02.05)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   203</td><td style=\"text-align: right;\">         1237.65</td><td style=\"text-align: right;\">812000</td><td style=\"text-align: right;\">-305.298</td><td style=\"text-align: right;\">           -0.588138</td><td style=\"text-align: right;\">            -1720.37</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 816000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-14-59\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.5881375089308005\n",
      "  episode_reward_mean: -265.4688372820784\n",
      "  episode_reward_min: -1389.558411158859\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4080\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.569531261920929\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: -0.34441134333610535\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009957151487469673\n",
      "          model: {}\n",
      "          policy_loss: -0.021051669493317604\n",
      "          total_loss: 547.090576171875\n",
      "          vf_explained_var: 0.569250762462616\n",
      "          vf_loss: 547.10595703125\n",
      "    num_agent_steps_sampled: 816000\n",
      "    num_agent_steps_trained: 816000\n",
      "    num_steps_sampled: 816000\n",
      "    num_steps_trained: 816000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 204\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.442857142857143\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1204400937897649\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17713785370988705\n",
      "    mean_inference_ms: 1.2178110196988539\n",
      "    mean_raw_obs_processing_ms: 0.10295471123961418\n",
      "  time_since_restore: 1243.6461081504822\n",
      "  time_this_iter_s: 5.99665379524231\n",
      "  time_total_s: 1243.6461081504822\n",
      "  timers:\n",
      "    learn_throughput: 1477.106\n",
      "    learn_time_ms: 2707.998\n",
      "    load_throughput: 5957606.619\n",
      "    load_time_ms: 0.671\n",
      "    sample_throughput: 663.244\n",
      "    sample_time_ms: 6030.96\n",
      "    update_time_ms: 2.84\n",
      "  timestamp: 1636042499\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 816000\n",
      "  training_iteration: 204\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:15:00 (running for 00:21:07.10)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   204</td><td style=\"text-align: right;\">         1243.65</td><td style=\"text-align: right;\">816000</td><td style=\"text-align: right;\">-265.469</td><td style=\"text-align: right;\">           -0.588138</td><td style=\"text-align: right;\">            -1389.56</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:15:05 (running for 00:21:12.11)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   204</td><td style=\"text-align: right;\">         1243.65</td><td style=\"text-align: right;\">816000</td><td style=\"text-align: right;\">-265.469</td><td style=\"text-align: right;\">           -0.588138</td><td style=\"text-align: right;\">            -1389.56</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 820000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-15-05\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.6003028198193634\n",
      "  episode_reward_mean: -260.3563947435712\n",
      "  episode_reward_min: -1375.540551206984\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4100\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.569531261920929\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: -0.11979367583990097\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009207559749484062\n",
      "          model: {}\n",
      "          policy_loss: -0.020204154774546623\n",
      "          total_loss: 332.0087585449219\n",
      "          vf_explained_var: 0.6305738091468811\n",
      "          vf_loss: 332.0237121582031\n",
      "    num_agent_steps_sampled: 820000\n",
      "    num_agent_steps_trained: 820000\n",
      "    num_steps_sampled: 820000\n",
      "    num_steps_trained: 820000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 205\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.4375\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12042806085603405\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17711718281325298\n",
      "    mean_inference_ms: 1.217640642201224\n",
      "    mean_raw_obs_processing_ms: 0.1029438902780586\n",
      "  time_since_restore: 1249.6781582832336\n",
      "  time_this_iter_s: 6.032050132751465\n",
      "  time_total_s: 1249.6781582832336\n",
      "  timers:\n",
      "    learn_throughput: 1475.235\n",
      "    learn_time_ms: 2711.433\n",
      "    load_throughput: 5935895.839\n",
      "    load_time_ms: 0.674\n",
      "    sample_throughput: 663.167\n",
      "    sample_time_ms: 6031.667\n",
      "    update_time_ms: 2.847\n",
      "  timestamp: 1636042505\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 820000\n",
      "  training_iteration: 205\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:15:10 (running for 00:21:17.18)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   205</td><td style=\"text-align: right;\">         1249.68</td><td style=\"text-align: right;\">820000</td><td style=\"text-align: right;\">-260.356</td><td style=\"text-align: right;\">             -1.6003</td><td style=\"text-align: right;\">            -1375.54</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 824000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-15-11\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.6003028198193634\n",
      "  episode_reward_mean: -280.40366788245655\n",
      "  episode_reward_min: -1349.501186130284\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4120\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.569531261920929\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.6434223055839539\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005676659289747477\n",
      "          model: {}\n",
      "          policy_loss: -0.012182570062577724\n",
      "          total_loss: 2218.025634765625\n",
      "          vf_explained_var: 0.5652329921722412\n",
      "          vf_loss: 2218.034423828125\n",
      "    num_agent_steps_sampled: 824000\n",
      "    num_agent_steps_trained: 824000\n",
      "    num_steps_sampled: 824000\n",
      "    num_steps_trained: 824000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 206\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.542857142857143\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12041522379919338\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1770951639923377\n",
      "    mean_inference_ms: 1.2174720465823436\n",
      "    mean_raw_obs_processing_ms: 0.10293256739248818\n",
      "  time_since_restore: 1255.5859122276306\n",
      "  time_this_iter_s: 5.907753944396973\n",
      "  time_total_s: 1255.5859122276306\n",
      "  timers:\n",
      "    learn_throughput: 1476.811\n",
      "    learn_time_ms: 2708.538\n",
      "    load_throughput: 5844090.846\n",
      "    load_time_ms: 0.684\n",
      "    sample_throughput: 663.429\n",
      "    sample_time_ms: 6029.283\n",
      "    update_time_ms: 2.855\n",
      "  timestamp: 1636042511\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 824000\n",
      "  training_iteration: 206\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:15:16 (running for 00:21:23.14)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   206</td><td style=\"text-align: right;\">         1255.59</td><td style=\"text-align: right;\">824000</td><td style=\"text-align: right;\">-280.404</td><td style=\"text-align: right;\">             -1.6003</td><td style=\"text-align: right;\">             -1349.5</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 828000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-15-17\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.740387032539888\n",
      "  episode_reward_mean: -309.62914355832083\n",
      "  episode_reward_min: -1766.3746255096974\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4140\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.569531261920929\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.8003378510475159\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005188565235584974\n",
      "          model: {}\n",
      "          policy_loss: -0.010074152611196041\n",
      "          total_loss: 4432.33154296875\n",
      "          vf_explained_var: 0.25556352734565735\n",
      "          vf_loss: 4432.3388671875\n",
      "    num_agent_steps_sampled: 828000\n",
      "    num_agent_steps_trained: 828000\n",
      "    num_steps_sampled: 828000\n",
      "    num_steps_trained: 828000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 207\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.5\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.120400370281382\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17706891171719755\n",
      "    mean_inference_ms: 1.2172904361557466\n",
      "    mean_raw_obs_processing_ms: 0.10292002253103619\n",
      "  time_since_restore: 1261.503573179245\n",
      "  time_this_iter_s: 5.91766095161438\n",
      "  time_total_s: 1261.503573179245\n",
      "  timers:\n",
      "    learn_throughput: 1477.329\n",
      "    learn_time_ms: 2707.589\n",
      "    load_throughput: 5850816.391\n",
      "    load_time_ms: 0.684\n",
      "    sample_throughput: 664.378\n",
      "    sample_time_ms: 6020.666\n",
      "    update_time_ms: 2.853\n",
      "  timestamp: 1636042517\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 828000\n",
      "  training_iteration: 207\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:15:22 (running for 00:21:29.10)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   207</td><td style=\"text-align: right;\">          1261.5</td><td style=\"text-align: right;\">828000</td><td style=\"text-align: right;\">-309.629</td><td style=\"text-align: right;\">            -2.74039</td><td style=\"text-align: right;\">            -1766.37</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 832000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-15-23\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -2.740387032539888\n",
      "  episode_reward_mean: -302.17488164895553\n",
      "  episode_reward_min: -1766.3746255096974\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4160\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.569531261920929\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: -0.3079918622970581\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009390806779265404\n",
      "          model: {}\n",
      "          policy_loss: -0.02140292152762413\n",
      "          total_loss: 168.0944366455078\n",
      "          vf_explained_var: 0.660768985748291\n",
      "          vf_loss: 168.11048889160156\n",
      "    num_agent_steps_sampled: 832000\n",
      "    num_agent_steps_trained: 832000\n",
      "    num_steps_sampled: 832000\n",
      "    num_steps_trained: 832000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 208\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.342857142857143\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12038593397992224\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17704348715894597\n",
      "    mean_inference_ms: 1.217111135281253\n",
      "    mean_raw_obs_processing_ms: 0.10290776914977974\n",
      "  time_since_restore: 1267.3937435150146\n",
      "  time_this_iter_s: 5.890170335769653\n",
      "  time_total_s: 1267.3937435150146\n",
      "  timers:\n",
      "    learn_throughput: 1478.496\n",
      "    learn_time_ms: 2705.452\n",
      "    load_throughput: 5844294.423\n",
      "    load_time_ms: 0.684\n",
      "    sample_throughput: 665.758\n",
      "    sample_time_ms: 6008.19\n",
      "    update_time_ms: 2.851\n",
      "  timestamp: 1636042523\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 832000\n",
      "  training_iteration: 208\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:15:28 (running for 00:21:35.04)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   208</td><td style=\"text-align: right;\">         1267.39</td><td style=\"text-align: right;\">832000</td><td style=\"text-align: right;\">-302.175</td><td style=\"text-align: right;\">            -2.74039</td><td style=\"text-align: right;\">            -1766.37</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 836000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-15-29\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -116.66678640725686\n",
      "  episode_reward_mean: -305.8594935680818\n",
      "  episode_reward_min: -1766.3746255096974\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4180\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.569531261920929\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: -0.23516973853111267\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01328708790242672\n",
      "          model: {}\n",
      "          policy_loss: -0.026513339951634407\n",
      "          total_loss: 224.87818908691406\n",
      "          vf_explained_var: 0.503187894821167\n",
      "          vf_loss: 224.8971405029297\n",
      "    num_agent_steps_sampled: 836000\n",
      "    num_agent_steps_trained: 836000\n",
      "    num_steps_sampled: 836000\n",
      "    num_steps_trained: 836000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 209\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.625\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12037209464314184\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17701957840156549\n",
      "    mean_inference_ms: 1.2169380639201917\n",
      "    mean_raw_obs_processing_ms: 0.10289608740583707\n",
      "  time_since_restore: 1273.4051702022552\n",
      "  time_this_iter_s: 6.011426687240601\n",
      "  time_total_s: 1273.4051702022552\n",
      "  timers:\n",
      "    learn_throughput: 1477.648\n",
      "    learn_time_ms: 2707.004\n",
      "    load_throughput: 5888808.705\n",
      "    load_time_ms: 0.679\n",
      "    sample_throughput: 665.63\n",
      "    sample_time_ms: 6009.343\n",
      "    update_time_ms: 2.856\n",
      "  timestamp: 1636042529\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 836000\n",
      "  training_iteration: 209\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:15:33 (running for 00:21:40.09)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   209</td><td style=\"text-align: right;\">         1273.41</td><td style=\"text-align: right;\">836000</td><td style=\"text-align: right;\">-305.859</td><td style=\"text-align: right;\">            -116.667</td><td style=\"text-align: right;\">            -1766.37</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 840000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-15-35\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.6066584202096308\n",
      "  episode_reward_mean: -313.58698937540385\n",
      "  episode_reward_min: -1766.3746255096974\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4200\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.569531261920929\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.30355119705200195\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0037453151308000088\n",
      "          model: {}\n",
      "          policy_loss: -0.011654617264866829\n",
      "          total_loss: 1958.995849609375\n",
      "          vf_explained_var: 0.67997145652771\n",
      "          vf_loss: 1959.00537109375\n",
      "    num_agent_steps_sampled: 840000\n",
      "    num_agent_steps_trained: 840000\n",
      "    num_steps_sampled: 840000\n",
      "    num_steps_trained: 840000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 210\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.314285714285715\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12035929371710917\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17699670985880533\n",
      "    mean_inference_ms: 1.216781737417993\n",
      "    mean_raw_obs_processing_ms: 0.1028836204676645\n",
      "  time_since_restore: 1279.405685186386\n",
      "  time_this_iter_s: 6.000514984130859\n",
      "  time_total_s: 1279.405685186386\n",
      "  timers:\n",
      "    learn_throughput: 1477.26\n",
      "    learn_time_ms: 2707.716\n",
      "    load_throughput: 5890876.404\n",
      "    load_time_ms: 0.679\n",
      "    sample_throughput: 665.78\n",
      "    sample_time_ms: 6007.994\n",
      "    update_time_ms: 2.889\n",
      "  timestamp: 1636042535\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 840000\n",
      "  training_iteration: 210\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:15:38 (running for 00:21:45.14)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   210</td><td style=\"text-align: right;\">         1279.41</td><td style=\"text-align: right;\">840000</td><td style=\"text-align: right;\">-313.587</td><td style=\"text-align: right;\">           -0.606658</td><td style=\"text-align: right;\">            -1766.37</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 844000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-15-41\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.6066584202096308\n",
      "  episode_reward_mean: -287.4613905934646\n",
      "  episode_reward_min: -1766.3746255096974\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4220\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656309604645\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: -0.01811891607940197\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01826225034892559\n",
      "          model: {}\n",
      "          policy_loss: -0.017055150121450424\n",
      "          total_loss: 1046.87060546875\n",
      "          vf_explained_var: 0.5679974555969238\n",
      "          vf_loss: 1046.8824462890625\n",
      "    num_agent_steps_sampled: 844000\n",
      "    num_agent_steps_trained: 844000\n",
      "    num_steps_sampled: 844000\n",
      "    num_steps_trained: 844000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 211\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.5\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12034799301599514\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17697643927050571\n",
      "    mean_inference_ms: 1.2166421429920977\n",
      "    mean_raw_obs_processing_ms: 0.10287190059490552\n",
      "  time_since_restore: 1285.368884563446\n",
      "  time_this_iter_s: 5.9631993770599365\n",
      "  time_total_s: 1285.368884563446\n",
      "  timers:\n",
      "    learn_throughput: 1476.888\n",
      "    learn_time_ms: 2708.398\n",
      "    load_throughput: 5888188.678\n",
      "    load_time_ms: 0.679\n",
      "    sample_throughput: 665.429\n",
      "    sample_time_ms: 6011.157\n",
      "    update_time_ms: 2.876\n",
      "  timestamp: 1636042541\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 844000\n",
      "  training_iteration: 211\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:15:43 (running for 00:21:50.15)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   211</td><td style=\"text-align: right;\">         1285.37</td><td style=\"text-align: right;\">844000</td><td style=\"text-align: right;\">-287.461</td><td style=\"text-align: right;\">           -0.606658</td><td style=\"text-align: right;\">            -1766.37</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 848000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-15-47\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.5957463009563716\n",
      "  episode_reward_mean: -312.96490827590264\n",
      "  episode_reward_min: -1773.5650544605567\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4240\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656309604645\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.5201009511947632\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008819998241961002\n",
      "          model: {}\n",
      "          policy_loss: -0.010041146539151669\n",
      "          total_loss: 6878.12890625\n",
      "          vf_explained_var: 0.3312915563583374\n",
      "          vf_loss: 6878.13671875\n",
      "    num_agent_steps_sampled: 848000\n",
      "    num_agent_steps_trained: 848000\n",
      "    num_steps_sampled: 848000\n",
      "    num_steps_trained: 848000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 212\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.371428571428572\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12033881615434316\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1769609876691647\n",
      "    mean_inference_ms: 1.216520925078793\n",
      "    mean_raw_obs_processing_ms: 0.10286216048483139\n",
      "  time_since_restore: 1291.3275105953217\n",
      "  time_this_iter_s: 5.95862603187561\n",
      "  time_total_s: 1291.3275105953217\n",
      "  timers:\n",
      "    learn_throughput: 1476.452\n",
      "    learn_time_ms: 2709.197\n",
      "    load_throughput: 5891290.119\n",
      "    load_time_ms: 0.679\n",
      "    sample_throughput: 665.831\n",
      "    sample_time_ms: 6007.527\n",
      "    update_time_ms: 2.894\n",
      "  timestamp: 1636042547\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 848000\n",
      "  training_iteration: 212\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:15:48 (running for 00:21:55.15)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   212</td><td style=\"text-align: right;\">         1291.33</td><td style=\"text-align: right;\">848000</td><td style=\"text-align: right;\">-312.965</td><td style=\"text-align: right;\">           -0.595746</td><td style=\"text-align: right;\">            -1773.57</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 852000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-15-53\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.5957463009563716\n",
      "  episode_reward_mean: -344.9058106823283\n",
      "  episode_reward_min: -1773.5650544605567\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4260\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656309604645\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.7767779231071472\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007533467374742031\n",
      "          model: {}\n",
      "          policy_loss: -0.012999220751225948\n",
      "          total_loss: 1826.12451171875\n",
      "          vf_explained_var: 0.4449162185192108\n",
      "          vf_loss: 1826.1353759765625\n",
      "    num_agent_steps_sampled: 852000\n",
      "    num_agent_steps_trained: 852000\n",
      "    num_steps_sampled: 852000\n",
      "    num_steps_trained: 852000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 213\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.600000000000001\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12033090842370128\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17694715692392493\n",
      "    mean_inference_ms: 1.216419312574617\n",
      "    mean_raw_obs_processing_ms: 0.10285291625693822\n",
      "  time_since_restore: 1297.3160004615784\n",
      "  time_this_iter_s: 5.988489866256714\n",
      "  time_total_s: 1297.3160004615784\n",
      "  timers:\n",
      "    learn_throughput: 1477.768\n",
      "    learn_time_ms: 2706.784\n",
      "    load_throughput: 5899369.176\n",
      "    load_time_ms: 0.678\n",
      "    sample_throughput: 664.873\n",
      "    sample_time_ms: 6016.182\n",
      "    update_time_ms: 2.856\n",
      "  timestamp: 1636042553\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 852000\n",
      "  training_iteration: 213\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:15:53 (running for 00:22:00.19)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   213</td><td style=\"text-align: right;\">         1297.32</td><td style=\"text-align: right;\">852000</td><td style=\"text-align: right;\">-344.906</td><td style=\"text-align: right;\">           -0.595746</td><td style=\"text-align: right;\">            -1773.57</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:15:58 (running for 00:22:05.20)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   213</td><td style=\"text-align: right;\">         1297.32</td><td style=\"text-align: right;\">852000</td><td style=\"text-align: right;\">-344.906</td><td style=\"text-align: right;\">           -0.595746</td><td style=\"text-align: right;\">            -1773.57</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 856000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-15-59\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.5957463009563716\n",
      "  episode_reward_mean: -367.9230801103351\n",
      "  episode_reward_min: -1773.5650544605567\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4280\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656309604645\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.4400692880153656\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014910636469721794\n",
      "          model: {}\n",
      "          policy_loss: -0.01722109690308571\n",
      "          total_loss: 1661.4578857421875\n",
      "          vf_explained_var: 0.3430333435535431\n",
      "          vf_loss: 1661.4708251953125\n",
      "    num_agent_steps_sampled: 856000\n",
      "    num_agent_steps_trained: 856000\n",
      "    num_steps_sampled: 856000\n",
      "    num_steps_trained: 856000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 214\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.299999999999999\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12032182470632083\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17693091359112714\n",
      "    mean_inference_ms: 1.2163066589462528\n",
      "    mean_raw_obs_processing_ms: 0.10284236991515844\n",
      "  time_since_restore: 1303.2533640861511\n",
      "  time_this_iter_s: 5.937363624572754\n",
      "  time_total_s: 1303.2533640861511\n",
      "  timers:\n",
      "    learn_throughput: 1477.923\n",
      "    learn_time_ms: 2706.502\n",
      "    load_throughput: 5937156.204\n",
      "    load_time_ms: 0.674\n",
      "    sample_throughput: 665.749\n",
      "    sample_time_ms: 6008.271\n",
      "    update_time_ms: 2.855\n",
      "  timestamp: 1636042559\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 856000\n",
      "  training_iteration: 214\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:16:04 (running for 00:22:11.18)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   214</td><td style=\"text-align: right;\">         1303.25</td><td style=\"text-align: right;\">856000</td><td style=\"text-align: right;\">-367.923</td><td style=\"text-align: right;\">           -0.595746</td><td style=\"text-align: right;\">            -1773.57</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 860000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-16-05\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.5957463009563716\n",
      "  episode_reward_mean: -349.35086103588526\n",
      "  episode_reward_min: -1773.5650544605567\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4300\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656309604645\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: -0.29730743169784546\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012490383349359035\n",
      "          model: {}\n",
      "          policy_loss: -0.02066371589899063\n",
      "          total_loss: 268.9999084472656\n",
      "          vf_explained_var: 0.2666080892086029\n",
      "          vf_loss: 269.0169982910156\n",
      "    num_agent_steps_sampled: 860000\n",
      "    num_agent_steps_trained: 860000\n",
      "    num_steps_sampled: 860000\n",
      "    num_steps_trained: 860000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 215\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.6375\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12031251179135753\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17691442048629574\n",
      "    mean_inference_ms: 1.2161817049390193\n",
      "    mean_raw_obs_processing_ms: 0.10283224052852895\n",
      "  time_since_restore: 1309.233674287796\n",
      "  time_this_iter_s: 5.9803102016448975\n",
      "  time_total_s: 1309.233674287796\n",
      "  timers:\n",
      "    learn_throughput: 1481.257\n",
      "    learn_time_ms: 2700.41\n",
      "    load_throughput: 5968203.194\n",
      "    load_time_ms: 0.67\n",
      "    sample_throughput: 665.735\n",
      "    sample_time_ms: 6008.396\n",
      "    update_time_ms: 2.873\n",
      "  timestamp: 1636042565\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 860000\n",
      "  training_iteration: 215\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:16:09 (running for 00:22:16.20)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   215</td><td style=\"text-align: right;\">         1309.23</td><td style=\"text-align: right;\">860000</td><td style=\"text-align: right;\">-349.351</td><td style=\"text-align: right;\">           -0.595746</td><td style=\"text-align: right;\">            -1773.57</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 864000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-16-11\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.5957463009563716\n",
      "  episode_reward_mean: -353.9588875838763\n",
      "  episode_reward_min: -1773.5650544605567\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4320\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656309604645\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: -0.03513609245419502\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016953155398368835\n",
      "          model: {}\n",
      "          policy_loss: -0.02865470200777054\n",
      "          total_loss: 741.4778442382812\n",
      "          vf_explained_var: 0.49629485607147217\n",
      "          vf_loss: 741.5015869140625\n",
      "    num_agent_steps_sampled: 864000\n",
      "    num_agent_steps_trained: 864000\n",
      "    num_steps_sampled: 864000\n",
      "    num_steps_trained: 864000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 216\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.299999999999999\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12030448154069891\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1768990785947694\n",
      "    mean_inference_ms: 1.216061090756579\n",
      "    mean_raw_obs_processing_ms: 0.10282330023146967\n",
      "  time_since_restore: 1315.2013003826141\n",
      "  time_this_iter_s: 5.967626094818115\n",
      "  time_total_s: 1315.2013003826141\n",
      "  timers:\n",
      "    learn_throughput: 1481.496\n",
      "    learn_time_ms: 2699.974\n",
      "    load_throughput: 6113922.962\n",
      "    load_time_ms: 0.654\n",
      "    sample_throughput: 665.694\n",
      "    sample_time_ms: 6008.767\n",
      "    update_time_ms: 2.872\n",
      "  timestamp: 1636042571\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 864000\n",
      "  training_iteration: 216\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:16:14 (running for 00:22:21.22)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   216</td><td style=\"text-align: right;\">          1315.2</td><td style=\"text-align: right;\">864000</td><td style=\"text-align: right;\">-353.959</td><td style=\"text-align: right;\">           -0.595746</td><td style=\"text-align: right;\">            -1773.57</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 868000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-16-17\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.9873874054245629\n",
      "  episode_reward_mean: -322.00564143483024\n",
      "  episode_reward_min: -1797.943682417325\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4340\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656309604645\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.4143250584602356\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011853219009935856\n",
      "          model: {}\n",
      "          policy_loss: 0.035006552934646606\n",
      "          total_loss: 1761.9197998046875\n",
      "          vf_explained_var: 0.5041411519050598\n",
      "          vf_loss: 1761.8814697265625\n",
      "    num_agent_steps_sampled: 868000\n",
      "    num_agent_steps_trained: 868000\n",
      "    num_steps_sampled: 868000\n",
      "    num_steps_trained: 868000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 217\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.2625\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12029651704623742\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1768827109376682\n",
      "    mean_inference_ms: 1.2159378888622374\n",
      "    mean_raw_obs_processing_ms: 0.10281387899707943\n",
      "  time_since_restore: 1321.178987979889\n",
      "  time_this_iter_s: 5.97768759727478\n",
      "  time_total_s: 1321.178987979889\n",
      "  timers:\n",
      "    learn_throughput: 1481.936\n",
      "    learn_time_ms: 2699.171\n",
      "    load_throughput: 6092608.49\n",
      "    load_time_ms: 0.657\n",
      "    sample_throughput: 664.989\n",
      "    sample_time_ms: 6015.138\n",
      "    update_time_ms: 2.878\n",
      "  timestamp: 1636042577\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 868000\n",
      "  training_iteration: 217\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:16:19 (running for 00:22:26.24)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   217</td><td style=\"text-align: right;\">         1321.18</td><td style=\"text-align: right;\">868000</td><td style=\"text-align: right;\">-322.006</td><td style=\"text-align: right;\">           -0.987387</td><td style=\"text-align: right;\">            -1797.94</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 872000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-16-23\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.9873874054245629\n",
      "  episode_reward_mean: -305.0444769141249\n",
      "  episode_reward_min: -1797.943682417325\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4360\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2847656309604645\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.1822308599948883\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.020616812631487846\n",
      "          model: {}\n",
      "          policy_loss: -0.024140410125255585\n",
      "          total_loss: 1608.5211181640625\n",
      "          vf_explained_var: 0.447742760181427\n",
      "          vf_loss: 1608.539306640625\n",
      "    num_agent_steps_sampled: 872000\n",
      "    num_agent_steps_trained: 872000\n",
      "    num_steps_sampled: 872000\n",
      "    num_steps_trained: 872000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 218\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.375\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12028942291632419\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17686769516020712\n",
      "    mean_inference_ms: 1.215824818115746\n",
      "    mean_raw_obs_processing_ms: 0.10280541031791318\n",
      "  time_since_restore: 1327.1957297325134\n",
      "  time_this_iter_s: 6.016741752624512\n",
      "  time_total_s: 1327.1957297325134\n",
      "  timers:\n",
      "    learn_throughput: 1480.122\n",
      "    learn_time_ms: 2702.48\n",
      "    load_throughput: 6052386.724\n",
      "    load_time_ms: 0.661\n",
      "    sample_throughput: 664.04\n",
      "    sample_time_ms: 6023.73\n",
      "    update_time_ms: 2.88\n",
      "  timestamp: 1636042583\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 872000\n",
      "  training_iteration: 218\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:16:24 (running for 00:22:31.34)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   218</td><td style=\"text-align: right;\">          1327.2</td><td style=\"text-align: right;\">872000</td><td style=\"text-align: right;\">-305.044</td><td style=\"text-align: right;\">           -0.987387</td><td style=\"text-align: right;\">            -1797.94</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 876000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-16-29\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.9873874054245629\n",
      "  episode_reward_mean: -299.82328534464295\n",
      "  episode_reward_min: -1797.943682417325\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4380\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4271484315395355\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.39409810304641724\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00613784184679389\n",
      "          model: {}\n",
      "          policy_loss: -0.00907908659428358\n",
      "          total_loss: 3546.35009765625\n",
      "          vf_explained_var: 0.4506024122238159\n",
      "          vf_loss: 3546.3564453125\n",
      "    num_agent_steps_sampled: 876000\n",
      "    num_agent_steps_trained: 876000\n",
      "    num_steps_sampled: 876000\n",
      "    num_steps_trained: 876000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 219\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.37142857142857\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12028138714823804\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17685158556442118\n",
      "    mean_inference_ms: 1.2157036768985776\n",
      "    mean_raw_obs_processing_ms: 0.10279666827334459\n",
      "  time_since_restore: 1333.108372926712\n",
      "  time_this_iter_s: 5.912643194198608\n",
      "  time_total_s: 1333.108372926712\n",
      "  timers:\n",
      "    learn_throughput: 1481.804\n",
      "    learn_time_ms: 2699.413\n",
      "    load_throughput: 6051731.775\n",
      "    load_time_ms: 0.661\n",
      "    sample_throughput: 664.0\n",
      "    sample_time_ms: 6024.101\n",
      "    update_time_ms: 2.877\n",
      "  timestamp: 1636042589\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 876000\n",
      "  training_iteration: 219\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:16:30 (running for 00:22:37.30)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   219</td><td style=\"text-align: right;\">         1333.11</td><td style=\"text-align: right;\">876000</td><td style=\"text-align: right;\">-299.823</td><td style=\"text-align: right;\">           -0.987387</td><td style=\"text-align: right;\">            -1797.94</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 880000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-16-35\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.0338819732594482\n",
      "  episode_reward_mean: -296.4143978634505\n",
      "  episode_reward_min: -1797.943682417325\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4400\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4271484315395355\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: -0.43301936984062195\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009447114542126656\n",
      "          model: {}\n",
      "          policy_loss: -0.0227035079151392\n",
      "          total_loss: 131.84722900390625\n",
      "          vf_explained_var: 0.5158435106277466\n",
      "          vf_loss: 131.8658905029297\n",
      "    num_agent_steps_sampled: 880000\n",
      "    num_agent_steps_trained: 880000\n",
      "    num_steps_sampled: 880000\n",
      "    num_steps_trained: 880000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 220\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.4375\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1202709039387938\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17683251800098496\n",
      "    mean_inference_ms: 1.215566796951193\n",
      "    mean_raw_obs_processing_ms: 0.102785996168442\n",
      "  time_since_restore: 1339.001900434494\n",
      "  time_this_iter_s: 5.893527507781982\n",
      "  time_total_s: 1339.001900434494\n",
      "  timers:\n",
      "    learn_throughput: 1482.62\n",
      "    learn_time_ms: 2697.926\n",
      "    load_throughput: 6004945.059\n",
      "    load_time_ms: 0.666\n",
      "    sample_throughput: 665.485\n",
      "    sample_time_ms: 6010.653\n",
      "    update_time_ms: 3.899\n",
      "  timestamp: 1636042595\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 880000\n",
      "  training_iteration: 220\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:16:35 (running for 00:22:42.32)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   220</td><td style=\"text-align: right;\">            1339</td><td style=\"text-align: right;\">880000</td><td style=\"text-align: right;\">-296.414</td><td style=\"text-align: right;\">            -1.03388</td><td style=\"text-align: right;\">            -1797.94</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:16:40 (running for 00:22:47.33)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   220</td><td style=\"text-align: right;\">            1339</td><td style=\"text-align: right;\">880000</td><td style=\"text-align: right;\">-296.414</td><td style=\"text-align: right;\">            -1.03388</td><td style=\"text-align: right;\">            -1797.94</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 884000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-16-41\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.0338819732594482\n",
      "  episode_reward_mean: -311.3702328229445\n",
      "  episode_reward_min: -1797.943682417325\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4420\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4271484315395355\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.7207579612731934\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0077537777833640575\n",
      "          model: {}\n",
      "          policy_loss: -0.008286620490252972\n",
      "          total_loss: 2592.0947265625\n",
      "          vf_explained_var: 0.29683157801628113\n",
      "          vf_loss: 2592.099609375\n",
      "    num_agent_steps_sampled: 884000\n",
      "    num_agent_steps_trained: 884000\n",
      "    num_steps_sampled: 884000\n",
      "    num_steps_trained: 884000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 221\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.2\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12025894606176425\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1768117134079828\n",
      "    mean_inference_ms: 1.2154177441630072\n",
      "    mean_raw_obs_processing_ms: 0.10277419575381487\n",
      "  time_since_restore: 1344.926735162735\n",
      "  time_this_iter_s: 5.924834728240967\n",
      "  time_total_s: 1344.926735162735\n",
      "  timers:\n",
      "    learn_throughput: 1483.03\n",
      "    learn_time_ms: 2697.18\n",
      "    load_throughput: 6007310.226\n",
      "    load_time_ms: 0.666\n",
      "    sample_throughput: 665.03\n",
      "    sample_time_ms: 6014.768\n",
      "    update_time_ms: 3.904\n",
      "  timestamp: 1636042601\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 884000\n",
      "  training_iteration: 221\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:16:46 (running for 00:22:53.29)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   221</td><td style=\"text-align: right;\">         1344.93</td><td style=\"text-align: right;\">884000</td><td style=\"text-align: right;\"> -311.37</td><td style=\"text-align: right;\">            -1.03388</td><td style=\"text-align: right;\">            -1797.94</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 888000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-16-47\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.0338819732594482\n",
      "  episode_reward_mean: -283.1953517522705\n",
      "  episode_reward_min: -1761.5954311116423\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4440\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4271484315395355\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: -0.4857395589351654\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012720993719995022\n",
      "          model: {}\n",
      "          policy_loss: -0.02206997200846672\n",
      "          total_loss: 184.1689910888672\n",
      "          vf_explained_var: 0.5099249482154846\n",
      "          vf_loss: 184.1856231689453\n",
      "    num_agent_steps_sampled: 888000\n",
      "    num_agent_steps_trained: 888000\n",
      "    num_steps_sampled: 888000\n",
      "    num_steps_trained: 888000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 222\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.362499999999999\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12024658812968901\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1767904846102986\n",
      "    mean_inference_ms: 1.215262411369328\n",
      "    mean_raw_obs_processing_ms: 0.10276189342555414\n",
      "  time_since_restore: 1350.8570919036865\n",
      "  time_this_iter_s: 5.930356740951538\n",
      "  time_total_s: 1350.8570919036865\n",
      "  timers:\n",
      "    learn_throughput: 1483.101\n",
      "    learn_time_ms: 2697.052\n",
      "    load_throughput: 6016142.289\n",
      "    load_time_ms: 0.665\n",
      "    sample_throughput: 665.398\n",
      "    sample_time_ms: 6011.441\n",
      "    update_time_ms: 3.92\n",
      "  timestamp: 1636042607\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 888000\n",
      "  training_iteration: 222\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:16:52 (running for 00:22:59.29)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   222</td><td style=\"text-align: right;\">         1350.86</td><td style=\"text-align: right;\">888000</td><td style=\"text-align: right;\">-283.195</td><td style=\"text-align: right;\">            -1.03388</td><td style=\"text-align: right;\">             -1761.6</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 892000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-16-53\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.108419303104213\n",
      "  episode_reward_mean: -255.57008615136382\n",
      "  episode_reward_min: -1761.5954311116423\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4460\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4271484315395355\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: -0.5711251497268677\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.021846020594239235\n",
      "          model: {}\n",
      "          policy_loss: -0.022426526993513107\n",
      "          total_loss: 133.42684936523438\n",
      "          vf_explained_var: 0.3177192807197571\n",
      "          vf_loss: 133.43995666503906\n",
      "    num_agent_steps_sampled: 892000\n",
      "    num_agent_steps_trained: 892000\n",
      "    num_steps_sampled: 892000\n",
      "    num_steps_trained: 892000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 223\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.428571428571429\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12023293873239271\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17676679480735052\n",
      "    mean_inference_ms: 1.2150851327519694\n",
      "    mean_raw_obs_processing_ms: 0.1027482727806452\n",
      "  time_since_restore: 1356.812290430069\n",
      "  time_this_iter_s: 5.955198526382446\n",
      "  time_total_s: 1356.812290430069\n",
      "  timers:\n",
      "    learn_throughput: 1482.605\n",
      "    learn_time_ms: 2697.955\n",
      "    load_throughput: 6013770.163\n",
      "    load_time_ms: 0.665\n",
      "    sample_throughput: 665.66\n",
      "    sample_time_ms: 6009.07\n",
      "    update_time_ms: 3.945\n",
      "  timestamp: 1636042613\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 892000\n",
      "  training_iteration: 223\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:16:57 (running for 00:23:04.29)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   223</td><td style=\"text-align: right;\">         1356.81</td><td style=\"text-align: right;\">892000</td><td style=\"text-align: right;\"> -255.57</td><td style=\"text-align: right;\">            -1.10842</td><td style=\"text-align: right;\">             -1761.6</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 896000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-16-59\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -1.108419303104213\n",
      "  episode_reward_mean: -235.13521799267252\n",
      "  episode_reward_min: -1758.1329513042836\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4480\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.6407226324081421\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: -0.29327404499053955\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.004416798707097769\n",
      "          model: {}\n",
      "          policy_loss: -0.016563374549150467\n",
      "          total_loss: 369.4483947753906\n",
      "          vf_explained_var: 0.4604814946651459\n",
      "          vf_loss: 369.46209716796875\n",
      "    num_agent_steps_sampled: 896000\n",
      "    num_agent_steps_trained: 896000\n",
      "    num_steps_sampled: 896000\n",
      "    num_steps_trained: 896000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 224\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.75\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12022088417751245\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17674520136809416\n",
      "    mean_inference_ms: 1.214922180386406\n",
      "    mean_raw_obs_processing_ms: 0.1027356432105328\n",
      "  time_since_restore: 1362.7753865718842\n",
      "  time_this_iter_s: 5.9630961418151855\n",
      "  time_total_s: 1362.7753865718842\n",
      "  timers:\n",
      "    learn_throughput: 1481.61\n",
      "    learn_time_ms: 2699.765\n",
      "    load_throughput: 6001508.138\n",
      "    load_time_ms: 0.666\n",
      "    sample_throughput: 665.478\n",
      "    sample_time_ms: 6010.717\n",
      "    update_time_ms: 3.935\n",
      "  timestamp: 1636042619\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 896000\n",
      "  training_iteration: 224\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:17:02 (running for 00:23:09.30)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   224</td><td style=\"text-align: right;\">         1362.78</td><td style=\"text-align: right;\">896000</td><td style=\"text-align: right;\">-235.135</td><td style=\"text-align: right;\">            -1.10842</td><td style=\"text-align: right;\">            -1758.13</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 900000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-17-05\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.4297937953890297\n",
      "  episode_reward_mean: -249.14673809368887\n",
      "  episode_reward_min: -1758.1329513042836\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4500\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.32036131620407104\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: -0.022335708141326904\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010650564916431904\n",
      "          model: {}\n",
      "          policy_loss: -0.015132161788642406\n",
      "          total_loss: 694.728759765625\n",
      "          vf_explained_var: 0.45266637206077576\n",
      "          vf_loss: 694.740478515625\n",
      "    num_agent_steps_sampled: 900000\n",
      "    num_agent_steps_trained: 900000\n",
      "    num_steps_sampled: 900000\n",
      "    num_steps_trained: 900000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 225\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.4\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12021128904634519\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17672671464567777\n",
      "    mean_inference_ms: 1.214775793080732\n",
      "    mean_raw_obs_processing_ms: 0.10272511566169655\n",
      "  time_since_restore: 1368.7443296909332\n",
      "  time_this_iter_s: 5.968943119049072\n",
      "  time_total_s: 1368.7443296909332\n",
      "  timers:\n",
      "    learn_throughput: 1480.945\n",
      "    learn_time_ms: 2700.979\n",
      "    load_throughput: 5991862.857\n",
      "    load_time_ms: 0.668\n",
      "    sample_throughput: 665.507\n",
      "    sample_time_ms: 6010.451\n",
      "    update_time_ms: 3.918\n",
      "  timestamp: 1636042625\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 900000\n",
      "  training_iteration: 225\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:17:07 (running for 00:23:14.32)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   225</td><td style=\"text-align: right;\">         1368.74</td><td style=\"text-align: right;\">900000</td><td style=\"text-align: right;\">-249.147</td><td style=\"text-align: right;\">           -0.429794</td><td style=\"text-align: right;\">            -1758.13</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 904000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-17-11\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.4297937953890297\n",
      "  episode_reward_mean: -222.40651392017742\n",
      "  episode_reward_min: -1693.7226614721933\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4520\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.32036131620407104\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: -0.14043116569519043\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010496568866074085\n",
      "          model: {}\n",
      "          policy_loss: -0.026242289692163467\n",
      "          total_loss: 1921.875732421875\n",
      "          vf_explained_var: 0.3413969874382019\n",
      "          vf_loss: 1921.8985595703125\n",
      "    num_agent_steps_sampled: 904000\n",
      "    num_agent_steps_trained: 904000\n",
      "    num_steps_sampled: 904000\n",
      "    num_steps_trained: 904000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 226\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.462499999999999\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12020277905272313\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17671003843834032\n",
      "    mean_inference_ms: 1.2146470109192236\n",
      "    mean_raw_obs_processing_ms: 0.10271608542774337\n",
      "  time_since_restore: 1374.7449395656586\n",
      "  time_this_iter_s: 6.000609874725342\n",
      "  time_total_s: 1374.7449395656586\n",
      "  timers:\n",
      "    learn_throughput: 1480.136\n",
      "    learn_time_ms: 2702.454\n",
      "    load_throughput: 5988440.891\n",
      "    load_time_ms: 0.668\n",
      "    sample_throughput: 665.182\n",
      "    sample_time_ms: 6013.389\n",
      "    update_time_ms: 3.912\n",
      "  timestamp: 1636042631\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 904000\n",
      "  training_iteration: 226\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:17:12 (running for 00:23:19.36)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   226</td><td style=\"text-align: right;\">         1374.74</td><td style=\"text-align: right;\">904000</td><td style=\"text-align: right;\">-222.407</td><td style=\"text-align: right;\">           -0.429794</td><td style=\"text-align: right;\">            -1693.72</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 908000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-17-17\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.4297937953890297\n",
      "  episode_reward_mean: -236.9682380602556\n",
      "  episode_reward_min: -1693.7226614721933\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4540\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.32036131620407104\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.016193127259612083\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009986931458115578\n",
      "          model: {}\n",
      "          policy_loss: -0.01926860213279724\n",
      "          total_loss: 1230.9583740234375\n",
      "          vf_explained_var: 0.3299112319946289\n",
      "          vf_loss: 1230.9744873046875\n",
      "    num_agent_steps_sampled: 908000\n",
      "    num_agent_steps_trained: 908000\n",
      "    num_steps_sampled: 908000\n",
      "    num_steps_trained: 908000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 227\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 9.457142857142857\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12019483876481975\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17669402837061413\n",
      "    mean_inference_ms: 1.2145255333102125\n",
      "    mean_raw_obs_processing_ms: 0.10270799254042234\n",
      "  time_since_restore: 1380.7033507823944\n",
      "  time_this_iter_s: 5.95841121673584\n",
      "  time_total_s: 1380.7033507823944\n",
      "  timers:\n",
      "    learn_throughput: 1479.704\n",
      "    learn_time_ms: 2703.243\n",
      "    load_throughput: 6013985.733\n",
      "    load_time_ms: 0.665\n",
      "    sample_throughput: 665.326\n",
      "    sample_time_ms: 6012.087\n",
      "    update_time_ms: 3.92\n",
      "  timestamp: 1636042637\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 908000\n",
      "  training_iteration: 227\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:17:17 (running for 00:23:24.37)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   227</td><td style=\"text-align: right;\">          1380.7</td><td style=\"text-align: right;\">908000</td><td style=\"text-align: right;\">-236.968</td><td style=\"text-align: right;\">           -0.429794</td><td style=\"text-align: right;\">            -1693.72</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:17:22 (running for 00:23:29.38)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   227</td><td style=\"text-align: right;\">          1380.7</td><td style=\"text-align: right;\">908000</td><td style=\"text-align: right;\">-236.968</td><td style=\"text-align: right;\">           -0.429794</td><td style=\"text-align: right;\">            -1693.72</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 912000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-17-23\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.4297937953890297\n",
      "  episode_reward_mean: -255.4716063035607\n",
      "  episode_reward_min: -1693.7226614721933\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4560\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.32036131620407104\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.08053802698850632\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015784908086061478\n",
      "          model: {}\n",
      "          policy_loss: -0.023222003132104874\n",
      "          total_loss: 538.92041015625\n",
      "          vf_explained_var: 0.5058586597442627\n",
      "          vf_loss: 538.9385375976562\n",
      "    num_agent_steps_sampled: 912000\n",
      "    num_agent_steps_trained: 912000\n",
      "    num_steps_sampled: 912000\n",
      "    num_steps_trained: 912000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 228\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 10.8\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12018999933006899\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17668358119780975\n",
      "    mean_inference_ms: 1.2144478447426514\n",
      "    mean_raw_obs_processing_ms: 0.10270299713011508\n",
      "  time_since_restore: 1386.7782053947449\n",
      "  time_this_iter_s: 6.074854612350464\n",
      "  time_total_s: 1386.7782053947449\n",
      "  timers:\n",
      "    learn_throughput: 1480.185\n",
      "    learn_time_ms: 2702.364\n",
      "    load_throughput: 6055445.03\n",
      "    load_time_ms: 0.661\n",
      "    sample_throughput: 664.481\n",
      "    sample_time_ms: 6019.737\n",
      "    update_time_ms: 3.926\n",
      "  timestamp: 1636042643\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 912000\n",
      "  training_iteration: 228\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:17:27 (running for 00:23:34.50)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   228</td><td style=\"text-align: right;\">         1386.78</td><td style=\"text-align: right;\">912000</td><td style=\"text-align: right;\">-255.472</td><td style=\"text-align: right;\">           -0.429794</td><td style=\"text-align: right;\">            -1693.72</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 916000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-17-29\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.4297937953890297\n",
      "  episode_reward_mean: -252.11649809113806\n",
      "  episode_reward_min: -1693.7226614721933\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4580\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.32036131620407104\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: -0.35396385192871094\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.03323986753821373\n",
      "          model: {}\n",
      "          policy_loss: -0.021174682304263115\n",
      "          total_loss: 246.1624298095703\n",
      "          vf_explained_var: 0.5646535754203796\n",
      "          vf_loss: 246.1729736328125\n",
      "    num_agent_steps_sampled: 916000\n",
      "    num_agent_steps_trained: 916000\n",
      "    num_steps_sampled: 916000\n",
      "    num_steps_trained: 916000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 229\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.385714285714284\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12018412017216916\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17667207330519652\n",
      "    mean_inference_ms: 1.2143622935166942\n",
      "    mean_raw_obs_processing_ms: 0.10269765083873264\n",
      "  time_since_restore: 1392.6888120174408\n",
      "  time_this_iter_s: 5.910606622695923\n",
      "  time_total_s: 1392.6888120174408\n",
      "  timers:\n",
      "    learn_throughput: 1480.287\n",
      "    learn_time_ms: 2702.179\n",
      "    load_throughput: 5987158.661\n",
      "    load_time_ms: 0.668\n",
      "    sample_throughput: 664.953\n",
      "    sample_time_ms: 6015.463\n",
      "    update_time_ms: 3.922\n",
      "  timestamp: 1636042649\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 916000\n",
      "  training_iteration: 229\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:17:33 (running for 00:23:40.46)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   229</td><td style=\"text-align: right;\">         1392.69</td><td style=\"text-align: right;\">916000</td><td style=\"text-align: right;\">-252.116</td><td style=\"text-align: right;\">           -0.429794</td><td style=\"text-align: right;\">            -1693.72</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 920000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-17-35\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.48477681434605085\n",
      "  episode_reward_mean: -290.2781858336932\n",
      "  episode_reward_min: -1736.172734602708\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4600\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.48054200410842896\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.4662836790084839\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006467936560511589\n",
      "          model: {}\n",
      "          policy_loss: -0.010318798944354057\n",
      "          total_loss: 5705.13134765625\n",
      "          vf_explained_var: 0.420562207698822\n",
      "          vf_loss: 5705.13818359375\n",
      "    num_agent_steps_sampled: 920000\n",
      "    num_agent_steps_trained: 920000\n",
      "    num_steps_sampled: 920000\n",
      "    num_steps_trained: 920000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 230\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.700000000000001\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12017755357126696\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17666018893796134\n",
      "    mean_inference_ms: 1.21427470593944\n",
      "    mean_raw_obs_processing_ms: 0.10269185112771498\n",
      "  time_since_restore: 1398.629501581192\n",
      "  time_this_iter_s: 5.940689563751221\n",
      "  time_total_s: 1398.629501581192\n",
      "  timers:\n",
      "    learn_throughput: 1480.211\n",
      "    learn_time_ms: 2702.317\n",
      "    load_throughput: 6058944.023\n",
      "    load_time_ms: 0.66\n",
      "    sample_throughput: 664.309\n",
      "    sample_time_ms: 6021.291\n",
      "    update_time_ms: 2.866\n",
      "  timestamp: 1636042655\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 920000\n",
      "  training_iteration: 230\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:17:39 (running for 00:23:46.45)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   230</td><td style=\"text-align: right;\">         1398.63</td><td style=\"text-align: right;\">920000</td><td style=\"text-align: right;\">-290.278</td><td style=\"text-align: right;\">           -0.484777</td><td style=\"text-align: right;\">            -1736.17</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 924000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-17-41\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.48477681434605085\n",
      "  episode_reward_mean: -295.0143566103108\n",
      "  episode_reward_min: -1736.172734602708\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4620\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.48054200410842896\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.15184736251831055\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.004089404828846455\n",
      "          model: {}\n",
      "          policy_loss: -0.00982341542840004\n",
      "          total_loss: 1213.9619140625\n",
      "          vf_explained_var: 0.3975302278995514\n",
      "          vf_loss: 1213.9698486328125\n",
      "    num_agent_steps_sampled: 924000\n",
      "    num_agent_steps_trained: 924000\n",
      "    num_steps_sampled: 924000\n",
      "    num_steps_trained: 924000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 231\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.4\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12016995662077852\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17664676404170407\n",
      "    mean_inference_ms: 1.2141691342566647\n",
      "    mean_raw_obs_processing_ms: 0.10268497548653321\n",
      "  time_since_restore: 1404.5640075206757\n",
      "  time_this_iter_s: 5.934505939483643\n",
      "  time_total_s: 1404.5640075206757\n",
      "  timers:\n",
      "    learn_throughput: 1480.49\n",
      "    learn_time_ms: 2701.808\n",
      "    load_throughput: 6020244.007\n",
      "    load_time_ms: 0.664\n",
      "    sample_throughput: 665.112\n",
      "    sample_time_ms: 6014.026\n",
      "    update_time_ms: 2.861\n",
      "  timestamp: 1636042661\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 924000\n",
      "  training_iteration: 231\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:17:45 (running for 00:23:52.43)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   231</td><td style=\"text-align: right;\">         1404.56</td><td style=\"text-align: right;\">924000</td><td style=\"text-align: right;\">-295.014</td><td style=\"text-align: right;\">           -0.484777</td><td style=\"text-align: right;\">            -1736.17</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 928000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-17-47\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.48477681434605085\n",
      "  episode_reward_mean: -298.6752091747737\n",
      "  episode_reward_min: -1736.172734602708\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4640\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.24027100205421448\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.22529086470603943\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010273094289004803\n",
      "          model: {}\n",
      "          policy_loss: -0.017959823831915855\n",
      "          total_loss: 912.4995727539062\n",
      "          vf_explained_var: 0.42874932289123535\n",
      "          vf_loss: 912.5151977539062\n",
      "    num_agent_steps_sampled: 928000\n",
      "    num_agent_steps_trained: 928000\n",
      "    num_steps_sampled: 928000\n",
      "    num_steps_trained: 928000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 232\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.5\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1201621924640289\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1766332026517641\n",
      "    mean_inference_ms: 1.2140683493217168\n",
      "    mean_raw_obs_processing_ms: 0.10267765107643527\n",
      "  time_since_restore: 1410.5151453018188\n",
      "  time_this_iter_s: 5.9511377811431885\n",
      "  time_total_s: 1410.5151453018188\n",
      "  timers:\n",
      "    learn_throughput: 1480.813\n",
      "    learn_time_ms: 2701.218\n",
      "    load_throughput: 5986945.009\n",
      "    load_time_ms: 0.668\n",
      "    sample_throughput: 664.882\n",
      "    sample_time_ms: 6016.104\n",
      "    update_time_ms: 2.831\n",
      "  timestamp: 1636042667\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 928000\n",
      "  training_iteration: 232\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:17:51 (running for 00:23:58.42)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   232</td><td style=\"text-align: right;\">         1410.52</td><td style=\"text-align: right;\">928000</td><td style=\"text-align: right;\">-298.675</td><td style=\"text-align: right;\">           -0.484777</td><td style=\"text-align: right;\">            -1736.17</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 932000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-17-53\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.41048060901300554\n",
      "  episode_reward_mean: -296.75355607470703\n",
      "  episode_reward_min: -1736.172734602708\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4660\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.24027100205421448\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.05374515429139137\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006736930925399065\n",
      "          model: {}\n",
      "          policy_loss: -0.015531349927186966\n",
      "          total_loss: 493.58978271484375\n",
      "          vf_explained_var: 0.37255287170410156\n",
      "          vf_loss: 493.60369873046875\n",
      "    num_agent_steps_sampled: 932000\n",
      "    num_agent_steps_trained: 932000\n",
      "    num_steps_sampled: 932000\n",
      "    num_steps_trained: 932000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 233\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.285714285714285\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12015073813596423\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17661375467115167\n",
      "    mean_inference_ms: 1.2139310765946056\n",
      "    mean_raw_obs_processing_ms: 0.10266700436254803\n",
      "  time_since_restore: 1416.4628369808197\n",
      "  time_this_iter_s: 5.9476916790008545\n",
      "  time_total_s: 1416.4628369808197\n",
      "  timers:\n",
      "    learn_throughput: 1480.467\n",
      "    learn_time_ms: 2701.85\n",
      "    load_throughput: 6012692.542\n",
      "    load_time_ms: 0.665\n",
      "    sample_throughput: 665.346\n",
      "    sample_time_ms: 6011.914\n",
      "    update_time_ms: 2.823\n",
      "  timestamp: 1636042673\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 932000\n",
      "  training_iteration: 233\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:17:57 (running for 00:24:04.42)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   233</td><td style=\"text-align: right;\">         1416.46</td><td style=\"text-align: right;\">932000</td><td style=\"text-align: right;\">-296.754</td><td style=\"text-align: right;\">           -0.410481</td><td style=\"text-align: right;\">            -1736.17</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 936000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-17-59\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.3444427814441205\n",
      "  episode_reward_mean: -302.95914473954656\n",
      "  episode_reward_min: -1736.172734602708\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4680\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.24027100205421448\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.08039593696594238\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008282044902443886\n",
      "          model: {}\n",
      "          policy_loss: -0.015229066833853722\n",
      "          total_loss: 1950.3843994140625\n",
      "          vf_explained_var: 0.16330692172050476\n",
      "          vf_loss: 1950.3975830078125\n",
      "    num_agent_steps_sampled: 936000\n",
      "    num_agent_steps_trained: 936000\n",
      "    num_steps_sampled: 936000\n",
      "    num_steps_trained: 936000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 234\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.9125\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12014010441206534\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1765949061266295\n",
      "    mean_inference_ms: 1.2137993443270256\n",
      "    mean_raw_obs_processing_ms: 0.10265692816203568\n",
      "  time_since_restore: 1422.3907995224\n",
      "  time_this_iter_s: 5.9279625415802\n",
      "  time_total_s: 1422.3907995224\n",
      "  timers:\n",
      "    learn_throughput: 1481.333\n",
      "    learn_time_ms: 2700.27\n",
      "    load_throughput: 6004085.46\n",
      "    load_time_ms: 0.666\n",
      "    sample_throughput: 665.506\n",
      "    sample_time_ms: 6010.461\n",
      "    update_time_ms: 2.834\n",
      "  timestamp: 1636042679\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 936000\n",
      "  training_iteration: 234\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:18:03 (running for 00:24:10.39)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   234</td><td style=\"text-align: right;\">         1422.39</td><td style=\"text-align: right;\">936000</td><td style=\"text-align: right;\">-302.959</td><td style=\"text-align: right;\">           -0.344443</td><td style=\"text-align: right;\">            -1736.17</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 940000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-18-05\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.3444427814441205\n",
      "  episode_reward_mean: -288.43334270692554\n",
      "  episode_reward_min: -1703.1960921303366\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4700\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.24027100205421448\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.7890337109565735\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02251630835235119\n",
      "          model: {}\n",
      "          policy_loss: -0.01999461092054844\n",
      "          total_loss: 3306.555908203125\n",
      "          vf_explained_var: 0.3741510808467865\n",
      "          vf_loss: 3306.5703125\n",
      "    num_agent_steps_sampled: 940000\n",
      "    num_agent_steps_trained: 940000\n",
      "    num_steps_sampled: 940000\n",
      "    num_steps_trained: 940000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 235\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.357142857142858\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1201298337309592\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1765760438371618\n",
      "    mean_inference_ms: 1.2136691493496183\n",
      "    mean_raw_obs_processing_ms: 0.10264716412410362\n",
      "  time_since_restore: 1428.3415260314941\n",
      "  time_this_iter_s: 5.950726509094238\n",
      "  time_total_s: 1428.3415260314941\n",
      "  timers:\n",
      "    learn_throughput: 1481.713\n",
      "    learn_time_ms: 2699.579\n",
      "    load_throughput: 5966505.21\n",
      "    load_time_ms: 0.67\n",
      "    sample_throughput: 665.801\n",
      "    sample_time_ms: 6007.801\n",
      "    update_time_ms: 2.849\n",
      "  timestamp: 1636042685\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 940000\n",
      "  training_iteration: 235\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:18:09 (running for 00:24:16.39)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   235</td><td style=\"text-align: right;\">         1428.34</td><td style=\"text-align: right;\">940000</td><td style=\"text-align: right;\">-288.433</td><td style=\"text-align: right;\">           -0.344443</td><td style=\"text-align: right;\">             -1703.2</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 944000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-18-11\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.3444427814441205\n",
      "  episode_reward_mean: -289.77504899488287\n",
      "  episode_reward_min: -1703.1960921303366\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4720\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3604064881801605\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.061865899711847305\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007361013442277908\n",
      "          model: {}\n",
      "          policy_loss: -0.018378006294369698\n",
      "          total_loss: 704.768798828125\n",
      "          vf_explained_var: 0.35267242789268494\n",
      "          vf_loss: 704.7845458984375\n",
      "    num_agent_steps_sampled: 944000\n",
      "    num_agent_steps_trained: 944000\n",
      "    num_steps_sampled: 944000\n",
      "    num_steps_trained: 944000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 236\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.55\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12011988683616061\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17655741963972674\n",
      "    mean_inference_ms: 1.2135439094863452\n",
      "    mean_raw_obs_processing_ms: 0.10263749246051157\n",
      "  time_since_restore: 1434.2733161449432\n",
      "  time_this_iter_s: 5.931790113449097\n",
      "  time_total_s: 1434.2733161449432\n",
      "  timers:\n",
      "    learn_throughput: 1481.984\n",
      "    learn_time_ms: 2699.085\n",
      "    load_throughput: 5927507.066\n",
      "    load_time_ms: 0.675\n",
      "    sample_throughput: 666.576\n",
      "    sample_time_ms: 6000.815\n",
      "    update_time_ms: 2.852\n",
      "  timestamp: 1636042691\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 944000\n",
      "  training_iteration: 236\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:18:15 (running for 00:24:22.37)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   236</td><td style=\"text-align: right;\">         1434.27</td><td style=\"text-align: right;\">944000</td><td style=\"text-align: right;\">-289.775</td><td style=\"text-align: right;\">           -0.344443</td><td style=\"text-align: right;\">             -1703.2</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 948000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-18-17\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.3444427814441205\n",
      "  episode_reward_mean: -277.83059298170036\n",
      "  episode_reward_min: -1703.1960921303366\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4740\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3604064881801605\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: -0.21742267906665802\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007761069107800722\n",
      "          model: {}\n",
      "          policy_loss: -0.017233001068234444\n",
      "          total_loss: 418.1377258300781\n",
      "          vf_explained_var: 0.38332095742225647\n",
      "          vf_loss: 418.15216064453125\n",
      "    num_agent_steps_sampled: 948000\n",
      "    num_agent_steps_trained: 948000\n",
      "    num_steps_sampled: 948000\n",
      "    num_steps_trained: 948000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 237\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.414285714285715\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12011207947477309\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17654180791058763\n",
      "    mean_inference_ms: 1.213434371616771\n",
      "    mean_raw_obs_processing_ms: 0.10262904427438267\n",
      "  time_since_restore: 1440.2997064590454\n",
      "  time_this_iter_s: 6.026390314102173\n",
      "  time_total_s: 1440.2997064590454\n",
      "  timers:\n",
      "    learn_throughput: 1482.621\n",
      "    learn_time_ms: 2697.925\n",
      "    load_throughput: 5908718.743\n",
      "    load_time_ms: 0.677\n",
      "    sample_throughput: 665.755\n",
      "    sample_time_ms: 6008.215\n",
      "    update_time_ms: 2.842\n",
      "  timestamp: 1636042697\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 948000\n",
      "  training_iteration: 237\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:18:20 (running for 00:24:27.44)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   237</td><td style=\"text-align: right;\">          1440.3</td><td style=\"text-align: right;\">948000</td><td style=\"text-align: right;\">-277.831</td><td style=\"text-align: right;\">           -0.344443</td><td style=\"text-align: right;\">             -1703.2</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 952000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-18-23\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.3444427814441205\n",
      "  episode_reward_mean: -306.049963826593\n",
      "  episode_reward_min: -1703.1960921303366\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4760\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3604064881801605\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.0558840036392212\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011401456780731678\n",
      "          model: {}\n",
      "          policy_loss: -0.01699153520166874\n",
      "          total_loss: 3467.91943359375\n",
      "          vf_explained_var: 0.27034106850624084\n",
      "          vf_loss: 3467.932373046875\n",
      "    num_agent_steps_sampled: 952000\n",
      "    num_agent_steps_trained: 952000\n",
      "    num_steps_sampled: 952000\n",
      "    num_steps_trained: 952000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 238\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.475000000000001\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12010487006973707\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17652729308794712\n",
      "    mean_inference_ms: 1.213326690866179\n",
      "    mean_raw_obs_processing_ms: 0.10262127486153556\n",
      "  time_since_restore: 1446.2611095905304\n",
      "  time_this_iter_s: 5.961403131484985\n",
      "  time_total_s: 1446.2611095905304\n",
      "  timers:\n",
      "    learn_throughput: 1482.921\n",
      "    learn_time_ms: 2697.38\n",
      "    load_throughput: 5848165.086\n",
      "    load_time_ms: 0.684\n",
      "    sample_throughput: 667.081\n",
      "    sample_time_ms: 5996.275\n",
      "    update_time_ms: 2.849\n",
      "  timestamp: 1636042703\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 952000\n",
      "  training_iteration: 238\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:18:25 (running for 00:24:32.45)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   238</td><td style=\"text-align: right;\">         1446.26</td><td style=\"text-align: right;\">952000</td><td style=\"text-align: right;\"> -306.05</td><td style=\"text-align: right;\">           -0.344443</td><td style=\"text-align: right;\">             -1703.2</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 956000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-18-29\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.4423730732758421\n",
      "  episode_reward_mean: -301.16539484387937\n",
      "  episode_reward_min: -1566.192088512489\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4780\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3604064881801605\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: -0.2529354691505432\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015929097309708595\n",
      "          model: {}\n",
      "          policy_loss: -0.02441689744591713\n",
      "          total_loss: 224.63003540039062\n",
      "          vf_explained_var: 0.5074716806411743\n",
      "          vf_loss: 224.6487274169922\n",
      "    num_agent_steps_sampled: 956000\n",
      "    num_agent_steps_trained: 956000\n",
      "    num_steps_sampled: 956000\n",
      "    num_steps_trained: 956000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 239\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.414285714285715\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1200966624350601\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17651187643228827\n",
      "    mean_inference_ms: 1.2132124329996679\n",
      "    mean_raw_obs_processing_ms: 0.10261326025566073\n",
      "  time_since_restore: 1452.1789140701294\n",
      "  time_this_iter_s: 5.917804479598999\n",
      "  time_total_s: 1452.1789140701294\n",
      "  timers:\n",
      "    learn_throughput: 1481.883\n",
      "    learn_time_ms: 2699.268\n",
      "    load_throughput: 5911633.545\n",
      "    load_time_ms: 0.677\n",
      "    sample_throughput: 667.318\n",
      "    sample_time_ms: 5994.139\n",
      "    update_time_ms: 2.852\n",
      "  timestamp: 1636042709\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 956000\n",
      "  training_iteration: 239\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:18:31 (running for 00:24:38.42)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   239</td><td style=\"text-align: right;\">         1452.18</td><td style=\"text-align: right;\">956000</td><td style=\"text-align: right;\">-301.165</td><td style=\"text-align: right;\">           -0.442373</td><td style=\"text-align: right;\">            -1566.19</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 960000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-18-35\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.4423730732758421\n",
      "  episode_reward_mean: -301.4337548682042\n",
      "  episode_reward_min: -1566.192088512489\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4800\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3604064881801605\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.7281898260116577\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009967324323952198\n",
      "          model: {}\n",
      "          policy_loss: -0.019881686195731163\n",
      "          total_loss: 1533.566650390625\n",
      "          vf_explained_var: 0.5493341684341431\n",
      "          vf_loss: 1533.583251953125\n",
      "    num_agent_steps_sampled: 960000\n",
      "    num_agent_steps_trained: 960000\n",
      "    num_steps_sampled: 960000\n",
      "    num_steps_trained: 960000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 240\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.912500000000001\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12008910880972205\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1764981093774095\n",
      "    mean_inference_ms: 1.2131045120559196\n",
      "    mean_raw_obs_processing_ms: 0.1026053783630206\n",
      "  time_since_restore: 1458.1373031139374\n",
      "  time_this_iter_s: 5.958389043807983\n",
      "  time_total_s: 1458.1373031139374\n",
      "  timers:\n",
      "    learn_throughput: 1481.268\n",
      "    learn_time_ms: 2700.388\n",
      "    load_throughput: 5901236.722\n",
      "    load_time_ms: 0.678\n",
      "    sample_throughput: 667.028\n",
      "    sample_time_ms: 5996.752\n",
      "    update_time_ms: 2.864\n",
      "  timestamp: 1636042715\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 960000\n",
      "  training_iteration: 240\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:18:36 (running for 00:24:43.42)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   240</td><td style=\"text-align: right;\">         1458.14</td><td style=\"text-align: right;\">960000</td><td style=\"text-align: right;\">-301.434</td><td style=\"text-align: right;\">           -0.442373</td><td style=\"text-align: right;\">            -1566.19</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 964000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-18-41\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.20133548406698307\n",
      "  episode_reward_mean: -299.18453401221143\n",
      "  episode_reward_min: -1719.4496555045273\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4820\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3604064881801605\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.11086214333772659\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01149810291826725\n",
      "          model: {}\n",
      "          policy_loss: -0.02109040692448616\n",
      "          total_loss: 3661.68896484375\n",
      "          vf_explained_var: 0.308706670999527\n",
      "          vf_loss: 3661.705810546875\n",
      "    num_agent_steps_sampled: 964000\n",
      "    num_agent_steps_trained: 964000\n",
      "    num_steps_sampled: 964000\n",
      "    num_steps_trained: 964000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 241\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.471428571428572\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12008105758899071\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17648377057731696\n",
      "    mean_inference_ms: 1.212997677366962\n",
      "    mean_raw_obs_processing_ms: 0.10259698536131012\n",
      "  time_since_restore: 1464.056728363037\n",
      "  time_this_iter_s: 5.9194252490997314\n",
      "  time_total_s: 1464.056728363037\n",
      "  timers:\n",
      "    learn_throughput: 1481.06\n",
      "    learn_time_ms: 2700.769\n",
      "    load_throughput: 5936525.954\n",
      "    load_time_ms: 0.674\n",
      "    sample_throughput: 667.119\n",
      "    sample_time_ms: 5995.93\n",
      "    update_time_ms: 2.861\n",
      "  timestamp: 1636042721\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 964000\n",
      "  training_iteration: 241\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:18:42 (running for 00:24:49.39)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   241</td><td style=\"text-align: right;\">         1464.06</td><td style=\"text-align: right;\">964000</td><td style=\"text-align: right;\">-299.185</td><td style=\"text-align: right;\">           -0.201335</td><td style=\"text-align: right;\">            -1719.45</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 968000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-18-47\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.20133548406698307\n",
      "  episode_reward_mean: -317.12029473694963\n",
      "  episode_reward_min: -1719.4496555045273\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4840\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3604064881801605\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.30350354313850403\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007211730349808931\n",
      "          model: {}\n",
      "          policy_loss: -0.016658654436469078\n",
      "          total_loss: 905.4865112304688\n",
      "          vf_explained_var: 0.480299174785614\n",
      "          vf_loss: 905.5005493164062\n",
      "    num_agent_steps_sampled: 968000\n",
      "    num_agent_steps_trained: 968000\n",
      "    num_steps_sampled: 968000\n",
      "    num_steps_trained: 968000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 242\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.6875\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1200717571732233\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17646782477062675\n",
      "    mean_inference_ms: 1.2128826431735613\n",
      "    mean_raw_obs_processing_ms: 0.10258810013375402\n",
      "  time_since_restore: 1470.0368473529816\n",
      "  time_this_iter_s: 5.980118989944458\n",
      "  time_total_s: 1470.0368473529816\n",
      "  timers:\n",
      "    learn_throughput: 1480.712\n",
      "    learn_time_ms: 2701.403\n",
      "    load_throughput: 5941150.891\n",
      "    load_time_ms: 0.673\n",
      "    sample_throughput: 666.819\n",
      "    sample_time_ms: 5998.633\n",
      "    update_time_ms: 2.869\n",
      "  timestamp: 1636042727\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 968000\n",
      "  training_iteration: 242\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:18:47 (running for 00:24:54.42)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   242</td><td style=\"text-align: right;\">         1470.04</td><td style=\"text-align: right;\">968000</td><td style=\"text-align: right;\"> -317.12</td><td style=\"text-align: right;\">           -0.201335</td><td style=\"text-align: right;\">            -1719.45</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:18:52 (running for 00:24:59.42)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   242</td><td style=\"text-align: right;\">         1470.04</td><td style=\"text-align: right;\">968000</td><td style=\"text-align: right;\"> -317.12</td><td style=\"text-align: right;\">           -0.201335</td><td style=\"text-align: right;\">            -1719.45</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 972000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-18-53\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.20133548406698307\n",
      "  episode_reward_mean: -289.0849086116895\n",
      "  episode_reward_min: -1719.4496555045273\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4860\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3604064881801605\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.023390628397464752\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013149986043572426\n",
      "          model: {}\n",
      "          policy_loss: -0.024598101153969765\n",
      "          total_loss: 681.6257934570312\n",
      "          vf_explained_var: 0.41804972290992737\n",
      "          vf_loss: 681.645751953125\n",
      "    num_agent_steps_sampled: 972000\n",
      "    num_agent_steps_trained: 972000\n",
      "    num_steps_sampled: 972000\n",
      "    num_steps_trained: 972000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 243\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.528571428571428\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12006256829402444\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17645232384467596\n",
      "    mean_inference_ms: 1.2127617850238486\n",
      "    mean_raw_obs_processing_ms: 0.10257905368572352\n",
      "  time_since_restore: 1475.9598701000214\n",
      "  time_this_iter_s: 5.923022747039795\n",
      "  time_total_s: 1475.9598701000214\n",
      "  timers:\n",
      "    learn_throughput: 1481.137\n",
      "    learn_time_ms: 2700.628\n",
      "    load_throughput: 5895844.813\n",
      "    load_time_ms: 0.678\n",
      "    sample_throughput: 666.917\n",
      "    sample_time_ms: 5997.744\n",
      "    update_time_ms: 2.859\n",
      "  timestamp: 1636042733\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 972000\n",
      "  training_iteration: 243\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:18:58 (running for 00:25:05.39)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   243</td><td style=\"text-align: right;\">         1475.96</td><td style=\"text-align: right;\">972000</td><td style=\"text-align: right;\">-289.085</td><td style=\"text-align: right;\">           -0.201335</td><td style=\"text-align: right;\">            -1719.45</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 976000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-18-59\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.20133548406698307\n",
      "  episode_reward_mean: -313.03951433521274\n",
      "  episode_reward_min: -1746.1977645437291\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4880\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3604064881801605\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.5765169262886047\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006871141493320465\n",
      "          model: {}\n",
      "          policy_loss: -0.015624226070940495\n",
      "          total_loss: 3579.816162109375\n",
      "          vf_explained_var: 0.3841802477836609\n",
      "          vf_loss: 3579.829345703125\n",
      "    num_agent_steps_sampled: 976000\n",
      "    num_agent_steps_trained: 976000\n",
      "    num_steps_sampled: 976000\n",
      "    num_steps_trained: 976000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 244\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.7875\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12005530949802667\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1764399964341707\n",
      "    mean_inference_ms: 1.2126597055067598\n",
      "    mean_raw_obs_processing_ms: 0.10257138322945876\n",
      "  time_since_restore: 1481.9362437725067\n",
      "  time_this_iter_s: 5.976373672485352\n",
      "  time_total_s: 1481.9362437725067\n",
      "  timers:\n",
      "    learn_throughput: 1481.633\n",
      "    learn_time_ms: 2699.723\n",
      "    load_throughput: 5888395.339\n",
      "    load_time_ms: 0.679\n",
      "    sample_throughput: 666.319\n",
      "    sample_time_ms: 6003.127\n",
      "    update_time_ms: 2.846\n",
      "  timestamp: 1636042739\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 976000\n",
      "  training_iteration: 244\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:19:03 (running for 00:25:10.42)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   244</td><td style=\"text-align: right;\">         1481.94</td><td style=\"text-align: right;\">976000</td><td style=\"text-align: right;\"> -313.04</td><td style=\"text-align: right;\">           -0.201335</td><td style=\"text-align: right;\">             -1746.2</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 980000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-19-05\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.20133548406698307\n",
      "  episode_reward_mean: -305.19250934245605\n",
      "  episode_reward_min: -1746.1977645437291\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4900\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3604064881801605\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.6344183087348938\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008328042924404144\n",
      "          model: {}\n",
      "          policy_loss: -0.013551365584135056\n",
      "          total_loss: 2927.17529296875\n",
      "          vf_explained_var: 0.5580344200134277\n",
      "          vf_loss: 2927.18603515625\n",
      "    num_agent_steps_sampled: 980000\n",
      "    num_agent_steps_trained: 980000\n",
      "    num_steps_sampled: 980000\n",
      "    num_steps_trained: 980000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 245\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.442857142857143\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12004635321721491\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17642497771557636\n",
      "    mean_inference_ms: 1.2125438516461533\n",
      "    mean_raw_obs_processing_ms: 0.10256270885506556\n",
      "  time_since_restore: 1487.837560892105\n",
      "  time_this_iter_s: 5.901317119598389\n",
      "  time_total_s: 1487.837560892105\n",
      "  timers:\n",
      "    learn_throughput: 1482.48\n",
      "    learn_time_ms: 2698.181\n",
      "    load_throughput: 5925204.309\n",
      "    load_time_ms: 0.675\n",
      "    sample_throughput: 666.802\n",
      "    sample_time_ms: 5998.782\n",
      "    update_time_ms: 2.858\n",
      "  timestamp: 1636042745\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 980000\n",
      "  training_iteration: 245\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:19:09 (running for 00:25:16.37)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   245</td><td style=\"text-align: right;\">         1487.84</td><td style=\"text-align: right;\">980000</td><td style=\"text-align: right;\">-305.193</td><td style=\"text-align: right;\">           -0.201335</td><td style=\"text-align: right;\">             -1746.2</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 984000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-19-11\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.9522583610129097\n",
      "  episode_reward_mean: -330.1844647275827\n",
      "  episode_reward_min: -1746.1977645437291\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4920\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3604064881801605\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.8502122163772583\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007262384984642267\n",
      "          model: {}\n",
      "          policy_loss: -0.014510458335280418\n",
      "          total_loss: 1943.4744873046875\n",
      "          vf_explained_var: 0.6344550848007202\n",
      "          vf_loss: 1943.486328125\n",
      "    num_agent_steps_sampled: 984000\n",
      "    num_agent_steps_trained: 984000\n",
      "    num_steps_sampled: 984000\n",
      "    num_steps_trained: 984000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 246\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.6625\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12003757054980566\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17641069906552537\n",
      "    mean_inference_ms: 1.2124262836572246\n",
      "    mean_raw_obs_processing_ms: 0.10255438808435224\n",
      "  time_since_restore: 1493.757860660553\n",
      "  time_this_iter_s: 5.920299768447876\n",
      "  time_total_s: 1493.757860660553\n",
      "  timers:\n",
      "    learn_throughput: 1482.571\n",
      "    learn_time_ms: 2698.015\n",
      "    load_throughput: 5947890.949\n",
      "    load_time_ms: 0.673\n",
      "    sample_throughput: 667.039\n",
      "    sample_time_ms: 5996.648\n",
      "    update_time_ms: 2.85\n",
      "  timestamp: 1636042751\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 984000\n",
      "  training_iteration: 246\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:19:15 (running for 00:25:22.34)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   246</td><td style=\"text-align: right;\">         1493.76</td><td style=\"text-align: right;\">984000</td><td style=\"text-align: right;\">-330.184</td><td style=\"text-align: right;\">           -0.952258</td><td style=\"text-align: right;\">             -1746.2</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 988000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-19-17\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.9522583610129097\n",
      "  episode_reward_mean: -324.92358635650896\n",
      "  episode_reward_min: -1785.677810508336\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4940\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3604064881801605\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.4129292666912079\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008698058314621449\n",
      "          model: {}\n",
      "          policy_loss: -0.016335362568497658\n",
      "          total_loss: 4055.028076171875\n",
      "          vf_explained_var: 0.2984515130519867\n",
      "          vf_loss: 4055.04150390625\n",
      "    num_agent_steps_sampled: 988000\n",
      "    num_agent_steps_trained: 988000\n",
      "    num_steps_sampled: 988000\n",
      "    num_steps_trained: 988000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 247\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.571428571428573\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12002828928249647\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17639575395422707\n",
      "    mean_inference_ms: 1.2122990918627026\n",
      "    mean_raw_obs_processing_ms: 0.10254599120216884\n",
      "  time_since_restore: 1499.7117528915405\n",
      "  time_this_iter_s: 5.953892230987549\n",
      "  time_total_s: 1499.7117528915405\n",
      "  timers:\n",
      "    learn_throughput: 1482.948\n",
      "    learn_time_ms: 2697.33\n",
      "    load_throughput: 5994003.573\n",
      "    load_time_ms: 0.667\n",
      "    sample_throughput: 667.757\n",
      "    sample_time_ms: 5990.199\n",
      "    update_time_ms: 2.841\n",
      "  timestamp: 1636042757\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 988000\n",
      "  training_iteration: 247\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:19:20 (running for 00:25:27.34)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   247</td><td style=\"text-align: right;\">         1499.71</td><td style=\"text-align: right;\">988000</td><td style=\"text-align: right;\">-324.924</td><td style=\"text-align: right;\">           -0.952258</td><td style=\"text-align: right;\">            -1785.68</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 992000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-19-23\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.3145586235473108\n",
      "  episode_reward_mean: -315.7551155329171\n",
      "  episode_reward_min: -1851.95747557049\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4960\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.3604064881801605\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: -0.10618028789758682\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.002688551554456353\n",
      "          model: {}\n",
      "          policy_loss: 0.04724306985735893\n",
      "          total_loss: 1611.9930419921875\n",
      "          vf_explained_var: 0.17161761224269867\n",
      "          vf_loss: 1611.9449462890625\n",
      "    num_agent_steps_sampled: 992000\n",
      "    num_agent_steps_trained: 992000\n",
      "    num_steps_sampled: 992000\n",
      "    num_steps_trained: 992000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 248\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.525\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12001886213856335\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1763806371947779\n",
      "    mean_inference_ms: 1.2121719493851244\n",
      "    mean_raw_obs_processing_ms: 0.102537781414709\n",
      "  time_since_restore: 1505.6415603160858\n",
      "  time_this_iter_s: 5.929807424545288\n",
      "  time_total_s: 1505.6415603160858\n",
      "  timers:\n",
      "    learn_throughput: 1483.357\n",
      "    learn_time_ms: 2696.587\n",
      "    load_throughput: 5986731.373\n",
      "    load_time_ms: 0.668\n",
      "    sample_throughput: 668.091\n",
      "    sample_time_ms: 5987.207\n",
      "    update_time_ms: 2.831\n",
      "  timestamp: 1636042763\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 992000\n",
      "  training_iteration: 248\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:19:26 (running for 00:25:33.32)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   248</td><td style=\"text-align: right;\">         1505.64</td><td style=\"text-align: right;\">992000</td><td style=\"text-align: right;\">-315.755</td><td style=\"text-align: right;\">           -0.314559</td><td style=\"text-align: right;\">            -1851.96</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 996000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-19-29\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.3145586235473108\n",
      "  episode_reward_mean: -295.6761436814528\n",
      "  episode_reward_min: -1851.95747557049\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4980\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.18020324409008026\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: -0.13253037631511688\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014717470854520798\n",
      "          model: {}\n",
      "          policy_loss: -0.016910728067159653\n",
      "          total_loss: 613.1573486328125\n",
      "          vf_explained_var: 0.4265807569026947\n",
      "          vf_loss: 613.1715087890625\n",
      "    num_agent_steps_sampled: 996000\n",
      "    num_agent_steps_trained: 996000\n",
      "    num_steps_sampled: 996000\n",
      "    num_steps_trained: 996000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 249\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.557142857142859\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1200085057322328\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17636347396533217\n",
      "    mean_inference_ms: 1.2120369565607272\n",
      "    mean_raw_obs_processing_ms: 0.10252858835941833\n",
      "  time_since_restore: 1511.5805170536041\n",
      "  time_this_iter_s: 5.9389567375183105\n",
      "  time_total_s: 1511.5805170536041\n",
      "  timers:\n",
      "    learn_throughput: 1483.556\n",
      "    learn_time_ms: 2696.225\n",
      "    load_throughput: 5952956.037\n",
      "    load_time_ms: 0.672\n",
      "    sample_throughput: 667.886\n",
      "    sample_time_ms: 5989.044\n",
      "    update_time_ms: 2.831\n",
      "  timestamp: 1636042769\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 996000\n",
      "  training_iteration: 249\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:19:32 (running for 00:25:39.30)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   249</td><td style=\"text-align: right;\">         1511.58</td><td style=\"text-align: right;\">996000</td><td style=\"text-align: right;\">-295.676</td><td style=\"text-align: right;\">           -0.314559</td><td style=\"text-align: right;\">            -1851.96</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 1000000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-19-35\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.3145586235473108\n",
      "  episode_reward_mean: -284.3441633232422\n",
      "  episode_reward_min: -1851.95747557049\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5000\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.18020324409008026\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.27143415808677673\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012045644223690033\n",
      "          model: {}\n",
      "          policy_loss: -0.01521763950586319\n",
      "          total_loss: 776.43359375\n",
      "          vf_explained_var: 0.4393116235733032\n",
      "          vf_loss: 776.4466552734375\n",
      "    num_agent_steps_sampled: 1000000\n",
      "    num_agent_steps_trained: 1000000\n",
      "    num_steps_sampled: 1000000\n",
      "    num_steps_trained: 1000000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 250\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.6875\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11999814889159661\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17634623817841302\n",
      "    mean_inference_ms: 1.2119006295117634\n",
      "    mean_raw_obs_processing_ms: 0.102519036157832\n",
      "  time_since_restore: 1517.4996082782745\n",
      "  time_this_iter_s: 5.91909122467041\n",
      "  time_total_s: 1517.4996082782745\n",
      "  timers:\n",
      "    learn_throughput: 1483.656\n",
      "    learn_time_ms: 2696.042\n",
      "    load_throughput: 5922903.34\n",
      "    load_time_ms: 0.675\n",
      "    sample_throughput: 668.381\n",
      "    sample_time_ms: 5984.613\n",
      "    update_time_ms: 2.83\n",
      "  timestamp: 1636042775\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1000000\n",
      "  training_iteration: 250\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:19:38 (running for 00:25:45.27)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   250</td><td style=\"text-align: right;\">          1517.5</td><td style=\"text-align: right;\">1000000</td><td style=\"text-align: right;\">-284.344</td><td style=\"text-align: right;\">           -0.314559</td><td style=\"text-align: right;\">            -1851.96</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 1004000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-19-41\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.31286263447746665\n",
      "  episode_reward_mean: -270.78823760489905\n",
      "  episode_reward_min: -1851.95747557049\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5020\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.18020324409008026\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.5572782754898071\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014201370999217033\n",
      "          model: {}\n",
      "          policy_loss: -0.018938083201646805\n",
      "          total_loss: 3015.467529296875\n",
      "          vf_explained_var: 0.36413154006004333\n",
      "          vf_loss: 3015.48388671875\n",
      "    num_agent_steps_sampled: 1004000\n",
      "    num_agent_steps_trained: 1004000\n",
      "    num_steps_sampled: 1004000\n",
      "    num_steps_trained: 1004000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 251\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.414285714285715\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11998874991981914\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17633016273922905\n",
      "    mean_inference_ms: 1.211774227903025\n",
      "    mean_raw_obs_processing_ms: 0.10251055233992928\n",
      "  time_since_restore: 1523.4622156620026\n",
      "  time_this_iter_s: 5.962607383728027\n",
      "  time_total_s: 1523.4622156620026\n",
      "  timers:\n",
      "    learn_throughput: 1483.359\n",
      "    learn_time_ms: 2696.582\n",
      "    load_throughput: 5916637.043\n",
      "    load_time_ms: 0.676\n",
      "    sample_throughput: 667.941\n",
      "    sample_time_ms: 5988.557\n",
      "    update_time_ms: 2.855\n",
      "  timestamp: 1636042781\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1004000\n",
      "  training_iteration: 251\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:19:43 (running for 00:25:50.28)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   251</td><td style=\"text-align: right;\">         1523.46</td><td style=\"text-align: right;\">1004000</td><td style=\"text-align: right;\">-270.788</td><td style=\"text-align: right;\">           -0.312863</td><td style=\"text-align: right;\">            -1851.96</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 1008000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-19-47\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.31286263447746665\n",
      "  episode_reward_mean: -251.0745469901288\n",
      "  episode_reward_min: -1851.95747557049\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5040\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.18020324409008026\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: -0.5556580424308777\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019741497933864594\n",
      "          model: {}\n",
      "          policy_loss: -0.028143228963017464\n",
      "          total_loss: 144.9324493408203\n",
      "          vf_explained_var: 0.3875163793563843\n",
      "          vf_loss: 144.95701599121094\n",
      "    num_agent_steps_sampled: 1008000\n",
      "    num_agent_steps_trained: 1008000\n",
      "    num_steps_sampled: 1008000\n",
      "    num_steps_trained: 1008000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 252\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.6125\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11997950794851221\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17631447176091797\n",
      "    mean_inference_ms: 1.2116603144932023\n",
      "    mean_raw_obs_processing_ms: 0.1025022768814053\n",
      "  time_since_restore: 1529.4357163906097\n",
      "  time_this_iter_s: 5.973500728607178\n",
      "  time_total_s: 1529.4357163906097\n",
      "  timers:\n",
      "    learn_throughput: 1483.935\n",
      "    learn_time_ms: 2695.536\n",
      "    load_throughput: 5934216.186\n",
      "    load_time_ms: 0.674\n",
      "    sample_throughput: 667.836\n",
      "    sample_time_ms: 5989.496\n",
      "    update_time_ms: 2.862\n",
      "  timestamp: 1636042787\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1008000\n",
      "  training_iteration: 252\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:19:48 (running for 00:25:55.30)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   252</td><td style=\"text-align: right;\">         1529.44</td><td style=\"text-align: right;\">1008000</td><td style=\"text-align: right;\">-251.075</td><td style=\"text-align: right;\">           -0.312863</td><td style=\"text-align: right;\">            -1851.96</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:19:53 (running for 00:26:00.31)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   252</td><td style=\"text-align: right;\">         1529.44</td><td style=\"text-align: right;\">1008000</td><td style=\"text-align: right;\">-251.075</td><td style=\"text-align: right;\">           -0.312863</td><td style=\"text-align: right;\">            -1851.96</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 1012000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-19-53\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.31286263447746665\n",
      "  episode_reward_mean: -269.5621673474125\n",
      "  episode_reward_min: -1708.039772511389\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5060\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.18020324409008026\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.16256479918956757\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.022967498749494553\n",
      "          model: {}\n",
      "          policy_loss: -0.027524292469024658\n",
      "          total_loss: 863.7724609375\n",
      "          vf_explained_var: 0.653756856918335\n",
      "          vf_loss: 863.7958374023438\n",
      "    num_agent_steps_sampled: 1012000\n",
      "    num_agent_steps_trained: 1012000\n",
      "    num_steps_sampled: 1012000\n",
      "    num_steps_trained: 1012000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 253\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.457142857142857\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11997158891980499\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17630063340581528\n",
      "    mean_inference_ms: 1.2115622133126156\n",
      "    mean_raw_obs_processing_ms: 0.10249502547201918\n",
      "  time_since_restore: 1535.445407629013\n",
      "  time_this_iter_s: 6.00969123840332\n",
      "  time_total_s: 1535.445407629013\n",
      "  timers:\n",
      "    learn_throughput: 1483.839\n",
      "    learn_time_ms: 2695.71\n",
      "    load_throughput: 5922485.174\n",
      "    load_time_ms: 0.675\n",
      "    sample_throughput: 667.0\n",
      "    sample_time_ms: 5997.0\n",
      "    update_time_ms: 2.858\n",
      "  timestamp: 1636042793\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1012000\n",
      "  training_iteration: 253\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:19:58 (running for 00:26:05.36)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   253</td><td style=\"text-align: right;\">         1535.45</td><td style=\"text-align: right;\">1012000</td><td style=\"text-align: right;\">-269.562</td><td style=\"text-align: right;\">           -0.312863</td><td style=\"text-align: right;\">            -1708.04</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 1016000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-19-59\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.15291211392138684\n",
      "  episode_reward_mean: -299.2398368996863\n",
      "  episode_reward_min: -1746.0024409003522\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5080\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2703048586845398\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.8968436121940613\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009961826726794243\n",
      "          model: {}\n",
      "          policy_loss: -0.016004102304577827\n",
      "          total_loss: 3876.29248046875\n",
      "          vf_explained_var: 0.43814700841903687\n",
      "          vf_loss: 3876.305419921875\n",
      "    num_agent_steps_sampled: 1016000\n",
      "    num_agent_steps_trained: 1016000\n",
      "    num_steps_sampled: 1016000\n",
      "    num_steps_trained: 1016000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 254\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.975000000000001\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1199640461092091\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17628749036378139\n",
      "    mean_inference_ms: 1.2114675270966366\n",
      "    mean_raw_obs_processing_ms: 0.10248813492055564\n",
      "  time_since_restore: 1541.3935241699219\n",
      "  time_this_iter_s: 5.9481165409088135\n",
      "  time_total_s: 1541.3935241699219\n",
      "  timers:\n",
      "    learn_throughput: 1483.744\n",
      "    learn_time_ms: 2695.882\n",
      "    load_throughput: 5944308.39\n",
      "    load_time_ms: 0.673\n",
      "    sample_throughput: 667.319\n",
      "    sample_time_ms: 5994.133\n",
      "    update_time_ms: 2.85\n",
      "  timestamp: 1636042799\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1016000\n",
      "  training_iteration: 254\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:20:04 (running for 00:26:11.36)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   254</td><td style=\"text-align: right;\">         1541.39</td><td style=\"text-align: right;\">1016000</td><td style=\"text-align: right;\"> -299.24</td><td style=\"text-align: right;\">           -0.152912</td><td style=\"text-align: right;\">               -1746</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 1020000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-20-05\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.14064503058188144\n",
      "  episode_reward_mean: -295.72076545099776\n",
      "  episode_reward_min: -1746.0024409003522\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5100\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2703048586845398\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.19705037772655487\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011714787222445011\n",
      "          model: {}\n",
      "          policy_loss: -0.017722127959132195\n",
      "          total_loss: 1158.396240234375\n",
      "          vf_explained_var: 0.34625181555747986\n",
      "          vf_loss: 1158.41064453125\n",
      "    num_agent_steps_sampled: 1020000\n",
      "    num_agent_steps_trained: 1020000\n",
      "    num_steps_sampled: 1020000\n",
      "    num_steps_trained: 1020000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 255\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.571428571428571\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11995611215512202\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17627426698660123\n",
      "    mean_inference_ms: 1.2113749056131915\n",
      "    mean_raw_obs_processing_ms: 0.10248154770378383\n",
      "  time_since_restore: 1547.3107681274414\n",
      "  time_this_iter_s: 5.917243957519531\n",
      "  time_total_s: 1547.3107681274414\n",
      "  timers:\n",
      "    learn_throughput: 1482.307\n",
      "    learn_time_ms: 2698.496\n",
      "    load_throughput: 5893359.562\n",
      "    load_time_ms: 0.679\n",
      "    sample_throughput: 667.445\n",
      "    sample_time_ms: 5993.0\n",
      "    update_time_ms: 2.814\n",
      "  timestamp: 1636042805\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1020000\n",
      "  training_iteration: 255\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:20:10 (running for 00:26:17.32)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   255</td><td style=\"text-align: right;\">         1547.31</td><td style=\"text-align: right;\">1020000</td><td style=\"text-align: right;\">-295.721</td><td style=\"text-align: right;\">           -0.140645</td><td style=\"text-align: right;\">               -1746</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 1024000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-20-11\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.14064503058188144\n",
      "  episode_reward_mean: -286.81316796927075\n",
      "  episode_reward_min: -1746.0024409003522\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5120\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2703048586845398\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: -0.017170190811157227\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012822292745113373\n",
      "          model: {}\n",
      "          policy_loss: -0.026403775438666344\n",
      "          total_loss: 393.2626037597656\n",
      "          vf_explained_var: 0.5769454836845398\n",
      "          vf_loss: 393.2855529785156\n",
      "    num_agent_steps_sampled: 1024000\n",
      "    num_agent_steps_trained: 1024000\n",
      "    num_steps_sampled: 1024000\n",
      "    num_steps_trained: 1024000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 256\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.4375\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11994730836384958\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17625969810103495\n",
      "    mean_inference_ms: 1.2112746345530678\n",
      "    mean_raw_obs_processing_ms: 0.1024741064408996\n",
      "  time_since_restore: 1553.2176752090454\n",
      "  time_this_iter_s: 5.906907081604004\n",
      "  time_total_s: 1553.2176752090454\n",
      "  timers:\n",
      "    learn_throughput: 1482.787\n",
      "    learn_time_ms: 2697.622\n",
      "    load_throughput: 5933376.715\n",
      "    load_time_ms: 0.674\n",
      "    sample_throughput: 667.164\n",
      "    sample_time_ms: 5995.526\n",
      "    update_time_ms: 2.804\n",
      "  timestamp: 1636042811\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1024000\n",
      "  training_iteration: 256\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:20:16 (running for 00:26:23.28)<br>Memory usage on this node: 11.4/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   256</td><td style=\"text-align: right;\">         1553.22</td><td style=\"text-align: right;\">1024000</td><td style=\"text-align: right;\">-286.813</td><td style=\"text-align: right;\">           -0.140645</td><td style=\"text-align: right;\">               -1746</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 1028000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-20-17\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.14064503058188144\n",
      "  episode_reward_mean: -304.3091049480444\n",
      "  episode_reward_min: -1746.0024409003522\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5140\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2703048586845398\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.06408672779798508\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008979988284409046\n",
      "          model: {}\n",
      "          policy_loss: -0.019757336005568504\n",
      "          total_loss: 501.19549560546875\n",
      "          vf_explained_var: 0.6679983735084534\n",
      "          vf_loss: 501.21282958984375\n",
      "    num_agent_steps_sampled: 1028000\n",
      "    num_agent_steps_trained: 1028000\n",
      "    num_steps_sampled: 1028000\n",
      "    num_steps_trained: 1028000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 257\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 9.37142857142857\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1199376898740944\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17624399355731846\n",
      "    mean_inference_ms: 1.2111564495391887\n",
      "    mean_raw_obs_processing_ms: 0.10246580964338646\n",
      "  time_since_restore: 1559.136525630951\n",
      "  time_this_iter_s: 5.918850421905518\n",
      "  time_total_s: 1559.136525630951\n",
      "  timers:\n",
      "    learn_throughput: 1481.568\n",
      "    learn_time_ms: 2699.843\n",
      "    load_throughput: 5904559.724\n",
      "    load_time_ms: 0.677\n",
      "    sample_throughput: 667.924\n",
      "    sample_time_ms: 5988.705\n",
      "    update_time_ms: 2.83\n",
      "  timestamp: 1636042817\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1028000\n",
      "  training_iteration: 257\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:20:22 (running for 00:26:29.25)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   257</td><td style=\"text-align: right;\">         1559.14</td><td style=\"text-align: right;\">1028000</td><td style=\"text-align: right;\">-304.309</td><td style=\"text-align: right;\">           -0.140645</td><td style=\"text-align: right;\">               -1746</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 1032000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-20-23\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.14064503058188144\n",
      "  episode_reward_mean: -319.55099024799574\n",
      "  episode_reward_min: -1746.0024409003522\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5160\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2703048586845398\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.8703708052635193\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014891340397298336\n",
      "          model: {}\n",
      "          policy_loss: -0.01828405261039734\n",
      "          total_loss: 3024.576416015625\n",
      "          vf_explained_var: 0.5276405215263367\n",
      "          vf_loss: 3024.5908203125\n",
      "    num_agent_steps_sampled: 1032000\n",
      "    num_agent_steps_trained: 1032000\n",
      "    num_steps_sampled: 1032000\n",
      "    num_steps_trained: 1032000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 258\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.0125\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1199290488895813\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.176229408734203\n",
      "    mean_inference_ms: 1.2110526718466286\n",
      "    mean_raw_obs_processing_ms: 0.10245950039785409\n",
      "  time_since_restore: 1565.1742219924927\n",
      "  time_this_iter_s: 6.037696361541748\n",
      "  time_total_s: 1565.1742219924927\n",
      "  timers:\n",
      "    learn_throughput: 1481.519\n",
      "    learn_time_ms: 2699.932\n",
      "    load_throughput: 5969264.926\n",
      "    load_time_ms: 0.67\n",
      "    sample_throughput: 666.501\n",
      "    sample_time_ms: 6001.49\n",
      "    update_time_ms: 2.826\n",
      "  timestamp: 1636042823\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1032000\n",
      "  training_iteration: 258\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:20:27 (running for 00:26:34.33)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   258</td><td style=\"text-align: right;\">         1565.17</td><td style=\"text-align: right;\">1032000</td><td style=\"text-align: right;\">-319.551</td><td style=\"text-align: right;\">           -0.140645</td><td style=\"text-align: right;\">               -1746</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 1036000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-20-29\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.12445832313570565\n",
      "  episode_reward_mean: -346.2186250007658\n",
      "  episode_reward_min: -1785.3435962350507\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5180\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2703048586845398\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 2.019979953765869\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011556328274309635\n",
      "          model: {}\n",
      "          policy_loss: -0.015795711427927017\n",
      "          total_loss: 7832.92138671875\n",
      "          vf_explained_var: 0.401279479265213\n",
      "          vf_loss: 7832.9345703125\n",
      "    num_agent_steps_sampled: 1036000\n",
      "    num_agent_steps_trained: 1036000\n",
      "    num_steps_sampled: 1036000\n",
      "    num_steps_trained: 1036000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 259\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.514285714285716\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11992000622503378\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17621435149765713\n",
      "    mean_inference_ms: 1.2109483875318658\n",
      "    mean_raw_obs_processing_ms: 0.10245234764231384\n",
      "  time_since_restore: 1571.1035571098328\n",
      "  time_this_iter_s: 5.929335117340088\n",
      "  time_total_s: 1571.1035571098328\n",
      "  timers:\n",
      "    learn_throughput: 1481.777\n",
      "    learn_time_ms: 2699.462\n",
      "    load_throughput: 6008385.918\n",
      "    load_time_ms: 0.666\n",
      "    sample_throughput: 666.56\n",
      "    sample_time_ms: 6000.964\n",
      "    update_time_ms: 2.835\n",
      "  timestamp: 1636042829\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1036000\n",
      "  training_iteration: 259\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:20:33 (running for 00:26:40.31)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   259</td><td style=\"text-align: right;\">          1571.1</td><td style=\"text-align: right;\">1036000</td><td style=\"text-align: right;\">-346.219</td><td style=\"text-align: right;\">           -0.124458</td><td style=\"text-align: right;\">            -1785.34</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 1040000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-20-35\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.12445832313570565\n",
      "  episode_reward_mean: -358.41043411076663\n",
      "  episode_reward_min: -1785.3435962350507\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5200\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2703048586845398\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.7702561020851135\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009463218040764332\n",
      "          model: {}\n",
      "          policy_loss: -0.015701044350862503\n",
      "          total_loss: 3286.841064453125\n",
      "          vf_explained_var: 0.3219088017940521\n",
      "          vf_loss: 3286.854248046875\n",
      "    num_agent_steps_sampled: 1040000\n",
      "    num_agent_steps_trained: 1040000\n",
      "    num_steps_sampled: 1040000\n",
      "    num_steps_trained: 1040000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 260\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.6\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11991211007319766\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17620044347408506\n",
      "    mean_inference_ms: 1.2108466623061118\n",
      "    mean_raw_obs_processing_ms: 0.10244549930075156\n",
      "  time_since_restore: 1577.052971124649\n",
      "  time_this_iter_s: 5.949414014816284\n",
      "  time_total_s: 1577.052971124649\n",
      "  timers:\n",
      "    learn_throughput: 1482.154\n",
      "    learn_time_ms: 2698.775\n",
      "    load_throughput: 6000649.523\n",
      "    load_time_ms: 0.667\n",
      "    sample_throughput: 666.164\n",
      "    sample_time_ms: 6004.526\n",
      "    update_time_ms: 2.849\n",
      "  timestamp: 1636042835\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1040000\n",
      "  training_iteration: 260\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:20:39 (running for 00:26:46.31)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   260</td><td style=\"text-align: right;\">         1577.05</td><td style=\"text-align: right;\">1040000</td><td style=\"text-align: right;\"> -358.41</td><td style=\"text-align: right;\">           -0.124458</td><td style=\"text-align: right;\">            -1785.34</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 1044000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-20-41\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.12445832313570565\n",
      "  episode_reward_mean: -368.05736475755964\n",
      "  episode_reward_min: -1785.3435962350507\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5220\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2703048586845398\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.39646416902542114\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012456377036869526\n",
      "          model: {}\n",
      "          policy_loss: -0.020440325140953064\n",
      "          total_loss: 2352.753173828125\n",
      "          vf_explained_var: 0.4926183521747589\n",
      "          vf_loss: 2352.769775390625\n",
      "    num_agent_steps_sampled: 1044000\n",
      "    num_agent_steps_trained: 1044000\n",
      "    num_steps_sampled: 1044000\n",
      "    num_steps_trained: 1044000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 261\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.585714285714285\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11990492264513972\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1761874456025789\n",
      "    mean_inference_ms: 1.2107484545429885\n",
      "    mean_raw_obs_processing_ms: 0.10243881134704322\n",
      "  time_since_restore: 1582.9721992015839\n",
      "  time_this_iter_s: 5.9192280769348145\n",
      "  time_total_s: 1582.9721992015839\n",
      "  timers:\n",
      "    learn_throughput: 1482.894\n",
      "    learn_time_ms: 2697.428\n",
      "    load_throughput: 6002581.753\n",
      "    load_time_ms: 0.666\n",
      "    sample_throughput: 666.604\n",
      "    sample_time_ms: 6000.56\n",
      "    update_time_ms: 2.834\n",
      "  timestamp: 1636042841\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1044000\n",
      "  training_iteration: 261\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:20:45 (running for 00:26:52.27)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   261</td><td style=\"text-align: right;\">         1582.97</td><td style=\"text-align: right;\">1044000</td><td style=\"text-align: right;\">-368.057</td><td style=\"text-align: right;\">           -0.124458</td><td style=\"text-align: right;\">            -1785.34</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 1048000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-20-47\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.12445832313570565\n",
      "  episode_reward_mean: -380.1732495884048\n",
      "  episode_reward_min: -1785.3435962350507\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5240\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2703048586845398\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.5918247103691101\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008601068519055843\n",
      "          model: {}\n",
      "          policy_loss: -0.012053604237735271\n",
      "          total_loss: 2453.693603515625\n",
      "          vf_explained_var: 0.4438055753707886\n",
      "          vf_loss: 2453.703125\n",
      "    num_agent_steps_sampled: 1048000\n",
      "    num_agent_steps_trained: 1048000\n",
      "    num_steps_sampled: 1048000\n",
      "    num_steps_trained: 1048000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 262\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.4875\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1198981933632695\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17617481173533997\n",
      "    mean_inference_ms: 1.2106568955908883\n",
      "    mean_raw_obs_processing_ms: 0.10243219347986816\n",
      "  time_since_restore: 1588.9055848121643\n",
      "  time_this_iter_s: 5.933385610580444\n",
      "  time_total_s: 1588.9055848121643\n",
      "  timers:\n",
      "    learn_throughput: 1482.356\n",
      "    learn_time_ms: 2698.406\n",
      "    load_throughput: 6005589.92\n",
      "    load_time_ms: 0.666\n",
      "    sample_throughput: 667.297\n",
      "    sample_time_ms: 5994.334\n",
      "    update_time_ms: 2.821\n",
      "  timestamp: 1636042847\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1048000\n",
      "  training_iteration: 262\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:20:51 (running for 00:26:58.25)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   262</td><td style=\"text-align: right;\">         1588.91</td><td style=\"text-align: right;\">1048000</td><td style=\"text-align: right;\">-380.173</td><td style=\"text-align: right;\">           -0.124458</td><td style=\"text-align: right;\">            -1785.34</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 1052000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-20-53\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.12445832313570565\n",
      "  episode_reward_mean: -376.33996202500236\n",
      "  episode_reward_min: -1785.3435962350507\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5260\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2703048586845398\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.0101300477981567\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008913501165807247\n",
      "          model: {}\n",
      "          policy_loss: 0.02330213598906994\n",
      "          total_loss: 4273.66162109375\n",
      "          vf_explained_var: 0.25021106004714966\n",
      "          vf_loss: 4273.63525390625\n",
      "    num_agent_steps_sampled: 1052000\n",
      "    num_agent_steps_trained: 1052000\n",
      "    num_steps_sampled: 1052000\n",
      "    num_steps_trained: 1052000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 263\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.371428571428572\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.119890507392166\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1761609232942385\n",
      "    mean_inference_ms: 1.2105483826259904\n",
      "    mean_raw_obs_processing_ms: 0.10242368995260497\n",
      "  time_since_restore: 1594.8713459968567\n",
      "  time_this_iter_s: 5.965761184692383\n",
      "  time_total_s: 1594.8713459968567\n",
      "  timers:\n",
      "    learn_throughput: 1482.911\n",
      "    learn_time_ms: 2697.397\n",
      "    load_throughput: 6072980.526\n",
      "    load_time_ms: 0.659\n",
      "    sample_throughput: 667.585\n",
      "    sample_time_ms: 5991.749\n",
      "    update_time_ms: 2.833\n",
      "  timestamp: 1636042853\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1052000\n",
      "  training_iteration: 263\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:20:56 (running for 00:27:03.27)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   263</td><td style=\"text-align: right;\">         1594.87</td><td style=\"text-align: right;\">1052000</td><td style=\"text-align: right;\"> -376.34</td><td style=\"text-align: right;\">           -0.124458</td><td style=\"text-align: right;\">            -1785.34</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 1056000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-20-59\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.13959404187797814\n",
      "  episode_reward_mean: -334.2674677209\n",
      "  episode_reward_min: -1768.8959789485807\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5280\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2703048586845398\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.5200719237327576\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008615165017545223\n",
      "          model: {}\n",
      "          policy_loss: -0.016615580767393112\n",
      "          total_loss: 1819.2998046875\n",
      "          vf_explained_var: 0.16299402713775635\n",
      "          vf_loss: 1819.3140869140625\n",
      "    num_agent_steps_sampled: 1056000\n",
      "    num_agent_steps_trained: 1056000\n",
      "    num_steps_sampled: 1056000\n",
      "    num_steps_trained: 1056000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 264\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.587499999999999\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11988263295552844\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17614734627869183\n",
      "    mean_inference_ms: 1.2104419891698137\n",
      "    mean_raw_obs_processing_ms: 0.10241596594931879\n",
      "  time_since_restore: 1600.816107749939\n",
      "  time_this_iter_s: 5.944761753082275\n",
      "  time_total_s: 1600.816107749939\n",
      "  timers:\n",
      "    learn_throughput: 1483.122\n",
      "    learn_time_ms: 2697.013\n",
      "    load_throughput: 6038010.509\n",
      "    load_time_ms: 0.662\n",
      "    sample_throughput: 667.683\n",
      "    sample_time_ms: 5990.865\n",
      "    update_time_ms: 2.862\n",
      "  timestamp: 1636042859\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1056000\n",
      "  training_iteration: 264\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:21:02 (running for 00:27:09.26)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   264</td><td style=\"text-align: right;\">         1600.82</td><td style=\"text-align: right;\">1056000</td><td style=\"text-align: right;\">-334.267</td><td style=\"text-align: right;\">           -0.139594</td><td style=\"text-align: right;\">             -1768.9</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 1060000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-21-05\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.29169747672119506\n",
      "  episode_reward_mean: -334.23738008075134\n",
      "  episode_reward_min: -1768.8959789485807\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5300\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2703048586845398\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.5827774405479431\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01274885330349207\n",
      "          model: {}\n",
      "          policy_loss: -0.019299671053886414\n",
      "          total_loss: 2046.6741943359375\n",
      "          vf_explained_var: 0.4939068853855133\n",
      "          vf_loss: 2046.6900634765625\n",
      "    num_agent_steps_sampled: 1060000\n",
      "    num_agent_steps_trained: 1060000\n",
      "    num_steps_sampled: 1060000\n",
      "    num_steps_trained: 1060000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 265\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.785714285714286\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11987459509210596\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17613343546138904\n",
      "    mean_inference_ms: 1.2103372321788328\n",
      "    mean_raw_obs_processing_ms: 0.10240829802543178\n",
      "  time_since_restore: 1606.7534761428833\n",
      "  time_this_iter_s: 5.937368392944336\n",
      "  time_total_s: 1606.7534761428833\n",
      "  timers:\n",
      "    learn_throughput: 1482.877\n",
      "    learn_time_ms: 2697.459\n",
      "    load_throughput: 6043883.425\n",
      "    load_time_ms: 0.662\n",
      "    sample_throughput: 667.528\n",
      "    sample_time_ms: 5992.261\n",
      "    update_time_ms: 2.872\n",
      "  timestamp: 1636042865\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1060000\n",
      "  training_iteration: 265\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:21:08 (running for 00:27:15.25)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   265</td><td style=\"text-align: right;\">         1606.75</td><td style=\"text-align: right;\">1060000</td><td style=\"text-align: right;\">-334.237</td><td style=\"text-align: right;\">           -0.291697</td><td style=\"text-align: right;\">             -1768.9</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 1064000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-21-11\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.20413267477192276\n",
      "  episode_reward_mean: -336.94282766663645\n",
      "  episode_reward_min: -1768.8959789485807\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5320\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2703048586845398\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.6071887612342834\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008155607618391514\n",
      "          model: {}\n",
      "          policy_loss: 0.042220283299684525\n",
      "          total_loss: 1738.3934326171875\n",
      "          vf_explained_var: 0.4562506675720215\n",
      "          vf_loss: 1738.3489990234375\n",
      "    num_agent_steps_sampled: 1064000\n",
      "    num_agent_steps_trained: 1064000\n",
      "    num_steps_sampled: 1064000\n",
      "    num_steps_trained: 1064000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 266\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.55\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1198671621763557\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17612020160993108\n",
      "    mean_inference_ms: 1.2102376642959667\n",
      "    mean_raw_obs_processing_ms: 0.10240113745986319\n",
      "  time_since_restore: 1612.708713054657\n",
      "  time_this_iter_s: 5.955236911773682\n",
      "  time_total_s: 1612.708713054657\n",
      "  timers:\n",
      "    learn_throughput: 1482.448\n",
      "    learn_time_ms: 2698.24\n",
      "    load_throughput: 6002366.999\n",
      "    load_time_ms: 0.666\n",
      "    sample_throughput: 667.098\n",
      "    sample_time_ms: 5996.124\n",
      "    update_time_ms: 2.873\n",
      "  timestamp: 1636042871\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1064000\n",
      "  training_iteration: 266\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:21:13 (running for 00:27:20.25)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   266</td><td style=\"text-align: right;\">         1612.71</td><td style=\"text-align: right;\">1064000</td><td style=\"text-align: right;\">-336.943</td><td style=\"text-align: right;\">           -0.204133</td><td style=\"text-align: right;\">             -1768.9</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 1068000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-21-17\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.1326057897127723\n",
      "  episode_reward_mean: -326.20174781404904\n",
      "  episode_reward_min: -1739.6397467082627\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5340\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2703048586845398\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.4205373525619507\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010110244154930115\n",
      "          model: {}\n",
      "          policy_loss: -0.015866277739405632\n",
      "          total_loss: 643.9497680664062\n",
      "          vf_explained_var: 0.38114333152770996\n",
      "          vf_loss: 643.962890625\n",
      "    num_agent_steps_sampled: 1068000\n",
      "    num_agent_steps_trained: 1068000\n",
      "    num_steps_sampled: 1068000\n",
      "    num_steps_trained: 1068000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 267\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.414285714285713\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11985978335824982\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17610719529499239\n",
      "    mean_inference_ms: 1.210138180540153\n",
      "    mean_raw_obs_processing_ms: 0.10239442482680924\n",
      "  time_since_restore: 1618.6500475406647\n",
      "  time_this_iter_s: 5.94133448600769\n",
      "  time_total_s: 1618.6500475406647\n",
      "  timers:\n",
      "    learn_throughput: 1483.747\n",
      "    learn_time_ms: 2695.878\n",
      "    load_throughput: 5988654.649\n",
      "    load_time_ms: 0.668\n",
      "    sample_throughput: 666.5\n",
      "    sample_time_ms: 6001.504\n",
      "    update_time_ms: 2.86\n",
      "  timestamp: 1636042877\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1068000\n",
      "  training_iteration: 267\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:21:19 (running for 00:27:26.24)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   267</td><td style=\"text-align: right;\">         1618.65</td><td style=\"text-align: right;\">1068000</td><td style=\"text-align: right;\">-326.202</td><td style=\"text-align: right;\">           -0.132606</td><td style=\"text-align: right;\">            -1739.64</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 1072000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-21-23\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.1326057897127723\n",
      "  episode_reward_mean: -296.2248362746818\n",
      "  episode_reward_min: -1734.2820125889561\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5360\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2703048586845398\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: -0.23775406181812286\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01226932555437088\n",
      "          model: {}\n",
      "          policy_loss: -0.02230484038591385\n",
      "          total_loss: 155.80043029785156\n",
      "          vf_explained_var: 0.3971674144268036\n",
      "          vf_loss: 155.81942749023438\n",
      "    num_agent_steps_sampled: 1072000\n",
      "    num_agent_steps_trained: 1072000\n",
      "    num_steps_sampled: 1072000\n",
      "    num_steps_trained: 1072000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 268\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.425\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11985110961814122\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17609232064211852\n",
      "    mean_inference_ms: 1.2100279751299907\n",
      "    mean_raw_obs_processing_ms: 0.10238659226314031\n",
      "  time_since_restore: 1624.7413563728333\n",
      "  time_this_iter_s: 6.091308832168579\n",
      "  time_total_s: 1624.7413563728333\n",
      "  timers:\n",
      "    learn_throughput: 1474.19\n",
      "    learn_time_ms: 2713.354\n",
      "    load_throughput: 5995502.984\n",
      "    load_time_ms: 0.667\n",
      "    sample_throughput: 668.074\n",
      "    sample_time_ms: 5987.361\n",
      "    update_time_ms: 2.85\n",
      "  timestamp: 1636042883\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1072000\n",
      "  training_iteration: 268\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:21:24 (running for 00:27:31.38)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   268</td><td style=\"text-align: right;\">         1624.74</td><td style=\"text-align: right;\">1072000</td><td style=\"text-align: right;\">-296.225</td><td style=\"text-align: right;\">           -0.132606</td><td style=\"text-align: right;\">            -1734.28</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 1076000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-21-29\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.1326057897127723\n",
      "  episode_reward_mean: -316.57994118922716\n",
      "  episode_reward_min: -1734.2820125889561\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5380\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2703048586845398\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.2856403589248657\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0115429125726223\n",
      "          model: {}\n",
      "          policy_loss: -0.01944197155535221\n",
      "          total_loss: 3914.3359375\n",
      "          vf_explained_var: 0.45735299587249756\n",
      "          vf_loss: 3914.352783203125\n",
      "    num_agent_steps_sampled: 1076000\n",
      "    num_agent_steps_trained: 1076000\n",
      "    num_steps_sampled: 1076000\n",
      "    num_steps_trained: 1076000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 269\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.6125\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11984237437487202\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.176076806998868\n",
      "    mean_inference_ms: 1.2099093888152268\n",
      "    mean_raw_obs_processing_ms: 0.10237821688039378\n",
      "  time_since_restore: 1630.6555817127228\n",
      "  time_this_iter_s: 5.914225339889526\n",
      "  time_total_s: 1630.6555817127228\n",
      "  timers:\n",
      "    learn_throughput: 1474.385\n",
      "    learn_time_ms: 2712.995\n",
      "    load_throughput: 5933586.561\n",
      "    load_time_ms: 0.674\n",
      "    sample_throughput: 666.286\n",
      "    sample_time_ms: 6003.425\n",
      "    update_time_ms: 2.838\n",
      "  timestamp: 1636042889\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1076000\n",
      "  training_iteration: 269\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:21:30 (running for 00:27:37.34)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   269</td><td style=\"text-align: right;\">         1630.66</td><td style=\"text-align: right;\">1076000</td><td style=\"text-align: right;\"> -316.58</td><td style=\"text-align: right;\">           -0.132606</td><td style=\"text-align: right;\">            -1734.28</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 1080000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-21-35\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.1326057897127723\n",
      "  episode_reward_mean: -311.21563745008166\n",
      "  episode_reward_min: -1734.2820125889561\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5400\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2703048586845398\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.39316749572753906\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012266856618225574\n",
      "          model: {}\n",
      "          policy_loss: -0.01918637752532959\n",
      "          total_loss: 2382.654296875\n",
      "          vf_explained_var: 0.5095441341400146\n",
      "          vf_loss: 2382.669921875\n",
      "    num_agent_steps_sampled: 1080000\n",
      "    num_agent_steps_trained: 1080000\n",
      "    num_steps_sampled: 1080000\n",
      "    num_steps_trained: 1080000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 270\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.471428571428572\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11983429190290996\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17606266408765994\n",
      "    mean_inference_ms: 1.2098025259975147\n",
      "    mean_raw_obs_processing_ms: 0.10237070857836666\n",
      "  time_since_restore: 1636.5982208251953\n",
      "  time_this_iter_s: 5.942639112472534\n",
      "  time_total_s: 1636.5982208251953\n",
      "  timers:\n",
      "    learn_throughput: 1474.594\n",
      "    learn_time_ms: 2712.61\n",
      "    load_throughput: 6008385.918\n",
      "    load_time_ms: 0.666\n",
      "    sample_throughput: 666.343\n",
      "    sample_time_ms: 6002.911\n",
      "    update_time_ms: 2.826\n",
      "  timestamp: 1636042895\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1080000\n",
      "  training_iteration: 270\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:21:36 (running for 00:27:43.33)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   270</td><td style=\"text-align: right;\">          1636.6</td><td style=\"text-align: right;\">1080000</td><td style=\"text-align: right;\">-311.216</td><td style=\"text-align: right;\">           -0.132606</td><td style=\"text-align: right;\">            -1734.28</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 1084000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-21-41\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.1326057897127723\n",
      "  episode_reward_mean: -300.8249034683855\n",
      "  episode_reward_min: -1726.4398226001097\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5420\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2703048586845398\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.32622653245925903\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01228273008018732\n",
      "          model: {}\n",
      "          policy_loss: -0.019828079268336296\n",
      "          total_loss: 757.0706787109375\n",
      "          vf_explained_var: 0.44236916303634644\n",
      "          vf_loss: 757.0872192382812\n",
      "    num_agent_steps_sampled: 1084000\n",
      "    num_agent_steps_trained: 1084000\n",
      "    num_steps_sampled: 1084000\n",
      "    num_steps_trained: 1084000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 271\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.5125\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11982698691396011\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1760496246375676\n",
      "    mean_inference_ms: 1.2097039103617058\n",
      "    mean_raw_obs_processing_ms: 0.10236389410258201\n",
      "  time_since_restore: 1642.5770099163055\n",
      "  time_this_iter_s: 5.9787890911102295\n",
      "  time_total_s: 1642.5770099163055\n",
      "  timers:\n",
      "    learn_throughput: 1473.284\n",
      "    learn_time_ms: 2715.023\n",
      "    load_throughput: 5970327.035\n",
      "    load_time_ms: 0.67\n",
      "    sample_throughput: 665.974\n",
      "    sample_time_ms: 6006.241\n",
      "    update_time_ms: 2.825\n",
      "  timestamp: 1636042901\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1084000\n",
      "  training_iteration: 271\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:21:41 (running for 00:27:48.36)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   271</td><td style=\"text-align: right;\">         1642.58</td><td style=\"text-align: right;\">1084000</td><td style=\"text-align: right;\">-300.825</td><td style=\"text-align: right;\">           -0.132606</td><td style=\"text-align: right;\">            -1726.44</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:21:46 (running for 00:27:53.37)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   271</td><td style=\"text-align: right;\">         1642.58</td><td style=\"text-align: right;\">1084000</td><td style=\"text-align: right;\">-300.825</td><td style=\"text-align: right;\">           -0.132606</td><td style=\"text-align: right;\">            -1726.44</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 1088000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-21-47\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.13969365794809596\n",
      "  episode_reward_mean: -285.2666550611435\n",
      "  episode_reward_min: -1726.4398226001097\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5440\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2703048586845398\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: -0.1478014886379242\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01623361185193062\n",
      "          model: {}\n",
      "          policy_loss: -0.02437962219119072\n",
      "          total_loss: 189.76904296875\n",
      "          vf_explained_var: 0.302813321352005\n",
      "          vf_loss: 189.78903198242188\n",
      "    num_agent_steps_sampled: 1088000\n",
      "    num_agent_steps_trained: 1088000\n",
      "    num_steps_sampled: 1088000\n",
      "    num_steps_trained: 1088000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 272\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.428571428571429\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11981918677949928\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17603560340148638\n",
      "    mean_inference_ms: 1.2096033233094219\n",
      "    mean_raw_obs_processing_ms: 0.10235665488821001\n",
      "  time_since_restore: 1648.482345342636\n",
      "  time_this_iter_s: 5.905335426330566\n",
      "  time_total_s: 1648.482345342636\n",
      "  timers:\n",
      "    learn_throughput: 1473.294\n",
      "    learn_time_ms: 2715.004\n",
      "    load_throughput: 5905391.059\n",
      "    load_time_ms: 0.677\n",
      "    sample_throughput: 666.052\n",
      "    sample_time_ms: 6005.535\n",
      "    update_time_ms: 2.845\n",
      "  timestamp: 1636042907\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1088000\n",
      "  training_iteration: 272\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:21:52 (running for 00:27:59.31)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   272</td><td style=\"text-align: right;\">         1648.48</td><td style=\"text-align: right;\">1088000</td><td style=\"text-align: right;\">-285.267</td><td style=\"text-align: right;\">           -0.139694</td><td style=\"text-align: right;\">            -1726.44</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 1092000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-21-53\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.22379249777972976\n",
      "  episode_reward_mean: -293.89709363274534\n",
      "  episode_reward_min: -1726.4398226001097\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5460\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2703048586845398\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.45117685198783875\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008810079656541348\n",
      "          model: {}\n",
      "          policy_loss: -0.0174397062510252\n",
      "          total_loss: 4169.75634765625\n",
      "          vf_explained_var: 0.09452974051237106\n",
      "          vf_loss: 4169.771484375\n",
      "    num_agent_steps_sampled: 1092000\n",
      "    num_agent_steps_trained: 1092000\n",
      "    num_steps_sampled: 1092000\n",
      "    num_steps_trained: 1092000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 273\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.4875\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11981152631914072\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17602145897025007\n",
      "    mean_inference_ms: 1.209501111806247\n",
      "    mean_raw_obs_processing_ms: 0.10234949742475191\n",
      "  time_since_restore: 1654.405175447464\n",
      "  time_this_iter_s: 5.922830104827881\n",
      "  time_total_s: 1654.405175447464\n",
      "  timers:\n",
      "    learn_throughput: 1471.987\n",
      "    learn_time_ms: 2717.416\n",
      "    load_throughput: 5896259.225\n",
      "    load_time_ms: 0.678\n",
      "    sample_throughput: 666.787\n",
      "    sample_time_ms: 5998.92\n",
      "    update_time_ms: 2.86\n",
      "  timestamp: 1636042913\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1092000\n",
      "  training_iteration: 273\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:21:58 (running for 00:28:05.31)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   273</td><td style=\"text-align: right;\">         1654.41</td><td style=\"text-align: right;\">1092000</td><td style=\"text-align: right;\">-293.897</td><td style=\"text-align: right;\">           -0.223792</td><td style=\"text-align: right;\">            -1726.44</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 1096000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-21-59\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.22379249777972976\n",
      "  episode_reward_mean: -290.8586731345655\n",
      "  episode_reward_min: -1702.9852415892876\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5480\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.2703048586845398\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.1832813024520874\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02542940340936184\n",
      "          model: {}\n",
      "          policy_loss: -0.025858886539936066\n",
      "          total_loss: 3691.162353515625\n",
      "          vf_explained_var: 0.24840563535690308\n",
      "          vf_loss: 3691.181640625\n",
      "    num_agent_steps_sampled: 1096000\n",
      "    num_agent_steps_trained: 1096000\n",
      "    num_steps_sampled: 1096000\n",
      "    num_steps_trained: 1096000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 274\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.6\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11980372576432269\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17600772836350656\n",
      "    mean_inference_ms: 1.2094031624705237\n",
      "    mean_raw_obs_processing_ms: 0.1023425276118213\n",
      "  time_since_restore: 1660.3390052318573\n",
      "  time_this_iter_s: 5.9338297843933105\n",
      "  time_total_s: 1660.3390052318573\n",
      "  timers:\n",
      "    learn_throughput: 1471.672\n",
      "    learn_time_ms: 2717.996\n",
      "    load_throughput: 5887982.031\n",
      "    load_time_ms: 0.679\n",
      "    sample_throughput: 666.473\n",
      "    sample_time_ms: 6001.74\n",
      "    update_time_ms: 2.86\n",
      "  timestamp: 1636042919\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1096000\n",
      "  training_iteration: 274\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:22:04 (running for 00:28:11.30)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   274</td><td style=\"text-align: right;\">         1660.34</td><td style=\"text-align: right;\">1096000</td><td style=\"text-align: right;\">-290.859</td><td style=\"text-align: right;\">           -0.223792</td><td style=\"text-align: right;\">            -1702.99</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 1100000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-22-05\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.22379249777972976\n",
      "  episode_reward_mean: -273.9897635726069\n",
      "  episode_reward_min: -1702.9852415892876\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5500\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4054573178291321\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.05526421591639519\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007630556356161833\n",
      "          model: {}\n",
      "          policy_loss: -0.024920519441366196\n",
      "          total_loss: 407.7188720703125\n",
      "          vf_explained_var: 0.5016279816627502\n",
      "          vf_loss: 407.7406921386719\n",
      "    num_agent_steps_sampled: 1100000\n",
      "    num_agent_steps_trained: 1100000\n",
      "    num_steps_sampled: 1100000\n",
      "    num_steps_trained: 1100000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 275\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.525\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.119796074095262\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17599413853617843\n",
      "    mean_inference_ms: 1.209303375427784\n",
      "    mean_raw_obs_processing_ms: 0.1023359057666767\n",
      "  time_since_restore: 1666.3105385303497\n",
      "  time_this_iter_s: 5.971533298492432\n",
      "  time_total_s: 1666.3105385303497\n",
      "  timers:\n",
      "    learn_throughput: 1471.561\n",
      "    learn_time_ms: 2718.201\n",
      "    load_throughput: 5874786.75\n",
      "    load_time_ms: 0.681\n",
      "    sample_throughput: 665.873\n",
      "    sample_time_ms: 6007.148\n",
      "    update_time_ms: 2.886\n",
      "  timestamp: 1636042925\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1100000\n",
      "  training_iteration: 275\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:22:09 (running for 00:28:16.32)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   275</td><td style=\"text-align: right;\">         1666.31</td><td style=\"text-align: right;\">1100000</td><td style=\"text-align: right;\"> -273.99</td><td style=\"text-align: right;\">           -0.223792</td><td style=\"text-align: right;\">            -1702.99</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 1104000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-22-11\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.1328667783215572\n",
      "  episode_reward_mean: -282.6993625884854\n",
      "  episode_reward_min: -1702.9852415892876\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5520\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4054573178291321\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.7144468426704407\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007381115108728409\n",
      "          model: {}\n",
      "          policy_loss: -0.016361255198717117\n",
      "          total_loss: 2169.8359375\n",
      "          vf_explained_var: 0.39837220311164856\n",
      "          vf_loss: 2169.849609375\n",
      "    num_agent_steps_sampled: 1104000\n",
      "    num_agent_steps_trained: 1104000\n",
      "    num_steps_sampled: 1104000\n",
      "    num_steps_trained: 1104000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 276\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.571428571428571\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11978681130757338\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17597876380985739\n",
      "    mean_inference_ms: 1.2091907774656467\n",
      "    mean_raw_obs_processing_ms: 0.10232858756330294\n",
      "  time_since_restore: 1672.223909854889\n",
      "  time_this_iter_s: 5.913371324539185\n",
      "  time_total_s: 1672.223909854889\n",
      "  timers:\n",
      "    learn_throughput: 1471.936\n",
      "    learn_time_ms: 2717.509\n",
      "    load_throughput: 5900406.556\n",
      "    load_time_ms: 0.678\n",
      "    sample_throughput: 666.265\n",
      "    sample_time_ms: 6003.614\n",
      "    update_time_ms: 2.896\n",
      "  timestamp: 1636042931\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1104000\n",
      "  training_iteration: 276\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:22:15 (running for 00:28:22.28)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   276</td><td style=\"text-align: right;\">         1672.22</td><td style=\"text-align: right;\">1104000</td><td style=\"text-align: right;\">-282.699</td><td style=\"text-align: right;\">           -0.132867</td><td style=\"text-align: right;\">            -1702.99</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 1108000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-22-17\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.1328667783215572\n",
      "  episode_reward_mean: -341.38815989165914\n",
      "  episode_reward_min: -1769.4850007941443\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5540\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4054573178291321\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.8779338598251343\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010516485199332237\n",
      "          model: {}\n",
      "          policy_loss: -0.01788017898797989\n",
      "          total_loss: 5816.0126953125\n",
      "          vf_explained_var: 0.5646659135818481\n",
      "          vf_loss: 5816.0263671875\n",
      "    num_agent_steps_sampled: 1108000\n",
      "    num_agent_steps_trained: 1108000\n",
      "    num_steps_sampled: 1108000\n",
      "    num_steps_trained: 1108000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 277\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.55\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11977846363882051\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17596522017892127\n",
      "    mean_inference_ms: 1.20908691481203\n",
      "    mean_raw_obs_processing_ms: 0.10232217371869959\n",
      "  time_since_restore: 1678.1833345890045\n",
      "  time_this_iter_s: 5.959424734115601\n",
      "  time_total_s: 1678.1833345890045\n",
      "  timers:\n",
      "    learn_throughput: 1470.96\n",
      "    learn_time_ms: 2719.313\n",
      "    load_throughput: 5943044.988\n",
      "    load_time_ms: 0.673\n",
      "    sample_throughput: 666.322\n",
      "    sample_time_ms: 6003.1\n",
      "    update_time_ms: 2.899\n",
      "  timestamp: 1636042937\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1108000\n",
      "  training_iteration: 277\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:22:20 (running for 00:28:27.29)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   277</td><td style=\"text-align: right;\">         1678.18</td><td style=\"text-align: right;\">1108000</td><td style=\"text-align: right;\">-341.388</td><td style=\"text-align: right;\">           -0.132867</td><td style=\"text-align: right;\">            -1769.49</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 1112000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-22-23\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.1328667783215572\n",
      "  episode_reward_mean: -375.4925523330496\n",
      "  episode_reward_min: -1831.0587437754602\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5560\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4054573178291321\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.7005565166473389\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0067367213778197765\n",
      "          model: {}\n",
      "          policy_loss: -0.013315761461853981\n",
      "          total_loss: 5757.484375\n",
      "          vf_explained_var: 0.31919172406196594\n",
      "          vf_loss: 5757.4951171875\n",
      "    num_agent_steps_sampled: 1112000\n",
      "    num_agent_steps_trained: 1112000\n",
      "    num_steps_sampled: 1112000\n",
      "    num_steps_trained: 1112000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 278\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.485714285714286\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11977112701805794\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17595393540225548\n",
      "    mean_inference_ms: 1.2089999662395037\n",
      "    mean_raw_obs_processing_ms: 0.10231710662134458\n",
      "  time_since_restore: 1684.1563863754272\n",
      "  time_this_iter_s: 5.9730517864227295\n",
      "  time_total_s: 1684.1563863754272\n",
      "  timers:\n",
      "    learn_throughput: 1480.145\n",
      "    learn_time_ms: 2702.438\n",
      "    load_throughput: 5943466.062\n",
      "    load_time_ms: 0.673\n",
      "    sample_throughput: 665.625\n",
      "    sample_time_ms: 6009.39\n",
      "    update_time_ms: 2.9\n",
      "  timestamp: 1636042943\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1112000\n",
      "  training_iteration: 278\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:22:25 (running for 00:28:32.30)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   278</td><td style=\"text-align: right;\">         1684.16</td><td style=\"text-align: right;\">1112000</td><td style=\"text-align: right;\">-375.493</td><td style=\"text-align: right;\">           -0.132867</td><td style=\"text-align: right;\">            -1831.06</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 1116000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-22-29\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.1328667783215572\n",
      "  episode_reward_mean: -354.28989079660175\n",
      "  episode_reward_min: -1831.0587437754602\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5580\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4054573178291321\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.5743705630302429\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007870743051171303\n",
      "          model: {}\n",
      "          policy_loss: -0.022300440818071365\n",
      "          total_loss: 1244.263427734375\n",
      "          vf_explained_var: 0.29105815291404724\n",
      "          vf_loss: 1244.282470703125\n",
      "    num_agent_steps_sampled: 1116000\n",
      "    num_agent_steps_trained: 1116000\n",
      "    num_steps_sampled: 1116000\n",
      "    num_steps_trained: 1116000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 279\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.537500000000001\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1197652653232177\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17594404025428836\n",
      "    mean_inference_ms: 1.2089274337254308\n",
      "    mean_raw_obs_processing_ms: 0.10231321542351221\n",
      "  time_since_restore: 1690.142195224762\n",
      "  time_this_iter_s: 5.985808849334717\n",
      "  time_total_s: 1690.142195224762\n",
      "  timers:\n",
      "    learn_throughput: 1479.787\n",
      "    learn_time_ms: 2703.092\n",
      "    load_throughput: 5992933.024\n",
      "    load_time_ms: 0.667\n",
      "    sample_throughput: 666.76\n",
      "    sample_time_ms: 5999.162\n",
      "    update_time_ms: 2.918\n",
      "  timestamp: 1636042949\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1116000\n",
      "  training_iteration: 279\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:22:30 (running for 00:28:37.34)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   279</td><td style=\"text-align: right;\">         1690.14</td><td style=\"text-align: right;\">1116000</td><td style=\"text-align: right;\"> -354.29</td><td style=\"text-align: right;\">           -0.132867</td><td style=\"text-align: right;\">            -1831.06</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 1120000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-22-35\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.1328667783215572\n",
      "  episode_reward_mean: -393.7414454155621\n",
      "  episode_reward_min: -1831.0587437754602\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5600\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4054573178291321\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.4089893102645874\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008205396123230457\n",
      "          model: {}\n",
      "          policy_loss: -0.014236915856599808\n",
      "          total_loss: 3606.7216796875\n",
      "          vf_explained_var: 0.5711876153945923\n",
      "          vf_loss: 3606.73291015625\n",
      "    num_agent_steps_sampled: 1120000\n",
      "    num_agent_steps_trained: 1120000\n",
      "    num_steps_sampled: 1120000\n",
      "    num_steps_trained: 1120000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 280\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.414285714285715\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11975929751153844\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17593434528438653\n",
      "    mean_inference_ms: 1.2088532808677377\n",
      "    mean_raw_obs_processing_ms: 0.10230898609629051\n",
      "  time_since_restore: 1696.0895891189575\n",
      "  time_this_iter_s: 5.947393894195557\n",
      "  time_total_s: 1696.0895891189575\n",
      "  timers:\n",
      "    learn_throughput: 1479.054\n",
      "    learn_time_ms: 2704.431\n",
      "    load_throughput: 5978411.431\n",
      "    load_time_ms: 0.669\n",
      "    sample_throughput: 666.799\n",
      "    sample_time_ms: 5998.814\n",
      "    update_time_ms: 2.922\n",
      "  timestamp: 1636042955\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1120000\n",
      "  training_iteration: 280\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:22:36 (running for 00:28:43.33)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   280</td><td style=\"text-align: right;\">         1696.09</td><td style=\"text-align: right;\">1120000</td><td style=\"text-align: right;\">-393.741</td><td style=\"text-align: right;\">           -0.132867</td><td style=\"text-align: right;\">            -1831.06</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 1124000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-22-41\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.1513329195885571\n",
      "  episode_reward_mean: -381.70080128225914\n",
      "  episode_reward_min: -1831.0587437754602\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5620\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4054573178291321\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.3207629919052124\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007224545814096928\n",
      "          model: {}\n",
      "          policy_loss: -0.016202818602323532\n",
      "          total_loss: 612.1283569335938\n",
      "          vf_explained_var: 0.41361385583877563\n",
      "          vf_loss: 612.1415405273438\n",
      "    num_agent_steps_sampled: 1124000\n",
      "    num_agent_steps_trained: 1124000\n",
      "    num_steps_sampled: 1124000\n",
      "    num_steps_trained: 1124000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 281\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 9.362499999999999\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11975480148290593\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1759260917628243\n",
      "    mean_inference_ms: 1.2087885419994657\n",
      "    mean_raw_obs_processing_ms: 0.10230532281582282\n",
      "  time_since_restore: 1702.053926229477\n",
      "  time_this_iter_s: 5.964337110519409\n",
      "  time_total_s: 1702.053926229477\n",
      "  timers:\n",
      "    learn_throughput: 1479.267\n",
      "    learn_time_ms: 2704.042\n",
      "    load_throughput: 5956126.101\n",
      "    load_time_ms: 0.672\n",
      "    sample_throughput: 666.785\n",
      "    sample_time_ms: 5998.936\n",
      "    update_time_ms: 2.919\n",
      "  timestamp: 1636042961\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1124000\n",
      "  training_iteration: 281\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:22:41 (running for 00:28:48.34)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   281</td><td style=\"text-align: right;\">         1702.05</td><td style=\"text-align: right;\">1124000</td><td style=\"text-align: right;\">-381.701</td><td style=\"text-align: right;\">           -0.151333</td><td style=\"text-align: right;\">            -1831.06</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:22:46 (running for 00:28:53.35)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   281</td><td style=\"text-align: right;\">         1702.05</td><td style=\"text-align: right;\">1124000</td><td style=\"text-align: right;\">-381.701</td><td style=\"text-align: right;\">           -0.151333</td><td style=\"text-align: right;\">            -1831.06</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 1128000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-22-47\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.1513329195885571\n",
      "  episode_reward_mean: -338.42818107628693\n",
      "  episode_reward_min: -1831.0587437754602\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5640\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4054573178291321\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.5964993834495544\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0059831030666828156\n",
      "          model: {}\n",
      "          policy_loss: -0.014514274895191193\n",
      "          total_loss: 2304.365234375\n",
      "          vf_explained_var: 0.21201935410499573\n",
      "          vf_loss: 2304.37744140625\n",
      "    num_agent_steps_sampled: 1128000\n",
      "    num_agent_steps_trained: 1128000\n",
      "    num_steps_sampled: 1128000\n",
      "    num_steps_trained: 1128000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 282\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.357142857142858\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11975105323570057\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17591912078871416\n",
      "    mean_inference_ms: 1.208730485869919\n",
      "    mean_raw_obs_processing_ms: 0.10230203771589551\n",
      "  time_since_restore: 1708.0298936367035\n",
      "  time_this_iter_s: 5.9759674072265625\n",
      "  time_total_s: 1708.0298936367035\n",
      "  timers:\n",
      "    learn_throughput: 1479.649\n",
      "    learn_time_ms: 2703.343\n",
      "    load_throughput: 6011184.522\n",
      "    load_time_ms: 0.665\n",
      "    sample_throughput: 665.941\n",
      "    sample_time_ms: 6006.54\n",
      "    update_time_ms: 2.905\n",
      "  timestamp: 1636042967\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1128000\n",
      "  training_iteration: 282\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:22:51 (running for 00:28:58.37)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   282</td><td style=\"text-align: right;\">         1708.03</td><td style=\"text-align: right;\">1128000</td><td style=\"text-align: right;\">-338.428</td><td style=\"text-align: right;\">           -0.151333</td><td style=\"text-align: right;\">            -1831.06</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 1132000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-22-53\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.1513329195885571\n",
      "  episode_reward_mean: -318.84834167782594\n",
      "  episode_reward_min: -1808.4859119429896\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5660\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4054573178291321\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.8747513890266418\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006335139740258455\n",
      "          model: {}\n",
      "          policy_loss: -0.015730421990156174\n",
      "          total_loss: 2744.41162109375\n",
      "          vf_explained_var: 0.4739033281803131\n",
      "          vf_loss: 2744.424560546875\n",
      "    num_agent_steps_sampled: 1132000\n",
      "    num_agent_steps_trained: 1132000\n",
      "    num_steps_sampled: 1132000\n",
      "    num_steps_trained: 1132000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 283\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.6875\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11974632739501267\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.175910385837666\n",
      "    mean_inference_ms: 1.208658607841803\n",
      "    mean_raw_obs_processing_ms: 0.10229747325917103\n",
      "  time_since_restore: 1713.949188709259\n",
      "  time_this_iter_s: 5.919295072555542\n",
      "  time_total_s: 1713.949188709259\n",
      "  timers:\n",
      "    learn_throughput: 1479.76\n",
      "    learn_time_ms: 2703.141\n",
      "    load_throughput: 5968627.842\n",
      "    load_time_ms: 0.67\n",
      "    sample_throughput: 666.029\n",
      "    sample_time_ms: 6005.747\n",
      "    update_time_ms: 2.891\n",
      "  timestamp: 1636042973\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1132000\n",
      "  training_iteration: 283\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:22:57 (running for 00:29:04.34)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   283</td><td style=\"text-align: right;\">         1713.95</td><td style=\"text-align: right;\">1132000</td><td style=\"text-align: right;\">-318.848</td><td style=\"text-align: right;\">           -0.151333</td><td style=\"text-align: right;\">            -1808.49</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 1136000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-22-59\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.15406966052220103\n",
      "  episode_reward_mean: -323.4958660999727\n",
      "  episode_reward_min: -1813.7603255030845\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5680\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.4054573178291321\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.6465117931365967\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.004007982090115547\n",
      "          model: {}\n",
      "          policy_loss: -0.009581870399415493\n",
      "          total_loss: 2705.9462890625\n",
      "          vf_explained_var: 0.5965080261230469\n",
      "          vf_loss: 2705.954345703125\n",
      "    num_agent_steps_sampled: 1136000\n",
      "    num_agent_steps_trained: 1136000\n",
      "    num_steps_sampled: 1136000\n",
      "    num_steps_trained: 1136000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 284\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.457142857142857\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11974082589401748\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1759006908959644\n",
      "    mean_inference_ms: 1.2085799794489904\n",
      "    mean_raw_obs_processing_ms: 0.10229234388923654\n",
      "  time_since_restore: 1719.8721396923065\n",
      "  time_this_iter_s: 5.922950983047485\n",
      "  time_total_s: 1719.8721396923065\n",
      "  timers:\n",
      "    learn_throughput: 1480.409\n",
      "    learn_time_ms: 2701.957\n",
      "    load_throughput: 5983742.064\n",
      "    load_time_ms: 0.668\n",
      "    sample_throughput: 666.315\n",
      "    sample_time_ms: 6003.167\n",
      "    update_time_ms: 2.882\n",
      "  timestamp: 1636042979\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1136000\n",
      "  training_iteration: 284\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:23:03 (running for 00:29:10.31)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   284</td><td style=\"text-align: right;\">         1719.87</td><td style=\"text-align: right;\">1136000</td><td style=\"text-align: right;\">-323.496</td><td style=\"text-align: right;\">            -0.15407</td><td style=\"text-align: right;\">            -1813.76</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 1140000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-23-05\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.15406966052220103\n",
      "  episode_reward_mean: -274.37707747021864\n",
      "  episode_reward_min: -1813.7603255030845\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5700\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20272865891456604\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: -0.21480031311511993\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015542333945631981\n",
      "          model: {}\n",
      "          policy_loss: -0.02159768156707287\n",
      "          total_loss: 189.12527465820312\n",
      "          vf_explained_var: 0.39262524247169495\n",
      "          vf_loss: 189.1437225341797\n",
      "    num_agent_steps_sampled: 1140000\n",
      "    num_agent_steps_trained: 1140000\n",
      "    num_steps_sampled: 1140000\n",
      "    num_steps_trained: 1140000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 285\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.5875\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11973416253555451\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1758889181632478\n",
      "    mean_inference_ms: 1.208491801995408\n",
      "    mean_raw_obs_processing_ms: 0.1022864155112487\n",
      "  time_since_restore: 1725.7942774295807\n",
      "  time_this_iter_s: 5.92213773727417\n",
      "  time_total_s: 1725.7942774295807\n",
      "  timers:\n",
      "    learn_throughput: 1480.482\n",
      "    learn_time_ms: 2701.824\n",
      "    load_throughput: 6043448.003\n",
      "    load_time_ms: 0.662\n",
      "    sample_throughput: 667.127\n",
      "    sample_time_ms: 5995.862\n",
      "    update_time_ms: 2.863\n",
      "  timestamp: 1636042985\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1140000\n",
      "  training_iteration: 285\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:23:09 (running for 00:29:16.28)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   285</td><td style=\"text-align: right;\">         1725.79</td><td style=\"text-align: right;\">1140000</td><td style=\"text-align: right;\">-274.377</td><td style=\"text-align: right;\">            -0.15407</td><td style=\"text-align: right;\">            -1813.76</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 1144000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-23-11\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.15406966052220103\n",
      "  episode_reward_mean: -289.94638185308776\n",
      "  episode_reward_min: -1813.7603255030845\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5720\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20272865891456604\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.9075402617454529\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012963110581040382\n",
      "          model: {}\n",
      "          policy_loss: -0.019947892054915428\n",
      "          total_loss: 2424.678466796875\n",
      "          vf_explained_var: 0.5831372737884521\n",
      "          vf_loss: 2424.69580078125\n",
      "    num_agent_steps_sampled: 1144000\n",
      "    num_agent_steps_trained: 1144000\n",
      "    num_steps_sampled: 1144000\n",
      "    num_steps_trained: 1144000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 286\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.257142857142856\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11972731948611148\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17587731864100264\n",
      "    mean_inference_ms: 1.208402949001471\n",
      "    mean_raw_obs_processing_ms: 0.10228041814958308\n",
      "  time_since_restore: 1731.7355597019196\n",
      "  time_this_iter_s: 5.941282272338867\n",
      "  time_total_s: 1731.7355597019196\n",
      "  timers:\n",
      "    learn_throughput: 1480.727\n",
      "    learn_time_ms: 2701.376\n",
      "    load_throughput: 6017436.964\n",
      "    load_time_ms: 0.665\n",
      "    sample_throughput: 666.761\n",
      "    sample_time_ms: 5999.148\n",
      "    update_time_ms: 2.85\n",
      "  timestamp: 1636042991\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1144000\n",
      "  training_iteration: 286\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:23:15 (running for 00:29:22.27)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   286</td><td style=\"text-align: right;\">         1731.74</td><td style=\"text-align: right;\">1144000</td><td style=\"text-align: right;\">-289.946</td><td style=\"text-align: right;\">            -0.15407</td><td style=\"text-align: right;\">            -1813.76</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 1148000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-23-17\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.15406966052220103\n",
      "  episode_reward_mean: -261.2367152806949\n",
      "  episode_reward_min: -1813.7603255030845\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5740\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20272865891456604\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: -0.3964390456676483\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0071556963957846165\n",
      "          model: {}\n",
      "          policy_loss: -0.015712713822722435\n",
      "          total_loss: 145.55636596679688\n",
      "          vf_explained_var: 0.38201963901519775\n",
      "          vf_loss: 145.5706329345703\n",
      "    num_agent_steps_sampled: 1148000\n",
      "    num_agent_steps_trained: 1148000\n",
      "    num_steps_sampled: 1148000\n",
      "    num_steps_trained: 1148000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 287\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 9.4625\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1197197275666748\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.175864202487607\n",
      "    mean_inference_ms: 1.2083064660948377\n",
      "    mean_raw_obs_processing_ms: 0.10227382733369797\n",
      "  time_since_restore: 1737.7075736522675\n",
      "  time_this_iter_s: 5.9720139503479\n",
      "  time_total_s: 1737.7075736522675\n",
      "  timers:\n",
      "    learn_throughput: 1479.547\n",
      "    learn_time_ms: 2703.53\n",
      "    load_throughput: 5980968.949\n",
      "    load_time_ms: 0.669\n",
      "    sample_throughput: 666.909\n",
      "    sample_time_ms: 5997.823\n",
      "    update_time_ms: 2.834\n",
      "  timestamp: 1636042997\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1148000\n",
      "  training_iteration: 287\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:23:20 (running for 00:29:27.29)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   287</td><td style=\"text-align: right;\">         1737.71</td><td style=\"text-align: right;\">1148000</td><td style=\"text-align: right;\">-261.237</td><td style=\"text-align: right;\">            -0.15407</td><td style=\"text-align: right;\">            -1813.76</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 1152000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-23-23\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.19873394446347725\n",
      "  episode_reward_mean: -253.68871819844324\n",
      "  episode_reward_min: -1813.7603255030845\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5760\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20272865891456604\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.51179438829422\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014183524996042252\n",
      "          model: {}\n",
      "          policy_loss: -0.023398451507091522\n",
      "          total_loss: 835.8727416992188\n",
      "          vf_explained_var: 0.39835479855537415\n",
      "          vf_loss: 835.8931884765625\n",
      "    num_agent_steps_sampled: 1152000\n",
      "    num_agent_steps_trained: 1152000\n",
      "    num_steps_sampled: 1152000\n",
      "    num_steps_trained: 1152000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 288\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 10.8\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11971465574325207\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1758554912296961\n",
      "    mean_inference_ms: 1.2082486511232926\n",
      "    mean_raw_obs_processing_ms: 0.1022697680231356\n",
      "  time_since_restore: 1743.7677474021912\n",
      "  time_this_iter_s: 6.060173749923706\n",
      "  time_total_s: 1743.7677474021912\n",
      "  timers:\n",
      "    learn_throughput: 1479.457\n",
      "    learn_time_ms: 2703.695\n",
      "    load_throughput: 5969052.549\n",
      "    load_time_ms: 0.67\n",
      "    sample_throughput: 665.697\n",
      "    sample_time_ms: 6008.741\n",
      "    update_time_ms: 2.845\n",
      "  timestamp: 1636043003\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1152000\n",
      "  training_iteration: 288\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:23:25 (running for 00:29:32.39)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   288</td><td style=\"text-align: right;\">         1743.77</td><td style=\"text-align: right;\">1152000</td><td style=\"text-align: right;\">-253.689</td><td style=\"text-align: right;\">           -0.198734</td><td style=\"text-align: right;\">            -1813.76</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 1156000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-23-29\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.19873394446347725\n",
      "  episode_reward_mean: -231.71218714781978\n",
      "  episode_reward_min: -1729.1410123467365\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5780\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20272865891456604\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.04070518538355827\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010342326015233994\n",
      "          model: {}\n",
      "          policy_loss: -0.017213545739650726\n",
      "          total_loss: 319.59893798828125\n",
      "          vf_explained_var: 0.37675175070762634\n",
      "          vf_loss: 319.6140441894531\n",
      "    num_agent_steps_sampled: 1156000\n",
      "    num_agent_steps_trained: 1156000\n",
      "    num_steps_sampled: 1156000\n",
      "    num_steps_trained: 1156000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 289\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.5\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11971086586168248\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17584875414611353\n",
      "    mean_inference_ms: 1.2082006481328704\n",
      "    mean_raw_obs_processing_ms: 0.10226632704268843\n",
      "  time_since_restore: 1749.7704856395721\n",
      "  time_this_iter_s: 6.0027382373809814\n",
      "  time_total_s: 1749.7704856395721\n",
      "  timers:\n",
      "    learn_throughput: 1479.414\n",
      "    learn_time_ms: 2703.774\n",
      "    load_throughput: 5985449.875\n",
      "    load_time_ms: 0.668\n",
      "    sample_throughput: 665.493\n",
      "    sample_time_ms: 6010.583\n",
      "    update_time_ms: 2.827\n",
      "  timestamp: 1636043009\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1156000\n",
      "  training_iteration: 289\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:23:30 (running for 00:29:37.44)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   289</td><td style=\"text-align: right;\">         1749.77</td><td style=\"text-align: right;\">1156000</td><td style=\"text-align: right;\">-231.712</td><td style=\"text-align: right;\">           -0.198734</td><td style=\"text-align: right;\">            -1729.14</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 1160000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-23-35\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.19873394446347725\n",
      "  episode_reward_mean: -245.18691322338452\n",
      "  episode_reward_min: -1823.1652471904513\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5800\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20272865891456604\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.45683571696281433\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01191797573119402\n",
      "          model: {}\n",
      "          policy_loss: -0.019778380170464516\n",
      "          total_loss: 2525.264404296875\n",
      "          vf_explained_var: 0.4419908821582794\n",
      "          vf_loss: 2525.28173828125\n",
      "    num_agent_steps_sampled: 1160000\n",
      "    num_agent_steps_trained: 1160000\n",
      "    num_steps_sampled: 1160000\n",
      "    num_steps_trained: 1160000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 290\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.485714285714286\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11970729337403448\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17584203681102029\n",
      "    mean_inference_ms: 1.2081550914048211\n",
      "    mean_raw_obs_processing_ms: 0.10226305855166654\n",
      "  time_since_restore: 1755.6734852790833\n",
      "  time_this_iter_s: 5.902999639511108\n",
      "  time_total_s: 1755.6734852790833\n",
      "  timers:\n",
      "    learn_throughput: 1479.749\n",
      "    learn_time_ms: 2703.162\n",
      "    load_throughput: 5991648.87\n",
      "    load_time_ms: 0.668\n",
      "    sample_throughput: 665.908\n",
      "    sample_time_ms: 6006.836\n",
      "    update_time_ms: 2.835\n",
      "  timestamp: 1636043015\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1160000\n",
      "  training_iteration: 290\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:23:36 (running for 00:29:43.40)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   290</td><td style=\"text-align: right;\">         1755.67</td><td style=\"text-align: right;\">1160000</td><td style=\"text-align: right;\">-245.187</td><td style=\"text-align: right;\">           -0.198734</td><td style=\"text-align: right;\">            -1823.17</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 1164000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-23-41\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.19873394446347725\n",
      "  episode_reward_mean: -226.65868203188816\n",
      "  episode_reward_min: -1823.1652471904513\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5820\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20272865891456604\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.3338525891304016\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017529888078570366\n",
      "          model: {}\n",
      "          policy_loss: -0.026101579889655113\n",
      "          total_loss: 1093.8828125\n",
      "          vf_explained_var: 0.3029511868953705\n",
      "          vf_loss: 1093.9053955078125\n",
      "    num_agent_steps_sampled: 1164000\n",
      "    num_agent_steps_trained: 1164000\n",
      "    num_steps_sampled: 1164000\n",
      "    num_steps_trained: 1164000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 291\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.537500000000001\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11970342851843395\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17583458619032832\n",
      "    mean_inference_ms: 1.2081110705427216\n",
      "    mean_raw_obs_processing_ms: 0.10225946404989943\n",
      "  time_since_restore: 1761.6399850845337\n",
      "  time_this_iter_s: 5.9664998054504395\n",
      "  time_total_s: 1761.6399850845337\n",
      "  timers:\n",
      "    learn_throughput: 1479.859\n",
      "    learn_time_ms: 2702.961\n",
      "    load_throughput: 6057412.716\n",
      "    load_time_ms: 0.66\n",
      "    sample_throughput: 665.901\n",
      "    sample_time_ms: 6006.897\n",
      "    update_time_ms: 2.843\n",
      "  timestamp: 1636043021\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1164000\n",
      "  training_iteration: 291\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:23:41 (running for 00:29:48.41)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   291</td><td style=\"text-align: right;\">         1761.64</td><td style=\"text-align: right;\">1164000</td><td style=\"text-align: right;\">-226.659</td><td style=\"text-align: right;\">           -0.198734</td><td style=\"text-align: right;\">            -1823.17</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:23:46 (running for 00:29:53.42)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   291</td><td style=\"text-align: right;\">         1761.64</td><td style=\"text-align: right;\">1164000</td><td style=\"text-align: right;\">-226.659</td><td style=\"text-align: right;\">           -0.198734</td><td style=\"text-align: right;\">            -1823.17</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 1168000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-23-47\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.19873394446347725\n",
      "  episode_reward_mean: -250.1161076520986\n",
      "  episode_reward_min: -1823.1652471904513\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5840\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20272865891456604\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.16979096829891205\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.039200857281684875\n",
      "          model: {}\n",
      "          policy_loss: -0.026674862951040268\n",
      "          total_loss: 225.05953979492188\n",
      "          vf_explained_var: 0.5369831919670105\n",
      "          vf_loss: 225.07826232910156\n",
      "    num_agent_steps_sampled: 1168000\n",
      "    num_agent_steps_trained: 1168000\n",
      "    num_steps_sampled: 1168000\n",
      "    num_steps_trained: 1168000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 292\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.114285714285714\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11969878284189298\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17582640703331492\n",
      "    mean_inference_ms: 1.2080622650129351\n",
      "    mean_raw_obs_processing_ms: 0.10225555183673336\n",
      "  time_since_restore: 1767.5603675842285\n",
      "  time_this_iter_s: 5.920382499694824\n",
      "  time_total_s: 1767.5603675842285\n",
      "  timers:\n",
      "    learn_throughput: 1479.306\n",
      "    learn_time_ms: 2703.971\n",
      "    load_throughput: 6058944.023\n",
      "    load_time_ms: 0.66\n",
      "    sample_throughput: 666.646\n",
      "    sample_time_ms: 6000.183\n",
      "    update_time_ms: 2.84\n",
      "  timestamp: 1636043027\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1168000\n",
      "  training_iteration: 292\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:23:52 (running for 00:29:59.38)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   292</td><td style=\"text-align: right;\">         1767.56</td><td style=\"text-align: right;\">1168000</td><td style=\"text-align: right;\">-250.116</td><td style=\"text-align: right;\">           -0.198734</td><td style=\"text-align: right;\">            -1823.17</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 1172000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-23-53\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.2326440616473788\n",
      "  episode_reward_mean: -257.6184888646164\n",
      "  episode_reward_min: -1823.1652471904513\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5860\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297347068787\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.6489023566246033\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015034261159598827\n",
      "          model: {}\n",
      "          policy_loss: -0.024581145495176315\n",
      "          total_loss: 1765.77099609375\n",
      "          vf_explained_var: 0.5477519035339355\n",
      "          vf_loss: 1765.791015625\n",
      "    num_agent_steps_sampled: 1172000\n",
      "    num_agent_steps_trained: 1172000\n",
      "    num_steps_sampled: 1172000\n",
      "    num_steps_trained: 1172000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 293\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 9.2125\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11969229306316477\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17581472891483563\n",
      "    mean_inference_ms: 1.207981057138458\n",
      "    mean_raw_obs_processing_ms: 0.10224976929155752\n",
      "  time_since_restore: 1773.7277295589447\n",
      "  time_this_iter_s: 6.1673619747161865\n",
      "  time_total_s: 1773.7277295589447\n",
      "  timers:\n",
      "    learn_throughput: 1468.435\n",
      "    learn_time_ms: 2723.988\n",
      "    load_throughput: 6018732.197\n",
      "    load_time_ms: 0.665\n",
      "    sample_throughput: 666.009\n",
      "    sample_time_ms: 6005.922\n",
      "    update_time_ms: 2.835\n",
      "  timestamp: 1636043033\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1172000\n",
      "  training_iteration: 293\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:23:57 (running for 00:30:04.60)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   293</td><td style=\"text-align: right;\">         1773.73</td><td style=\"text-align: right;\">1172000</td><td style=\"text-align: right;\">-257.618</td><td style=\"text-align: right;\">           -0.232644</td><td style=\"text-align: right;\">            -1823.17</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 1176000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-23-59\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.2326440616473788\n",
      "  episode_reward_mean: -300.5794275998901\n",
      "  episode_reward_min: -1823.1652471904513\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5880\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297347068787\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.2454769611358643\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017613140866160393\n",
      "          model: {}\n",
      "          policy_loss: -0.025912845507264137\n",
      "          total_loss: 3157.92236328125\n",
      "          vf_explained_var: 0.4720589220523834\n",
      "          vf_loss: 3157.943115234375\n",
      "    num_agent_steps_sampled: 1176000\n",
      "    num_agent_steps_trained: 1176000\n",
      "    num_steps_sampled: 1176000\n",
      "    num_steps_trained: 1176000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 294\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.549999999999999\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11968605433432558\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17580375692744338\n",
      "    mean_inference_ms: 1.207898439633655\n",
      "    mean_raw_obs_processing_ms: 0.10224445614065775\n",
      "  time_since_restore: 1779.7311563491821\n",
      "  time_this_iter_s: 6.003426790237427\n",
      "  time_total_s: 1779.7311563491821\n",
      "  timers:\n",
      "    learn_throughput: 1467.85\n",
      "    learn_time_ms: 2725.075\n",
      "    load_throughput: 6050640.508\n",
      "    load_time_ms: 0.661\n",
      "    sample_throughput: 662.995\n",
      "    sample_time_ms: 6033.232\n",
      "    update_time_ms: 2.838\n",
      "  timestamp: 1636043039\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1176000\n",
      "  training_iteration: 294\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:24:03 (running for 00:30:09.65)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   294</td><td style=\"text-align: right;\">         1779.73</td><td style=\"text-align: right;\">1176000</td><td style=\"text-align: right;\">-300.579</td><td style=\"text-align: right;\">           -0.232644</td><td style=\"text-align: right;\">            -1823.17</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 1180000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-24-05\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.20835919728967395\n",
      "  episode_reward_mean: -301.7065204668481\n",
      "  episode_reward_min: -1478.884412290423\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5900\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297347068787\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.1887218952178955\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01701214164495468\n",
      "          model: {}\n",
      "          policy_loss: -0.025956176221370697\n",
      "          total_loss: 734.0760498046875\n",
      "          vf_explained_var: 0.3617728650569916\n",
      "          vf_loss: 734.0968627929688\n",
      "    num_agent_steps_sampled: 1180000\n",
      "    num_agent_steps_trained: 1180000\n",
      "    num_steps_sampled: 1180000\n",
      "    num_steps_trained: 1180000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 295\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.714285714285714\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11968088302327988\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17579454702768152\n",
      "    mean_inference_ms: 1.2078236885901092\n",
      "    mean_raw_obs_processing_ms: 0.10223975887049132\n",
      "  time_since_restore: 1785.6825454235077\n",
      "  time_this_iter_s: 5.9513890743255615\n",
      "  time_total_s: 1785.6825454235077\n",
      "  timers:\n",
      "    learn_throughput: 1468.787\n",
      "    learn_time_ms: 2723.335\n",
      "    load_throughput: 6057194.021\n",
      "    load_time_ms: 0.66\n",
      "    sample_throughput: 662.412\n",
      "    sample_time_ms: 6038.534\n",
      "    update_time_ms: 2.828\n",
      "  timestamp: 1636043045\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1180000\n",
      "  training_iteration: 295\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:24:08 (running for 00:30:14.65)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   295</td><td style=\"text-align: right;\">         1785.68</td><td style=\"text-align: right;\">1180000</td><td style=\"text-align: right;\">-301.707</td><td style=\"text-align: right;\">           -0.208359</td><td style=\"text-align: right;\">            -1478.88</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 1184000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-24-11\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.1299796606333658\n",
      "  episode_reward_mean: -287.74469242550254\n",
      "  episode_reward_min: -1478.884412290423\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5920\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297347068787\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: -0.248648002743721\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008131145499646664\n",
      "          model: {}\n",
      "          policy_loss: -0.021480277180671692\n",
      "          total_loss: 172.18777465820312\n",
      "          vf_explained_var: 0.2620590925216675\n",
      "          vf_loss: 172.206787109375\n",
      "    num_agent_steps_sampled: 1184000\n",
      "    num_agent_steps_trained: 1184000\n",
      "    num_steps_sampled: 1184000\n",
      "    num_steps_trained: 1184000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 296\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.3875\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1196747790535438\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1757839799767016\n",
      "    mean_inference_ms: 1.207738553454908\n",
      "    mean_raw_obs_processing_ms: 0.10223451107908929\n",
      "  time_since_restore: 1791.6058855056763\n",
      "  time_this_iter_s: 5.923340082168579\n",
      "  time_total_s: 1791.6058855056763\n",
      "  timers:\n",
      "    learn_throughput: 1467.687\n",
      "    learn_time_ms: 2725.377\n",
      "    load_throughput: 6091502.433\n",
      "    load_time_ms: 0.657\n",
      "    sample_throughput: 662.93\n",
      "    sample_time_ms: 6033.823\n",
      "    update_time_ms: 2.859\n",
      "  timestamp: 1636043051\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1184000\n",
      "  training_iteration: 296\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:24:13 (running for 00:30:20.62)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   296</td><td style=\"text-align: right;\">         1791.61</td><td style=\"text-align: right;\">1184000</td><td style=\"text-align: right;\">-287.745</td><td style=\"text-align: right;\">            -0.12998</td><td style=\"text-align: right;\">            -1478.88</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 1188000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-24-17\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.1299796606333658\n",
      "  episode_reward_mean: -346.1634913697542\n",
      "  episode_reward_min: -1722.7751266148773\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5940\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297347068787\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 2.7996270656585693\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012551981024444103\n",
      "          model: {}\n",
      "          policy_loss: -0.015734978020191193\n",
      "          total_loss: 9307.3359375\n",
      "          vf_explained_var: 0.34431192278862\n",
      "          vf_loss: 9307.34765625\n",
      "    num_agent_steps_sampled: 1188000\n",
      "    num_agent_steps_trained: 1188000\n",
      "    num_steps_sampled: 1188000\n",
      "    num_steps_trained: 1188000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 297\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.485714285714286\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11966886241850241\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17577334941418762\n",
      "    mean_inference_ms: 1.2076518370685734\n",
      "    mean_raw_obs_processing_ms: 0.10222940291387492\n",
      "  time_since_restore: 1797.527747631073\n",
      "  time_this_iter_s: 5.9218621253967285\n",
      "  time_total_s: 1797.527747631073\n",
      "  timers:\n",
      "    learn_throughput: 1468.975\n",
      "    learn_time_ms: 2722.988\n",
      "    load_throughput: 6081125.086\n",
      "    load_time_ms: 0.658\n",
      "    sample_throughput: 662.999\n",
      "    sample_time_ms: 6033.195\n",
      "    update_time_ms: 2.857\n",
      "  timestamp: 1636043057\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1188000\n",
      "  training_iteration: 297\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:24:19 (running for 00:30:26.60)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   297</td><td style=\"text-align: right;\">         1797.53</td><td style=\"text-align: right;\">1188000</td><td style=\"text-align: right;\">-346.163</td><td style=\"text-align: right;\">            -0.12998</td><td style=\"text-align: right;\">            -1722.78</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 1192000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-24-23\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.1299796606333658\n",
      "  episode_reward_mean: -325.25230475948786\n",
      "  episode_reward_min: -1722.7751266148773\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5960\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30409297347068787\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.1416684091091156\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.025769460946321487\n",
      "          model: {}\n",
      "          policy_loss: -0.01755456067621708\n",
      "          total_loss: 135.87710571289062\n",
      "          vf_explained_var: 0.4501700699329376\n",
      "          vf_loss: 135.88682556152344\n",
      "    num_agent_steps_sampled: 1192000\n",
      "    num_agent_steps_trained: 1192000\n",
      "    num_steps_sampled: 1192000\n",
      "    num_steps_trained: 1192000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 298\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.487499999999999\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11966298436725661\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17576311877870932\n",
      "    mean_inference_ms: 1.2075640206143035\n",
      "    mean_raw_obs_processing_ms: 0.1022245044661292\n",
      "  time_since_restore: 1803.488531112671\n",
      "  time_this_iter_s: 5.9607834815979\n",
      "  time_total_s: 1803.488531112671\n",
      "  timers:\n",
      "    learn_throughput: 1468.25\n",
      "    learn_time_ms: 2724.332\n",
      "    load_throughput: 6075179.606\n",
      "    load_time_ms: 0.658\n",
      "    sample_throughput: 664.51\n",
      "    sample_time_ms: 6019.472\n",
      "    update_time_ms: 2.866\n",
      "  timestamp: 1636043063\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1192000\n",
      "  training_iteration: 298\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:24:24 (running for 00:30:31.60)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   298</td><td style=\"text-align: right;\">         1803.49</td><td style=\"text-align: right;\">1192000</td><td style=\"text-align: right;\">-325.252</td><td style=\"text-align: right;\">            -0.12998</td><td style=\"text-align: right;\">            -1722.78</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 1196000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-24-29\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.1299796606333658\n",
      "  episode_reward_mean: -320.6017061375282\n",
      "  episode_reward_min: -1753.9596973917005\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5980\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.456139475107193\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 1.3860023021697998\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006502613890916109\n",
      "          model: {}\n",
      "          policy_loss: -0.013555729761719704\n",
      "          total_loss: 3159.806396484375\n",
      "          vf_explained_var: 0.27910465002059937\n",
      "          vf_loss: 3159.817138671875\n",
      "    num_agent_steps_sampled: 1196000\n",
      "    num_agent_steps_trained: 1196000\n",
      "    num_steps_sampled: 1196000\n",
      "    num_steps_trained: 1196000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 299\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.457142857142857\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.02971768202080238\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1196559130655883\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17575133090557007\n",
      "    mean_inference_ms: 1.2074735132054775\n",
      "    mean_raw_obs_processing_ms: 0.10221869486295367\n",
      "  time_since_restore: 1809.4554238319397\n",
      "  time_this_iter_s: 5.966892719268799\n",
      "  time_total_s: 1809.4554238319397\n",
      "  timers:\n",
      "    learn_throughput: 1468.278\n",
      "    learn_time_ms: 2724.28\n",
      "    load_throughput: 6004300.336\n",
      "    load_time_ms: 0.666\n",
      "    sample_throughput: 664.737\n",
      "    sample_time_ms: 6017.418\n",
      "    update_time_ms: 2.873\n",
      "  timestamp: 1636043069\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1196000\n",
      "  training_iteration: 299\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:24:29 (running for 00:30:36.62)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   299</td><td style=\"text-align: right;\">         1809.46</td><td style=\"text-align: right;\">1196000</td><td style=\"text-align: right;\">-320.602</td><td style=\"text-align: right;\">            -0.12998</td><td style=\"text-align: right;\">            -1753.96</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:24:34 (running for 00:30:41.62)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>RUNNING </td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   299</td><td style=\"text-align: right;\">         1809.46</td><td style=\"text-align: right;\">1196000</td><td style=\"text-align: right;\">-320.602</td><td style=\"text-align: right;\">            -0.12998</td><td style=\"text-align: right;\">            -1753.96</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_Pendulum-v1_6902c_00000:\n",
      "  agent_timesteps_total: 1200000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-11-04_16-24-35\n",
      "  done: true\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -0.1299796606333658\n",
      "  episode_reward_mean: -338.25304887775155\n",
      "  episode_reward_min: -1753.9596973917005\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6000\n",
      "  experiment_id: 974a35a0b9a841189eaddc4ce6bd27a1\n",
      "  hostname: ip-172-16-0-96\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.456139475107193\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.9954953789710999\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01134356390684843\n",
      "          model: {}\n",
      "          policy_loss: -0.02483336627483368\n",
      "          total_loss: 1037.4683837890625\n",
      "          vf_explained_var: 0.4962584674358368\n",
      "          vf_loss: 1037.488037109375\n",
      "    num_agent_steps_sampled: 1200000\n",
      "    num_agent_steps_trained: 1200000\n",
      "    num_steps_sampled: 1200000\n",
      "    num_steps_trained: 1200000\n",
      "    num_steps_trained_this_iter: 0\n",
      "  iterations_since_restore: 300\n",
      "  node_ip: 172.16.0.96\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 8.575\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.0\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.4\n",
      "    vram_util_percent0: 0.029717682020802376\n",
      "    vram_util_percent1: 0.00026221484136002097\n",
      "    vram_util_percent2: 0.00026221484136002097\n",
      "    vram_util_percent3: 0.00026221484136002097\n",
      "    vram_util_percent4: 0.00026221484136002097\n",
      "    vram_util_percent5: 0.00026221484136002097\n",
      "    vram_util_percent6: 0.00026221484136002097\n",
      "    vram_util_percent7: 0.00026221484136002097\n",
      "  pid: 83435\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11964956237967246\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1757408951717285\n",
      "    mean_inference_ms: 1.2073902262235974\n",
      "    mean_raw_obs_processing_ms: 0.10221342381383504\n",
      "  time_since_restore: 1815.4463782310486\n",
      "  time_this_iter_s: 5.990954399108887\n",
      "  time_total_s: 1815.4463782310486\n",
      "  timers:\n",
      "    learn_throughput: 1468.094\n",
      "    learn_time_ms: 2724.62\n",
      "    load_throughput: 5977133.492\n",
      "    load_time_ms: 0.669\n",
      "    sample_throughput: 663.822\n",
      "    sample_time_ms: 6025.711\n",
      "    update_time_ms: 2.858\n",
      "  timestamp: 1636043075\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_this_iter: 0\n",
      "  timesteps_total: 1200000\n",
      "  training_iteration: 300\n",
      "  trial_id: 6902c_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2021-11-04 16:24:36 (running for 00:30:42.72)<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/32 CPUs, 0/8 GPUs, 0.0/322.92 GiB heap, 0.0/142.39 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status    </th><th>loc              </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_Pendulum-v1_6902c_00000</td><td>TERMINATED</td><td>172.16.0.96:83435</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">   300</td><td style=\"text-align: right;\">         1815.45</td><td style=\"text-align: right;\">1200000</td><td style=\"text-align: right;\">-338.253</td><td style=\"text-align: right;\">            -0.12998</td><td style=\"text-align: right;\">            -1753.96</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=83422)\u001b[0m [2021-11-04 16:24:36,495 E 83422 84184] raylet_client.cc:159: IOError: Broken pipe [RayletClient] Failed to disconnect from raylet.\n",
      "2021-11-04 16:24:36,598\tINFO tune.py:630 -- Total run time: 1843.27 seconds (1842.66 seconds for the tuning loop).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7f66b21f3908>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ray\n",
    "import ray.rllib.agents.sac as sac\n",
    "import ray.rllib.agents.ppo as ppo\n",
    "from ray import tune\n",
    "\n",
    "alg = 'PPO'\n",
    "tune.run(alg,\n",
    "    stop={\"training_iteration\": 300},\n",
    "    config={\n",
    "        'env':'Pendulum-v1',\n",
    "        'num_gpus':0,\n",
    "        'num_workers':2,\n",
    "        'lr':tune.grid_search([.0001])     \n",
    "    }\n",
    ")\n",
    "# Pendulum-v1\n",
    "#CartPole-v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b8e709",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
