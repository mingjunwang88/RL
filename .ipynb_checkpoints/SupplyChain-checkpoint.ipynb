{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'Envs.SupplyChain' from '/Users/mingjunwang/Documents/Backup/RL/Envs/SupplyChain.py'>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ray\n",
    "from ray import tune\n",
    "\n",
    "from ray.rllib.agents import ddpg, ppo, dqn\n",
    "from ray.tune.logger import pretty_print\n",
    "\n",
    "from Envs import SupplyChain as e\n",
    "import importlib\n",
    "importlib.reload(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.6/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 5/12 CPUs, 0/0 GPUs, 0.0/4.79 GiB heap, 0.0/1.61 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 1/2 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_SimpleSupplyChain_1a2f8_00000</td><td>RUNNING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=8114)\u001b[0m WARNING:tensorflow:From /Users/mingjunwang/opt/miniconda3/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=8114)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=8114)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=8118)\u001b[0m WARNING:tensorflow:From /Users/mingjunwang/opt/miniconda3/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=8118)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=8118)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=8114)\u001b[0m 2021-05-08 18:07:08,328\tINFO trainer.py:591 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=8114)\u001b[0m 2021-05-08 18:07:08,328\tINFO trainer.py:616 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=8118)\u001b[0m 2021-05-08 18:07:08,328\tINFO trainer.py:591 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=8118)\u001b[0m 2021-05-08 18:07:08,328\tINFO trainer.py:616 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=8131)\u001b[0m WARNING:tensorflow:From /Users/mingjunwang/opt/miniconda3/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=8131)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=8131)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=8134)\u001b[0m WARNING:tensorflow:From /Users/mingjunwang/opt/miniconda3/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=8134)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=8134)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=8132)\u001b[0m WARNING:tensorflow:From /Users/mingjunwang/opt/miniconda3/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=8132)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=8132)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=8133)\u001b[0m WARNING:tensorflow:From /Users/mingjunwang/opt/miniconda3/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=8133)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=8133)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=8135)\u001b[0m WARNING:tensorflow:From /Users/mingjunwang/opt/miniconda3/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=8135)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=8135)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=8138)\u001b[0m WARNING:tensorflow:From /Users/mingjunwang/opt/miniconda3/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=8138)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=8138)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=8136)\u001b[0m WARNING:tensorflow:From /Users/mingjunwang/opt/miniconda3/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=8136)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=8136)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=8137)\u001b[0m WARNING:tensorflow:From /Users/mingjunwang/opt/miniconda3/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=8137)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=8137)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=8118)\u001b[0m 2021-05-08 18:07:18,343\tINFO trainable.py:99 -- Trainable.setup took 10.016 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=8131)\u001b[0m 2021-05-08 18:07:18,356\tWARNING deprecation.py:29 -- DeprecationWarning: `env_index` has been deprecated. Use `episode.env_id` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=8132)\u001b[0m 2021-05-08 18:07:18,392\tWARNING deprecation.py:29 -- DeprecationWarning: `env_index` has been deprecated. Use `episode.env_id` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_SimpleSupplyChain_1a2f8_00001:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-08_18-07-20\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_reward_max: -377897.88405697414\n",
      "  episode_reward_mean: -598967.3841606939\n",
      "  episode_reward_min: -795854.671078611\n",
      "  episodes_this_iter: 60\n",
      "  episodes_total: 60\n",
      "  experiment_id: d723ce1e54d6484893e40b17874a26d0\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 1500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: -0.35458388924598694\n",
      "        mean_q: -22.15859031677246\n",
      "        min_q: -44.98976135253906\n",
      "        model: {}\n",
      "    num_steps_sampled: 1500\n",
      "    num_steps_trained: 256\n",
      "    num_target_updates: 1\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 46.166666666666664\n",
      "    ram_util_percent: 68.06666666666666\n",
      "  pid: 8114\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.13366309886283062\n",
      "    mean_env_wait_ms: 0.3338598824561911\n",
      "    mean_inference_ms: 1.0891349074688366\n",
      "    mean_raw_obs_processing_ms: 0.2988022058568102\n",
      "  time_since_restore: 1.843479871749878\n",
      "  time_this_iter_s: 1.843479871749878\n",
      "  time_total_s: 1.843479871749878\n",
      "  timers:\n",
      "    learn_throughput: 1639.165\n",
      "    learn_time_ms: 156.177\n",
      "    update_time_ms: 5.264\n",
      "  timestamp: 1620515240\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1500\n",
      "  training_iteration: 1\n",
      "  trial_id: 1a2f8_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 10/12 CPUs, 0/0 GPUs, 0.0/4.79 GiB heap, 0.0/1.61 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_SimpleSupplyChain_1a2f8_00000</td><td>RUNNING </td><td>                 </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>DDPG_SimpleSupplyChain_1a2f8_00001</td><td>RUNNING </td><td>192.168.0.23:8114</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.84348</td><td style=\"text-align: right;\">1500</td><td style=\"text-align: right;\"> -598967</td><td style=\"text-align: right;\">             -377898</td><td style=\"text-align: right;\">             -795855</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_SimpleSupplyChain_1a2f8_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-08_18-07-20\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_reward_max: -437144.7401069886\n",
      "  episode_reward_mean: -613921.4990750158\n",
      "  episode_reward_min: -806274.8348902947\n",
      "  episodes_this_iter: 60\n",
      "  episodes_total: 60\n",
      "  experiment_id: 925d870c9bcc486a8beaa0dfc6db064c\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 1500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: 55.72043228149414\n",
      "        mean_q: 22.70733642578125\n",
      "        min_q: 0.06055539846420288\n",
      "        model: {}\n",
      "    num_steps_sampled: 1500\n",
      "    num_steps_trained: 256\n",
      "    num_target_updates: 1\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 46.13333333333333\n",
      "    ram_util_percent: 68.1\n",
      "  pid: 8118\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.13343514280116306\n",
      "    mean_env_wait_ms: 0.33386384553097664\n",
      "    mean_inference_ms: 1.0788908347170405\n",
      "    mean_raw_obs_processing_ms: 0.2967247620541998\n",
      "  time_since_restore: 1.8537688255310059\n",
      "  time_this_iter_s: 1.8537688255310059\n",
      "  time_total_s: 1.8537688255310059\n",
      "  timers:\n",
      "    learn_throughput: 1738.656\n",
      "    learn_time_ms: 147.24\n",
      "    update_time_ms: 4.612\n",
      "  timestamp: 1620515240\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1500\n",
      "  training_iteration: 1\n",
      "  trial_id: 1a2f8_00000\n",
      "  \n",
      "Result for DDPG_SimpleSupplyChain_1a2f8_00001:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-08_18-07-30\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_reward_max: -164794.23606545097\n",
      "  episode_reward_mean: -454815.36102870555\n",
      "  episode_reward_min: -795854.671078611\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 100\n",
      "  experiment_id: d723ce1e54d6484893e40b17874a26d0\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 2500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: -75.45525360107422\n",
      "        mean_q: -22218.474609375\n",
      "        min_q: -58652.0390625\n",
      "        model: {}\n",
      "    num_steps_sampled: 2500\n",
      "    num_steps_trained: 64256\n",
      "    num_target_updates: 251\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 45.25714285714286\n",
      "    ram_util_percent: 67.97142857142856\n",
      "  pid: 8114\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.13220152174394917\n",
      "    mean_env_wait_ms: 0.32850788655137464\n",
      "    mean_inference_ms: 1.05447951400639\n",
      "    mean_raw_obs_processing_ms: 0.29405633218185834\n",
      "  time_since_restore: 11.818572998046875\n",
      "  time_this_iter_s: 9.975093126296997\n",
      "  time_total_s: 11.818572998046875\n",
      "  timers:\n",
      "    learn_throughput: 51348.918\n",
      "    learn_time_ms: 4.985\n",
      "    update_time_ms: 2.799\n",
      "  timestamp: 1620515250\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2500\n",
      "  training_iteration: 2\n",
      "  trial_id: 1a2f8_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 10/12 CPUs, 0/0 GPUs, 0.0/4.79 GiB heap, 0.0/1.61 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_SimpleSupplyChain_1a2f8_00000</td><td>RUNNING </td><td>192.168.0.23:8118</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.85377</td><td style=\"text-align: right;\">1500</td><td style=\"text-align: right;\"> -613921</td><td style=\"text-align: right;\">             -437145</td><td style=\"text-align: right;\">             -806275</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "<tr><td>DDPG_SimpleSupplyChain_1a2f8_00001</td><td>RUNNING </td><td>192.168.0.23:8114</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        11.8186 </td><td style=\"text-align: right;\">2500</td><td style=\"text-align: right;\"> -454815</td><td style=\"text-align: right;\">             -164794</td><td style=\"text-align: right;\">             -795855</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_SimpleSupplyChain_1a2f8_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-08_18-07-30\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_reward_max: -155538.8243689052\n",
      "  episode_reward_mean: -487813.734948087\n",
      "  episode_reward_min: -806274.8348902947\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 100\n",
      "  experiment_id: 925d870c9bcc486a8beaa0dfc6db064c\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 2500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: -55.063392639160156\n",
      "        mean_q: -26401.5703125\n",
      "        min_q: -62077.921875\n",
      "        model: {}\n",
      "    num_steps_sampled: 2500\n",
      "    num_steps_trained: 64256\n",
      "    num_target_updates: 251\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 45.221428571428575\n",
      "    ram_util_percent: 67.97142857142856\n",
      "  pid: 8118\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.13128920217623852\n",
      "    mean_env_wait_ms: 0.32741944226780406\n",
      "    mean_inference_ms: 1.0396840364950353\n",
      "    mean_raw_obs_processing_ms: 0.2905832801221075\n",
      "  time_since_restore: 11.859790802001953\n",
      "  time_this_iter_s: 10.006021976470947\n",
      "  time_total_s: 11.859790802001953\n",
      "  timers:\n",
      "    learn_throughput: 52299.329\n",
      "    learn_time_ms: 4.895\n",
      "    update_time_ms: 3.504\n",
      "  timestamp: 1620515250\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2500\n",
      "  training_iteration: 2\n",
      "  trial_id: 1a2f8_00000\n",
      "  \n",
      "Result for DDPG_SimpleSupplyChain_1a2f8_00001:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-08_18-07-40\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_reward_max: -164794.23606545097\n",
      "  episode_reward_mean: -320273.55505652085\n",
      "  episode_reward_min: -795854.671078611\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 140\n",
      "  experiment_id: d723ce1e54d6484893e40b17874a26d0\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 3500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: -28.505054473876953\n",
      "        mean_q: -31765.78515625\n",
      "        min_q: -72439.9375\n",
      "        model: {}\n",
      "    num_steps_sampled: 3500\n",
      "    num_steps_trained: 128256\n",
      "    num_target_updates: 501\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.642857142857146\n",
      "    ram_util_percent: 67.81428571428573\n",
      "  pid: 8114\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12911764717338672\n",
      "    mean_env_wait_ms: 0.3198994021974826\n",
      "    mean_inference_ms: 1.0003692946442932\n",
      "    mean_raw_obs_processing_ms: 0.2881094838882786\n",
      "  time_since_restore: 21.71768069267273\n",
      "  time_this_iter_s: 9.899107694625854\n",
      "  time_total_s: 21.71768069267273\n",
      "  timers:\n",
      "    learn_throughput: 54110.506\n",
      "    learn_time_ms: 4.731\n",
      "    update_time_ms: 2.467\n",
      "  timestamp: 1620515260\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3500\n",
      "  training_iteration: 3\n",
      "  trial_id: 1a2f8_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 10/12 CPUs, 0/0 GPUs, 0.0/4.79 GiB heap, 0.0/1.61 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_SimpleSupplyChain_1a2f8_00000</td><td>RUNNING </td><td>192.168.0.23:8118</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         11.8598</td><td style=\"text-align: right;\">2500</td><td style=\"text-align: right;\"> -487814</td><td style=\"text-align: right;\">             -155539</td><td style=\"text-align: right;\">             -806275</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "<tr><td>DDPG_SimpleSupplyChain_1a2f8_00001</td><td>RUNNING </td><td>192.168.0.23:8114</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         21.7177</td><td style=\"text-align: right;\">3500</td><td style=\"text-align: right;\"> -320274</td><td style=\"text-align: right;\">             -164794</td><td style=\"text-align: right;\">             -795855</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_SimpleSupplyChain_1a2f8_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-08_18-07-40\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_reward_max: -155538.8243689052\n",
      "  episode_reward_mean: -318481.2832325769\n",
      "  episode_reward_min: -806274.8348902947\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 140\n",
      "  experiment_id: 925d870c9bcc486a8beaa0dfc6db064c\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 3500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: -55.65253829956055\n",
      "        mean_q: -38518.34375\n",
      "        min_q: -73212.015625\n",
      "        model: {}\n",
      "    num_steps_sampled: 3500\n",
      "    num_steps_trained: 128256\n",
      "    num_target_updates: 501\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.82142857142858\n",
      "    ram_util_percent: 67.81428571428573\n",
      "  pid: 8118\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12636128929162205\n",
      "    mean_env_wait_ms: 0.3138451423992006\n",
      "    mean_inference_ms: 0.9776724184295742\n",
      "    mean_raw_obs_processing_ms: 0.2783483963101294\n",
      "  time_since_restore: 21.730416536331177\n",
      "  time_this_iter_s: 9.870625734329224\n",
      "  time_total_s: 21.730416536331177\n",
      "  timers:\n",
      "    learn_throughput: 46670.02\n",
      "    learn_time_ms: 5.485\n",
      "    update_time_ms: 2.406\n",
      "  timestamp: 1620515260\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3500\n",
      "  training_iteration: 3\n",
      "  trial_id: 1a2f8_00000\n",
      "  \n",
      "Result for DDPG_SimpleSupplyChain_1a2f8_00001:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-08_18-07-50\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_reward_max: -160694.44802369914\n",
      "  episode_reward_mean: -231670.8075398144\n",
      "  episode_reward_min: -679955.4442514663\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 180\n",
      "  experiment_id: d723ce1e54d6484893e40b17874a26d0\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 4500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: -1.2606863975524902\n",
      "        mean_q: -38156.9765625\n",
      "        min_q: -79493.03125\n",
      "        model: {}\n",
      "    num_steps_sampled: 4500\n",
      "    num_steps_trained: 192256\n",
      "    num_target_updates: 751\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 45.38666666666666\n",
      "    ram_util_percent: 67.55333333333334\n",
      "  pid: 8114\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12766732018667393\n",
      "    mean_env_wait_ms: 0.3150312381920685\n",
      "    mean_inference_ms: 0.96112383142389\n",
      "    mean_raw_obs_processing_ms: 0.2834553308419865\n",
      "  time_since_restore: 32.23854875564575\n",
      "  time_this_iter_s: 10.520868062973022\n",
      "  time_total_s: 32.23854875564575\n",
      "  timers:\n",
      "    learn_throughput: 33720.188\n",
      "    learn_time_ms: 7.592\n",
      "    update_time_ms: 3.148\n",
      "  timestamp: 1620515270\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4500\n",
      "  training_iteration: 4\n",
      "  trial_id: 1a2f8_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 10/12 CPUs, 0/0 GPUs, 0.0/4.79 GiB heap, 0.0/1.61 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_SimpleSupplyChain_1a2f8_00000</td><td>RUNNING </td><td>192.168.0.23:8118</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         21.7304</td><td style=\"text-align: right;\">3500</td><td style=\"text-align: right;\"> -318481</td><td style=\"text-align: right;\">             -155539</td><td style=\"text-align: right;\">             -806275</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "<tr><td>DDPG_SimpleSupplyChain_1a2f8_00001</td><td>RUNNING </td><td>192.168.0.23:8114</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         32.2385</td><td style=\"text-align: right;\">4500</td><td style=\"text-align: right;\"> -231671</td><td style=\"text-align: right;\">             -160694</td><td style=\"text-align: right;\">             -679955</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_SimpleSupplyChain_1a2f8_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-08_18-07-50\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_reward_max: -59738.63463478153\n",
      "  episode_reward_mean: -196948.9735166737\n",
      "  episode_reward_min: -724508.0315402446\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 180\n",
      "  experiment_id: 925d870c9bcc486a8beaa0dfc6db064c\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 4500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: 184.14886474609375\n",
      "        mean_q: -38880.140625\n",
      "        min_q: -87303.96875\n",
      "        model: {}\n",
      "    num_steps_sampled: 4500\n",
      "    num_steps_trained: 192256\n",
      "    num_target_updates: 751\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 45.4\n",
      "    ram_util_percent: 67.55333333333334\n",
      "  pid: 8118\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12436364640800641\n",
      "    mean_env_wait_ms: 0.3074535698662089\n",
      "    mean_inference_ms: 0.9343410126600782\n",
      "    mean_raw_obs_processing_ms: 0.272119682458152\n",
      "  time_since_restore: 32.28600835800171\n",
      "  time_this_iter_s: 10.555591821670532\n",
      "  time_total_s: 32.28600835800171\n",
      "  timers:\n",
      "    learn_throughput: 40010.352\n",
      "    learn_time_ms: 6.398\n",
      "    update_time_ms: 2.934\n",
      "  timestamp: 1620515270\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4500\n",
      "  training_iteration: 4\n",
      "  trial_id: 1a2f8_00000\n",
      "  \n",
      "Result for DDPG_SimpleSupplyChain_1a2f8_00001:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-08_18-08-00\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_reward_max: -160694.44802369914\n",
      "  episode_reward_mean: -208081.45947399864\n",
      "  episode_reward_min: -369318.4433310366\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 220\n",
      "  experiment_id: d723ce1e54d6484893e40b17874a26d0\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 5500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: -18.32662010192871\n",
      "        mean_q: -39338.5390625\n",
      "        min_q: -93640.5703125\n",
      "        model: {}\n",
      "    num_steps_sampled: 5500\n",
      "    num_steps_trained: 256256\n",
      "    num_target_updates: 1001\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.46428571428571\n",
      "    ram_util_percent: 67.67142857142855\n",
      "  pid: 8114\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1271389852085252\n",
      "    mean_env_wait_ms: 0.31403038801610805\n",
      "    mean_inference_ms: 0.9468254576274343\n",
      "    mean_raw_obs_processing_ms: 0.2829852091267074\n",
      "  time_since_restore: 42.12086009979248\n",
      "  time_this_iter_s: 9.882311344146729\n",
      "  time_total_s: 42.12086009979248\n",
      "  timers:\n",
      "    learn_throughput: 43296.74\n",
      "    learn_time_ms: 5.913\n",
      "    update_time_ms: 3.361\n",
      "  timestamp: 1620515280\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 5500\n",
      "  training_iteration: 5\n",
      "  trial_id: 1a2f8_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 10/12 CPUs, 0/0 GPUs, 0.0/4.79 GiB heap, 0.0/1.61 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_SimpleSupplyChain_1a2f8_00000</td><td>RUNNING </td><td>192.168.0.23:8118</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         32.286 </td><td style=\"text-align: right;\">4500</td><td style=\"text-align: right;\"> -196949</td><td style=\"text-align: right;\">            -59738.6</td><td style=\"text-align: right;\">             -724508</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "<tr><td>DDPG_SimpleSupplyChain_1a2f8_00001</td><td>RUNNING </td><td>192.168.0.23:8114</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         42.1209</td><td style=\"text-align: right;\">5500</td><td style=\"text-align: right;\"> -208081</td><td style=\"text-align: right;\">           -160694  </td><td style=\"text-align: right;\">             -369318</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_SimpleSupplyChain_1a2f8_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-08_18-08-00\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_reward_max: -59738.63463478153\n",
      "  episode_reward_mean: -150797.12433720237\n",
      "  episode_reward_min: -209931.62035256243\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 220\n",
      "  experiment_id: 925d870c9bcc486a8beaa0dfc6db064c\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 5500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: 305.88275146484375\n",
      "        mean_q: -41187.34765625\n",
      "        min_q: -102739.328125\n",
      "        model: {}\n",
      "    num_steps_sampled: 5500\n",
      "    num_steps_trained: 256256\n",
      "    num_target_updates: 1001\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 37.278571428571425\n",
      "    ram_util_percent: 67.67857142857142\n",
      "  pid: 8118\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1230642696169111\n",
      "    mean_env_wait_ms: 0.3041618509290183\n",
      "    mean_inference_ms: 0.9112620995745041\n",
      "    mean_raw_obs_processing_ms: 0.2689522641724087\n",
      "  time_since_restore: 42.17130208015442\n",
      "  time_this_iter_s: 9.88529372215271\n",
      "  time_total_s: 42.17130208015442\n",
      "  timers:\n",
      "    learn_throughput: 45594.326\n",
      "    learn_time_ms: 5.615\n",
      "    update_time_ms: 3.231\n",
      "  timestamp: 1620515280\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 5500\n",
      "  training_iteration: 5\n",
      "  trial_id: 1a2f8_00000\n",
      "  \n",
      "Result for DDPG_SimpleSupplyChain_1a2f8_00001:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-08_18-08-10\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_reward_max: -160694.44802369914\n",
      "  episode_reward_mean: -191510.48170265462\n",
      "  episode_reward_min: -219035.01130082077\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 260\n",
      "  experiment_id: d723ce1e54d6484893e40b17874a26d0\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 6500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: -364.20697021484375\n",
      "        mean_q: -42244.3203125\n",
      "        min_q: -112192.0859375\n",
      "        model: {}\n",
      "    num_steps_sampled: 6500\n",
      "    num_steps_trained: 320256\n",
      "    num_target_updates: 1251\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 45.47857142857142\n",
      "    ram_util_percent: 67.78571428571429\n",
      "  pid: 8114\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12662454215265673\n",
      "    mean_env_wait_ms: 0.31342569414628\n",
      "    mean_inference_ms: 0.9378800740539197\n",
      "    mean_raw_obs_processing_ms: 0.28247616580868606\n",
      "  time_since_restore: 52.49738883972168\n",
      "  time_this_iter_s: 10.3765287399292\n",
      "  time_total_s: 52.49738883972168\n",
      "  timers:\n",
      "    learn_throughput: 48577.03\n",
      "    learn_time_ms: 5.27\n",
      "    update_time_ms: 2.684\n",
      "  timestamp: 1620515290\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 6500\n",
      "  training_iteration: 6\n",
      "  trial_id: 1a2f8_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 10/12 CPUs, 0/0 GPUs, 0.0/4.79 GiB heap, 0.0/1.61 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_SimpleSupplyChain_1a2f8_00000</td><td>RUNNING </td><td>192.168.0.23:8118</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         42.1713</td><td style=\"text-align: right;\">5500</td><td style=\"text-align: right;\"> -150797</td><td style=\"text-align: right;\">            -59738.6</td><td style=\"text-align: right;\">             -209932</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "<tr><td>DDPG_SimpleSupplyChain_1a2f8_00001</td><td>RUNNING </td><td>192.168.0.23:8114</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         52.4974</td><td style=\"text-align: right;\">6500</td><td style=\"text-align: right;\"> -191510</td><td style=\"text-align: right;\">           -160694  </td><td style=\"text-align: right;\">             -219035</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_SimpleSupplyChain_1a2f8_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-08_18-08-10\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_reward_max: -86314.60287881258\n",
      "  episode_reward_mean: -147074.08772061727\n",
      "  episode_reward_min: -220017.96333603925\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 260\n",
      "  experiment_id: 925d870c9bcc486a8beaa0dfc6db064c\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 6500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: 5.274119853973389\n",
      "        mean_q: -40947.84375\n",
      "        min_q: -117333.015625\n",
      "        model: {}\n",
      "    num_steps_sampled: 6500\n",
      "    num_steps_trained: 320256\n",
      "    num_target_updates: 1251\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.00714285714285\n",
      "    ram_util_percent: 67.78571428571429\n",
      "  pid: 8118\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12221720209344678\n",
      "    mean_env_wait_ms: 0.3020868315255085\n",
      "    mean_inference_ms: 0.8971784384582142\n",
      "    mean_raw_obs_processing_ms: 0.26704237619218796\n",
      "  time_since_restore: 52.543219327926636\n",
      "  time_this_iter_s: 10.371917247772217\n",
      "  time_total_s: 52.543219327926636\n",
      "  timers:\n",
      "    learn_throughput: 48034.188\n",
      "    learn_time_ms: 5.33\n",
      "    update_time_ms: 2.706\n",
      "  timestamp: 1620515290\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 6500\n",
      "  training_iteration: 6\n",
      "  trial_id: 1a2f8_00000\n",
      "  \n",
      "Result for DDPG_SimpleSupplyChain_1a2f8_00001:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-08_18-08-21\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_reward_max: -157551.57476716107\n",
      "  episode_reward_mean: -190600.63792738307\n",
      "  episode_reward_min: -219035.01130082077\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 300\n",
      "  experiment_id: d723ce1e54d6484893e40b17874a26d0\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 7500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: -267.17242431640625\n",
      "        mean_q: -45653.68359375\n",
      "        min_q: -116836.9296875\n",
      "        model: {}\n",
      "    num_steps_sampled: 7500\n",
      "    num_steps_trained: 384256\n",
      "    num_target_updates: 1501\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.566666666666656\n",
      "    ram_util_percent: 67.89333333333333\n",
      "  pid: 8114\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12589388543471416\n",
      "    mean_env_wait_ms: 0.31181968710910274\n",
      "    mean_inference_ms: 0.9279823708878812\n",
      "    mean_raw_obs_processing_ms: 0.2809322120390312\n",
      "  time_since_restore: 62.79679870605469\n",
      "  time_this_iter_s: 10.299409866333008\n",
      "  time_total_s: 62.79679870605469\n",
      "  timers:\n",
      "    learn_throughput: 40429.919\n",
      "    learn_time_ms: 6.332\n",
      "    update_time_ms: 3.567\n",
      "  timestamp: 1620515301\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 7500\n",
      "  training_iteration: 7\n",
      "  trial_id: 1a2f8_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 10/12 CPUs, 0/0 GPUs, 0.0/4.79 GiB heap, 0.0/1.61 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_SimpleSupplyChain_1a2f8_00000</td><td>RUNNING </td><td>192.168.0.23:8118</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         52.5432</td><td style=\"text-align: right;\">6500</td><td style=\"text-align: right;\"> -147074</td><td style=\"text-align: right;\">            -86314.6</td><td style=\"text-align: right;\">             -220018</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "<tr><td>DDPG_SimpleSupplyChain_1a2f8_00001</td><td>RUNNING </td><td>192.168.0.23:8114</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         62.7968</td><td style=\"text-align: right;\">7500</td><td style=\"text-align: right;\"> -190601</td><td style=\"text-align: right;\">           -157552  </td><td style=\"text-align: right;\">             -219035</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_SimpleSupplyChain_1a2f8_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-08_18-08-21\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_reward_max: -102768.5310530997\n",
      "  episode_reward_mean: -158132.60806337695\n",
      "  episode_reward_min: -220017.96333603925\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 300\n",
      "  experiment_id: 925d870c9bcc486a8beaa0dfc6db064c\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 7500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: 5478.20703125\n",
      "        mean_q: -41080.703125\n",
      "        min_q: -128609.4375\n",
      "        model: {}\n",
      "    num_steps_sampled: 7500\n",
      "    num_steps_trained: 384256\n",
      "    num_target_updates: 1501\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.70666666666667\n",
      "    ram_util_percent: 67.87999999999998\n",
      "  pid: 8118\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12123036944972339\n",
      "    mean_env_wait_ms: 0.29981337209352543\n",
      "    mean_inference_ms: 0.8863184150642661\n",
      "    mean_raw_obs_processing_ms: 0.2650204858459208\n",
      "  time_since_restore: 62.856006383895874\n",
      "  time_this_iter_s: 10.312787055969238\n",
      "  time_total_s: 62.856006383895874\n",
      "  timers:\n",
      "    learn_throughput: 40811.633\n",
      "    learn_time_ms: 6.273\n",
      "    update_time_ms: 3.174\n",
      "  timestamp: 1620515301\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 7500\n",
      "  training_iteration: 7\n",
      "  trial_id: 1a2f8_00000\n",
      "  \n",
      "Result for DDPG_SimpleSupplyChain_1a2f8_00001:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-08_18-08-31\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_reward_max: -157551.57476716107\n",
      "  episode_reward_mean: -191296.30040011505\n",
      "  episode_reward_min: -219035.01130082077\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 340\n",
      "  experiment_id: d723ce1e54d6484893e40b17874a26d0\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 8500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: 1510.16015625\n",
      "        mean_q: -47137.8125\n",
      "        min_q: -131271.4375\n",
      "        model: {}\n",
      "    num_steps_sampled: 8500\n",
      "    num_steps_trained: 448256\n",
      "    num_target_updates: 1751\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.0\n",
      "    ram_util_percent: 68.0142857142857\n",
      "  pid: 8114\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12456463416382338\n",
      "    mean_env_wait_ms: 0.3082844098230948\n",
      "    mean_inference_ms: 0.9147335007984959\n",
      "    mean_raw_obs_processing_ms: 0.27747516701950475\n",
      "  time_since_restore: 72.72218871116638\n",
      "  time_this_iter_s: 9.925390005111694\n",
      "  time_total_s: 72.72218871116638\n",
      "  timers:\n",
      "    learn_throughput: 53097.969\n",
      "    learn_time_ms: 4.821\n",
      "    update_time_ms: 4.815\n",
      "  timestamp: 1620515311\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 8500\n",
      "  training_iteration: 8\n",
      "  trial_id: 1a2f8_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 10/12 CPUs, 0/0 GPUs, 0.0/4.79 GiB heap, 0.0/1.61 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_SimpleSupplyChain_1a2f8_00000</td><td>RUNNING </td><td>192.168.0.23:8118</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         62.856 </td><td style=\"text-align: right;\">7500</td><td style=\"text-align: right;\"> -158133</td><td style=\"text-align: right;\">             -102769</td><td style=\"text-align: right;\">             -220018</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "<tr><td>DDPG_SimpleSupplyChain_1a2f8_00001</td><td>RUNNING </td><td>192.168.0.23:8114</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         72.7222</td><td style=\"text-align: right;\">8500</td><td style=\"text-align: right;\"> -191296</td><td style=\"text-align: right;\">             -157552</td><td style=\"text-align: right;\">             -219035</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_SimpleSupplyChain_1a2f8_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-08_18-08-31\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_reward_max: -106673.99136994398\n",
      "  episode_reward_mean: -151196.7035557851\n",
      "  episode_reward_min: -199087.456807997\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 340\n",
      "  experiment_id: 925d870c9bcc486a8beaa0dfc6db064c\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 8500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: 786.6741943359375\n",
      "        mean_q: -39287.80078125\n",
      "        min_q: -146071.171875\n",
      "        model: {}\n",
      "    num_steps_sampled: 8500\n",
      "    num_steps_trained: 448256\n",
      "    num_target_updates: 1751\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.09285714285714\n",
      "    ram_util_percent: 68.0142857142857\n",
      "  pid: 8118\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12043774534464673\n",
      "    mean_env_wait_ms: 0.2978027352478127\n",
      "    mean_inference_ms: 0.8773195429388108\n",
      "    mean_raw_obs_processing_ms: 0.26343439224549103\n",
      "  time_since_restore: 72.7804434299469\n",
      "  time_this_iter_s: 9.924437046051025\n",
      "  time_total_s: 72.7804434299469\n",
      "  timers:\n",
      "    learn_throughput: 45146.312\n",
      "    learn_time_ms: 5.67\n",
      "    update_time_ms: 4.158\n",
      "  timestamp: 1620515311\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 8500\n",
      "  training_iteration: 8\n",
      "  trial_id: 1a2f8_00000\n",
      "  \n",
      "Result for DDPG_SimpleSupplyChain_1a2f8_00001:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-08_18-08-41\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_reward_max: -157551.57476716107\n",
      "  episode_reward_mean: -190583.3572596433\n",
      "  episode_reward_min: -220630.90022173143\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 380\n",
      "  experiment_id: d723ce1e54d6484893e40b17874a26d0\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 9500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: 17.68059730529785\n",
      "        mean_q: -51171.79296875\n",
      "        min_q: -155558.984375\n",
      "        model: {}\n",
      "    num_steps_sampled: 9500\n",
      "    num_steps_trained: 512256\n",
      "    num_target_updates: 2001\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.14285714285713\n",
      "    ram_util_percent: 67.6\n",
      "  pid: 8114\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12340462190087152\n",
      "    mean_env_wait_ms: 0.3053784897945867\n",
      "    mean_inference_ms: 0.9041454698310379\n",
      "    mean_raw_obs_processing_ms: 0.27465416975524076\n",
      "  time_since_restore: 83.0380859375\n",
      "  time_this_iter_s: 10.315897226333618\n",
      "  time_total_s: 83.0380859375\n",
      "  timers:\n",
      "    learn_throughput: 41718.639\n",
      "    learn_time_ms: 6.136\n",
      "    update_time_ms: 3.524\n",
      "  timestamp: 1620515321\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 9500\n",
      "  training_iteration: 9\n",
      "  trial_id: 1a2f8_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 10/12 CPUs, 0/0 GPUs, 0.0/4.79 GiB heap, 0.0/1.61 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_SimpleSupplyChain_1a2f8_00000</td><td>RUNNING </td><td>192.168.0.23:8118</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         72.7804</td><td style=\"text-align: right;\">8500</td><td style=\"text-align: right;\"> -151197</td><td style=\"text-align: right;\">             -106674</td><td style=\"text-align: right;\">             -199087</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "<tr><td>DDPG_SimpleSupplyChain_1a2f8_00001</td><td>RUNNING </td><td>192.168.0.23:8114</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         83.0381</td><td style=\"text-align: right;\">9500</td><td style=\"text-align: right;\"> -190583</td><td style=\"text-align: right;\">             -157552</td><td style=\"text-align: right;\">             -220631</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_SimpleSupplyChain_1a2f8_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-08_18-08-41\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_reward_max: -106673.99136994398\n",
      "  episode_reward_mean: -142026.35986610394\n",
      "  episode_reward_min: -189100.1920730657\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 380\n",
      "  experiment_id: 925d870c9bcc486a8beaa0dfc6db064c\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 9500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: 516.3848266601562\n",
      "        mean_q: -39458.515625\n",
      "        min_q: -169312.703125\n",
      "        model: {}\n",
      "    num_steps_sampled: 9500\n",
      "    num_steps_trained: 512256\n",
      "    num_target_updates: 2001\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.10714285714285\n",
      "    ram_util_percent: 67.58571428571427\n",
      "  pid: 8118\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11985130396884917\n",
      "    mean_env_wait_ms: 0.2962891551235829\n",
      "    mean_inference_ms: 0.8706421639030267\n",
      "    mean_raw_obs_processing_ms: 0.2622289614644948\n",
      "  time_since_restore: 83.08756947517395\n",
      "  time_this_iter_s: 10.30712604522705\n",
      "  time_total_s: 83.08756947517395\n",
      "  timers:\n",
      "    learn_throughput: 43271.963\n",
      "    learn_time_ms: 5.916\n",
      "    update_time_ms: 2.701\n",
      "  timestamp: 1620515321\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 9500\n",
      "  training_iteration: 9\n",
      "  trial_id: 1a2f8_00000\n",
      "  \n",
      "Result for DDPG_SimpleSupplyChain_1a2f8_00001:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-08_18-08-52\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_reward_max: -163687.16436692412\n",
      "  episode_reward_mean: -191659.61005516193\n",
      "  episode_reward_min: -220630.90022173143\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 420\n",
      "  experiment_id: d723ce1e54d6484893e40b17874a26d0\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 10500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: -707.4987182617188\n",
      "        mean_q: -46197.05859375\n",
      "        min_q: -152054.3125\n",
      "        model: {}\n",
      "    num_steps_sampled: 10500\n",
      "    num_steps_trained: 576256\n",
      "    num_target_updates: 2251\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.91333333333333\n",
      "    ram_util_percent: 67.25333333333336\n",
      "  pid: 8114\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12281991963166189\n",
      "    mean_env_wait_ms: 0.30398451465430937\n",
      "    mean_inference_ms: 0.8985739558167538\n",
      "    mean_raw_obs_processing_ms: 0.2730913348429766\n",
      "  time_since_restore: 93.50546598434448\n",
      "  time_this_iter_s: 10.467380046844482\n",
      "  time_total_s: 93.50546598434448\n",
      "  timers:\n",
      "    learn_throughput: 53456.427\n",
      "    learn_time_ms: 4.789\n",
      "    update_time_ms: 2.598\n",
      "  timestamp: 1620515332\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10500\n",
      "  training_iteration: 10\n",
      "  trial_id: 1a2f8_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 10/12 CPUs, 0/0 GPUs, 0.0/4.79 GiB heap, 0.0/1.61 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_SimpleSupplyChain_1a2f8_00000</td><td>RUNNING </td><td>192.168.0.23:8118</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         83.0876</td><td style=\"text-align: right;\"> 9500</td><td style=\"text-align: right;\"> -142026</td><td style=\"text-align: right;\">             -106674</td><td style=\"text-align: right;\">             -189100</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "<tr><td>DDPG_SimpleSupplyChain_1a2f8_00001</td><td>RUNNING </td><td>192.168.0.23:8114</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         93.5055</td><td style=\"text-align: right;\">10500</td><td style=\"text-align: right;\"> -191660</td><td style=\"text-align: right;\">             -163687</td><td style=\"text-align: right;\">             -220631</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_SimpleSupplyChain_1a2f8_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-08_18-08-52\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_reward_max: -110874.33508585757\n",
      "  episode_reward_mean: -148769.6680220362\n",
      "  episode_reward_min: -209112.01437455034\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 420\n",
      "  experiment_id: 925d870c9bcc486a8beaa0dfc6db064c\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 10500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: -425.86614990234375\n",
      "        mean_q: -42757.05078125\n",
      "        min_q: -185901.25\n",
      "        model: {}\n",
      "    num_steps_sampled: 10500\n",
      "    num_steps_trained: 576256\n",
      "    num_target_updates: 2251\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 38.16\n",
      "    ram_util_percent: 67.25333333333336\n",
      "  pid: 8118\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11946041066001108\n",
      "    mean_env_wait_ms: 0.2954849623437197\n",
      "    mean_inference_ms: 0.866573476410652\n",
      "    mean_raw_obs_processing_ms: 0.2616567477928657\n",
      "  time_since_restore: 93.54356741905212\n",
      "  time_this_iter_s: 10.455997943878174\n",
      "  time_total_s: 93.54356741905212\n",
      "  timers:\n",
      "    learn_throughput: 44753.788\n",
      "    learn_time_ms: 5.72\n",
      "    update_time_ms: 2.91\n",
      "  timestamp: 1620515332\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10500\n",
      "  training_iteration: 10\n",
      "  trial_id: 1a2f8_00000\n",
      "  \n",
      "Result for DDPG_SimpleSupplyChain_1a2f8_00001:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-08_18-09-02\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_reward_max: -164933.81712504095\n",
      "  episode_reward_mean: -193117.47970717924\n",
      "  episode_reward_min: -216144.1686373166\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 460\n",
      "  experiment_id: d723ce1e54d6484893e40b17874a26d0\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 11500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: 858.6271362304688\n",
      "        mean_q: -51257.9375\n",
      "        min_q: -183187.140625\n",
      "        model: {}\n",
      "    num_steps_sampled: 11500\n",
      "    num_steps_trained: 640256\n",
      "    num_target_updates: 2501\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.892857142857146\n",
      "    ram_util_percent: 67.32857142857142\n",
      "  pid: 8114\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12256882230174929\n",
      "    mean_env_wait_ms: 0.3032593186444022\n",
      "    mean_inference_ms: 0.8952518915919901\n",
      "    mean_raw_obs_processing_ms: 0.2720403560266983\n",
      "  time_since_restore: 103.79288601875305\n",
      "  time_this_iter_s: 10.28742003440857\n",
      "  time_total_s: 103.79288601875305\n",
      "  timers:\n",
      "    learn_throughput: 46533.093\n",
      "    learn_time_ms: 5.501\n",
      "    update_time_ms: 2.759\n",
      "  timestamp: 1620515342\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 11500\n",
      "  training_iteration: 11\n",
      "  trial_id: 1a2f8_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 10/12 CPUs, 0/0 GPUs, 0.0/4.79 GiB heap, 0.0/1.61 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_SimpleSupplyChain_1a2f8_00000</td><td>RUNNING </td><td>192.168.0.23:8118</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         93.5436</td><td style=\"text-align: right;\">10500</td><td style=\"text-align: right;\"> -148770</td><td style=\"text-align: right;\">             -110874</td><td style=\"text-align: right;\">             -209112</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "<tr><td>DDPG_SimpleSupplyChain_1a2f8_00001</td><td>RUNNING </td><td>192.168.0.23:8114</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">        103.793 </td><td style=\"text-align: right;\">11500</td><td style=\"text-align: right;\"> -193117</td><td style=\"text-align: right;\">             -164934</td><td style=\"text-align: right;\">             -216144</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_SimpleSupplyChain_1a2f8_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-08_18-09-02\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_reward_max: -117454.05474898583\n",
      "  episode_reward_mean: -163831.54291979675\n",
      "  episode_reward_min: -209112.01437455034\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 460\n",
      "  experiment_id: 925d870c9bcc486a8beaa0dfc6db064c\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 11500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: -1099.2589111328125\n",
      "        mean_q: -45763.96875\n",
      "        min_q: -181383.0625\n",
      "        model: {}\n",
      "    num_steps_sampled: 11500\n",
      "    num_steps_trained: 640256\n",
      "    num_target_updates: 2501\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 37.464285714285715\n",
      "    ram_util_percent: 67.32142857142857\n",
      "  pid: 8118\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11928788574637685\n",
      "    mean_env_wait_ms: 0.295177447547642\n",
      "    mean_inference_ms: 0.8645372163106128\n",
      "    mean_raw_obs_processing_ms: 0.2614851493497191\n",
      "  time_since_restore: 103.79286050796509\n",
      "  time_this_iter_s: 10.249293088912964\n",
      "  time_total_s: 103.79286050796509\n",
      "  timers:\n",
      "    learn_throughput: 53419.725\n",
      "    learn_time_ms: 4.792\n",
      "    update_time_ms: 2.558\n",
      "  timestamp: 1620515342\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 11500\n",
      "  training_iteration: 11\n",
      "  trial_id: 1a2f8_00000\n",
      "  \n",
      "Result for DDPG_SimpleSupplyChain_1a2f8_00001:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-08_18-09-12\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_reward_max: -164549.75688923514\n",
      "  episode_reward_mean: -193397.69633673274\n",
      "  episode_reward_min: -216144.1686373166\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 500\n",
      "  experiment_id: d723ce1e54d6484893e40b17874a26d0\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 12500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: -2405.8330078125\n",
      "        mean_q: -54498.78125\n",
      "        min_q: -193224.875\n",
      "        model: {}\n",
      "    num_steps_sampled: 12500\n",
      "    num_steps_trained: 704256\n",
      "    num_target_updates: 2751\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.77142857142856\n",
      "    ram_util_percent: 67.62142857142858\n",
      "  pid: 8114\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1222809337654449\n",
      "    mean_env_wait_ms: 0.30256337667322797\n",
      "    mean_inference_ms: 0.8924178603261652\n",
      "    mean_raw_obs_processing_ms: 0.27118410960245826\n",
      "  time_since_restore: 114.08139085769653\n",
      "  time_this_iter_s: 10.288504838943481\n",
      "  time_total_s: 114.08139085769653\n",
      "  timers:\n",
      "    learn_throughput: 40690.38\n",
      "    learn_time_ms: 6.291\n",
      "    update_time_ms: 2.983\n",
      "  timestamp: 1620515352\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12500\n",
      "  training_iteration: 12\n",
      "  trial_id: 1a2f8_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 10/12 CPUs, 0/0 GPUs, 0.0/4.79 GiB heap, 0.0/1.61 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_SimpleSupplyChain_1a2f8_00000</td><td>RUNNING </td><td>192.168.0.23:8118</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         103.793</td><td style=\"text-align: right;\">11500</td><td style=\"text-align: right;\"> -163832</td><td style=\"text-align: right;\">             -117454</td><td style=\"text-align: right;\">             -209112</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "<tr><td>DDPG_SimpleSupplyChain_1a2f8_00001</td><td>RUNNING </td><td>192.168.0.23:8114</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         114.081</td><td style=\"text-align: right;\">12500</td><td style=\"text-align: right;\"> -193398</td><td style=\"text-align: right;\">             -164550</td><td style=\"text-align: right;\">             -216144</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_SimpleSupplyChain_1a2f8_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-08_18-09-12\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_reward_max: -123293.54927100605\n",
      "  episode_reward_mean: -175656.24181503773\n",
      "  episode_reward_min: -214769.67202437288\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 500\n",
      "  experiment_id: 925d870c9bcc486a8beaa0dfc6db064c\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 12500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: -418.5176696777344\n",
      "        mean_q: -52965.16796875\n",
      "        min_q: -208773.796875\n",
      "        model: {}\n",
      "    num_steps_sampled: 12500\n",
      "    num_steps_trained: 704256\n",
      "    num_target_updates: 2751\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.75\n",
      "    ram_util_percent: 67.62142857142858\n",
      "  pid: 8118\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11910665188302572\n",
      "    mean_env_wait_ms: 0.2948900570853798\n",
      "    mean_inference_ms: 0.862819717108601\n",
      "    mean_raw_obs_processing_ms: 0.2613434768219232\n",
      "  time_since_restore: 114.08735871315002\n",
      "  time_this_iter_s: 10.294498205184937\n",
      "  time_total_s: 114.08735871315002\n",
      "  timers:\n",
      "    learn_throughput: 41576.646\n",
      "    learn_time_ms: 6.157\n",
      "    update_time_ms: 3.202\n",
      "  timestamp: 1620515352\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12500\n",
      "  training_iteration: 12\n",
      "  trial_id: 1a2f8_00000\n",
      "  \n",
      "Result for DDPG_SimpleSupplyChain_1a2f8_00001:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-08_18-09-23\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_reward_max: -115779.09221773213\n",
      "  episode_reward_mean: -196799.60674105\n",
      "  episode_reward_min: -551283.728361409\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 540\n",
      "  experiment_id: d723ce1e54d6484893e40b17874a26d0\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 13500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: 3716.44775390625\n",
      "        mean_q: -55108.2734375\n",
      "        min_q: -198028.484375\n",
      "        model: {}\n",
      "    num_steps_sampled: 13500\n",
      "    num_steps_trained: 768256\n",
      "    num_target_updates: 3001\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.362500000000004\n",
      "    ram_util_percent: 67.79375\n",
      "  pid: 8114\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12227958042059349\n",
      "    mean_env_wait_ms: 0.3026290151181595\n",
      "    mean_inference_ms: 0.8920415410080241\n",
      "    mean_raw_obs_processing_ms: 0.2710929176675229\n",
      "  time_since_restore: 124.8279128074646\n",
      "  time_this_iter_s: 10.746521949768066\n",
      "  time_total_s: 124.8279128074646\n",
      "  timers:\n",
      "    learn_throughput: 41249.215\n",
      "    learn_time_ms: 6.206\n",
      "    update_time_ms: 4.042\n",
      "  timestamp: 1620515363\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 13500\n",
      "  training_iteration: 13\n",
      "  trial_id: 1a2f8_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 10/12 CPUs, 0/0 GPUs, 0.0/4.79 GiB heap, 0.0/1.61 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_SimpleSupplyChain_1a2f8_00000</td><td>RUNNING </td><td>192.168.0.23:8118</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         114.087</td><td style=\"text-align: right;\">12500</td><td style=\"text-align: right;\"> -175656</td><td style=\"text-align: right;\">             -123294</td><td style=\"text-align: right;\">             -214770</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "<tr><td>DDPG_SimpleSupplyChain_1a2f8_00001</td><td>RUNNING </td><td>192.168.0.23:8114</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         124.828</td><td style=\"text-align: right;\">13500</td><td style=\"text-align: right;\"> -196800</td><td style=\"text-align: right;\">             -115779</td><td style=\"text-align: right;\">             -551284</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_SimpleSupplyChain_1a2f8_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-08_18-09-23\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_reward_max: -142683.54206972188\n",
      "  episode_reward_mean: -178643.50023525555\n",
      "  episode_reward_min: -214769.67202437288\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 540\n",
      "  experiment_id: 925d870c9bcc486a8beaa0dfc6db064c\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 13500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: 9747.900390625\n",
      "        mean_q: -53897.57421875\n",
      "        min_q: -227090.125\n",
      "        model: {}\n",
      "    num_steps_sampled: 13500\n",
      "    num_steps_trained: 768256\n",
      "    num_target_updates: 3001\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.5\n",
      "    ram_util_percent: 67.80000000000001\n",
      "  pid: 8118\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11913274260800584\n",
      "    mean_env_wait_ms: 0.29507515410314283\n",
      "    mean_inference_ms: 0.862964629522929\n",
      "    mean_raw_obs_processing_ms: 0.2614925203460244\n",
      "  time_since_restore: 124.82386755943298\n",
      "  time_this_iter_s: 10.736508846282959\n",
      "  time_total_s: 124.82386755943298\n",
      "  timers:\n",
      "    learn_throughput: 41550.421\n",
      "    learn_time_ms: 6.161\n",
      "    update_time_ms: 3.651\n",
      "  timestamp: 1620515363\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 13500\n",
      "  training_iteration: 13\n",
      "  trial_id: 1a2f8_00000\n",
      "  \n",
      "Result for DDPG_SimpleSupplyChain_1a2f8_00001:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-08_18-09-34\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_reward_max: -115779.09221773213\n",
      "  episode_reward_mean: -185451.7583662333\n",
      "  episode_reward_min: -551283.728361409\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 580\n",
      "  experiment_id: d723ce1e54d6484893e40b17874a26d0\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 14500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: 7781.8388671875\n",
      "        mean_q: -47430.84375\n",
      "        min_q: -217646.59375\n",
      "        model: {}\n",
      "    num_steps_sampled: 14500\n",
      "    num_steps_trained: 832256\n",
      "    num_target_updates: 3251\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.79333333333333\n",
      "    ram_util_percent: 68.42000000000002\n",
      "  pid: 8114\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12261945304679589\n",
      "    mean_env_wait_ms: 0.3034988759229059\n",
      "    mean_inference_ms: 0.8940949460054148\n",
      "    mean_raw_obs_processing_ms: 0.2716679799452506\n",
      "  time_since_restore: 136.03744077682495\n",
      "  time_this_iter_s: 11.209527969360352\n",
      "  time_total_s: 136.03744077682495\n",
      "  timers:\n",
      "    learn_throughput: 46328.503\n",
      "    learn_time_ms: 5.526\n",
      "    update_time_ms: 2.81\n",
      "  timestamp: 1620515374\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 14500\n",
      "  training_iteration: 14\n",
      "  trial_id: 1a2f8_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 10/12 CPUs, 0/0 GPUs, 0.0/4.79 GiB heap, 0.0/1.61 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_SimpleSupplyChain_1a2f8_00000</td><td>RUNNING </td><td>192.168.0.23:8118</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         124.824</td><td style=\"text-align: right;\">13500</td><td style=\"text-align: right;\"> -178644</td><td style=\"text-align: right;\">             -142684</td><td style=\"text-align: right;\">             -214770</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "<tr><td>DDPG_SimpleSupplyChain_1a2f8_00001</td><td>RUNNING </td><td>192.168.0.23:8114</td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">         136.037</td><td style=\"text-align: right;\">14500</td><td style=\"text-align: right;\"> -185452</td><td style=\"text-align: right;\">             -115779</td><td style=\"text-align: right;\">             -551284</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_SimpleSupplyChain_1a2f8_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-08_18-09-34\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_reward_max: -144926.29804793364\n",
      "  episode_reward_mean: -178516.03794342256\n",
      "  episode_reward_min: -213863.89428693792\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 580\n",
      "  experiment_id: 925d870c9bcc486a8beaa0dfc6db064c\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 14500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: 3494.81884765625\n",
      "        mean_q: -50538.20703125\n",
      "        min_q: -223588.671875\n",
      "        model: {}\n",
      "    num_steps_sampled: 14500\n",
      "    num_steps_trained: 832256\n",
      "    num_target_updates: 3251\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.69375\n",
      "    ram_util_percent: 68.425\n",
      "  pid: 8118\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11931785871422251\n",
      "    mean_env_wait_ms: 0.2955172777955953\n",
      "    mean_inference_ms: 0.8645162612260875\n",
      "    mean_raw_obs_processing_ms: 0.26169072887336947\n",
      "  time_since_restore: 136.02910232543945\n",
      "  time_this_iter_s: 11.20523476600647\n",
      "  time_total_s: 136.02910232543945\n",
      "  timers:\n",
      "    learn_throughput: 52319.206\n",
      "    learn_time_ms: 4.893\n",
      "    update_time_ms: 3.115\n",
      "  timestamp: 1620515374\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 14500\n",
      "  training_iteration: 14\n",
      "  trial_id: 1a2f8_00000\n",
      "  \n",
      "Result for DDPG_SimpleSupplyChain_1a2f8_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-08_18-09-45\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_reward_max: -144926.29804793364\n",
      "  episode_reward_mean: -177817.6394637695\n",
      "  episode_reward_min: -206868.56832006937\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 620\n",
      "  experiment_id: 925d870c9bcc486a8beaa0dfc6db064c\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 15500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: -1238.9644775390625\n",
      "        mean_q: -54187.91796875\n",
      "        min_q: -233153.6875\n",
      "        model: {}\n",
      "    num_steps_sampled: 15500\n",
      "    num_steps_trained: 896256\n",
      "    num_target_updates: 3501\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.873333333333335\n",
      "    ram_util_percent: 69.47333333333334\n",
      "  pid: 8118\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1195001410461108\n",
      "    mean_env_wait_ms: 0.29589864950965067\n",
      "    mean_inference_ms: 0.8658584985725551\n",
      "    mean_raw_obs_processing_ms: 0.2618699108234971\n",
      "  time_since_restore: 146.82372331619263\n",
      "  time_this_iter_s: 10.794620990753174\n",
      "  time_total_s: 146.82372331619263\n",
      "  timers:\n",
      "    learn_throughput: 38824.914\n",
      "    learn_time_ms: 6.594\n",
      "    update_time_ms: 3.518\n",
      "  timestamp: 1620515385\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 15500\n",
      "  training_iteration: 15\n",
      "  trial_id: 1a2f8_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 10/12 CPUs, 0/0 GPUs, 0.0/4.79 GiB heap, 0.0/1.61 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_SimpleSupplyChain_1a2f8_00000</td><td>RUNNING </td><td>192.168.0.23:8118</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         146.824</td><td style=\"text-align: right;\">15500</td><td style=\"text-align: right;\"> -177818</td><td style=\"text-align: right;\">             -144926</td><td style=\"text-align: right;\">             -206869</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "<tr><td>DDPG_SimpleSupplyChain_1a2f8_00001</td><td>RUNNING </td><td>192.168.0.23:8114</td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">         136.037</td><td style=\"text-align: right;\">14500</td><td style=\"text-align: right;\"> -185452</td><td style=\"text-align: right;\">             -115779</td><td style=\"text-align: right;\">             -551284</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_SimpleSupplyChain_1a2f8_00001:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-08_18-09-45\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_reward_max: -115779.09221773213\n",
      "  episode_reward_mean: -169453.84289862952\n",
      "  episode_reward_min: -479059.21197478834\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 620\n",
      "  experiment_id: d723ce1e54d6484893e40b17874a26d0\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 15500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: 2730.994384765625\n",
      "        mean_q: -53182.0390625\n",
      "        min_q: -230265.875\n",
      "        model: {}\n",
      "    num_steps_sampled: 15500\n",
      "    num_steps_trained: 896256\n",
      "    num_target_updates: 3501\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.96666666666667\n",
      "    ram_util_percent: 69.44666666666667\n",
      "  pid: 8114\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12286399862054846\n",
      "    mean_env_wait_ms: 0.30412271766861265\n",
      "    mean_inference_ms: 0.8951649261643922\n",
      "    mean_raw_obs_processing_ms: 0.27193936038782235\n",
      "  time_since_restore: 146.8666934967041\n",
      "  time_this_iter_s: 10.82925271987915\n",
      "  time_total_s: 146.8666934967041\n",
      "  timers:\n",
      "    learn_throughput: 39840.667\n",
      "    learn_time_ms: 6.426\n",
      "    update_time_ms: 3.5\n",
      "  timestamp: 1620515385\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 15500\n",
      "  training_iteration: 15\n",
      "  trial_id: 1a2f8_00001\n",
      "  \n",
      "Result for DDPG_SimpleSupplyChain_1a2f8_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-08_18-09-56\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_reward_max: -144926.29804793364\n",
      "  episode_reward_mean: -179660.9583067021\n",
      "  episode_reward_min: -207980.60350320852\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 660\n",
      "  experiment_id: 925d870c9bcc486a8beaa0dfc6db064c\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 16500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: 1399.7939453125\n",
      "        mean_q: -55527.265625\n",
      "        min_q: -245628.25\n",
      "        model: {}\n",
      "    num_steps_sampled: 16500\n",
      "    num_steps_trained: 960256\n",
      "    num_target_updates: 3751\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.266666666666666\n",
      "    ram_util_percent: 69.39333333333335\n",
      "  pid: 8118\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11948604839263599\n",
      "    mean_env_wait_ms: 0.2957582689361057\n",
      "    mean_inference_ms: 0.8657308848664708\n",
      "    mean_raw_obs_processing_ms: 0.26173814842118875\n",
      "  time_since_restore: 157.55520129203796\n",
      "  time_this_iter_s: 10.731477975845337\n",
      "  time_total_s: 157.55520129203796\n",
      "  timers:\n",
      "    learn_throughput: 45724.413\n",
      "    learn_time_ms: 5.599\n",
      "    update_time_ms: 2.599\n",
      "  timestamp: 1620515396\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 16500\n",
      "  training_iteration: 16\n",
      "  trial_id: 1a2f8_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 10/12 CPUs, 0/0 GPUs, 0.0/4.79 GiB heap, 0.0/1.61 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_SimpleSupplyChain_1a2f8_00000</td><td>RUNNING </td><td>192.168.0.23:8118</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         157.555</td><td style=\"text-align: right;\">16500</td><td style=\"text-align: right;\"> -179661</td><td style=\"text-align: right;\">             -144926</td><td style=\"text-align: right;\">             -207981</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "<tr><td>DDPG_SimpleSupplyChain_1a2f8_00001</td><td>RUNNING </td><td>192.168.0.23:8114</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         146.867</td><td style=\"text-align: right;\">15500</td><td style=\"text-align: right;\"> -169454</td><td style=\"text-align: right;\">             -115779</td><td style=\"text-align: right;\">             -479059</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_SimpleSupplyChain_1a2f8_00001:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-08_18-09-56\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_reward_max: -116470.56759108967\n",
      "  episode_reward_mean: -155273.06720547716\n",
      "  episode_reward_min: -196412.6755786351\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 660\n",
      "  experiment_id: d723ce1e54d6484893e40b17874a26d0\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 16500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: 2362.8876953125\n",
      "        mean_q: -49370.9296875\n",
      "        min_q: -229914.6875\n",
      "        model: {}\n",
      "    num_steps_sampled: 16500\n",
      "    num_steps_trained: 960256\n",
      "    num_target_updates: 3751\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 45.04\n",
      "    ram_util_percent: 69.3\n",
      "  pid: 8114\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12277778075121294\n",
      "    mean_env_wait_ms: 0.30398639250208226\n",
      "    mean_inference_ms: 0.893745299438641\n",
      "    mean_raw_obs_processing_ms: 0.27147048978254273\n",
      "  time_since_restore: 157.60097074508667\n",
      "  time_this_iter_s: 10.734277248382568\n",
      "  time_total_s: 157.60097074508667\n",
      "  timers:\n",
      "    learn_throughput: 52708.039\n",
      "    learn_time_ms: 4.857\n",
      "    update_time_ms: 2.563\n",
      "  timestamp: 1620515396\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 16500\n",
      "  training_iteration: 16\n",
      "  trial_id: 1a2f8_00001\n",
      "  \n",
      "Result for DDPG_SimpleSupplyChain_1a2f8_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-08_18-10-06\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_reward_max: -142386.21985884375\n",
      "  episode_reward_mean: -180716.10069572832\n",
      "  episode_reward_min: -207980.60350320852\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 700\n",
      "  experiment_id: 925d870c9bcc486a8beaa0dfc6db064c\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 17500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: -643.3870849609375\n",
      "        mean_q: -59088.8125\n",
      "        min_q: -253690.171875\n",
      "        model: {}\n",
      "    num_steps_sampled: 17500\n",
      "    num_steps_trained: 1024256\n",
      "    num_target_updates: 4001\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 47.32142857142858\n",
      "    ram_util_percent: 69.22857142857143\n",
      "  pid: 8118\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11935997134133647\n",
      "    mean_env_wait_ms: 0.29550282058681226\n",
      "    mean_inference_ms: 0.8648316015311944\n",
      "    mean_raw_obs_processing_ms: 0.26159768476093154\n",
      "  time_since_restore: 167.90871739387512\n",
      "  time_this_iter_s: 10.353516101837158\n",
      "  time_total_s: 167.90871739387512\n",
      "  timers:\n",
      "    learn_throughput: 51541.422\n",
      "    learn_time_ms: 4.967\n",
      "    update_time_ms: 2.759\n",
      "  timestamp: 1620515406\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 17500\n",
      "  training_iteration: 17\n",
      "  trial_id: 1a2f8_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 10/12 CPUs, 0/0 GPUs, 0.0/4.79 GiB heap, 0.0/1.61 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_SimpleSupplyChain_1a2f8_00000</td><td>RUNNING </td><td>192.168.0.23:8118</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         167.909</td><td style=\"text-align: right;\">17500</td><td style=\"text-align: right;\"> -180716</td><td style=\"text-align: right;\">             -142386</td><td style=\"text-align: right;\">             -207981</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "<tr><td>DDPG_SimpleSupplyChain_1a2f8_00001</td><td>RUNNING </td><td>192.168.0.23:8114</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         157.601</td><td style=\"text-align: right;\">16500</td><td style=\"text-align: right;\"> -155273</td><td style=\"text-align: right;\">             -116471</td><td style=\"text-align: right;\">             -196413</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_SimpleSupplyChain_1a2f8_00001:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-08_18-10-06\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_reward_max: -102703.68414212382\n",
      "  episode_reward_mean: -149254.0832091443\n",
      "  episode_reward_min: -191069.14780467367\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 700\n",
      "  experiment_id: d723ce1e54d6484893e40b17874a26d0\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 17500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: 2133.52734375\n",
      "        mean_q: -50472.9140625\n",
      "        min_q: -243952.71875\n",
      "        model: {}\n",
      "    num_steps_sampled: 17500\n",
      "    num_steps_trained: 1024256\n",
      "    num_target_updates: 4001\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 47.17333333333333\n",
      "    ram_util_percent: 69.26\n",
      "  pid: 8114\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12242740550579981\n",
      "    mean_env_wait_ms: 0.30324154494776406\n",
      "    mean_inference_ms: 0.8907669955916683\n",
      "    mean_raw_obs_processing_ms: 0.27043323967689836\n",
      "  time_since_restore: 167.9678544998169\n",
      "  time_this_iter_s: 10.366883754730225\n",
      "  time_total_s: 167.9678544998169\n",
      "  timers:\n",
      "    learn_throughput: 50086.148\n",
      "    learn_time_ms: 5.111\n",
      "    update_time_ms: 3.224\n",
      "  timestamp: 1620515406\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 17500\n",
      "  training_iteration: 17\n",
      "  trial_id: 1a2f8_00001\n",
      "  \n",
      "Result for DDPG_SimpleSupplyChain_1a2f8_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-08_18-10-17\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_reward_max: -142386.21985884375\n",
      "  episode_reward_mean: -181439.42751493974\n",
      "  episode_reward_min: -212861.6606281436\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 740\n",
      "  experiment_id: 925d870c9bcc486a8beaa0dfc6db064c\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 18500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: 21368.193359375\n",
      "        mean_q: -63344.54296875\n",
      "        min_q: -263105.15625\n",
      "        model: {}\n",
      "    num_steps_sampled: 18500\n",
      "    num_steps_trained: 1088256\n",
      "    num_target_updates: 4251\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.526666666666664\n",
      "    ram_util_percent: 68.78\n",
      "  pid: 8118\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11908796786050935\n",
      "    mean_env_wait_ms: 0.29488991391033487\n",
      "    mean_inference_ms: 0.8628516415203002\n",
      "    mean_raw_obs_processing_ms: 0.26114470108445215\n",
      "  time_since_restore: 178.35961937904358\n",
      "  time_this_iter_s: 10.450901985168457\n",
      "  time_total_s: 178.35961937904358\n",
      "  timers:\n",
      "    learn_throughput: 46488.367\n",
      "    learn_time_ms: 5.507\n",
      "    update_time_ms: 2.706\n",
      "  timestamp: 1620515417\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 18500\n",
      "  training_iteration: 18\n",
      "  trial_id: 1a2f8_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 10/12 CPUs, 0/0 GPUs, 0.0/4.79 GiB heap, 0.0/1.61 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_SimpleSupplyChain_1a2f8_00000</td><td>RUNNING </td><td>192.168.0.23:8118</td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">         178.36 </td><td style=\"text-align: right;\">18500</td><td style=\"text-align: right;\"> -181439</td><td style=\"text-align: right;\">             -142386</td><td style=\"text-align: right;\">             -212862</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "<tr><td>DDPG_SimpleSupplyChain_1a2f8_00001</td><td>RUNNING </td><td>192.168.0.23:8114</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         167.968</td><td style=\"text-align: right;\">17500</td><td style=\"text-align: right;\"> -149254</td><td style=\"text-align: right;\">             -102704</td><td style=\"text-align: right;\">             -191069</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_SimpleSupplyChain_1a2f8_00001:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-08_18-10-17\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_reward_max: -101775.910005588\n",
      "  episode_reward_mean: -144705.5577928732\n",
      "  episode_reward_min: -177959.33638403183\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 740\n",
      "  experiment_id: d723ce1e54d6484893e40b17874a26d0\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 18500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: 1272.952392578125\n",
      "        mean_q: -52192.96875\n",
      "        min_q: -239762.3125\n",
      "        model: {}\n",
      "    num_steps_sampled: 18500\n",
      "    num_steps_trained: 1088256\n",
      "    num_target_updates: 4251\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.5\n",
      "    ram_util_percent: 68.75\n",
      "  pid: 8114\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12192584998292003\n",
      "    mean_env_wait_ms: 0.30215439369215646\n",
      "    mean_inference_ms: 0.8873119634796863\n",
      "    mean_raw_obs_processing_ms: 0.26929812588551455\n",
      "  time_since_restore: 178.39887762069702\n",
      "  time_this_iter_s: 10.431023120880127\n",
      "  time_total_s: 178.39887762069702\n",
      "  timers:\n",
      "    learn_throughput: 53291.403\n",
      "    learn_time_ms: 4.804\n",
      "    update_time_ms: 2.611\n",
      "  timestamp: 1620515417\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 18500\n",
      "  training_iteration: 18\n",
      "  trial_id: 1a2f8_00001\n",
      "  \n",
      "Result for DDPG_SimpleSupplyChain_1a2f8_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-08_18-10-28\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_reward_max: -141207.37488438553\n",
      "  episode_reward_mean: -180478.0372245917\n",
      "  episode_reward_min: -212861.6606281436\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 780\n",
      "  experiment_id: 925d870c9bcc486a8beaa0dfc6db064c\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 19500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: 5120.32177734375\n",
      "        mean_q: -55434.12890625\n",
      "        min_q: -248850.8125\n",
      "        model: {}\n",
      "    num_steps_sampled: 19500\n",
      "    num_steps_trained: 1152256\n",
      "    num_target_updates: 4501\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 46.60666666666666\n",
      "    ram_util_percent: 69.18666666666668\n",
      "  pid: 8118\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11894153257430913\n",
      "    mean_env_wait_ms: 0.29456983109452345\n",
      "    mean_inference_ms: 0.862087498090741\n",
      "    mean_raw_obs_processing_ms: 0.2609728355420199\n",
      "  time_since_restore: 189.49810361862183\n",
      "  time_this_iter_s: 11.138484239578247\n",
      "  time_total_s: 189.49810361862183\n",
      "  timers:\n",
      "    learn_throughput: 52095.571\n",
      "    learn_time_ms: 4.914\n",
      "    update_time_ms: 4.581\n",
      "  timestamp: 1620515428\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 19500\n",
      "  training_iteration: 19\n",
      "  trial_id: 1a2f8_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 10/12 CPUs, 0/0 GPUs, 0.0/4.79 GiB heap, 0.0/1.61 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_SimpleSupplyChain_1a2f8_00000</td><td>RUNNING </td><td>192.168.0.23:8118</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         189.498</td><td style=\"text-align: right;\">19500</td><td style=\"text-align: right;\"> -180478</td><td style=\"text-align: right;\">             -141207</td><td style=\"text-align: right;\">             -212862</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "<tr><td>DDPG_SimpleSupplyChain_1a2f8_00001</td><td>RUNNING </td><td>192.168.0.23:8114</td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">         178.399</td><td style=\"text-align: right;\">18500</td><td style=\"text-align: right;\"> -144706</td><td style=\"text-align: right;\">             -101776</td><td style=\"text-align: right;\">             -177959</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_SimpleSupplyChain_1a2f8_00001:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-08_18-10-28\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_reward_max: -101775.910005588\n",
      "  episode_reward_mean: -145211.00956846622\n",
      "  episode_reward_min: -177959.33638403183\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 780\n",
      "  experiment_id: d723ce1e54d6484893e40b17874a26d0\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 19500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: 87.07831573486328\n",
      "        mean_q: -53976.7578125\n",
      "        min_q: -271926.75\n",
      "        model: {}\n",
      "    num_steps_sampled: 19500\n",
      "    num_steps_trained: 1152256\n",
      "    num_target_updates: 4501\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 46.3\n",
      "    ram_util_percent: 69.1875\n",
      "  pid: 8114\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12164629281768846\n",
      "    mean_env_wait_ms: 0.30151199065730006\n",
      "    mean_inference_ms: 0.8857926453270091\n",
      "    mean_raw_obs_processing_ms: 0.2686557450097072\n",
      "  time_since_restore: 189.50764870643616\n",
      "  time_this_iter_s: 11.108771085739136\n",
      "  time_total_s: 189.50764870643616\n",
      "  timers:\n",
      "    learn_throughput: 41938.617\n",
      "    learn_time_ms: 6.104\n",
      "    update_time_ms: 4.13\n",
      "  timestamp: 1620515428\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 19500\n",
      "  training_iteration: 19\n",
      "  trial_id: 1a2f8_00001\n",
      "  \n",
      "Result for DDPG_SimpleSupplyChain_1a2f8_00001:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-08_18-10-39\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_reward_max: -93582.59101583427\n",
      "  episode_reward_mean: -146462.8077337933\n",
      "  episode_reward_min: -166553.67296626038\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 820\n",
      "  experiment_id: d723ce1e54d6484893e40b17874a26d0\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 20500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: -1006.0154418945312\n",
      "        mean_q: -59874.8125\n",
      "        min_q: -266972.8125\n",
      "        model: {}\n",
      "    num_steps_sampled: 20500\n",
      "    num_steps_trained: 1216256\n",
      "    num_target_updates: 4751\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.559999999999995\n",
      "    ram_util_percent: 69.29333333333334\n",
      "  pid: 8114\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1214854652973986\n",
      "    mean_env_wait_ms: 0.30118879528234815\n",
      "    mean_inference_ms: 0.8849332670590206\n",
      "    mean_raw_obs_processing_ms: 0.2682823650931271\n",
      "  time_since_restore: 200.5762596130371\n",
      "  time_this_iter_s: 11.068610906600952\n",
      "  time_total_s: 200.5762596130371\n",
      "  timers:\n",
      "    learn_throughput: 48564.287\n",
      "    learn_time_ms: 5.271\n",
      "    update_time_ms: 2.675\n",
      "  timestamp: 1620515439\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 20500\n",
      "  training_iteration: 20\n",
      "  trial_id: 1a2f8_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 10/12 CPUs, 0/0 GPUs, 0.0/4.79 GiB heap, 0.0/1.61 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_SimpleSupplyChain_1a2f8_00000</td><td>RUNNING </td><td>192.168.0.23:8118</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         189.498</td><td style=\"text-align: right;\">19500</td><td style=\"text-align: right;\"> -180478</td><td style=\"text-align: right;\">           -141207  </td><td style=\"text-align: right;\">             -212862</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "<tr><td>DDPG_SimpleSupplyChain_1a2f8_00001</td><td>RUNNING </td><td>192.168.0.23:8114</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         200.576</td><td style=\"text-align: right;\">20500</td><td style=\"text-align: right;\"> -146463</td><td style=\"text-align: right;\">            -93582.6</td><td style=\"text-align: right;\">             -166554</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_SimpleSupplyChain_1a2f8_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-08_18-10-39\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_reward_max: -141207.37488438553\n",
      "  episode_reward_mean: -180459.00914875217\n",
      "  episode_reward_min: -212488.73796955115\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 820\n",
      "  experiment_id: 925d870c9bcc486a8beaa0dfc6db064c\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 20500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: 2587.066650390625\n",
      "        mean_q: -67428.2421875\n",
      "        min_q: -279453.0\n",
      "        model: {}\n",
      "    num_steps_sampled: 20500\n",
      "    num_steps_trained: 1216256\n",
      "    num_target_updates: 4751\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 46.5125\n",
      "    ram_util_percent: 69.28125\n",
      "  pid: 8118\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11890816753968336\n",
      "    mean_env_wait_ms: 0.2944887035434498\n",
      "    mean_inference_ms: 0.8623178487322121\n",
      "    mean_raw_obs_processing_ms: 0.2609470095781553\n",
      "  time_since_restore: 200.66765332221985\n",
      "  time_this_iter_s: 11.169549703598022\n",
      "  time_total_s: 200.66765332221985\n",
      "  timers:\n",
      "    learn_throughput: 48147.698\n",
      "    learn_time_ms: 5.317\n",
      "    update_time_ms: 2.971\n",
      "  timestamp: 1620515439\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 20500\n",
      "  training_iteration: 20\n",
      "  trial_id: 1a2f8_00000\n",
      "  \n",
      "Result for DDPG_SimpleSupplyChain_1a2f8_00001:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-08_18-10-50\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_reward_max: -93582.59101583427\n",
      "  episode_reward_mean: -149443.40142565072\n",
      "  episode_reward_min: -172644.58514500028\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 860\n",
      "  experiment_id: d723ce1e54d6484893e40b17874a26d0\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 21500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: 1071.5517578125\n",
      "        mean_q: -54664.38671875\n",
      "        min_q: -301328.09375\n",
      "        model: {}\n",
      "    num_steps_sampled: 21500\n",
      "    num_steps_trained: 1280256\n",
      "    num_target_updates: 5001\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.143750000000004\n",
      "    ram_util_percent: 68.76875\n",
      "  pid: 8114\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12139367193675814\n",
      "    mean_env_wait_ms: 0.30099652228871693\n",
      "    mean_inference_ms: 0.8840643919667633\n",
      "    mean_raw_obs_processing_ms: 0.26798254413538347\n",
      "  time_since_restore: 211.48971843719482\n",
      "  time_this_iter_s: 10.913458824157715\n",
      "  time_total_s: 211.48971843719482\n",
      "  timers:\n",
      "    learn_throughput: 37199.52\n",
      "    learn_time_ms: 6.882\n",
      "    update_time_ms: 4.49\n",
      "  timestamp: 1620515450\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 21500\n",
      "  training_iteration: 21\n",
      "  trial_id: 1a2f8_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 10/12 CPUs, 0/0 GPUs, 0.0/4.79 GiB heap, 0.0/1.61 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_SimpleSupplyChain_1a2f8_00000</td><td>RUNNING </td><td>192.168.0.23:8118</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         200.668</td><td style=\"text-align: right;\">20500</td><td style=\"text-align: right;\"> -180459</td><td style=\"text-align: right;\">           -141207  </td><td style=\"text-align: right;\">             -212489</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "<tr><td>DDPG_SimpleSupplyChain_1a2f8_00001</td><td>RUNNING </td><td>192.168.0.23:8114</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">         211.49 </td><td style=\"text-align: right;\">21500</td><td style=\"text-align: right;\"> -149443</td><td style=\"text-align: right;\">            -93582.6</td><td style=\"text-align: right;\">             -172645</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_SimpleSupplyChain_1a2f8_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-08_18-10-50\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_reward_max: -141207.37488438553\n",
      "  episode_reward_mean: -182621.70867260275\n",
      "  episode_reward_min: -213451.6911592371\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 860\n",
      "  experiment_id: 925d870c9bcc486a8beaa0dfc6db064c\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 21500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: 4029.734619140625\n",
      "        mean_q: -63034.59375\n",
      "        min_q: -299720.5\n",
      "        model: {}\n",
      "    num_steps_sampled: 21500\n",
      "    num_steps_trained: 1280256\n",
      "    num_target_updates: 5001\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.00000000000001\n",
      "    ram_util_percent: 68.79333333333334\n",
      "  pid: 8118\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11891643168817252\n",
      "    mean_env_wait_ms: 0.2945823403075577\n",
      "    mean_inference_ms: 0.8626515655722673\n",
      "    mean_raw_obs_processing_ms: 0.2610552406112483\n",
      "  time_since_restore: 211.58732533454895\n",
      "  time_this_iter_s: 10.919672012329102\n",
      "  time_total_s: 211.58732533454895\n",
      "  timers:\n",
      "    learn_throughput: 39213.991\n",
      "    learn_time_ms: 6.528\n",
      "    update_time_ms: 3.476\n",
      "  timestamp: 1620515450\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 21500\n",
      "  training_iteration: 21\n",
      "  trial_id: 1a2f8_00000\n",
      "  \n",
      "Result for DDPG_SimpleSupplyChain_1a2f8_00001:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-08_18-11-01\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_reward_max: -93582.59101583427\n",
      "  episode_reward_mean: -148405.77326015607\n",
      "  episode_reward_min: -178341.07803128278\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 900\n",
      "  experiment_id: d723ce1e54d6484893e40b17874a26d0\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 22500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: -1521.41650390625\n",
      "        mean_q: -62744.578125\n",
      "        min_q: -301972.375\n",
      "        model: {}\n",
      "    num_steps_sampled: 22500\n",
      "    num_steps_trained: 1344256\n",
      "    num_target_updates: 5251\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.559999999999995\n",
      "    ram_util_percent: 68.70666666666668\n",
      "  pid: 8114\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12129448815092897\n",
      "    mean_env_wait_ms: 0.30080849782800717\n",
      "    mean_inference_ms: 0.8831955732207157\n",
      "    mean_raw_obs_processing_ms: 0.2676252936930406\n",
      "  time_since_restore: 222.38072729110718\n",
      "  time_this_iter_s: 10.891008853912354\n",
      "  time_total_s: 222.38072729110718\n",
      "  timers:\n",
      "    learn_throughput: 53282.147\n",
      "    learn_time_ms: 4.805\n",
      "    update_time_ms: 2.96\n",
      "  timestamp: 1620515461\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 22500\n",
      "  training_iteration: 22\n",
      "  trial_id: 1a2f8_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 10/12 CPUs, 0/0 GPUs, 0.0/4.79 GiB heap, 0.0/1.61 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_SimpleSupplyChain_1a2f8_00000</td><td>RUNNING </td><td>192.168.0.23:8118</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">         211.587</td><td style=\"text-align: right;\">21500</td><td style=\"text-align: right;\"> -182622</td><td style=\"text-align: right;\">           -141207  </td><td style=\"text-align: right;\">             -213452</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "<tr><td>DDPG_SimpleSupplyChain_1a2f8_00001</td><td>RUNNING </td><td>192.168.0.23:8114</td><td style=\"text-align: right;\">    22</td><td style=\"text-align: right;\">         222.381</td><td style=\"text-align: right;\">22500</td><td style=\"text-align: right;\"> -148406</td><td style=\"text-align: right;\">            -93582.6</td><td style=\"text-align: right;\">             -178341</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_SimpleSupplyChain_1a2f8_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-08_18-11-01\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_reward_max: -153871.89733370012\n",
      "  episode_reward_mean: -188252.31398129306\n",
      "  episode_reward_min: -216975.9890509545\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 900\n",
      "  experiment_id: 925d870c9bcc486a8beaa0dfc6db064c\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 22500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: 3410.23486328125\n",
      "        mean_q: -66299.4375\n",
      "        min_q: -315944.96875\n",
      "        model: {}\n",
      "    num_steps_sampled: 22500\n",
      "    num_steps_trained: 1344256\n",
      "    num_target_updates: 5251\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.92666666666667\n",
      "    ram_util_percent: 68.69333333333334\n",
      "  pid: 8118\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11889663678481996\n",
      "    mean_env_wait_ms: 0.2945270114016842\n",
      "    mean_inference_ms: 0.8626424760606441\n",
      "    mean_raw_obs_processing_ms: 0.26116841870508845\n",
      "  time_since_restore: 222.48808217048645\n",
      "  time_this_iter_s: 10.9007568359375\n",
      "  time_total_s: 222.48808217048645\n",
      "  timers:\n",
      "    learn_throughput: 51137.625\n",
      "    learn_time_ms: 5.006\n",
      "    update_time_ms: 3.042\n",
      "  timestamp: 1620515461\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 22500\n",
      "  training_iteration: 22\n",
      "  trial_id: 1a2f8_00000\n",
      "  \n",
      "Result for DDPG_SimpleSupplyChain_1a2f8_00001:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-08_18-11-11\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_reward_max: -103199.17043151325\n",
      "  episode_reward_mean: -147995.19240814028\n",
      "  episode_reward_min: -178341.07803128278\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 940\n",
      "  experiment_id: d723ce1e54d6484893e40b17874a26d0\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 23500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: -2720.52490234375\n",
      "        mean_q: -57098.734375\n",
      "        min_q: -308854.90625\n",
      "        model: {}\n",
      "    num_steps_sampled: 23500\n",
      "    num_steps_trained: 1408256\n",
      "    num_target_updates: 5501\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 37.553333333333335\n",
      "    ram_util_percent: 68.02666666666666\n",
      "  pid: 8114\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12117990169288508\n",
      "    mean_env_wait_ms: 0.3004972465651614\n",
      "    mean_inference_ms: 0.8822914697137176\n",
      "    mean_raw_obs_processing_ms: 0.2671796873185209\n",
      "  time_since_restore: 232.85228753089905\n",
      "  time_this_iter_s: 10.47156023979187\n",
      "  time_total_s: 232.85228753089905\n",
      "  timers:\n",
      "    learn_throughput: 51935.564\n",
      "    learn_time_ms: 4.929\n",
      "    update_time_ms: 3.55\n",
      "  timestamp: 1620515471\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 23500\n",
      "  training_iteration: 23\n",
      "  trial_id: 1a2f8_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 10/12 CPUs, 0/0 GPUs, 0.0/4.79 GiB heap, 0.0/1.61 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_SimpleSupplyChain_1a2f8_00000</td><td>RUNNING </td><td>192.168.0.23:8118</td><td style=\"text-align: right;\">    22</td><td style=\"text-align: right;\">         222.488</td><td style=\"text-align: right;\">22500</td><td style=\"text-align: right;\"> -188252</td><td style=\"text-align: right;\">             -153872</td><td style=\"text-align: right;\">             -216976</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "<tr><td>DDPG_SimpleSupplyChain_1a2f8_00001</td><td>RUNNING </td><td>192.168.0.23:8114</td><td style=\"text-align: right;\">    23</td><td style=\"text-align: right;\">         232.852</td><td style=\"text-align: right;\">23500</td><td style=\"text-align: right;\"> -147995</td><td style=\"text-align: right;\">             -103199</td><td style=\"text-align: right;\">             -178341</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_SimpleSupplyChain_1a2f8_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-08_18-11-11\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_reward_max: -166566.72813412166\n",
      "  episode_reward_mean: -191711.64753334023\n",
      "  episode_reward_min: -226449.12652984625\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 940\n",
      "  experiment_id: 925d870c9bcc486a8beaa0dfc6db064c\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 23500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: -3135.076171875\n",
      "        mean_q: -76900.046875\n",
      "        min_q: -316085.25\n",
      "        model: {}\n",
      "    num_steps_sampled: 23500\n",
      "    num_steps_trained: 1408256\n",
      "    num_target_updates: 5501\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.11333333333334\n",
      "    ram_util_percent: 68.02666666666666\n",
      "  pid: 8118\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11884161919423762\n",
      "    mean_env_wait_ms: 0.29432031680229476\n",
      "    mean_inference_ms: 0.8622614807332121\n",
      "    mean_raw_obs_processing_ms: 0.26114526257596715\n",
      "  time_since_restore: 232.9591522216797\n",
      "  time_this_iter_s: 10.471070051193237\n",
      "  time_total_s: 232.9591522216797\n",
      "  timers:\n",
      "    learn_throughput: 45518.944\n",
      "    learn_time_ms: 5.624\n",
      "    update_time_ms: 2.949\n",
      "  timestamp: 1620515471\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 23500\n",
      "  training_iteration: 23\n",
      "  trial_id: 1a2f8_00000\n",
      "  \n",
      "Result for DDPG_SimpleSupplyChain_1a2f8_00001:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-08_18-11-23\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_reward_max: -122276.12489214531\n",
      "  episode_reward_mean: -150028.51760719373\n",
      "  episode_reward_min: -175155.60722683734\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 980\n",
      "  experiment_id: d723ce1e54d6484893e40b17874a26d0\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 24500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: -3685.985595703125\n",
      "        mean_q: -68660.0078125\n",
      "        min_q: -304206.53125\n",
      "        model: {}\n",
      "    num_steps_sampled: 24500\n",
      "    num_steps_trained: 1472256\n",
      "    num_target_updates: 5751\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.11333333333334\n",
      "    ram_util_percent: 69.27333333333334\n",
      "  pid: 8114\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12118948692142434\n",
      "    mean_env_wait_ms: 0.3004913357489708\n",
      "    mean_inference_ms: 0.8826829327919985\n",
      "    mean_raw_obs_processing_ms: 0.26699087774731817\n",
      "  time_since_restore: 244.20658040046692\n",
      "  time_this_iter_s: 11.354292869567871\n",
      "  time_total_s: 244.20658040046692\n",
      "  timers:\n",
      "    learn_throughput: 39920.653\n",
      "    learn_time_ms: 6.413\n",
      "    update_time_ms: 3.266\n",
      "  timestamp: 1620515483\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 24500\n",
      "  training_iteration: 24\n",
      "  trial_id: 1a2f8_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 10/12 CPUs, 0/0 GPUs, 0.0/4.79 GiB heap, 0.0/1.61 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_SimpleSupplyChain_1a2f8_00000</td><td>RUNNING </td><td>192.168.0.23:8118</td><td style=\"text-align: right;\">    23</td><td style=\"text-align: right;\">         232.959</td><td style=\"text-align: right;\">23500</td><td style=\"text-align: right;\"> -191712</td><td style=\"text-align: right;\">             -166567</td><td style=\"text-align: right;\">             -226449</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "<tr><td>DDPG_SimpleSupplyChain_1a2f8_00001</td><td>RUNNING </td><td>192.168.0.23:8114</td><td style=\"text-align: right;\">    24</td><td style=\"text-align: right;\">         244.207</td><td style=\"text-align: right;\">24500</td><td style=\"text-align: right;\"> -150029</td><td style=\"text-align: right;\">             -122276</td><td style=\"text-align: right;\">             -175156</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_SimpleSupplyChain_1a2f8_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-08_18-11-23\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_reward_max: -150581.97379596895\n",
      "  episode_reward_mean: -192573.962458221\n",
      "  episode_reward_min: -226449.12652984625\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 980\n",
      "  experiment_id: 925d870c9bcc486a8beaa0dfc6db064c\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 24500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: -4821.0185546875\n",
      "        mean_q: -76291.8125\n",
      "        min_q: -313595.0\n",
      "        model: {}\n",
      "    num_steps_sampled: 24500\n",
      "    num_steps_trained: 1472256\n",
      "    num_target_updates: 5751\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.59375\n",
      "    ram_util_percent: 69.25625\n",
      "  pid: 8118\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11890069643735661\n",
      "    mean_env_wait_ms: 0.2943366222642002\n",
      "    mean_inference_ms: 0.8630210559845216\n",
      "    mean_raw_obs_processing_ms: 0.26133202582927806\n",
      "  time_since_restore: 244.3848419189453\n",
      "  time_this_iter_s: 11.425689697265625\n",
      "  time_total_s: 244.3848419189453\n",
      "  timers:\n",
      "    learn_throughput: 42515.0\n",
      "    learn_time_ms: 6.021\n",
      "    update_time_ms: 3.76\n",
      "  timestamp: 1620515483\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 24500\n",
      "  training_iteration: 24\n",
      "  trial_id: 1a2f8_00000\n",
      "  \n",
      "Result for DDPG_SimpleSupplyChain_1a2f8_00001:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-08_18-11-34\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_reward_max: -111885.00373232728\n",
      "  episode_reward_mean: -150568.49737621655\n",
      "  episode_reward_min: -175155.60722683734\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1020\n",
      "  experiment_id: d723ce1e54d6484893e40b17874a26d0\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 25500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: -3382.351318359375\n",
      "        mean_q: -62310.57421875\n",
      "        min_q: -298487.46875\n",
      "        model: {}\n",
      "    num_steps_sampled: 25500\n",
      "    num_steps_trained: 1536256\n",
      "    num_target_updates: 6001\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.7\n",
      "    ram_util_percent: 68.81875\n",
      "  pid: 8114\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.121096049569102\n",
      "    mean_env_wait_ms: 0.3002252904731878\n",
      "    mean_inference_ms: 0.8824096596675182\n",
      "    mean_raw_obs_processing_ms: 0.2666150750689304\n",
      "  time_since_restore: 255.14853930473328\n",
      "  time_this_iter_s: 10.941958904266357\n",
      "  time_total_s: 255.14853930473328\n",
      "  timers:\n",
      "    learn_throughput: 48408.616\n",
      "    learn_time_ms: 5.288\n",
      "    update_time_ms: 3.093\n",
      "  timestamp: 1620515494\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 25500\n",
      "  training_iteration: 25\n",
      "  trial_id: 1a2f8_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 10/12 CPUs, 0/0 GPUs, 0.0/4.79 GiB heap, 0.0/1.61 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_SimpleSupplyChain_1a2f8_00000</td><td>RUNNING </td><td>192.168.0.23:8118</td><td style=\"text-align: right;\">    24</td><td style=\"text-align: right;\">         244.385</td><td style=\"text-align: right;\">24500</td><td style=\"text-align: right;\"> -192574</td><td style=\"text-align: right;\">             -150582</td><td style=\"text-align: right;\">             -226449</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "<tr><td>DDPG_SimpleSupplyChain_1a2f8_00001</td><td>RUNNING </td><td>192.168.0.23:8114</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         255.149</td><td style=\"text-align: right;\">25500</td><td style=\"text-align: right;\"> -150568</td><td style=\"text-align: right;\">             -111885</td><td style=\"text-align: right;\">             -175156</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_SimpleSupplyChain_1a2f8_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-08_18-11-34\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_reward_max: -66476.10517819588\n",
      "  episode_reward_mean: -175348.77145094317\n",
      "  episode_reward_min: -227653.56382361418\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1020\n",
      "  experiment_id: 925d870c9bcc486a8beaa0dfc6db064c\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 25500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: -8485.00390625\n",
      "        mean_q: -73611.796875\n",
      "        min_q: -326397.25\n",
      "        model: {}\n",
      "    num_steps_sampled: 25500\n",
      "    num_steps_trained: 1536256\n",
      "    num_target_updates: 6001\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.63333333333334\n",
      "    ram_util_percent: 68.82666666666665\n",
      "  pid: 8118\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11902273813426667\n",
      "    mean_env_wait_ms: 0.2944508538489587\n",
      "    mean_inference_ms: 0.8642170681062143\n",
      "    mean_raw_obs_processing_ms: 0.26151476411858166\n",
      "  time_since_restore: 255.30935502052307\n",
      "  time_this_iter_s: 10.924513101577759\n",
      "  time_total_s: 255.30935502052307\n",
      "  timers:\n",
      "    learn_throughput: 36877.688\n",
      "    learn_time_ms: 6.942\n",
      "    update_time_ms: 3.272\n",
      "  timestamp: 1620515494\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 25500\n",
      "  training_iteration: 25\n",
      "  trial_id: 1a2f8_00000\n",
      "  \n",
      "Result for DDPG_SimpleSupplyChain_1a2f8_00001:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-08_18-11-45\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_reward_max: -111885.00373232728\n",
      "  episode_reward_mean: -150271.40832228065\n",
      "  episode_reward_min: -175155.60722683734\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1060\n",
      "  experiment_id: d723ce1e54d6484893e40b17874a26d0\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 26500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: -2047.577880859375\n",
      "        mean_q: -66985.890625\n",
      "        min_q: -317586.6875\n",
      "        model: {}\n",
      "    num_steps_sampled: 26500\n",
      "    num_steps_trained: 1600256\n",
      "    num_target_updates: 6251\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 37.78666666666666\n",
      "    ram_util_percent: 67.71333333333335\n",
      "  pid: 8114\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12103526594692113\n",
      "    mean_env_wait_ms: 0.30004086472684216\n",
      "    mean_inference_ms: 0.8819593221589862\n",
      "    mean_raw_obs_processing_ms: 0.2663459882751687\n",
      "  time_since_restore: 266.23748421669006\n",
      "  time_this_iter_s: 11.088944911956787\n",
      "  time_total_s: 266.23748421669006\n",
      "  timers:\n",
      "    learn_throughput: 50242.703\n",
      "    learn_time_ms: 5.095\n",
      "    update_time_ms: 4.65\n",
      "  timestamp: 1620515505\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 26500\n",
      "  training_iteration: 26\n",
      "  trial_id: 1a2f8_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 10/12 CPUs, 0/0 GPUs, 0.0/4.79 GiB heap, 0.0/1.61 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_SimpleSupplyChain_1a2f8_00000</td><td>RUNNING </td><td>192.168.0.23:8118</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         255.309</td><td style=\"text-align: right;\">25500</td><td style=\"text-align: right;\"> -175349</td><td style=\"text-align: right;\">            -66476.1</td><td style=\"text-align: right;\">             -227654</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "<tr><td>DDPG_SimpleSupplyChain_1a2f8_00001</td><td>RUNNING </td><td>192.168.0.23:8114</td><td style=\"text-align: right;\">    26</td><td style=\"text-align: right;\">         266.237</td><td style=\"text-align: right;\">26500</td><td style=\"text-align: right;\"> -150271</td><td style=\"text-align: right;\">           -111885  </td><td style=\"text-align: right;\">             -175156</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_SimpleSupplyChain_1a2f8_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-08_18-11-45\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_reward_max: -66476.10517819588\n",
      "  episode_reward_mean: -141350.255720479\n",
      "  episode_reward_min: -227653.56382361418\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1060\n",
      "  experiment_id: 925d870c9bcc486a8beaa0dfc6db064c\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 26500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: 1806.01025390625\n",
      "        mean_q: -68135.1875\n",
      "        min_q: -322665.0625\n",
      "        model: {}\n",
      "    num_steps_sampled: 26500\n",
      "    num_steps_trained: 1600256\n",
      "    num_target_updates: 6251\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.63333333333334\n",
      "    ram_util_percent: 67.71333333333335\n",
      "  pid: 8118\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11915576870722928\n",
      "    mean_env_wait_ms: 0.29473863938209227\n",
      "    mean_inference_ms: 0.8654247840570357\n",
      "    mean_raw_obs_processing_ms: 0.2617779415849552\n",
      "  time_since_restore: 266.38168692588806\n",
      "  time_this_iter_s: 11.07233190536499\n",
      "  time_total_s: 266.38168692588806\n",
      "  timers:\n",
      "    learn_throughput: 46804.083\n",
      "    learn_time_ms: 5.47\n",
      "    update_time_ms: 3.518\n",
      "  timestamp: 1620515505\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 26500\n",
      "  training_iteration: 26\n",
      "  trial_id: 1a2f8_00000\n",
      "  \n",
      "Result for DDPG_SimpleSupplyChain_1a2f8_00001:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-08_18-11-55\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_reward_max: -114104.73751667506\n",
      "  episode_reward_mean: -149904.4336748742\n",
      "  episode_reward_min: -175034.00972689307\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1100\n",
      "  experiment_id: d723ce1e54d6484893e40b17874a26d0\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 27500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: -6089.470703125\n",
      "        mean_q: -66987.859375\n",
      "        min_q: -334565.6875\n",
      "        model: {}\n",
      "    num_steps_sampled: 27500\n",
      "    num_steps_trained: 1664256\n",
      "    num_target_updates: 6501\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.173333333333325\n",
      "    ram_util_percent: 67.84666666666665\n",
      "  pid: 8114\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12095900734879696\n",
      "    mean_env_wait_ms: 0.2999356992927148\n",
      "    mean_inference_ms: 0.8813459507710399\n",
      "    mean_raw_obs_processing_ms: 0.2662015482458\n",
      "  time_since_restore: 277.0975091457367\n",
      "  time_this_iter_s: 10.86002492904663\n",
      "  time_total_s: 277.0975091457367\n",
      "  timers:\n",
      "    learn_throughput: 54779.672\n",
      "    learn_time_ms: 4.673\n",
      "    update_time_ms: 3.071\n",
      "  timestamp: 1620515515\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 27500\n",
      "  training_iteration: 27\n",
      "  trial_id: 1a2f8_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 10/12 CPUs, 0/0 GPUs, 0.0/4.79 GiB heap, 0.0/1.61 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_SimpleSupplyChain_1a2f8_00000</td><td>RUNNING </td><td>192.168.0.23:8118</td><td style=\"text-align: right;\">    26</td><td style=\"text-align: right;\">         266.382</td><td style=\"text-align: right;\">26500</td><td style=\"text-align: right;\"> -141350</td><td style=\"text-align: right;\">            -66476.1</td><td style=\"text-align: right;\">             -227654</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "<tr><td>DDPG_SimpleSupplyChain_1a2f8_00001</td><td>RUNNING </td><td>192.168.0.23:8114</td><td style=\"text-align: right;\">    27</td><td style=\"text-align: right;\">         277.098</td><td style=\"text-align: right;\">27500</td><td style=\"text-align: right;\"> -149904</td><td style=\"text-align: right;\">           -114105  </td><td style=\"text-align: right;\">             -175034</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_SimpleSupplyChain_1a2f8_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-08_18-11-56\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_reward_max: -48267.5758558995\n",
      "  episode_reward_mean: -104055.56287351316\n",
      "  episode_reward_min: -226884.02946506804\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1100\n",
      "  experiment_id: 925d870c9bcc486a8beaa0dfc6db064c\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 27500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: 22311.3125\n",
      "        mean_q: -70607.21875\n",
      "        min_q: -326559.46875\n",
      "        model: {}\n",
      "    num_steps_sampled: 27500\n",
      "    num_steps_trained: 1664256\n",
      "    num_target_updates: 6501\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.15333333333333\n",
      "    ram_util_percent: 67.84666666666665\n",
      "  pid: 8118\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11928152084127\n",
      "    mean_env_wait_ms: 0.29516065509963135\n",
      "    mean_inference_ms: 0.8665299295286765\n",
      "    mean_raw_obs_processing_ms: 0.26212314809562076\n",
      "  time_since_restore: 277.25731587409973\n",
      "  time_this_iter_s: 10.87562894821167\n",
      "  time_total_s: 277.25731587409973\n",
      "  timers:\n",
      "    learn_throughput: 45276.142\n",
      "    learn_time_ms: 5.654\n",
      "    update_time_ms: 2.842\n",
      "  timestamp: 1620515516\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 27500\n",
      "  training_iteration: 27\n",
      "  trial_id: 1a2f8_00000\n",
      "  \n",
      "Result for DDPG_SimpleSupplyChain_1a2f8_00001:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-08_18-12-06\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_reward_max: -104459.38468481964\n",
      "  episode_reward_mean: -149593.3749790649\n",
      "  episode_reward_min: -175034.00972689307\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1140\n",
      "  experiment_id: d723ce1e54d6484893e40b17874a26d0\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 28500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: -2687.670166015625\n",
      "        mean_q: -70001.984375\n",
      "        min_q: -339054.5625\n",
      "        model: {}\n",
      "    num_steps_sampled: 28500\n",
      "    num_steps_trained: 1728256\n",
      "    num_target_updates: 6751\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.29333333333334\n",
      "    ram_util_percent: 68.01333333333334\n",
      "  pid: 8114\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12081818810205135\n",
      "    mean_env_wait_ms: 0.29956432606173894\n",
      "    mean_inference_ms: 0.8800680397806854\n",
      "    mean_raw_obs_processing_ms: 0.26583863707283156\n",
      "  time_since_restore: 287.9491219520569\n",
      "  time_this_iter_s: 10.85161280632019\n",
      "  time_total_s: 287.9491219520569\n",
      "  timers:\n",
      "    learn_throughput: 49698.075\n",
      "    learn_time_ms: 5.151\n",
      "    update_time_ms: 4.295\n",
      "  timestamp: 1620515526\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 28500\n",
      "  training_iteration: 28\n",
      "  trial_id: 1a2f8_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 10/12 CPUs, 0/0 GPUs, 0.0/4.79 GiB heap, 0.0/1.61 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_SimpleSupplyChain_1a2f8_00000</td><td>RUNNING </td><td>192.168.0.23:8118</td><td style=\"text-align: right;\">    27</td><td style=\"text-align: right;\">         277.257</td><td style=\"text-align: right;\">27500</td><td style=\"text-align: right;\"> -104056</td><td style=\"text-align: right;\">            -48267.6</td><td style=\"text-align: right;\">             -226884</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "<tr><td>DDPG_SimpleSupplyChain_1a2f8_00001</td><td>RUNNING </td><td>192.168.0.23:8114</td><td style=\"text-align: right;\">    28</td><td style=\"text-align: right;\">         287.949</td><td style=\"text-align: right;\">28500</td><td style=\"text-align: right;\"> -149593</td><td style=\"text-align: right;\">           -104459  </td><td style=\"text-align: right;\">             -175034</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_SimpleSupplyChain_1a2f8_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-08_18-12-06\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_reward_max: -44031.21173143749\n",
      "  episode_reward_mean: -85448.9356887007\n",
      "  episode_reward_min: -130789.89036739236\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1140\n",
      "  experiment_id: 925d870c9bcc486a8beaa0dfc6db064c\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 28500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: -3635.66748046875\n",
      "        mean_q: -68220.640625\n",
      "        min_q: -373559.125\n",
      "        model: {}\n",
      "    num_steps_sampled: 28500\n",
      "    num_steps_trained: 1728256\n",
      "    num_target_updates: 6751\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.27333333333334\n",
      "    ram_util_percent: 68.01333333333334\n",
      "  pid: 8118\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11924955864130235\n",
      "    mean_env_wait_ms: 0.2951295692049807\n",
      "    mean_inference_ms: 0.8662856433956166\n",
      "    mean_raw_obs_processing_ms: 0.2620559696247996\n",
      "  time_since_restore: 288.04782581329346\n",
      "  time_this_iter_s: 10.790509939193726\n",
      "  time_total_s: 288.04782581329346\n",
      "  timers:\n",
      "    learn_throughput: 46476.898\n",
      "    learn_time_ms: 5.508\n",
      "    update_time_ms: 3.358\n",
      "  timestamp: 1620515526\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 28500\n",
      "  training_iteration: 28\n",
      "  trial_id: 1a2f8_00000\n",
      "  \n",
      "Result for DDPG_SimpleSupplyChain_1a2f8_00001:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-08_18-12-17\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_reward_max: -104459.38468481964\n",
      "  episode_reward_mean: -149936.08009809867\n",
      "  episode_reward_min: -175007.97091783947\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1180\n",
      "  experiment_id: d723ce1e54d6484893e40b17874a26d0\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 29500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: -7177.841796875\n",
      "        mean_q: -70857.296875\n",
      "        min_q: -324445.65625\n",
      "        model: {}\n",
      "    num_steps_sampled: 29500\n",
      "    num_steps_trained: 1792256\n",
      "    num_target_updates: 7001\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 38.35333333333334\n",
      "    ram_util_percent: 68.37999999999998\n",
      "  pid: 8114\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12050246227452018\n",
      "    mean_env_wait_ms: 0.29872340353349564\n",
      "    mean_inference_ms: 0.8776046939396145\n",
      "    mean_raw_obs_processing_ms: 0.26502012707411743\n",
      "  time_since_restore: 298.5028033256531\n",
      "  time_this_iter_s: 10.553681373596191\n",
      "  time_total_s: 298.5028033256531\n",
      "  timers:\n",
      "    learn_throughput: 41975.177\n",
      "    learn_time_ms: 6.099\n",
      "    update_time_ms: 3.839\n",
      "  timestamp: 1620515537\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 29500\n",
      "  training_iteration: 29\n",
      "  trial_id: 1a2f8_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 10/12 CPUs, 0/0 GPUs, 0.0/4.79 GiB heap, 0.0/1.61 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_SimpleSupplyChain_1a2f8_00000</td><td>RUNNING </td><td>192.168.0.23:8118</td><td style=\"text-align: right;\">    28</td><td style=\"text-align: right;\">         288.048</td><td style=\"text-align: right;\">28500</td><td style=\"text-align: right;\"> -85448.9</td><td style=\"text-align: right;\">            -44031.2</td><td style=\"text-align: right;\">             -130790</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "<tr><td>DDPG_SimpleSupplyChain_1a2f8_00001</td><td>RUNNING </td><td>192.168.0.23:8114</td><td style=\"text-align: right;\">    29</td><td style=\"text-align: right;\">         298.503</td><td style=\"text-align: right;\">29500</td><td style=\"text-align: right;\">-149936  </td><td style=\"text-align: right;\">           -104459  </td><td style=\"text-align: right;\">             -175008</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_SimpleSupplyChain_1a2f8_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-08_18-12-17\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_reward_max: -44031.21173143749\n",
      "  episode_reward_mean: -85804.15666188368\n",
      "  episode_reward_min: -127599.06456317371\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1180\n",
      "  experiment_id: 925d870c9bcc486a8beaa0dfc6db064c\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 29500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: 16661.013671875\n",
      "        mean_q: -67678.0859375\n",
      "        min_q: -346071.125\n",
      "        model: {}\n",
      "    num_steps_sampled: 29500\n",
      "    num_steps_trained: 1792256\n",
      "    num_target_updates: 7001\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.15333333333333\n",
      "    ram_util_percent: 68.37999999999998\n",
      "  pid: 8118\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11919828176499767\n",
      "    mean_env_wait_ms: 0.2950328094455734\n",
      "    mean_inference_ms: 0.8657849973158804\n",
      "    mean_raw_obs_processing_ms: 0.2619910859500373\n",
      "  time_since_restore: 298.6442470550537\n",
      "  time_this_iter_s: 10.596421241760254\n",
      "  time_total_s: 298.6442470550537\n",
      "  timers:\n",
      "    learn_throughput: 43612.23\n",
      "    learn_time_ms: 5.87\n",
      "    update_time_ms: 2.911\n",
      "  timestamp: 1620515537\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 29500\n",
      "  training_iteration: 29\n",
      "  trial_id: 1a2f8_00000\n",
      "  \n",
      "Result for DDPG_SimpleSupplyChain_1a2f8_00001:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-08_18-12-28\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_reward_max: -109186.95941849417\n",
      "  episode_reward_mean: -149178.80502684094\n",
      "  episode_reward_min: -179742.8105296827\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1220\n",
      "  experiment_id: d723ce1e54d6484893e40b17874a26d0\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 30500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: -2254.2314453125\n",
      "        mean_q: -68159.46875\n",
      "        min_q: -301413.1875\n",
      "        model: {}\n",
      "    num_steps_sampled: 30500\n",
      "    num_steps_trained: 1856256\n",
      "    num_target_updates: 7251\n",
      "  iterations_since_restore: 30\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.27333333333334\n",
      "    ram_util_percent: 68.74\n",
      "  pid: 8114\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12018552308883988\n",
      "    mean_env_wait_ms: 0.2979211549843414\n",
      "    mean_inference_ms: 0.8752135000166031\n",
      "    mean_raw_obs_processing_ms: 0.2642029969749351\n",
      "  time_since_restore: 309.2162711620331\n",
      "  time_this_iter_s: 10.713467836380005\n",
      "  time_total_s: 309.2162711620331\n",
      "  timers:\n",
      "    learn_throughput: 49621.365\n",
      "    learn_time_ms: 5.159\n",
      "    update_time_ms: 2.625\n",
      "  timestamp: 1620515548\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 30500\n",
      "  training_iteration: 30\n",
      "  trial_id: 1a2f8_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 10/12 CPUs, 0/0 GPUs, 0.0/4.79 GiB heap, 0.0/1.61 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_SimpleSupplyChain_1a2f8_00000</td><td>RUNNING </td><td>192.168.0.23:8118</td><td style=\"text-align: right;\">    29</td><td style=\"text-align: right;\">         298.644</td><td style=\"text-align: right;\">29500</td><td style=\"text-align: right;\"> -85804.2</td><td style=\"text-align: right;\">            -44031.2</td><td style=\"text-align: right;\">             -127599</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "<tr><td>DDPG_SimpleSupplyChain_1a2f8_00001</td><td>RUNNING </td><td>192.168.0.23:8114</td><td style=\"text-align: right;\">    30</td><td style=\"text-align: right;\">         309.216</td><td style=\"text-align: right;\">30500</td><td style=\"text-align: right;\">-149179  </td><td style=\"text-align: right;\">           -109187  </td><td style=\"text-align: right;\">             -179743</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_SimpleSupplyChain_1a2f8_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-08_18-12-28\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_reward_max: -44031.21173143749\n",
      "  episode_reward_mean: -95601.96560988543\n",
      "  episode_reward_min: -129353.76756379372\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1220\n",
      "  experiment_id: 925d870c9bcc486a8beaa0dfc6db064c\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 30500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: 268.6824951171875\n",
      "        mean_q: -61342.8984375\n",
      "        min_q: -339545.34375\n",
      "        model: {}\n",
      "    num_steps_sampled: 30500\n",
      "    num_steps_trained: 1856256\n",
      "    num_target_updates: 7251\n",
      "  iterations_since_restore: 30\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.21333333333333\n",
      "    ram_util_percent: 68.75333333333333\n",
      "  pid: 8118\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11917591823986026\n",
      "    mean_env_wait_ms: 0.2950833069434875\n",
      "    mean_inference_ms: 0.8656390010455997\n",
      "    mean_raw_obs_processing_ms: 0.2620992204655278\n",
      "  time_since_restore: 309.3729648590088\n",
      "  time_this_iter_s: 10.728717803955078\n",
      "  time_total_s: 309.3729648590088\n",
      "  timers:\n",
      "    learn_throughput: 49845.036\n",
      "    learn_time_ms: 5.136\n",
      "    update_time_ms: 2.573\n",
      "  timestamp: 1620515548\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 30500\n",
      "  training_iteration: 30\n",
      "  trial_id: 1a2f8_00000\n",
      "  \n",
      "Result for DDPG_SimpleSupplyChain_1a2f8_00001:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-08_18-12-39\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_reward_max: -109186.95941849417\n",
      "  episode_reward_mean: -150672.2467722462\n",
      "  episode_reward_min: -179742.8105296827\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1260\n",
      "  experiment_id: d723ce1e54d6484893e40b17874a26d0\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 31500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: -1544.9168701171875\n",
      "        mean_q: -70681.0390625\n",
      "        min_q: -323731.4375\n",
      "        model: {}\n",
      "    num_steps_sampled: 31500\n",
      "    num_steps_trained: 1920256\n",
      "    num_target_updates: 7501\n",
      "  iterations_since_restore: 31\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.30666666666666\n",
      "    ram_util_percent: 68.98666666666668\n",
      "  pid: 8114\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11992764511569906\n",
      "    mean_env_wait_ms: 0.2973313923679975\n",
      "    mean_inference_ms: 0.87333270679502\n",
      "    mean_raw_obs_processing_ms: 0.2635499145863472\n",
      "  time_since_restore: 320.1837589740753\n",
      "  time_this_iter_s: 10.967487812042236\n",
      "  time_total_s: 320.1837589740753\n",
      "  timers:\n",
      "    learn_throughput: 50024.777\n",
      "    learn_time_ms: 5.117\n",
      "    update_time_ms: 3.968\n",
      "  timestamp: 1620515559\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 31500\n",
      "  training_iteration: 31\n",
      "  trial_id: 1a2f8_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 10/12 CPUs, 0/0 GPUs, 0.0/4.79 GiB heap, 0.0/1.61 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_SimpleSupplyChain_1a2f8_00000</td><td>RUNNING </td><td>192.168.0.23:8118</td><td style=\"text-align: right;\">    30</td><td style=\"text-align: right;\">         309.373</td><td style=\"text-align: right;\">30500</td><td style=\"text-align: right;\">  -95602</td><td style=\"text-align: right;\">            -44031.2</td><td style=\"text-align: right;\">             -129354</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "<tr><td>DDPG_SimpleSupplyChain_1a2f8_00001</td><td>RUNNING </td><td>192.168.0.23:8114</td><td style=\"text-align: right;\">    31</td><td style=\"text-align: right;\">         320.184</td><td style=\"text-align: right;\">31500</td><td style=\"text-align: right;\"> -150672</td><td style=\"text-align: right;\">           -109187  </td><td style=\"text-align: right;\">             -179743</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_SimpleSupplyChain_1a2f8_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-08_18-12-39\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_reward_max: -63438.47346276407\n",
      "  episode_reward_mean: -103003.87542076813\n",
      "  episode_reward_min: -129353.76756379372\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1260\n",
      "  experiment_id: 925d870c9bcc486a8beaa0dfc6db064c\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 31500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: 1979.68310546875\n",
      "        mean_q: -61834.4296875\n",
      "        min_q: -378818.15625\n",
      "        model: {}\n",
      "    num_steps_sampled: 31500\n",
      "    num_steps_trained: 1920256\n",
      "    num_target_updates: 7501\n",
      "  iterations_since_restore: 31\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.10666666666667\n",
      "    ram_util_percent: 68.98\n",
      "  pid: 8118\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11915220274443289\n",
      "    mean_env_wait_ms: 0.29511741812649567\n",
      "    mean_inference_ms: 0.8653901525294385\n",
      "    mean_raw_obs_processing_ms: 0.262225679758338\n",
      "  time_since_restore: 320.35101079940796\n",
      "  time_this_iter_s: 10.97804594039917\n",
      "  time_total_s: 320.35101079940796\n",
      "  timers:\n",
      "    learn_throughput: 50048.794\n",
      "    learn_time_ms: 5.115\n",
      "    update_time_ms: 3.003\n",
      "  timestamp: 1620515559\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 31500\n",
      "  training_iteration: 31\n",
      "  trial_id: 1a2f8_00000\n",
      "  \n",
      "Result for DDPG_SimpleSupplyChain_1a2f8_00001:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-08_18-12-50\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_reward_max: -109186.95941849417\n",
      "  episode_reward_mean: -148118.04425549883\n",
      "  episode_reward_min: -180479.28705383456\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1300\n",
      "  experiment_id: d723ce1e54d6484893e40b17874a26d0\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 32500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: 4599.19580078125\n",
      "        mean_q: -78987.984375\n",
      "        min_q: -354883.34375\n",
      "        model: {}\n",
      "    num_steps_sampled: 32500\n",
      "    num_steps_trained: 1984256\n",
      "    num_target_updates: 7751\n",
      "  iterations_since_restore: 32\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.41875\n",
      "    ram_util_percent: 68.31875\n",
      "  pid: 8114\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11980405397319789\n",
      "    mean_env_wait_ms: 0.29710729378348316\n",
      "    mean_inference_ms: 0.8723760274890273\n",
      "    mean_raw_obs_processing_ms: 0.26324845959597243\n",
      "  time_since_restore: 331.4785439968109\n",
      "  time_this_iter_s: 11.294785022735596\n",
      "  time_total_s: 331.4785439968109\n",
      "  timers:\n",
      "    learn_throughput: 55318.147\n",
      "    learn_time_ms: 4.628\n",
      "    update_time_ms: 2.789\n",
      "  timestamp: 1620515570\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 32500\n",
      "  training_iteration: 32\n",
      "  trial_id: 1a2f8_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 10/12 CPUs, 0/0 GPUs, 0.0/4.79 GiB heap, 0.0/1.61 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_SimpleSupplyChain_1a2f8_00000</td><td>RUNNING </td><td>192.168.0.23:8118</td><td style=\"text-align: right;\">    31</td><td style=\"text-align: right;\">         320.351</td><td style=\"text-align: right;\">31500</td><td style=\"text-align: right;\"> -103004</td><td style=\"text-align: right;\">            -63438.5</td><td style=\"text-align: right;\">             -129354</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "<tr><td>DDPG_SimpleSupplyChain_1a2f8_00001</td><td>RUNNING </td><td>192.168.0.23:8114</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">         331.479</td><td style=\"text-align: right;\">32500</td><td style=\"text-align: right;\"> -148118</td><td style=\"text-align: right;\">           -109187  </td><td style=\"text-align: right;\">             -180479</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_SimpleSupplyChain_1a2f8_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-08_18-12-50\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_reward_max: -49550.78880565231\n",
      "  episode_reward_mean: -103042.98083750028\n",
      "  episode_reward_min: -129353.76756379372\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1300\n",
      "  experiment_id: 925d870c9bcc486a8beaa0dfc6db064c\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 32500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: -1878.2784423828125\n",
      "        mean_q: -62263.2890625\n",
      "        min_q: -344217.53125\n",
      "        model: {}\n",
      "    num_steps_sampled: 32500\n",
      "    num_steps_trained: 1984256\n",
      "    num_target_updates: 7751\n",
      "  iterations_since_restore: 32\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 40.36875\n",
      "    ram_util_percent: 68.29374999999999\n",
      "  pid: 8118\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11913017086697053\n",
      "    mean_env_wait_ms: 0.29505713220660645\n",
      "    mean_inference_ms: 0.8652042440824715\n",
      "    mean_raw_obs_processing_ms: 0.2622314274266696\n",
      "  time_since_restore: 331.681138753891\n",
      "  time_this_iter_s: 11.330127954483032\n",
      "  time_total_s: 331.681138753891\n",
      "  timers:\n",
      "    learn_throughput: 47820.901\n",
      "    learn_time_ms: 5.353\n",
      "    update_time_ms: 2.592\n",
      "  timestamp: 1620515570\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 32500\n",
      "  training_iteration: 32\n",
      "  trial_id: 1a2f8_00000\n",
      "  \n",
      "Result for DDPG_SimpleSupplyChain_1a2f8_00001:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-08_18-13-01\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_reward_max: -108806.83570199451\n",
      "  episode_reward_mean: -148772.91982720903\n",
      "  episode_reward_min: -180479.28705383456\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1340\n",
      "  experiment_id: d723ce1e54d6484893e40b17874a26d0\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 33500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: -868.0689086914062\n",
      "        mean_q: -75416.484375\n",
      "        min_q: -339646.375\n",
      "        model: {}\n",
      "    num_steps_sampled: 33500\n",
      "    num_steps_trained: 2048256\n",
      "    num_target_updates: 8001\n",
      "  iterations_since_restore: 33\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.55625\n",
      "    ram_util_percent: 68.49374999999999\n",
      "  pid: 8114\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1198228706576155\n",
      "    mean_env_wait_ms: 0.2971629556020603\n",
      "    mean_inference_ms: 0.872307691874327\n",
      "    mean_raw_obs_processing_ms: 0.26322577011241227\n",
      "  time_since_restore: 342.550861120224\n",
      "  time_this_iter_s: 11.072317123413086\n",
      "  time_total_s: 342.550861120224\n",
      "  timers:\n",
      "    learn_throughput: 52790.445\n",
      "    learn_time_ms: 4.849\n",
      "    update_time_ms: 2.684\n",
      "  timestamp: 1620515581\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 33500\n",
      "  training_iteration: 33\n",
      "  trial_id: 1a2f8_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 10/12 CPUs, 0/0 GPUs, 0.0/4.79 GiB heap, 0.0/1.61 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_SimpleSupplyChain_1a2f8_00000</td><td>RUNNING </td><td>192.168.0.23:8118</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">         331.681</td><td style=\"text-align: right;\">32500</td><td style=\"text-align: right;\"> -103043</td><td style=\"text-align: right;\">            -49550.8</td><td style=\"text-align: right;\">             -129354</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "<tr><td>DDPG_SimpleSupplyChain_1a2f8_00001</td><td>RUNNING </td><td>192.168.0.23:8114</td><td style=\"text-align: right;\">    33</td><td style=\"text-align: right;\">         342.551</td><td style=\"text-align: right;\">33500</td><td style=\"text-align: right;\"> -148773</td><td style=\"text-align: right;\">           -108807  </td><td style=\"text-align: right;\">             -180479</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_SimpleSupplyChain_1a2f8_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-08_18-13-01\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_reward_max: -49550.78880565231\n",
      "  episode_reward_mean: -107826.14394171545\n",
      "  episode_reward_min: -178606.0706149525\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1340\n",
      "  experiment_id: 925d870c9bcc486a8beaa0dfc6db064c\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 33500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: 26431.96875\n",
      "        mean_q: -60312.6484375\n",
      "        min_q: -380381.53125\n",
      "        model: {}\n",
      "    num_steps_sampled: 33500\n",
      "    num_steps_trained: 2048256\n",
      "    num_target_updates: 8001\n",
      "  iterations_since_restore: 33\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.70625\n",
      "    ram_util_percent: 68.54374999999999\n",
      "  pid: 8118\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11911429759573804\n",
      "    mean_env_wait_ms: 0.29500805910647615\n",
      "    mean_inference_ms: 0.8650953977809493\n",
      "    mean_raw_obs_processing_ms: 0.26222177469378005\n",
      "  time_since_restore: 342.7647726535797\n",
      "  time_this_iter_s: 11.08363389968872\n",
      "  time_total_s: 342.7647726535797\n",
      "  timers:\n",
      "    learn_throughput: 46174.699\n",
      "    learn_time_ms: 5.544\n",
      "    update_time_ms: 2.898\n",
      "  timestamp: 1620515581\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 33500\n",
      "  training_iteration: 33\n",
      "  trial_id: 1a2f8_00000\n",
      "  \n",
      "Result for DDPG_SimpleSupplyChain_1a2f8_00001:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-08_18-13-13\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_reward_max: -98518.89414604372\n",
      "  episode_reward_mean: -146501.66142594226\n",
      "  episode_reward_min: -180479.28705383456\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1380\n",
      "  experiment_id: d723ce1e54d6484893e40b17874a26d0\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 34500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: 3799.89111328125\n",
      "        mean_q: -81106.921875\n",
      "        min_q: -352954.4375\n",
      "        model: {}\n",
      "    num_steps_sampled: 34500\n",
      "    num_steps_trained: 2112256\n",
      "    num_target_updates: 8251\n",
      "  iterations_since_restore: 34\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 45.5625\n",
      "    ram_util_percent: 68.25\n",
      "  pid: 8114\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11991419737851611\n",
      "    mean_env_wait_ms: 0.2973155566797896\n",
      "    mean_inference_ms: 0.8727141156825821\n",
      "    mean_raw_obs_processing_ms: 0.26331924368458504\n",
      "  time_since_restore: 354.0473732948303\n",
      "  time_this_iter_s: 11.496512174606323\n",
      "  time_total_s: 354.0473732948303\n",
      "  timers:\n",
      "    learn_throughput: 41491.015\n",
      "    learn_time_ms: 6.17\n",
      "    update_time_ms: 3.1\n",
      "  timestamp: 1620515593\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 34500\n",
      "  training_iteration: 34\n",
      "  trial_id: 1a2f8_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 10/12 CPUs, 0/0 GPUs, 0.0/4.79 GiB heap, 0.0/1.61 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_SimpleSupplyChain_1a2f8_00000</td><td>RUNNING </td><td>192.168.0.23:8118</td><td style=\"text-align: right;\">    33</td><td style=\"text-align: right;\">         342.765</td><td style=\"text-align: right;\">33500</td><td style=\"text-align: right;\"> -107826</td><td style=\"text-align: right;\">            -49550.8</td><td style=\"text-align: right;\">             -178606</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "<tr><td>DDPG_SimpleSupplyChain_1a2f8_00001</td><td>RUNNING </td><td>192.168.0.23:8114</td><td style=\"text-align: right;\">    34</td><td style=\"text-align: right;\">         354.047</td><td style=\"text-align: right;\">34500</td><td style=\"text-align: right;\"> -146502</td><td style=\"text-align: right;\">            -98518.9</td><td style=\"text-align: right;\">             -180479</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_SimpleSupplyChain_1a2f8_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-08_18-13-13\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_reward_max: -62557.51227517788\n",
      "  episode_reward_mean: -109254.02181015411\n",
      "  episode_reward_min: -178606.0706149525\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1380\n",
      "  experiment_id: 925d870c9bcc486a8beaa0dfc6db064c\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 34500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: -1019.148681640625\n",
      "        mean_q: -62002.15234375\n",
      "        min_q: -374664.5625\n",
      "        model: {}\n",
      "    num_steps_sampled: 34500\n",
      "    num_steps_trained: 2112256\n",
      "    num_target_updates: 8251\n",
      "  iterations_since_restore: 34\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.575\n",
      "    ram_util_percent: 68.25\n",
      "  pid: 8118\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11916731847663063\n",
      "    mean_env_wait_ms: 0.2950683873226183\n",
      "    mean_inference_ms: 0.8654766963486162\n",
      "    mean_raw_obs_processing_ms: 0.2622524705366704\n",
      "  time_since_restore: 354.305029630661\n",
      "  time_this_iter_s: 11.540256977081299\n",
      "  time_total_s: 354.305029630661\n",
      "  timers:\n",
      "    learn_throughput: 43477.314\n",
      "    learn_time_ms: 5.888\n",
      "    update_time_ms: 3.436\n",
      "  timestamp: 1620515593\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 34500\n",
      "  training_iteration: 34\n",
      "  trial_id: 1a2f8_00000\n",
      "  \n",
      "Result for DDPG_SimpleSupplyChain_1a2f8_00001:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-08_18-13-24\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_reward_max: -98518.89414604372\n",
      "  episode_reward_mean: -146370.41887587946\n",
      "  episode_reward_min: -178152.8078064657\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1420\n",
      "  experiment_id: d723ce1e54d6484893e40b17874a26d0\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 35500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: -4778.22021484375\n",
      "        mean_q: -72595.7109375\n",
      "        min_q: -285597.84375\n",
      "        model: {}\n",
      "    num_steps_sampled: 35500\n",
      "    num_steps_trained: 2176256\n",
      "    num_target_updates: 8501\n",
      "  iterations_since_restore: 35\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.087500000000006\n",
      "    ram_util_percent: 67.7875\n",
      "  pid: 8114\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12000435830487262\n",
      "    mean_env_wait_ms: 0.2974735262861167\n",
      "    mean_inference_ms: 0.8732266775983202\n",
      "    mean_raw_obs_processing_ms: 0.2634156831020427\n",
      "  time_since_restore: 365.45680117607117\n",
      "  time_this_iter_s: 11.409427881240845\n",
      "  time_total_s: 365.45680117607117\n",
      "  timers:\n",
      "    learn_throughput: 50650.349\n",
      "    learn_time_ms: 5.054\n",
      "    update_time_ms: 2.618\n",
      "  timestamp: 1620515604\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 35500\n",
      "  training_iteration: 35\n",
      "  trial_id: 1a2f8_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 10/12 CPUs, 0/0 GPUs, 0.0/4.79 GiB heap, 0.0/1.61 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_SimpleSupplyChain_1a2f8_00000</td><td>RUNNING </td><td>192.168.0.23:8118</td><td style=\"text-align: right;\">    34</td><td style=\"text-align: right;\">         354.305</td><td style=\"text-align: right;\">34500</td><td style=\"text-align: right;\"> -109254</td><td style=\"text-align: right;\">            -62557.5</td><td style=\"text-align: right;\">             -178606</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "<tr><td>DDPG_SimpleSupplyChain_1a2f8_00001</td><td>RUNNING </td><td>192.168.0.23:8114</td><td style=\"text-align: right;\">    35</td><td style=\"text-align: right;\">         365.457</td><td style=\"text-align: right;\">35500</td><td style=\"text-align: right;\"> -146370</td><td style=\"text-align: right;\">            -98518.9</td><td style=\"text-align: right;\">             -178153</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_SimpleSupplyChain_1a2f8_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-08_18-13-24\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_reward_max: -40538.57414710139\n",
      "  episode_reward_mean: -114438.32969665033\n",
      "  episode_reward_min: -178606.0706149525\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1420\n",
      "  experiment_id: 925d870c9bcc486a8beaa0dfc6db064c\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 35500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: 47764.6484375\n",
      "        mean_q: -68925.625\n",
      "        min_q: -369917.375\n",
      "        model: {}\n",
      "    num_steps_sampled: 35500\n",
      "    num_steps_trained: 2176256\n",
      "    num_target_updates: 8501\n",
      "  iterations_since_restore: 35\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.973333333333336\n",
      "    ram_util_percent: 67.79333333333332\n",
      "  pid: 8118\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11926041308473544\n",
      "    mean_env_wait_ms: 0.29518884491686603\n",
      "    mean_inference_ms: 0.8660760291598224\n",
      "    mean_raw_obs_processing_ms: 0.26234292187457947\n",
      "  time_since_restore: 365.6681146621704\n",
      "  time_this_iter_s: 11.3630850315094\n",
      "  time_total_s: 365.6681146621704\n",
      "  timers:\n",
      "    learn_throughput: 51465.826\n",
      "    learn_time_ms: 4.974\n",
      "    update_time_ms: 2.502\n",
      "  timestamp: 1620515604\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 35500\n",
      "  training_iteration: 35\n",
      "  trial_id: 1a2f8_00000\n",
      "  \n",
      "Result for DDPG_SimpleSupplyChain_1a2f8_00001:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-08_18-13-35\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_reward_max: -99297.11360653854\n",
      "  episode_reward_mean: -147241.45496699688\n",
      "  episode_reward_min: -178152.8078064657\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1460\n",
      "  experiment_id: d723ce1e54d6484893e40b17874a26d0\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 36500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: -580.4089965820312\n",
      "        mean_q: -76557.9453125\n",
      "        min_q: -335081.96875\n",
      "        model: {}\n",
      "    num_steps_sampled: 36500\n",
      "    num_steps_trained: 2240256\n",
      "    num_target_updates: 8751\n",
      "  iterations_since_restore: 36\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.12\n",
      "    ram_util_percent: 67.61333333333334\n",
      "  pid: 8114\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12001461042306204\n",
      "    mean_env_wait_ms: 0.2974985559750741\n",
      "    mean_inference_ms: 0.8733309938706805\n",
      "    mean_raw_obs_processing_ms: 0.2634519821976732\n",
      "  time_since_restore: 376.40595841407776\n",
      "  time_this_iter_s: 10.949157238006592\n",
      "  time_total_s: 376.40595841407776\n",
      "  timers:\n",
      "    learn_throughput: 49081.526\n",
      "    learn_time_ms: 5.216\n",
      "    update_time_ms: 2.66\n",
      "  timestamp: 1620515615\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 36500\n",
      "  training_iteration: 36\n",
      "  trial_id: 1a2f8_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 10/12 CPUs, 0/0 GPUs, 0.0/4.79 GiB heap, 0.0/1.61 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_SimpleSupplyChain_1a2f8_00000</td><td>RUNNING </td><td>192.168.0.23:8118</td><td style=\"text-align: right;\">    35</td><td style=\"text-align: right;\">         365.668</td><td style=\"text-align: right;\">35500</td><td style=\"text-align: right;\"> -114438</td><td style=\"text-align: right;\">            -40538.6</td><td style=\"text-align: right;\">             -178606</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "<tr><td>DDPG_SimpleSupplyChain_1a2f8_00001</td><td>RUNNING </td><td>192.168.0.23:8114</td><td style=\"text-align: right;\">    36</td><td style=\"text-align: right;\">         376.406</td><td style=\"text-align: right;\">36500</td><td style=\"text-align: right;\"> -147241</td><td style=\"text-align: right;\">            -99297.1</td><td style=\"text-align: right;\">             -178153</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_SimpleSupplyChain_1a2f8_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-08_18-13-35\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_reward_max: -40538.57414710139\n",
      "  episode_reward_mean: -104375.88332331735\n",
      "  episode_reward_min: -168947.0041477344\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1460\n",
      "  experiment_id: 925d870c9bcc486a8beaa0dfc6db064c\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 36500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: -9502.9736328125\n",
      "        mean_q: -59812.7265625\n",
      "        min_q: -375861.5\n",
      "        model: {}\n",
      "    num_steps_sampled: 36500\n",
      "    num_steps_trained: 2240256\n",
      "    num_target_updates: 8751\n",
      "  iterations_since_restore: 36\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.16875\n",
      "    ram_util_percent: 67.63125\n",
      "  pid: 8118\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11927362608984096\n",
      "    mean_env_wait_ms: 0.29518715380823096\n",
      "    mean_inference_ms: 0.8662167512262247\n",
      "    mean_raw_obs_processing_ms: 0.2623374976259112\n",
      "  time_since_restore: 376.68878960609436\n",
      "  time_this_iter_s: 11.02067494392395\n",
      "  time_total_s: 376.68878960609436\n",
      "  timers:\n",
      "    learn_throughput: 41115.274\n",
      "    learn_time_ms: 6.226\n",
      "    update_time_ms: 3.281\n",
      "  timestamp: 1620515615\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 36500\n",
      "  training_iteration: 36\n",
      "  trial_id: 1a2f8_00000\n",
      "  \n",
      "Result for DDPG_SimpleSupplyChain_1a2f8_00001:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-08_18-13-46\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_reward_max: -99297.11360653854\n",
      "  episode_reward_mean: -148694.91032897762\n",
      "  episode_reward_min: -174829.6044290102\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1500\n",
      "  experiment_id: d723ce1e54d6484893e40b17874a26d0\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 37500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: -3269.8720703125\n",
      "        mean_q: -80428.8515625\n",
      "        min_q: -364940.875\n",
      "        model: {}\n",
      "    num_steps_sampled: 37500\n",
      "    num_steps_trained: 2304256\n",
      "    num_target_updates: 9001\n",
      "  iterations_since_restore: 37\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.593333333333334\n",
      "    ram_util_percent: 67.65333333333335\n",
      "  pid: 8114\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11994210493629728\n",
      "    mean_env_wait_ms: 0.29736956296920347\n",
      "    mean_inference_ms: 0.8729596791002954\n",
      "    mean_raw_obs_processing_ms: 0.2633673908017227\n",
      "  time_since_restore: 387.28310441970825\n",
      "  time_this_iter_s: 10.877146005630493\n",
      "  time_total_s: 387.28310441970825\n",
      "  timers:\n",
      "    learn_throughput: 39574.302\n",
      "    learn_time_ms: 6.469\n",
      "    update_time_ms: 3.086\n",
      "  timestamp: 1620515626\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 37500\n",
      "  training_iteration: 37\n",
      "  trial_id: 1a2f8_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 10/12 CPUs, 0/0 GPUs, 0.0/4.79 GiB heap, 0.0/1.61 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_SimpleSupplyChain_1a2f8_00000</td><td>RUNNING </td><td>192.168.0.23:8118</td><td style=\"text-align: right;\">    36</td><td style=\"text-align: right;\">         376.689</td><td style=\"text-align: right;\">36500</td><td style=\"text-align: right;\"> -104376</td><td style=\"text-align: right;\">            -40538.6</td><td style=\"text-align: right;\">             -168947</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "<tr><td>DDPG_SimpleSupplyChain_1a2f8_00001</td><td>RUNNING </td><td>192.168.0.23:8114</td><td style=\"text-align: right;\">    37</td><td style=\"text-align: right;\">         387.283</td><td style=\"text-align: right;\">37500</td><td style=\"text-align: right;\"> -148695</td><td style=\"text-align: right;\">            -99297.1</td><td style=\"text-align: right;\">             -174830</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_SimpleSupplyChain_1a2f8_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-08_18-13-46\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_reward_max: -40538.57414710139\n",
      "  episode_reward_mean: -95724.52754360504\n",
      "  episode_reward_min: -162914.74015741414\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1500\n",
      "  experiment_id: 925d870c9bcc486a8beaa0dfc6db064c\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 37500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: -12740.845703125\n",
      "        mean_q: -62570.0234375\n",
      "        min_q: -356354.34375\n",
      "        model: {}\n",
      "    num_steps_sampled: 37500\n",
      "    num_steps_trained: 2304256\n",
      "    num_target_updates: 9001\n",
      "  iterations_since_restore: 37\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.42666666666667\n",
      "    ram_util_percent: 67.64000000000001\n",
      "  pid: 8118\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11916031095382941\n",
      "    mean_env_wait_ms: 0.2949278022611718\n",
      "    mean_inference_ms: 0.865573721054682\n",
      "    mean_raw_obs_processing_ms: 0.26208747858819165\n",
      "  time_since_restore: 387.5956015586853\n",
      "  time_this_iter_s: 10.906811952590942\n",
      "  time_total_s: 387.5956015586853\n",
      "  timers:\n",
      "    learn_throughput: 45154.476\n",
      "    learn_time_ms: 5.669\n",
      "    update_time_ms: 3.367\n",
      "  timestamp: 1620515626\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 37500\n",
      "  training_iteration: 37\n",
      "  trial_id: 1a2f8_00000\n",
      "  \n",
      "Result for DDPG_SimpleSupplyChain_1a2f8_00001:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-08_18-13-57\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_reward_max: -99297.11360653854\n",
      "  episode_reward_mean: -147100.29767022084\n",
      "  episode_reward_min: -171857.5895277447\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1540\n",
      "  experiment_id: d723ce1e54d6484893e40b17874a26d0\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 38500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: 276.076171875\n",
      "        mean_q: -79319.328125\n",
      "        min_q: -384029.6875\n",
      "        model: {}\n",
      "    num_steps_sampled: 38500\n",
      "    num_steps_trained: 2368256\n",
      "    num_target_updates: 9251\n",
      "  iterations_since_restore: 38\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.175\n",
      "    ram_util_percent: 67.5375\n",
      "  pid: 8114\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11987189214564252\n",
      "    mean_env_wait_ms: 0.2972637020094926\n",
      "    mean_inference_ms: 0.8725323930632853\n",
      "    mean_raw_obs_processing_ms: 0.263261354767038\n",
      "  time_since_restore: 398.4924385547638\n",
      "  time_this_iter_s: 11.209334135055542\n",
      "  time_total_s: 398.4924385547638\n",
      "  timers:\n",
      "    learn_throughput: 52342.925\n",
      "    learn_time_ms: 4.891\n",
      "    update_time_ms: 2.77\n",
      "  timestamp: 1620515637\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 38500\n",
      "  training_iteration: 38\n",
      "  trial_id: 1a2f8_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 10/12 CPUs, 0/0 GPUs, 0.0/4.79 GiB heap, 0.0/1.61 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_SimpleSupplyChain_1a2f8_00000</td><td>RUNNING </td><td>192.168.0.23:8118</td><td style=\"text-align: right;\">    37</td><td style=\"text-align: right;\">         387.596</td><td style=\"text-align: right;\">37500</td><td style=\"text-align: right;\"> -95724.5</td><td style=\"text-align: right;\">            -40538.6</td><td style=\"text-align: right;\">             -162915</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "<tr><td>DDPG_SimpleSupplyChain_1a2f8_00001</td><td>RUNNING </td><td>192.168.0.23:8114</td><td style=\"text-align: right;\">    38</td><td style=\"text-align: right;\">         398.492</td><td style=\"text-align: right;\">38500</td><td style=\"text-align: right;\">-147100  </td><td style=\"text-align: right;\">            -99297.1</td><td style=\"text-align: right;\">             -171858</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_SimpleSupplyChain_1a2f8_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-08_18-13-57\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_reward_max: -48758.612681988525\n",
      "  episode_reward_mean: -90629.43269274797\n",
      "  episode_reward_min: -147947.38315212345\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1540\n",
      "  experiment_id: 925d870c9bcc486a8beaa0dfc6db064c\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 38500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: 34253.8984375\n",
      "        mean_q: -49564.5859375\n",
      "        min_q: -289501.46875\n",
      "        model: {}\n",
      "    num_steps_sampled: 38500\n",
      "    num_steps_trained: 2368256\n",
      "    num_target_updates: 9251\n",
      "  iterations_since_restore: 38\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.65333333333333\n",
      "    ram_util_percent: 67.53333333333333\n",
      "  pid: 8118\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1190100822223593\n",
      "    mean_env_wait_ms: 0.29460419838954116\n",
      "    mean_inference_ms: 0.8647177618381372\n",
      "    mean_raw_obs_processing_ms: 0.2617603897505887\n",
      "  time_since_restore: 398.78631949424744\n",
      "  time_this_iter_s: 11.190717935562134\n",
      "  time_total_s: 398.78631949424744\n",
      "  timers:\n",
      "    learn_throughput: 51980.066\n",
      "    learn_time_ms: 4.925\n",
      "    update_time_ms: 2.491\n",
      "  timestamp: 1620515637\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 38500\n",
      "  training_iteration: 38\n",
      "  trial_id: 1a2f8_00000\n",
      "  \n",
      "Result for DDPG_SimpleSupplyChain_1a2f8_00001:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-08_18-14-08\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_reward_max: -108103.16629524893\n",
      "  episode_reward_mean: -148139.9999497675\n",
      "  episode_reward_min: -171857.5895277447\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1580\n",
      "  experiment_id: d723ce1e54d6484893e40b17874a26d0\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 39500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: 4373.98681640625\n",
      "        mean_q: -89158.796875\n",
      "        min_q: -336062.3125\n",
      "        model: {}\n",
      "    num_steps_sampled: 39500\n",
      "    num_steps_trained: 2432256\n",
      "    num_target_updates: 9501\n",
      "  iterations_since_restore: 39\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.10666666666666\n",
      "    ram_util_percent: 67.75333333333332\n",
      "  pid: 8114\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11986025551131782\n",
      "    mean_env_wait_ms: 0.2972744574001304\n",
      "    mean_inference_ms: 0.872276538486893\n",
      "    mean_raw_obs_processing_ms: 0.2632563815363868\n",
      "  time_since_restore: 409.7140085697174\n",
      "  time_this_iter_s: 11.221570014953613\n",
      "  time_total_s: 409.7140085697174\n",
      "  timers:\n",
      "    learn_throughput: 34002.737\n",
      "    learn_time_ms: 7.529\n",
      "    update_time_ms: 4.057\n",
      "  timestamp: 1620515648\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 39500\n",
      "  training_iteration: 39\n",
      "  trial_id: 1a2f8_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 10/12 CPUs, 0/0 GPUs, 0.0/4.79 GiB heap, 0.0/1.61 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status  </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_SimpleSupplyChain_1a2f8_00000</td><td>RUNNING </td><td>192.168.0.23:8118</td><td style=\"text-align: right;\">    38</td><td style=\"text-align: right;\">         398.786</td><td style=\"text-align: right;\">38500</td><td style=\"text-align: right;\"> -90629.4</td><td style=\"text-align: right;\">            -48758.6</td><td style=\"text-align: right;\">             -147947</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "<tr><td>DDPG_SimpleSupplyChain_1a2f8_00001</td><td>RUNNING </td><td>192.168.0.23:8114</td><td style=\"text-align: right;\">    39</td><td style=\"text-align: right;\">         409.714</td><td style=\"text-align: right;\">39500</td><td style=\"text-align: right;\">-148140  </td><td style=\"text-align: right;\">           -108103  </td><td style=\"text-align: right;\">             -171858</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_SimpleSupplyChain_1a2f8_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-08_18-14-09\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_reward_max: -40889.998754370856\n",
      "  episode_reward_mean: -90245.59254976214\n",
      "  episode_reward_min: -147947.38315212345\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1580\n",
      "  experiment_id: 925d870c9bcc486a8beaa0dfc6db064c\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 39500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: 12991.830078125\n",
      "        mean_q: -59121.3828125\n",
      "        min_q: -364204.09375\n",
      "        model: {}\n",
      "    num_steps_sampled: 39500\n",
      "    num_steps_trained: 2432256\n",
      "    num_target_updates: 9501\n",
      "  iterations_since_restore: 39\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.2625\n",
      "    ram_util_percent: 67.775\n",
      "  pid: 8118\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11900772730494716\n",
      "    mean_env_wait_ms: 0.29461206152006303\n",
      "    mean_inference_ms: 0.864840076941126\n",
      "    mean_raw_obs_processing_ms: 0.2617257974604737\n",
      "  time_since_restore: 410.09938955307007\n",
      "  time_this_iter_s: 11.313070058822632\n",
      "  time_total_s: 410.09938955307007\n",
      "  timers:\n",
      "    learn_throughput: 41664.571\n",
      "    learn_time_ms: 6.144\n",
      "    update_time_ms: 2.897\n",
      "  timestamp: 1620515649\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 39500\n",
      "  training_iteration: 39\n",
      "  trial_id: 1a2f8_00000\n",
      "  \n",
      "Result for DDPG_SimpleSupplyChain_1a2f8_00001:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-08_18-14-20\n",
      "  done: true\n",
      "  episode_len_mean: 25.0\n",
      "  episode_reward_max: -108103.16629524893\n",
      "  episode_reward_mean: -147375.13436764263\n",
      "  episode_reward_min: -173501.87420206136\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1620\n",
      "  experiment_id: d723ce1e54d6484893e40b17874a26d0\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 40500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: -2636.769775390625\n",
      "        mean_q: -87200.84375\n",
      "        min_q: -358049.78125\n",
      "        model: {}\n",
      "    num_steps_sampled: 40500\n",
      "    num_steps_trained: 2496256\n",
      "    num_target_updates: 9751\n",
      "  iterations_since_restore: 40\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.8\n",
      "    ram_util_percent: 68.04374999999999\n",
      "  pid: 8114\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11986277192496197\n",
      "    mean_env_wait_ms: 0.2973188165647935\n",
      "    mean_inference_ms: 0.8722150285643909\n",
      "    mean_raw_obs_processing_ms: 0.26328159370207915\n",
      "  time_since_restore: 421.0794506072998\n",
      "  time_this_iter_s: 11.365442037582397\n",
      "  time_total_s: 421.0794506072998\n",
      "  timers:\n",
      "    learn_throughput: 47017.433\n",
      "    learn_time_ms: 5.445\n",
      "    update_time_ms: 2.831\n",
      "  timestamp: 1620515660\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 40500\n",
      "  training_iteration: 40\n",
      "  trial_id: 1a2f8_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 5/12 CPUs, 0/0 GPUs, 0.0/4.79 GiB heap, 0.0/1.61 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 2/2 (1 RUNNING, 1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status    </th><th>loc              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_SimpleSupplyChain_1a2f8_00000</td><td>RUNNING   </td><td>192.168.0.23:8118</td><td style=\"text-align: right;\">    39</td><td style=\"text-align: right;\">         410.099</td><td style=\"text-align: right;\">39500</td><td style=\"text-align: right;\"> -90245.6</td><td style=\"text-align: right;\">              -40890</td><td style=\"text-align: right;\">             -147947</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "<tr><td>DDPG_SimpleSupplyChain_1a2f8_00001</td><td>TERMINATED</td><td>                 </td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">         421.079</td><td style=\"text-align: right;\">40500</td><td style=\"text-align: right;\">-147375  </td><td style=\"text-align: right;\">             -108103</td><td style=\"text-align: right;\">             -173502</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_SimpleSupplyChain_1a2f8_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-05-08_18-14-20\n",
      "  done: true\n",
      "  episode_len_mean: 25.0\n",
      "  episode_reward_max: -31561.08626760905\n",
      "  episode_reward_mean: -87616.20442982737\n",
      "  episode_reward_min: -133028.22900434202\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 1620\n",
      "  experiment_id: 925d870c9bcc486a8beaa0dfc6db064c\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 40500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: 10398.5615234375\n",
      "        mean_q: -58210.53125\n",
      "        min_q: -360204.0625\n",
      "        model: {}\n",
      "    num_steps_sampled: 40500\n",
      "    num_steps_trained: 2496256\n",
      "    num_target_updates: 9751\n",
      "  iterations_since_restore: 40\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 4\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.3125\n",
      "    ram_util_percent: 68.04374999999999\n",
      "  pid: 8118\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11914746119320373\n",
      "    mean_env_wait_ms: 0.29502211304886056\n",
      "    mean_inference_ms: 0.8662066295842348\n",
      "    mean_raw_obs_processing_ms: 0.26207257584142957\n",
      "  time_since_restore: 421.4724485874176\n",
      "  time_this_iter_s: 11.373059034347534\n",
      "  time_total_s: 421.4724485874176\n",
      "  timers:\n",
      "    learn_throughput: 44330.113\n",
      "    learn_time_ms: 5.775\n",
      "    update_time_ms: 2.977\n",
      "  timestamp: 1620515660\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 40500\n",
      "  training_iteration: 40\n",
      "  trial_id: 1a2f8_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/12 CPUs, 0/0 GPUs, 0.0/4.79 GiB heap, 0.0/1.61 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 2/2 (2 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                        </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">   reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_SimpleSupplyChain_1a2f8_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">         421.472</td><td style=\"text-align: right;\">40500</td><td style=\"text-align: right;\"> -87616.2</td><td style=\"text-align: right;\">            -31561.1</td><td style=\"text-align: right;\">             -133028</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "<tr><td>DDPG_SimpleSupplyChain_1a2f8_00001</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">         421.079</td><td style=\"text-align: right;\">40500</td><td style=\"text-align: right;\">-147375  </td><td style=\"text-align: right;\">           -108103  </td><td style=\"text-align: right;\">             -173502</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-08 18:14:21,133\tINFO tune.py:448 -- Total run time: 437.33 seconds (436.85 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    stop = {\n",
    "            \"training_iteration\": 40,\n",
    "            \"episode_reward_mean\": 10000000,\n",
    "        }\n",
    "\n",
    "    config = {\n",
    "            \"env\": e.SimpleSupplyChain, \n",
    "            # Use GPUs iff `RLLIB_NUM_GPUS` env var set to > 0.\n",
    "            \"num_gpus\": 0,\n",
    "            \"num_workers\": 4 , # parallelism,\n",
    "            'framework': 'tf'\n",
    "        }\n",
    "    \n",
    "    tune.run('DDPG',num_samples=2,\n",
    "        stop=stop,\n",
    "        config=config\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/mingjunwang/opt/miniconda3/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "File descriptor limit 256 is too low for production servers and may result in connection errors. At least 8192 is recommended. --- Fix with 'ulimit -n 8192'\n",
      "2021-05-08 18:25:37,853\tINFO services.py:1171 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n",
      "2021-05-08 18:25:39,742\tINFO trainer.py:591 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "2021-05-08 18:25:40,055\tINFO dynamic_tf_policy.py:445 -- Testing `compute_actions` w/ dummy batch.\n",
      "2021-05-08 18:25:40,056\tINFO tf_run_builder.py:86 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "2021-05-08 18:25:40,122\tINFO dynamic_tf_policy.py:453 -- Adding extra-action-fetch `action_prob` to view-reqs.\n",
      "2021-05-08 18:25:40,123\tINFO dynamic_tf_policy.py:453 -- Adding extra-action-fetch `action_logp` to view-reqs.\n",
      "2021-05-08 18:25:40,124\tINFO dynamic_tf_policy.py:453 -- Adding extra-action-fetch `action_dist_inputs` to view-reqs.\n",
      "2021-05-08 18:25:40,125\tINFO dynamic_tf_policy.py:496 -- Testing `postprocess_trajectory` w/ dummy batch.\n",
      "2021-05-08 18:25:40,677\tINFO rollout_worker.py:1080 -- Built policy map: {'default_policy': <ray.rllib.policy.tf_policy_template.DDPGTFPolicy object at 0x7ffb81331e80>}\n",
      "2021-05-08 18:25:40,678\tINFO rollout_worker.py:1081 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7ffb81331be0>}\n",
      "2021-05-08 18:25:40,678\tINFO rollout_worker.py:509 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x7ffb815989a0>}\n",
      "2021-05-08 18:25:40,684\tINFO rollout_worker.py:635 -- Generating sample batch of size 1\n",
      "2021-05-08 18:25:40,690\tINFO sampler.py:558 -- Raw obs from env: { 0: { 'agent0': np.ndarray((17,), dtype=float64, min=0.0, max=0.0, mean=0.0)}}\n",
      "2021-05-08 18:25:40,691\tINFO sampler.py:560 -- Info return from env: {0: {'agent0': None}}\n",
      "2021-05-08 18:25:40,692\tWARNING deprecation.py:29 -- DeprecationWarning: `env_index` has been deprecated. Use `episode.env_id` instead. This will raise an error in the future!\n",
      "2021-05-08 18:25:40,693\tINFO sampler.py:1023 -- Preprocessed obs: np.ndarray((17,), dtype=float64, min=0.0, max=0.0, mean=0.0)\n",
      "2021-05-08 18:25:40,694\tINFO sampler.py:1028 -- Filtered obs: np.ndarray((17,), dtype=float64, min=0.0, max=0.0, mean=0.0)\n",
      "2021-05-08 18:25:40,695\tINFO sampler.py:1299 -- Inputs to compute_actions():\n",
      "\n",
      "{ 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
      "                                  'env_id': 0,\n",
      "                                  'info': None,\n",
      "                                  'obs': np.ndarray((17,), dtype=float64, min=0.0, max=0.0, mean=0.0),\n",
      "                                  'prev_action': None,\n",
      "                                  'prev_reward': 0.0,\n",
      "                                  'rnn_state': None},\n",
      "                        'type': 'PolicyEvalData'}]}\n",
      "\n",
      "2021-05-08 18:25:40,755\tINFO sampler.py:1317 -- Outputs of compute_actions():\n",
      "\n",
      "{ 'default_policy': ( np.ndarray((1, 4), dtype=float32, min=2.529, max=15.439, mean=10.022),\n",
      "                      [],\n",
      "                      { 'action_dist_inputs': np.ndarray((1, 4), dtype=float32, min=10.0, max=10.0, mean=10.0),\n",
      "                        'action_logp': np.ndarray((1,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "                        'action_prob': np.ndarray((1,), dtype=float32, min=1.0, max=1.0, mean=1.0)})}\n",
      "\n",
      "2021-05-08 18:25:40,758\tINFO simple_list_collector.py:538 -- Trajectory fragment after postprocess_trajectory():\n",
      "\n",
      "{ 'agent0': { 'data': { 'actions': np.ndarray((1, 4), dtype=float32, min=2.529, max=15.439, mean=10.022),\n",
      "                        'agent_index': np.ndarray((1,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "                        'dones': np.ndarray((1,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "                        'eps_id': np.ndarray((1,), dtype=int64, min=190315070.0, max=190315070.0, mean=190315070.0),\n",
      "                        'new_obs': np.ndarray((1, 17), dtype=float32, min=-9.211, max=8.0, mean=0.164),\n",
      "                        'obs': np.ndarray((1, 17), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "                        'rewards': np.ndarray((1,), dtype=float32, min=437.282, max=437.282, mean=437.282),\n",
      "                        'unroll_id': np.ndarray((1,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "                        'weights': np.ndarray((1,), dtype=float32, min=1.0, max=1.0, mean=1.0)},\n",
      "              'type': 'SampleBatch'}}\n",
      "\n",
      "2021-05-08 18:25:40,759\tINFO rollout_worker.py:669 -- Completed sample batch:\n",
      "\n",
      "{ 'data': { 'actions': np.ndarray((1, 4), dtype=float32, min=2.529, max=15.439, mean=10.022),\n",
      "            'agent_index': np.ndarray((1,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "            'dones': np.ndarray((1,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "            'eps_id': np.ndarray((1,), dtype=int64, min=190315070.0, max=190315070.0, mean=190315070.0),\n",
      "            'new_obs': np.ndarray((1, 17), dtype=float32, min=-9.211, max=8.0, mean=0.164),\n",
      "            'obs': np.ndarray((1, 17), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "            'rewards': np.ndarray((1,), dtype=float32, min=437.282, max=437.282, mean=437.282),\n",
      "            'unroll_id': np.ndarray((1,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "            'weights': np.ndarray((1,), dtype=float32, min=1.0, max=1.0, mean=1.0)},\n",
      "  'type': 'SampleBatch'}\n",
      "\n",
      "2021-05-08 18:25:40,765\tINFO replay_buffer.py:44 -- Estimated max memory usage for replay buffer is 0.00912 GB (10000.0 batches of size 1, 912 bytes each), available system memory is 17.179869184 GB\n",
      "2021-05-08 18:25:42,397\tINFO rollout_worker.py:809 -- Training on concatenated sample batches:\n",
      "\n",
      "{ 'count': 256,\n",
      "  'policy_batches': { 'default_policy': { 'data': { 'actions': np.ndarray((256, 4), dtype=float32, min=0.0, max=19.969, mean=8.103),\n",
      "                                                    'agent_index': np.ndarray((256,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "                                                    'batch_indexes': np.ndarray((256,), dtype=int64, min=2.0, max=1468.0, mean=751.516),\n",
      "                                                    'dones': np.ndarray((256,), dtype=bool, min=0.0, max=1.0, mean=0.066),\n",
      "                                                    'eps_id': np.ndarray((256,), dtype=int64, min=556056.0, max=1997007732.0, mean=902276348.602),\n",
      "                                                    'new_obs': np.ndarray((256, 17), dtype=float32, min=-598.562, max=40.0, mean=-6.355),\n",
      "                                                    'obs': np.ndarray((256, 17), dtype=float32, min=-563.268, max=40.0, mean=-5.672),\n",
      "                                                    'rewards': np.ndarray((256,), dtype=float32, min=-56566.418, max=1343.697, mean=-18201.652),\n",
      "                                                    'unroll_id': np.ndarray((256,), dtype=int64, min=2.0, max=1468.0, mean=751.516),\n",
      "                                                    'weights': np.ndarray((256,), dtype=float64, min=1.0, max=1.0, mean=1.0)},\n",
      "                                          'type': 'SampleBatch'}},\n",
      "  'type': 'MultiAgentBatch'}\n",
      "\n",
      "2021-05-08 18:25:42,398\tINFO tf_policy.py:634 -- Optimizing variable <tf.Variable 'default_policy/actor_hidden_0/kernel:0' shape=(17, 512) dtype=float32>\n",
      "2021-05-08 18:25:42,399\tINFO tf_policy.py:634 -- Optimizing variable <tf.Variable 'default_policy/actor_hidden_0/bias:0' shape=(512,) dtype=float32>\n",
      "2021-05-08 18:25:42,399\tINFO tf_policy.py:634 -- Optimizing variable <tf.Variable 'default_policy/actor_hidden_1/kernel:0' shape=(512, 512) dtype=float32>\n",
      "2021-05-08 18:25:42,399\tINFO tf_policy.py:634 -- Optimizing variable <tf.Variable 'default_policy/actor_hidden_1/bias:0' shape=(512,) dtype=float32>\n",
      "2021-05-08 18:25:42,400\tINFO tf_policy.py:634 -- Optimizing variable <tf.Variable 'default_policy/actor_out/kernel:0' shape=(512, 4) dtype=float32>\n",
      "2021-05-08 18:25:42,400\tINFO tf_policy.py:634 -- Optimizing variable <tf.Variable 'default_policy/actor_out/bias:0' shape=(4,) dtype=float32>\n",
      "2021-05-08 18:25:42,401\tINFO tf_policy.py:634 -- Optimizing variable <tf.Variable 'default_policy/sequential/q_hidden_0/kernel:0' shape=(21, 512) dtype=float32>\n",
      "2021-05-08 18:25:42,401\tINFO tf_policy.py:634 -- Optimizing variable <tf.Variable 'default_policy/sequential/q_hidden_0/bias:0' shape=(512,) dtype=float32>\n",
      "2021-05-08 18:25:42,402\tINFO tf_policy.py:634 -- Optimizing variable <tf.Variable 'default_policy/sequential/q_hidden_1/kernel:0' shape=(512, 512) dtype=float32>\n",
      "2021-05-08 18:25:42,402\tINFO tf_policy.py:634 -- Optimizing variable <tf.Variable 'default_policy/sequential/q_hidden_1/bias:0' shape=(512,) dtype=float32>\n",
      "2021-05-08 18:25:42,403\tINFO tf_policy.py:634 -- Optimizing variable <tf.Variable 'default_policy/sequential/q_out/kernel:0' shape=(512, 1) dtype=float32>\n",
      "2021-05-08 18:25:42,403\tINFO tf_policy.py:634 -- Optimizing variable <tf.Variable 'default_policy/sequential/q_out/bias:0' shape=(1,) dtype=float32>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  0\n",
      "iteration:  10\n",
      "iteration:  20\n",
      "iteration:  30\n",
      "iteration:  40\n",
      "iteration:  50\n",
      "iteration:  60\n",
      "iteration:  70\n",
      "iteration:  80\n",
      "iteration:  90\n",
      "iteration:  100\n",
      "iteration:  110\n",
      "iteration:  120\n",
      "iteration:  130\n",
      "iteration:  140\n",
      "iteration:  150\n",
      "iteration:  160\n",
      "iteration:  170\n",
      "iteration:  180\n",
      "iteration:  190\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "from ray.rllib.agents import dqn\n",
    "from ray import tune\n",
    "\n",
    "from Envs import SupplyChain as e\n",
    "\n",
    "import ray.rllib.agents.ddpg as ddpg\n",
    "from ray.tune.logger import pretty_print\n",
    "\n",
    "import importlib\n",
    "importlib.reload(e)\n",
    "\n",
    "ray.shutdown()\n",
    "ray.init()\n",
    "\n",
    "def train_ddpg():\n",
    "    config = ddpg.DEFAULT_CONFIG.copy()\n",
    "    config[\"log_level\"] = \"INFO\"\n",
    "    config[\"actor_hiddens\"] = [512, 512] \n",
    "    config[\"critic_hiddens\"] = [512, 512]\n",
    "    config[\"gamma\"] = 0.95\n",
    "    config[\"timesteps_per_iteration\"] = 1000\n",
    "    config[\"target_network_update_freq\"] = 5\n",
    "    config[\"buffer_size\"] = 10000\n",
    "    config['framework'] = 'tf'\n",
    "    \n",
    "    trainer = ddpg.DDPGTrainer(config=config, env=e.SimpleSupplyChain)\n",
    "    for i in range(200):\n",
    "        pass\n",
    "        result = trainer.train()\n",
    "        if i % 10 == 0:\n",
    "            print(\"iteration: \", i)\n",
    "        #print(pretty_print(result))\n",
    "        #checkpoint = trainer.save()\n",
    "        #print(\"Checkpoint saved at\", checkpoint)\n",
    "\n",
    "train_ddpg()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-09 11:31:23,006\tWARNING worker.py:622 -- File descriptor limit 256 is too low for production servers and may result in connection errors. At least 8192 is recommended. --- Fix with 'ulimit -n 8192'\n",
      "2021-05-09 11:31:23,398\tINFO services.py:1171 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Unknown config parameter `twin_q` ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-afeff6e27f63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m#print(\"Checkpoint saved at\", checkpoint)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mtrain_ppo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-afeff6e27f63>\u001b[0m in \u001b[0;36mtrain_ppo\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'framework'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'tf'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mppo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPPOTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSimpleSupplyChain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.8/site-packages/ray/rllib/agents/trainer_template.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, env, logger_creator)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger_creator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0mTrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger_creator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         def _init(self, config: TrainerConfigDict,\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, env, logger_creator)\u001b[0m\n\u001b[1;32m    463\u001b[0m             \u001b[0mlogger_creator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefault_logger_creator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger_creator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.8/site-packages/ray/tune/trainable.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, logger_creator)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0msetup_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msetup_time\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mSETUP_TIME_THRESHOLD\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\u001b[0m in \u001b[0;36msetup\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    575\u001b[0m         \u001b[0;31m# user-provided one.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_user_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m         self.config = Trainer.merge_trainer_configs(self._default_config,\n\u001b[0m\u001b[1;32m    578\u001b[0m                                                     config)\n\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\u001b[0m in \u001b[0;36mmerge_trainer_configs\u001b[0;34m(cls, config1, config2, _allow_unknown_configs)\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_allow_unknown_configs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m             \u001b[0m_allow_unknown_configs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_allow_unknown_configs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1055\u001b[0;31m         return deep_update(config1, config2, _allow_unknown_configs,\n\u001b[0m\u001b[1;32m   1056\u001b[0m                            \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_allow_unknown_subkeys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1057\u001b[0m                            cls._override_all_subkeys_if_type_changes)\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.8/site-packages/ray/tune/utils/util.py\u001b[0m in \u001b[0;36mdeep_update\u001b[0;34m(original, new_dict, new_keys_allowed, allow_new_subkey_list, override_all_if_type_changes)\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnew_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moriginal\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnew_keys_allowed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unknown config parameter `{}` \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;31m# Both orginal value and new one are dicts.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Unknown config parameter `twin_q` "
     ]
    }
   ],
   "source": [
    "import ray\n",
    "from ray import tune\n",
    "\n",
    "from ray.rllib.agents import ddpg, ppo, dqn\n",
    "from ray.tune.logger import pretty_print\n",
    "\n",
    "from Envs import SupplyChain as e\n",
    "import importlib\n",
    "importlib.reload(e)\n",
    "\n",
    "ray.shutdown()\n",
    "ray.init()\n",
    "\n",
    "def train_ppo():\n",
    "    config = ddpg.DEFAULT_CONFIG.copy()\n",
    "    config[\"log_level\"] = \"INFO\"\n",
    "    #config[\"actor_hiddens\"] = [512, 512] \n",
    "    #config[\"critic_hiddens\"] = [512, 512]\n",
    "    config[\"gamma\"] = 0.99\n",
    "    #config[\"timesteps_per_iteration\"] = 1000\n",
    "    #config[\"target_network_update_freq\"] = 5\n",
    "    #config[\"buffer_size\"] = 10000\n",
    "    config['framework'] = 'tf'\n",
    "    \n",
    "    trainer = ppo.PPOTrainer(config=config, env=e.SimpleSupplyChain)\n",
    "    for i in range(200):\n",
    "        pass\n",
    "        result = trainer.train()\n",
    "        if i % 10 == 0:\n",
    "            print(\"iteration: \", i)\n",
    "        #print(pretty_print(result))\n",
    "        #checkpoint = trainer.save()\n",
    "        #print(\"Checkpoint saved at\", checkpoint)\n",
    "\n",
    "train_ppo()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
