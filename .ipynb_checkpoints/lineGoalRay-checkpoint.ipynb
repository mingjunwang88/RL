{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.utils import seeding\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.agents import ppo\n",
    "from ray.tune.registry import register_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Space Attributes\n",
    "    Most environments have two special attributes: action_space, observation_space\n",
    "\n",
    "    These contain instances of gym.spaces classes\n",
    "    Makes it easy to find out what are valid states and actions I\n",
    "    There is a convenient sample method to generate uniform random samples in the space.\n",
    "#### gym.spaces\n",
    "    Action spaces and State spaces are defined by instances of classes of the gym.spaces modules\n",
    "\n",
    "    Included types are:\n",
    "      gym.spaces.Discrete\n",
    "\n",
    "      gym.spaces.MultiDiscrete\n",
    "\n",
    "      gym.spaces.Box\n",
    "\n",
    "      gym.spaces.Tuple\n",
    "    All instances have a sample method which will sample random instances within the space\n",
    "#### gym.spaces.Discrete\n",
    "    The homework environments will use this type of space Specifies a space containing n discrete points\n",
    "    Each point is mapped to an integer from [0 ,n−1]\n",
    "    Discrete(10) A space containing 10 items mapped to integers in [0,9] sample will return integers such as 0, 3, and 9.\n",
    "#### gym.spaces.MultiDiscrete\n",
    "    You will use this to implement an environment in the homework\n",
    "    Species a space containing k dimensions each with a separate number of discrete points.\n",
    "    Each point in the space is represented by a vector of integers of length k\n",
    "    MultiDiscrete([(1, 3), (0, 5)]) A space with k= 2 dimensions First dimension has 4 points mapped to integers in [1,3] Second dimension has 6 points mapped to integers in [0,5] sample will return a vector such as [2,5] and [1,3]\n",
    "#### gym.spaces.Box\n",
    "    Used for multidimensional continuous spaces with bounds\n",
    "    You will see environments with these types of state and action spaces in future homeworks\n",
    "    Box(np.array((-1.0, -2.0)), np.array((1.0, 2.0))) A 2D continous state spaceI First dimension has values in range [−1.0,1.0) Second dimension has values in range [−2.0,2.0) sample will return a vector such as [−.55,2.] and [.768,−1.55]\n",
    "\n",
    "### Creating an Environment\n",
    "#### gym.Env Class\n",
    "    All environments should inherit from gym.Env\n",
    "    At a minimum you must override a handful of methods:\n",
    "    step()\n",
    "    reset()\n",
    "    At a minimum you must provide the following attributes action_space, observation_space\n",
    "#### Subclass Methods\n",
    "    _step is the same api as the step function used in the example\n",
    "    _reset is the same api as the reset function in the example\n",
    "    You may also provide the following methods for additionalfunctionality:\n",
    "\n",
    "    _render\n",
    "    _close\n",
    "    _configure\n",
    "    _seed\n",
    "\n",
    "#### Attributes\n",
    "    observation_space represents the state space\n",
    "    action_space represents the action space\n",
    "    Both are instances of gym.spaces classes\n",
    "    You can also provide a reward_range , but this defaults to (−∞,∞)\n",
    "#### Registration\n",
    "    How do you get your environment to work with gym.make()? You must register it\n",
    "#### Registration Example\n",
    "    from gym.envs.registration import register\n",
    "\n",
    "    register(\n",
    "      id='Deterministic-4x4-FrozenLake-v0',\n",
    "      entry_point='gym.envs.toy_text.frozen_lake:FrozenLakeEnv',\n",
    "      kwargs={'map_name': '4x4',\n",
    "      'is_slippery': False})\n",
    "    id: the environment name used with gym.make\n",
    "    entry_point: module path and class name of environment\n",
    "    kwargs: dictionary of keyword arguments to environment constructor\n",
    "#### Discrete Environment Class\n",
    "    A subclass of the gym.Env which provides the followingattributes\n",
    "    nS: number of states\n",
    "    nA: number of actions\n",
    "    P: model of environment\n",
    "    isd: initial state distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEnv (gym.Env):\n",
    "    # possible actions\n",
    "    MOVE_LF = 0\n",
    "    MOVE_RT = 1\n",
    "\n",
    "    # possible positions\n",
    "    LF_MIN = 1\n",
    "    RT_MAX = 10\n",
    "\n",
    "    # land on the GOAL position within MAX_STEPS steps\n",
    "    MAX_STEPS = 10\n",
    "\n",
    "    # possible rewards\n",
    "    REWARD_AWAY = -2\n",
    "    REWARD_STEP = -1\n",
    "    REWARD_GOAL = MAX_STEPS\n",
    "\n",
    "    metadata = {\n",
    "        \"render.modes\": [\"human\"]\n",
    "        }\n",
    "\n",
    "\n",
    "    def __init__ (self, config):\n",
    "        # the action space ranges [0, 1] where:\n",
    "        #  `0` move left\n",
    "        #  `1` move right\n",
    "        self.action_space = gym.spaces.Discrete(2)\n",
    "\n",
    "        # NB: Ray throws exceptions for any `0` value Discrete\n",
    "        # observations so we'll make position a 1's based value\n",
    "        self.observation_space = gym.spaces.Discrete(self.RT_MAX + 1)\n",
    "\n",
    "        # possible positions to chose on `reset()`\n",
    "        self.goal = int((self.LF_MIN + self.RT_MAX - 1) / 2)\n",
    "\n",
    "        self.init_positions = list(range(self.LF_MIN, self.RT_MAX))\n",
    "        self.init_positions.remove(self.goal)\n",
    "\n",
    "        # NB: change to guarantee the sequence of pseudorandom numbers\n",
    "        # (e.g., for debugging)\n",
    "        self.seed()\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "\n",
    "    def reset (self):\n",
    "        \"\"\"\n",
    "        Reset the state of the environment and returns an initial observation.\n",
    "        Returns\n",
    "        -------\n",
    "        observation (object): the initial observation of the space.\n",
    "        \"\"\"\n",
    "        self.position = self.np_random.choice(self.init_positions)\n",
    "        self.count = 0\n",
    "\n",
    "        # for this environment, state is simply the position\n",
    "        self.state = self.position\n",
    "        self.reward = 0\n",
    "        self.done = False\n",
    "        self.info = {}\n",
    "\n",
    "        return self.state\n",
    "\n",
    "\n",
    "    def step (self, action):\n",
    "        \"\"\"\n",
    "        The agent takes a step in the environment.\n",
    "        Parameters\n",
    "        ----------\n",
    "        action : Discrete\n",
    "        Returns\n",
    "        -------\n",
    "        observation, reward, done, info : tuple\n",
    "            observation (object) :\n",
    "                an environment-specific object representing your observation of\n",
    "                the environment.\n",
    "            reward (float) :\n",
    "                amount of reward achieved by the previous action. The scale\n",
    "                varies between environments, but the goal is always to increase\n",
    "                your total reward.\n",
    "            done (bool) :\n",
    "                whether it's time to reset the environment again. Most (but not\n",
    "                all) tasks are divided up into well-defined episodes, and done\n",
    "                being True indicates the episode has terminated. (For example,\n",
    "                perhaps the pole tipped too far, or you lost your last life.)\n",
    "            info (dict) :\n",
    "                 diagnostic information useful for debugging. It can sometimes\n",
    "                 be useful for learning (for example, it might contain the raw\n",
    "                 probabilities behind the environment's last state change).\n",
    "                 However, official evaluations of your agent are not allowed to\n",
    "                 use this for learning.\n",
    "        \"\"\"\n",
    "        if self.done:\n",
    "            # code should never reach this point\n",
    "            print(\"EPISODE DONE!!!\")\n",
    "\n",
    "        elif self.count == self.MAX_STEPS:\n",
    "            self.done = True;\n",
    "\n",
    "        else:\n",
    "            assert self.action_space.contains(action)\n",
    "            self.count += 1\n",
    "\n",
    "            if action == self.MOVE_LF:\n",
    "                if self.position == self.LF_MIN:\n",
    "                    # invalid\n",
    "                    self.reward = self.REWARD_AWAY\n",
    "                else:\n",
    "                    self.position -= 1\n",
    "\n",
    "                    if self.position == self.goal:\n",
    "                        # on goal now\n",
    "                        self.reward = self.REWARD_GOAL\n",
    "                        self.done = 1\n",
    "                    elif self.position < self.goal:\n",
    "                        # moving away from goal\n",
    "                        self.reward = self.REWARD_AWAY\n",
    "                    else:\n",
    "                        # moving toward goal\n",
    "                        self.reward = self.REWARD_STEP\n",
    "\n",
    "            elif action == self.MOVE_RT:\n",
    "                if self.position == self.RT_MAX:\n",
    "                    # invalid\n",
    "                    self.reward = self.REWARD_AWAY\n",
    "                else:\n",
    "                    self.position += 1\n",
    "\n",
    "                    if self.position == self.goal:\n",
    "                        # on goal now\n",
    "                        self.reward = self.REWARD_GOAL\n",
    "                        self.done = 1\n",
    "                    elif self.position > self.goal:\n",
    "                        # moving away from goal\n",
    "                        self.reward = self.REWARD_AWAY\n",
    "                    else:\n",
    "                        # moving toward goal\n",
    "                        self.reward = self.REWARD_STEP\n",
    "\n",
    "            self.state = self.position\n",
    "            self.info[\"dist\"] = self.goal - self.position\n",
    "\n",
    "        try:\n",
    "            assert self.observation_space.contains(self.state)\n",
    "        except AssertionError:\n",
    "            print(\"INVALID STATE\", self.state)\n",
    "\n",
    "        return [self.state, self.reward, self.done, self.info]\n",
    "\n",
    "\n",
    "    def render (self, mode=\"human\"):\n",
    "        \"\"\"Renders the environment.\n",
    "        The set of supported modes varies per environment. (And some\n",
    "        environments do not support rendering at all.) By convention,\n",
    "        if mode is:\n",
    "        - human: render to the current display or terminal and\n",
    "          return nothing. Usually for human consumption.\n",
    "        - rgb_array: Return an numpy.ndarray with shape (x, y, 3),\n",
    "          representing RGB values for an x-by-y pixel image, suitable\n",
    "          for turning into a video.\n",
    "        - ansi: Return a string (str) or StringIO.StringIO containing a\n",
    "          terminal-style text representation. The text can include newlines\n",
    "          and ANSI escape sequences (e.g. for colors).\n",
    "        Note:\n",
    "            Make sure that your class's metadata 'render.modes' key includes\n",
    "              the list of supported modes. It's recommended to call super()\n",
    "              in implementations to use the functionality of this method.\n",
    "        Args:\n",
    "            mode (str): the mode to render with\n",
    "        \"\"\"\n",
    "        s = \"position: {:2d}  reward: {:2d}  info: {}\"\n",
    "        print(s.format(self.state, self.reward, self.info))\n",
    "\n",
    "\n",
    "    def seed (self, seed=None):\n",
    "        \"\"\"Sets the seed for this env's random number generator(s).\n",
    "        Note:\n",
    "            Some environments use multiple pseudorandom number generators.\n",
    "            We want to capture all such seeds used in order to ensure that\n",
    "            there aren't accidental correlations between multiple generators.\n",
    "        Returns:\n",
    "            list<bigint>: Returns the list of seeds used in this env's random\n",
    "              number generators. The first value in the list should be the\n",
    "              \"main\" seed, or the value which a reproducer should pass to\n",
    "              'seed'. Often, the main seed equals the provided 'seed', but\n",
    "              this won't be true if seed=None, for example.\n",
    "        \"\"\"\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "\n",
    "    def close (self):\n",
    "        \"\"\"Override close in your subclass to perform any necessary cleanup.\n",
    "        Environments will automatically close() themselves when\n",
    "        garbage collected or when the program exits.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '172.16.35.94',\n",
       " 'raylet_ip_address': '172.16.35.94',\n",
       " 'redis_address': '172.16.35.94:46312',\n",
       " 'object_store_address': '/tmp/ray/session_2021-10-15_14-41-51_900331_1008/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2021-10-15_14-41-51_900331_1008/sockets/raylet',\n",
       " 'webui_url': None,\n",
       " 'session_dir': '/tmp/ray/session_2021-10-15_14-41-51_900331_1008',\n",
       " 'metrics_export_port': 50367,\n",
       " 'node_id': '0164695434c0689e41765ad9152be00ecfb71aabe528847ddac76c8c'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using train: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(iters =20, path='checkpoint'):\n",
    "    config = {\n",
    "        \"env\": CustomEnv,  # or \"corridor\" if registered above\n",
    "        # Use GPUs iff `RLLIB_NUM_GPUS` env var set to > 0.\n",
    "        \"num_gpus\": 0,\n",
    "        \"num_workers\": 5  # parallelism\n",
    "    }\n",
    "    \n",
    "    # register the custom environment\n",
    "    #register_env(\"customEnv\", CustomEnv())\n",
    "    #trainer = ppo.PPOTrainer(env=\"customEnv\")\n",
    "    \n",
    "    #trainer = ppo.PPOTrainer(config=config, env=CustomEnv)\n",
    "    trainer = ppo.PPOTrainer(env=CustomEnv, config=config)\n",
    "    n=0\n",
    "    while True:\n",
    "        if n>=iters:\n",
    "            break\n",
    "        trainer.train()\n",
    "        chkpt = trainer.save(path)\n",
    "        \n",
    "        n=+1\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    pass\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Tune: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/32 CPUs, 0/8 GPUs, 0.0/323.85 GiB heap, 0.0/142.78 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 2/2 (2 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CustomEnv_114a8_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "<tr><td>PPO_CustomEnv_114a8_00001</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=1248)\u001b[0m 2021-10-15 14:42:08,170\tINFO ppo.py:165 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(pid=1248)\u001b[0m 2021-10-15 14:42:08,171\tINFO trainer.py:760 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=1259)\u001b[0m 2021-10-15 14:42:08,147\tINFO ppo.py:165 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(pid=1259)\u001b[0m 2021-10-15 14:42:08,148\tINFO trainer.py:760 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "2021-10-15 14:42:10,049\tERROR trial_runner.py:846 -- Trial PPO_CustomEnv_114a8_00000: Error processing event.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ray/tune/trial_runner.py\", line 812, in _process_trial\n",
      "    results = self.trial_executor.fetch_result(trial)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ray/tune/ray_trial_executor.py\", line 767, in fetch_result\n",
      "    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ray/_private/client_mode_hook.py\", line 89, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ray/worker.py\", line 1623, in get\n",
      "    raise value\n",
      "ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::PPO.__init__()\u001b[39m (pid=1259, ip=172.16.35.94)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ray/rllib/agents/trainer_template.py\", line 137, in __init__\n",
      "    Trainer.__init__(self, config, env, logger_creator)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ray/rllib/agents/trainer.py\", line 611, in __init__\n",
      "    super().__init__(config, logger_creator)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ray/tune/trainable.py\", line 106, in __init__\n",
      "    self.setup(copy.deepcopy(self.config))\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ray/rllib/agents/trainer_template.py\", line 147, in setup\n",
      "    super().setup(config)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ray/rllib/agents/trainer.py\", line 764, in setup\n",
      "    self._init(self.config, self.env_creator)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ray/rllib/agents/trainer_template.py\", line 176, in _init\n",
      "    num_workers=self.config[\"num_workers\"])\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ray/rllib/agents/trainer.py\", line 852, in _make_workers\n",
      "    logdir=self.logdir)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ray/rllib/evaluation/worker_set.py\", line 85, in __init__\n",
      "    lambda p, pid: (pid, p.observation_space, p.action_space)))\n",
      "ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=1244, ip=172.16.35.94)\n",
      "AttributeError: 'NoneType' object has no attribute 'config'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=1244, ip=172.16.35.94)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 564, in __init__\n",
      "    devices = get_tf_gpu_devices()\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ray/rllib/utils/tf_ops.py\", line 53, in get_gpu_devices\n",
      "    devices = tf.config.experimental.list_physical_devices()\n",
      "AttributeError: 'NoneType' object has no attribute 'config'\n",
      "2021-10-15 14:42:10,056\tERROR trial_runner.py:846 -- Trial PPO_CustomEnv_114a8_00001: Error processing event.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ray/tune/trial_runner.py\", line 812, in _process_trial\n",
      "    results = self.trial_executor.fetch_result(trial)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ray/tune/ray_trial_executor.py\", line 767, in fetch_result\n",
      "    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ray/_private/client_mode_hook.py\", line 89, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ray/worker.py\", line 1623, in get\n",
      "    raise value\n",
      "ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::PPO.__init__()\u001b[39m (pid=1248, ip=172.16.35.94)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ray/rllib/agents/trainer_template.py\", line 137, in __init__\n",
      "    Trainer.__init__(self, config, env, logger_creator)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ray/rllib/agents/trainer.py\", line 611, in __init__\n",
      "    super().__init__(config, logger_creator)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ray/tune/trainable.py\", line 106, in __init__\n",
      "    self.setup(copy.deepcopy(self.config))\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ray/rllib/agents/trainer_template.py\", line 147, in setup\n",
      "    super().setup(config)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ray/rllib/agents/trainer.py\", line 764, in setup\n",
      "    self._init(self.config, self.env_creator)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ray/rllib/agents/trainer_template.py\", line 176, in _init\n",
      "    num_workers=self.config[\"num_workers\"])\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ray/rllib/agents/trainer.py\", line 852, in _make_workers\n",
      "    logdir=self.logdir)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ray/rllib/evaluation/worker_set.py\", line 85, in __init__\n",
      "    lambda p, pid: (pid, p.observation_space, p.action_space)))\n",
      "ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=1258, ip=172.16.35.94)\n",
      "AttributeError: 'NoneType' object has no attribute 'config'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=1258, ip=172.16.35.94)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 564, in __init__\n",
      "    devices = get_tf_gpu_devices()\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ray/rllib/utils/tf_ops.py\", line 53, in get_gpu_devices\n",
      "    devices = tf.config.experimental.list_physical_devices()\n",
      "AttributeError: 'NoneType' object has no attribute 'config'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CustomEnv_114a8_00000:\n",
      "  {}\n",
      "  \n",
      "Result for PPO_CustomEnv_114a8_00001:\n",
      "  {}\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/32 CPUs, 0/8 GPUs, 0.0/323.85 GiB heap, 0.0/142.78 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 2/2 (2 ERROR)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CustomEnv_114a8_00000</td><td>ERROR   </td><td>     </td></tr>\n",
       "<tr><td>PPO_CustomEnv_114a8_00001</td><td>ERROR   </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 2<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                              </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CustomEnv_114a8_00000</td><td style=\"text-align: right;\">           1</td><td>/home/ec2-user/ray_results/PPO/PPO_CustomEnv_114a8_00000_0_2021-10-15_14-42-06/error.txt</td></tr>\n",
       "<tr><td>PPO_CustomEnv_114a8_00001</td><td style=\"text-align: right;\">           1</td><td>/home/ec2-user/ray_results/PPO/PPO_CustomEnv_114a8_00001_1_2021-10-15_14-42-06/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=1248)\u001b[0m 2021-10-15 14:42:10,050\tERROR worker.py:428 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::PPO.__init__()\u001b[39m (pid=1248, ip=172.16.35.94)\n",
      "\u001b[2m\u001b[36m(pid=1248)\u001b[0m   File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ray/rllib/agents/trainer_template.py\", line 137, in __init__\n",
      "\u001b[2m\u001b[36m(pid=1248)\u001b[0m     Trainer.__init__(self, config, env, logger_creator)\n",
      "\u001b[2m\u001b[36m(pid=1248)\u001b[0m   File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ray/rllib/agents/trainer.py\", line 611, in __init__\n",
      "\u001b[2m\u001b[36m(pid=1248)\u001b[0m     super().__init__(config, logger_creator)\n",
      "\u001b[2m\u001b[36m(pid=1248)\u001b[0m   File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ray/tune/trainable.py\", line 106, in __init__\n",
      "\u001b[2m\u001b[36m(pid=1248)\u001b[0m     self.setup(copy.deepcopy(self.config))\n",
      "\u001b[2m\u001b[36m(pid=1248)\u001b[0m   File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ray/rllib/agents/trainer_template.py\", line 147, in setup\n",
      "\u001b[2m\u001b[36m(pid=1248)\u001b[0m     super().setup(config)\n",
      "\u001b[2m\u001b[36m(pid=1248)\u001b[0m   File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ray/rllib/agents/trainer.py\", line 764, in setup\n",
      "\u001b[2m\u001b[36m(pid=1248)\u001b[0m     self._init(self.config, self.env_creator)\n",
      "\u001b[2m\u001b[36m(pid=1248)\u001b[0m   File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ray/rllib/agents/trainer_template.py\", line 176, in _init\n",
      "\u001b[2m\u001b[36m(pid=1248)\u001b[0m     num_workers=self.config[\"num_workers\"])\n",
      "\u001b[2m\u001b[36m(pid=1248)\u001b[0m   File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ray/rllib/agents/trainer.py\", line 852, in _make_workers\n",
      "\u001b[2m\u001b[36m(pid=1248)\u001b[0m     logdir=self.logdir)\n",
      "\u001b[2m\u001b[36m(pid=1248)\u001b[0m   File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ray/rllib/evaluation/worker_set.py\", line 85, in __init__\n",
      "\u001b[2m\u001b[36m(pid=1248)\u001b[0m     lambda p, pid: (pid, p.observation_space, p.action_space)))\n",
      "\u001b[2m\u001b[36m(pid=1248)\u001b[0m ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=1258, ip=172.16.35.94)\n",
      "\u001b[2m\u001b[36m(pid=1248)\u001b[0m AttributeError: 'NoneType' object has no attribute 'config'\n",
      "\u001b[2m\u001b[36m(pid=1248)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=1248)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(pid=1248)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=1248)\u001b[0m \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=1258, ip=172.16.35.94)\n",
      "\u001b[2m\u001b[36m(pid=1248)\u001b[0m   File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 564, in __init__\n",
      "\u001b[2m\u001b[36m(pid=1248)\u001b[0m     devices = get_tf_gpu_devices()\n",
      "\u001b[2m\u001b[36m(pid=1248)\u001b[0m   File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ray/rllib/utils/tf_ops.py\", line 53, in get_gpu_devices\n",
      "\u001b[2m\u001b[36m(pid=1248)\u001b[0m     devices = tf.config.experimental.list_physical_devices()\n",
      "\u001b[2m\u001b[36m(pid=1248)\u001b[0m AttributeError: 'NoneType' object has no attribute 'config'\n",
      "\u001b[2m\u001b[36m(pid=1259)\u001b[0m 2021-10-15 14:42:10,046\tERROR worker.py:428 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::PPO.__init__()\u001b[39m (pid=1259, ip=172.16.35.94)\n",
      "\u001b[2m\u001b[36m(pid=1259)\u001b[0m   File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ray/rllib/agents/trainer_template.py\", line 137, in __init__\n",
      "\u001b[2m\u001b[36m(pid=1259)\u001b[0m     Trainer.__init__(self, config, env, logger_creator)\n",
      "\u001b[2m\u001b[36m(pid=1259)\u001b[0m   File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ray/rllib/agents/trainer.py\", line 611, in __init__\n",
      "\u001b[2m\u001b[36m(pid=1259)\u001b[0m     super().__init__(config, logger_creator)\n",
      "\u001b[2m\u001b[36m(pid=1259)\u001b[0m   File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ray/tune/trainable.py\", line 106, in __init__\n",
      "\u001b[2m\u001b[36m(pid=1259)\u001b[0m     self.setup(copy.deepcopy(self.config))\n",
      "\u001b[2m\u001b[36m(pid=1259)\u001b[0m   File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ray/rllib/agents/trainer_template.py\", line 147, in setup\n",
      "\u001b[2m\u001b[36m(pid=1259)\u001b[0m     super().setup(config)\n",
      "\u001b[2m\u001b[36m(pid=1259)\u001b[0m   File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ray/rllib/agents/trainer.py\", line 764, in setup\n",
      "\u001b[2m\u001b[36m(pid=1259)\u001b[0m     self._init(self.config, self.env_creator)\n",
      "\u001b[2m\u001b[36m(pid=1259)\u001b[0m   File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ray/rllib/agents/trainer_template.py\", line 176, in _init\n",
      "\u001b[2m\u001b[36m(pid=1259)\u001b[0m     num_workers=self.config[\"num_workers\"])\n",
      "\u001b[2m\u001b[36m(pid=1259)\u001b[0m   File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ray/rllib/agents/trainer.py\", line 852, in _make_workers\n",
      "\u001b[2m\u001b[36m(pid=1259)\u001b[0m     logdir=self.logdir)\n",
      "\u001b[2m\u001b[36m(pid=1259)\u001b[0m   File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ray/rllib/evaluation/worker_set.py\", line 85, in __init__\n",
      "\u001b[2m\u001b[36m(pid=1259)\u001b[0m     lambda p, pid: (pid, p.observation_space, p.action_space)))\n",
      "\u001b[2m\u001b[36m(pid=1259)\u001b[0m ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=1244, ip=172.16.35.94)\n",
      "\u001b[2m\u001b[36m(pid=1259)\u001b[0m AttributeError: 'NoneType' object has no attribute 'config'\n",
      "\u001b[2m\u001b[36m(pid=1259)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=1259)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(pid=1259)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=1259)\u001b[0m \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=1244, ip=172.16.35.94)\n",
      "\u001b[2m\u001b[36m(pid=1259)\u001b[0m   File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 564, in __init__\n",
      "\u001b[2m\u001b[36m(pid=1259)\u001b[0m     devices = get_tf_gpu_devices()\n",
      "\u001b[2m\u001b[36m(pid=1259)\u001b[0m   File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ray/rllib/utils/tf_ops.py\", line 53, in get_gpu_devices\n",
      "\u001b[2m\u001b[36m(pid=1259)\u001b[0m     devices = tf.config.experimental.list_physical_devices()\n",
      "\u001b[2m\u001b[36m(pid=1259)\u001b[0m AttributeError: 'NoneType' object has no attribute 'config'\n",
      "\u001b[2m\u001b[36m(pid=1258)\u001b[0m 2021-10-15 14:42:10,042\tERROR worker.py:428 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=1258, ip=172.16.35.94)\n",
      "\u001b[2m\u001b[36m(pid=1258)\u001b[0m AttributeError: 'NoneType' object has no attribute 'config'\n",
      "\u001b[2m\u001b[36m(pid=1258)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=1258)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(pid=1258)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=1258)\u001b[0m \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=1258, ip=172.16.35.94)\n",
      "\u001b[2m\u001b[36m(pid=1258)\u001b[0m   File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 564, in __init__\n",
      "\u001b[2m\u001b[36m(pid=1258)\u001b[0m     devices = get_tf_gpu_devices()\n",
      "\u001b[2m\u001b[36m(pid=1258)\u001b[0m   File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ray/rllib/utils/tf_ops.py\", line 53, in get_gpu_devices\n",
      "\u001b[2m\u001b[36m(pid=1258)\u001b[0m     devices = tf.config.experimental.list_physical_devices()\n",
      "\u001b[2m\u001b[36m(pid=1258)\u001b[0m AttributeError: 'NoneType' object has no attribute 'config'\n",
      "\u001b[2m\u001b[36m(pid=1244)\u001b[0m 2021-10-15 14:42:10,038\tERROR worker.py:428 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=1244, ip=172.16.35.94)\n",
      "\u001b[2m\u001b[36m(pid=1244)\u001b[0m AttributeError: 'NoneType' object has no attribute 'config'\n",
      "\u001b[2m\u001b[36m(pid=1244)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=1244)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(pid=1244)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=1244)\u001b[0m \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=1244, ip=172.16.35.94)\n",
      "\u001b[2m\u001b[36m(pid=1244)\u001b[0m   File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 564, in __init__\n",
      "\u001b[2m\u001b[36m(pid=1244)\u001b[0m     devices = get_tf_gpu_devices()\n",
      "\u001b[2m\u001b[36m(pid=1244)\u001b[0m   File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ray/rllib/utils/tf_ops.py\", line 53, in get_gpu_devices\n",
      "\u001b[2m\u001b[36m(pid=1244)\u001b[0m     devices = tf.config.experimental.list_physical_devices()\n",
      "\u001b[2m\u001b[36m(pid=1244)\u001b[0m AttributeError: 'NoneType' object has no attribute 'config'\n"
     ]
    },
    {
     "ename": "TuneError",
     "evalue": "('Trials did not complete', [PPO_CustomEnv_114a8_00000, PPO_CustomEnv_114a8_00001])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTuneError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-3c2a983e6c4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m tune.run('PPO',num_samples=2,\n\u001b[1;32m     14\u001b[0m     \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m )\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ray/tune/tune.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, local_dir, search_alg, scheduler, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, sync_config, export_formats, max_failures, fail_fast, restore, server_port, resume, queue_trials, reuse_actors, trial_executor, raise_on_failed_trial, callbacks, max_concurrent_trials, loggers, ray_auto_init, run_errored_only, global_checkpoint_period, with_server, upload_dir, sync_to_cloud, sync_to_driver, sync_on_checkpoint, _remote)\u001b[0m\n\u001b[1;32m    609\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mincomplete_trials\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mraise_on_failed_trial\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSIGINT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTuneError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trials did not complete\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincomplete_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trials did not complete: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincomplete_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTuneError\u001b[0m: ('Trials did not complete', [PPO_CustomEnv_114a8_00000, PPO_CustomEnv_114a8_00001])"
     ]
    }
   ],
   "source": [
    "stop = {\n",
    "        \"training_iteration\": 10,\n",
    "        \"episode_reward_mean\": 100,\n",
    "    }\n",
    "\n",
    "config = {\n",
    "        \"env\": CustomEnv, \n",
    "        # Use GPUs iff `RLLIB_NUM_GPUS` env var set to > 0.\n",
    "        \"num_gpus\": 0,\n",
    "        \"num_workers\": 1  # parallelism\n",
    "    }\n",
    "    \n",
    "tune.run('PPO',num_samples=2,\n",
    "    stop=stop,\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
