{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To view the tensorboard: \n",
    "    1: tensorboard --logdir ray_results \n",
    "    2: see http://localhost:6006/ in browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/mingjunwang/opt/miniconda3/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "import ray.rllib.agents.ppo as ppo\n",
    "from ray.tune.logger import pretty_print\n",
    "from ray import tune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0: RLlib Training APIs: \n",
    "1: At a high level, RLlib provides an Trainer class which holds a policy for environment interaction. Through the trainer interface, the policy can be trained, checkpointed, or an action computed. In multi-agent training, the trainer manages the querying and optimization of multiple policies at once.\n",
    "\n",
    "2: rllib train --run DQN --env CartPole-v0  --config '{\"num_workers\": 8}'\n",
    "    To see the tensorboard: tensorboard --logdir=~/ray_results\n",
    "\n",
    "3: rllib rollout ~/ray_results/default/DQN_CartPole-v0_0upjmdgr0/checkpoint_1/checkpoint-1 \\\n",
    "    --run DQN --env CartPole-v0 --steps 10000\n",
    "\n",
    "4: Loading and restoring a trained agent from a checkpoint is simple:\n",
    "    \n",
    "    agent = ppo.PPOTrainer(config=config, env=env_class)\n",
    "    agent.restore(checkpoint_path)\n",
    "    \n",
    "5: Computing Actions\n",
    "\n",
    "The simplest way to programmatically compute actions from a trained agent is to use trainer.compute_action(). This method preprocesses and filters the observation before passing it to the agent policy. Here is a simple example of testing a trained agent for one episode:\n",
    "\n",
    "    # instantiate env class\n",
    "    env = env_class(env_config)\n",
    "\n",
    "    # run until episode ends\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "    while not done:\n",
    "        action = agent.compute_action(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        episode_reward += reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "File descriptor limit 256 is too low for production servers and may result in connection errors. At least 8192 is recommended. --- Fix with 'ulimit -n 8192'\n",
      "2021-07-13 20:54:19,623\tINFO services.py:1171 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '192.168.0.23',\n",
       " 'raylet_ip_address': '192.168.0.23',\n",
       " 'redis_address': '192.168.0.23:6379',\n",
       " 'object_store_address': '/tmp/ray/session_2021-07-13_20-54-18_787023_17501/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2021-07-13_20-54-18_787023_17501/sockets/raylet',\n",
       " 'webui_url': '127.0.0.1:8265',\n",
       " 'session_dir': '/tmp/ray/session_2021-07-13_20-54-18_787023_17501',\n",
       " 'metrics_export_port': 59855,\n",
       " 'node_id': '1a7564717afe72242af78d4f7c0bf6686fde4112'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.shutdown()\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 Example of Traing a PPO Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ppo.DEFAULT_CONFIG.copy()\n",
    "config['num_gpus'] = 0\n",
    "config['num_workers'] = 2\n",
    "trainer = ppo.PPOTrainer(config = config, env='CartPole-v0') \n",
    "\n",
    "for i in range(30):\n",
    "    result = trainer.train()\n",
    "    if i % 10 ==0:\n",
    "        checkpoint = trainer.save()\n",
    "        print('checkpoint saved')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 Example of Using Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nalg = 'PPO'\\ntune.run(alg,\\n    stop={'episode_reward_mean':200},\\n    config={\\n        'env':'CartPole-v0',\\n        'num_gpus':0,\\n        'num_workers':2,\\n        'lr':tune.grid_search([.01,.001,.0001])     \\n    }\\n)\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alg = 'PPO'\n",
    "tune.run(alg,\n",
    "    stop={'episode_reward_mean':200},\n",
    "    config={\n",
    "        'env':'CartPole-v0',\n",
    "        'num_gpus':0,\n",
    "        'num_workers':2,\n",
    "        'lr':tune.grid_search([.01,.001,.0001])     \n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/12 CPUs, 0/0 GPUs, 0.0/3.81 GiB heap, 0.0/1.32 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">   lr</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_Pendulum-v0_6b1dd_00000</td><td>RUNNING </td><td>     </td><td style=\"text-align: right;\">0.001</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17553)\u001b[0m WARNING:tensorflow:From /Users/mingjunwang/opt/miniconda3/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=17553)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=17553)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=17553)\u001b[0m 2021-07-13 20:54:31,552\tINFO trainer.py:591 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=17553)\u001b[0m 2021-07-13 20:54:31,552\tINFO trainer.py:616 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=17556)\u001b[0m WARNING:tensorflow:From /Users/mingjunwang/opt/miniconda3/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=17556)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=17556)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=17555)\u001b[0m WARNING:tensorflow:From /Users/mingjunwang/opt/miniconda3/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=17555)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=17555)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=17555)\u001b[0m 2021-07-13 20:54:38,692\tWARNING deprecation.py:29 -- DeprecationWarning: `env_index` has been deprecated. Use `episode.env_id` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_Pendulum-v0_6b1dd_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-13_20-54-41\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_reward_max: -781.1207639597676\n",
      "  episode_reward_mean: -1131.8902231793475\n",
      "  episode_reward_min: -1410.9375467664963\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 6\n",
      "  experiment_id: 43107a19371e49529eb221515032f0aa\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 1500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: 0.1796478033065796\n",
      "        mean_q: -0.10519210994243622\n",
      "        min_q: -0.6069953441619873\n",
      "        model: {}\n",
      "    num_steps_sampled: 1500\n",
      "    num_steps_trained: 256\n",
      "    num_target_updates: 1\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.799999999999997\n",
      "    ram_util_percent: 63.55\n",
      "  pid: 17553\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0973585918644932\n",
      "    mean_env_wait_ms: 0.14473166510204818\n",
      "    mean_inference_ms: 0.7737403544541523\n",
      "    mean_raw_obs_processing_ms: 0.22314836118573675\n",
      "  time_since_restore: 2.5313608646392822\n",
      "  time_this_iter_s: 2.5313608646392822\n",
      "  time_total_s: 2.5313608646392822\n",
      "  timers:\n",
      "    learn_throughput: 1305.163\n",
      "    learn_time_ms: 196.144\n",
      "    update_time_ms: 4.681\n",
      "  timestamp: 1626227681\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1500\n",
      "  training_iteration: 1\n",
      "  trial_id: 6b1dd_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/12 CPUs, 0/0 GPUs, 0.0/3.81 GiB heap, 0.0/1.32 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_Pendulum-v0_6b1dd_00000</td><td>RUNNING </td><td>192.168.0.23:17553</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.53136</td><td style=\"text-align: right;\">1500</td><td style=\"text-align: right;\">-1131.89</td><td style=\"text-align: right;\">            -781.121</td><td style=\"text-align: right;\">            -1410.94</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_Pendulum-v0_6b1dd_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-13_20-54-58\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_reward_max: -781.1207639597676\n",
      "  episode_reward_mean: -1331.3202113249129\n",
      "  episode_reward_min: -1796.9534032278605\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 12\n",
      "  experiment_id: 43107a19371e49529eb221515032f0aa\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 2500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: -0.50157630443573\n",
      "        mean_q: -11.473435401916504\n",
      "        min_q: -24.651611328125\n",
      "        model: {}\n",
      "    num_steps_sampled: 2500\n",
      "    num_steps_trained: 128256\n",
      "    num_target_updates: 501\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.545833333333334\n",
      "    ram_util_percent: 63.48333333333334\n",
      "  pid: 17553\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09797157483190518\n",
      "    mean_env_wait_ms: 0.14469709336634642\n",
      "    mean_inference_ms: 0.7615721799224051\n",
      "    mean_raw_obs_processing_ms: 0.22257846912676293\n",
      "  time_since_restore: 19.607420921325684\n",
      "  time_this_iter_s: 17.0760600566864\n",
      "  time_total_s: 19.607420921325684\n",
      "  timers:\n",
      "    learn_throughput: 58299.452\n",
      "    learn_time_ms: 4.391\n",
      "    update_time_ms: 2.605\n",
      "  timestamp: 1626227698\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2500\n",
      "  training_iteration: 2\n",
      "  trial_id: 6b1dd_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/12 CPUs, 0/0 GPUs, 0.0/3.81 GiB heap, 0.0/1.32 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_Pendulum-v0_6b1dd_00000</td><td>RUNNING </td><td>192.168.0.23:17553</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         19.6074</td><td style=\"text-align: right;\">2500</td><td style=\"text-align: right;\">-1331.32</td><td style=\"text-align: right;\">            -781.121</td><td style=\"text-align: right;\">            -1796.95</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_Pendulum-v0_6b1dd_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-13_20-55-15\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_reward_max: -781.1207639597676\n",
      "  episode_reward_mean: -1398.030697460801\n",
      "  episode_reward_min: -1796.9534032278605\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 16\n",
      "  experiment_id: 43107a19371e49529eb221515032f0aa\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 3500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: -0.1660407930612564\n",
      "        mean_q: -18.283344268798828\n",
      "        min_q: -33.41071319580078\n",
      "        model: {}\n",
      "    num_steps_sampled: 3500\n",
      "    num_steps_trained: 256256\n",
      "    num_target_updates: 1001\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.1125\n",
      "    ram_util_percent: 64.00416666666666\n",
      "  pid: 17553\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09813306505833767\n",
      "    mean_env_wait_ms: 0.1444267892224933\n",
      "    mean_inference_ms: 0.7552609488782389\n",
      "    mean_raw_obs_processing_ms: 0.22187214395356114\n",
      "  time_since_restore: 36.444782733917236\n",
      "  time_this_iter_s: 16.837361812591553\n",
      "  time_total_s: 36.444782733917236\n",
      "  timers:\n",
      "    learn_throughput: 67208.837\n",
      "    learn_time_ms: 3.809\n",
      "    update_time_ms: 2.164\n",
      "  timestamp: 1626227715\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3500\n",
      "  training_iteration: 3\n",
      "  trial_id: 6b1dd_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/12 CPUs, 0/0 GPUs, 0.0/3.81 GiB heap, 0.0/1.32 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_Pendulum-v0_6b1dd_00000</td><td>RUNNING </td><td>192.168.0.23:17553</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         36.4448</td><td style=\"text-align: right;\">3500</td><td style=\"text-align: right;\">-1398.03</td><td style=\"text-align: right;\">            -781.121</td><td style=\"text-align: right;\">            -1796.95</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_Pendulum-v0_6b1dd_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-13_20-55-32\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_reward_max: -781.1207639597676\n",
      "  episode_reward_mean: -1425.8668538197078\n",
      "  episode_reward_min: -1796.9534032278605\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 22\n",
      "  experiment_id: 43107a19371e49529eb221515032f0aa\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 4500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: -0.6811314225196838\n",
      "        mean_q: -23.862667083740234\n",
      "        min_q: -44.24247360229492\n",
      "        model: {}\n",
      "    num_steps_sampled: 4500\n",
      "    num_steps_trained: 384256\n",
      "    num_target_updates: 1501\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.125\n",
      "    ram_util_percent: 64.6375\n",
      "  pid: 17553\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09838679174479754\n",
      "    mean_env_wait_ms: 0.1443084430306832\n",
      "    mean_inference_ms: 0.7492579731789636\n",
      "    mean_raw_obs_processing_ms: 0.22119187406960872\n",
      "  time_since_restore: 53.642000675201416\n",
      "  time_this_iter_s: 17.19721794128418\n",
      "  time_total_s: 53.642000675201416\n",
      "  timers:\n",
      "    learn_throughput: 69144.745\n",
      "    learn_time_ms: 3.702\n",
      "    update_time_ms: 2.206\n",
      "  timestamp: 1626227732\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4500\n",
      "  training_iteration: 4\n",
      "  trial_id: 6b1dd_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/12 CPUs, 0/0 GPUs, 0.0/3.81 GiB heap, 0.0/1.32 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_Pendulum-v0_6b1dd_00000</td><td>RUNNING </td><td>192.168.0.23:17553</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">          53.642</td><td style=\"text-align: right;\">4500</td><td style=\"text-align: right;\">-1425.87</td><td style=\"text-align: right;\">            -781.121</td><td style=\"text-align: right;\">            -1796.95</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_Pendulum-v0_6b1dd_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-13_20-55-50\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_reward_max: -781.1207639597676\n",
      "  episode_reward_mean: -1444.8272991160627\n",
      "  episode_reward_min: -1796.9534032278605\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 26\n",
      "  experiment_id: 43107a19371e49529eb221515032f0aa\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 5500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: 0.3510587811470032\n",
      "        mean_q: -29.40019416809082\n",
      "        min_q: -47.306602478027344\n",
      "        model: {}\n",
      "    num_steps_sampled: 5500\n",
      "    num_steps_trained: 512256\n",
      "    num_target_updates: 2001\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.332000000000004\n",
      "    ram_util_percent: 64.232\n",
      "  pid: 17553\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09862273063120153\n",
      "    mean_env_wait_ms: 0.14439809792260078\n",
      "    mean_inference_ms: 0.7471738745456697\n",
      "    mean_raw_obs_processing_ms: 0.22106400627231673\n",
      "  time_since_restore: 71.59774255752563\n",
      "  time_this_iter_s: 17.95574188232422\n",
      "  time_total_s: 71.59774255752563\n",
      "  timers:\n",
      "    learn_throughput: 66434.142\n",
      "    learn_time_ms: 3.853\n",
      "    update_time_ms: 2.271\n",
      "  timestamp: 1626227750\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 5500\n",
      "  training_iteration: 5\n",
      "  trial_id: 6b1dd_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/12 CPUs, 0/0 GPUs, 0.0/3.81 GiB heap, 0.0/1.32 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_Pendulum-v0_6b1dd_00000</td><td>RUNNING </td><td>192.168.0.23:17553</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         71.5977</td><td style=\"text-align: right;\">5500</td><td style=\"text-align: right;\">-1444.83</td><td style=\"text-align: right;\">            -781.121</td><td style=\"text-align: right;\">            -1796.95</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_Pendulum-v0_6b1dd_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-13_20-56-07\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_reward_max: -781.1207639597676\n",
      "  episode_reward_mean: -1436.744265061477\n",
      "  episode_reward_min: -1796.9534032278605\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 32\n",
      "  experiment_id: 43107a19371e49529eb221515032f0aa\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 6500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: -0.09803511202335358\n",
      "        mean_q: -34.90961456298828\n",
      "        min_q: -63.530277252197266\n",
      "        model: {}\n",
      "    num_steps_sampled: 6500\n",
      "    num_steps_trained: 640256\n",
      "    num_target_updates: 2501\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.962500000000002\n",
      "    ram_util_percent: 63.50416666666667\n",
      "  pid: 17553\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09891302519653711\n",
      "    mean_env_wait_ms: 0.1445202853097181\n",
      "    mean_inference_ms: 0.7445685789103453\n",
      "    mean_raw_obs_processing_ms: 0.22094713838037758\n",
      "  time_since_restore: 89.20183849334717\n",
      "  time_this_iter_s: 17.604095935821533\n",
      "  time_total_s: 89.20183849334717\n",
      "  timers:\n",
      "    learn_throughput: 66701.153\n",
      "    learn_time_ms: 3.838\n",
      "    update_time_ms: 2.108\n",
      "  timestamp: 1626227767\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 6500\n",
      "  training_iteration: 6\n",
      "  trial_id: 6b1dd_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/12 CPUs, 0/0 GPUs, 0.0/3.81 GiB heap, 0.0/1.32 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_Pendulum-v0_6b1dd_00000</td><td>RUNNING </td><td>192.168.0.23:17553</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         89.2018</td><td style=\"text-align: right;\">6500</td><td style=\"text-align: right;\">-1436.74</td><td style=\"text-align: right;\">            -781.121</td><td style=\"text-align: right;\">            -1796.95</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_Pendulum-v0_6b1dd_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-13_20-56-26\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_reward_max: -781.1207639597676\n",
      "  episode_reward_mean: -1438.6196838828807\n",
      "  episode_reward_min: -1796.9534032278605\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 36\n",
      "  experiment_id: 43107a19371e49529eb221515032f0aa\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 7500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: 0.008759599179029465\n",
      "        mean_q: -40.90607452392578\n",
      "        min_q: -63.068790435791016\n",
      "        model: {}\n",
      "    num_steps_sampled: 7500\n",
      "    num_steps_trained: 768256\n",
      "    num_target_updates: 3001\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.47307692307692\n",
      "    ram_util_percent: 65.36153846153846\n",
      "  pid: 17553\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09913483847547423\n",
      "    mean_env_wait_ms: 0.14468110819074503\n",
      "    mean_inference_ms: 0.7435151127284532\n",
      "    mean_raw_obs_processing_ms: 0.2210145012650037\n",
      "  time_since_restore: 107.37493371963501\n",
      "  time_this_iter_s: 18.173095226287842\n",
      "  time_total_s: 107.37493371963501\n",
      "  timers:\n",
      "    learn_throughput: 55955.82\n",
      "    learn_time_ms: 4.575\n",
      "    update_time_ms: 2.636\n",
      "  timestamp: 1626227786\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 7500\n",
      "  training_iteration: 7\n",
      "  trial_id: 6b1dd_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/12 CPUs, 0/0 GPUs, 0.0/3.81 GiB heap, 0.0/1.32 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_Pendulum-v0_6b1dd_00000</td><td>RUNNING </td><td>192.168.0.23:17553</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         107.375</td><td style=\"text-align: right;\">7500</td><td style=\"text-align: right;\">-1438.62</td><td style=\"text-align: right;\">            -781.121</td><td style=\"text-align: right;\">            -1796.95</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_Pendulum-v0_6b1dd_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-13_20-56-44\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_reward_max: -781.1207639597676\n",
      "  episode_reward_mean: -1412.6190355939557\n",
      "  episode_reward_min: -1796.9534032278605\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 42\n",
      "  experiment_id: 43107a19371e49529eb221515032f0aa\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 8500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: 1.5346838235855103\n",
      "        mean_q: -44.83782958984375\n",
      "        min_q: -71.63545227050781\n",
      "        model: {}\n",
      "    num_steps_sampled: 8500\n",
      "    num_steps_trained: 896256\n",
      "    num_target_updates: 3501\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.483999999999995\n",
      "    ram_util_percent: 60.803999999999995\n",
      "  pid: 17553\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09941605964921256\n",
      "    mean_env_wait_ms: 0.14489890198444585\n",
      "    mean_inference_ms: 0.742334410942045\n",
      "    mean_raw_obs_processing_ms: 0.2211894070115637\n",
      "  time_since_restore: 125.71777677536011\n",
      "  time_this_iter_s: 18.342843055725098\n",
      "  time_total_s: 125.71777677536011\n",
      "  timers:\n",
      "    learn_throughput: 66089.841\n",
      "    learn_time_ms: 3.874\n",
      "    update_time_ms: 2.326\n",
      "  timestamp: 1626227804\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 8500\n",
      "  training_iteration: 8\n",
      "  trial_id: 6b1dd_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.7/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/12 CPUs, 0/0 GPUs, 0.0/3.81 GiB heap, 0.0/1.32 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_Pendulum-v0_6b1dd_00000</td><td>RUNNING </td><td>192.168.0.23:17553</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         125.718</td><td style=\"text-align: right;\">8500</td><td style=\"text-align: right;\">-1412.62</td><td style=\"text-align: right;\">            -781.121</td><td style=\"text-align: right;\">            -1796.95</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_Pendulum-v0_6b1dd_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-13_20-57-02\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_reward_max: -781.1207639597676\n",
      "  episode_reward_mean: -1398.6154951909177\n",
      "  episode_reward_min: -1796.9534032278605\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 46\n",
      "  experiment_id: 43107a19371e49529eb221515032f0aa\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 9500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: -0.022161278873682022\n",
      "        mean_q: -50.20433044433594\n",
      "        min_q: -73.2963638305664\n",
      "        model: {}\n",
      "    num_steps_sampled: 9500\n",
      "    num_steps_trained: 1024256\n",
      "    num_target_updates: 4001\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.676\n",
      "    ram_util_percent: 60.964\n",
      "  pid: 17553\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09955409264438625\n",
      "    mean_env_wait_ms: 0.14501209496502157\n",
      "    mean_inference_ms: 0.7415564006076368\n",
      "    mean_raw_obs_processing_ms: 0.22126649260221518\n",
      "  time_since_restore: 143.43251085281372\n",
      "  time_this_iter_s: 17.714734077453613\n",
      "  time_total_s: 143.43251085281372\n",
      "  timers:\n",
      "    learn_throughput: 63325.184\n",
      "    learn_time_ms: 4.043\n",
      "    update_time_ms: 2.284\n",
      "  timestamp: 1626227822\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 9500\n",
      "  training_iteration: 9\n",
      "  trial_id: 6b1dd_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/12 CPUs, 0/0 GPUs, 0.0/3.81 GiB heap, 0.0/1.32 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_Pendulum-v0_6b1dd_00000</td><td>RUNNING </td><td>192.168.0.23:17553</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         143.433</td><td style=\"text-align: right;\">9500</td><td style=\"text-align: right;\">-1398.62</td><td style=\"text-align: right;\">            -781.121</td><td style=\"text-align: right;\">            -1796.95</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_Pendulum-v0_6b1dd_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-13_20-57-20\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_reward_max: -11.722382280782014\n",
      "  episode_reward_mean: -1348.1319338104424\n",
      "  episode_reward_min: -1796.9534032278605\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 52\n",
      "  experiment_id: 43107a19371e49529eb221515032f0aa\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 10500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: 0.9547820687294006\n",
      "        mean_q: -53.53691864013672\n",
      "        min_q: -75.73155975341797\n",
      "        model: {}\n",
      "    num_steps_sampled: 10500\n",
      "    num_steps_trained: 1152256\n",
      "    num_target_updates: 4501\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.58076923076923\n",
      "    ram_util_percent: 62.80384615384616\n",
      "  pid: 17553\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09973805000463255\n",
      "    mean_env_wait_ms: 0.14517130740979722\n",
      "    mean_inference_ms: 0.7405815724945874\n",
      "    mean_raw_obs_processing_ms: 0.221378329083068\n",
      "  time_since_restore: 161.5978705883026\n",
      "  time_this_iter_s: 18.16535973548889\n",
      "  time_total_s: 161.5978705883026\n",
      "  timers:\n",
      "    learn_throughput: 61002.512\n",
      "    learn_time_ms: 4.197\n",
      "    update_time_ms: 2.488\n",
      "  timestamp: 1626227840\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10500\n",
      "  training_iteration: 10\n",
      "  trial_id: 6b1dd_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/12 CPUs, 0/0 GPUs, 0.0/3.81 GiB heap, 0.0/1.32 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_Pendulum-v0_6b1dd_00000</td><td>RUNNING </td><td>192.168.0.23:17553</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         161.598</td><td style=\"text-align: right;\">10500</td><td style=\"text-align: right;\">-1348.13</td><td style=\"text-align: right;\">            -11.7224</td><td style=\"text-align: right;\">            -1796.95</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_Pendulum-v0_6b1dd_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-13_20-57-38\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_reward_max: -11.722382280782014\n",
      "  episode_reward_mean: -1336.2461581544399\n",
      "  episode_reward_min: -1796.9534032278605\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 56\n",
      "  experiment_id: 43107a19371e49529eb221515032f0aa\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 11500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: -1.291930079460144\n",
      "        mean_q: -56.992698669433594\n",
      "        min_q: -83.67649841308594\n",
      "        model: {}\n",
      "    num_steps_sampled: 11500\n",
      "    num_steps_trained: 1280256\n",
      "    num_target_updates: 5001\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.019999999999996\n",
      "    ram_util_percent: 63.53200000000001\n",
      "  pid: 17553\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09984162821424539\n",
      "    mean_env_wait_ms: 0.1452610650848398\n",
      "    mean_inference_ms: 0.7400414020223257\n",
      "    mean_raw_obs_processing_ms: 0.22145066812292358\n",
      "  time_since_restore: 179.77703475952148\n",
      "  time_this_iter_s: 18.179164171218872\n",
      "  time_total_s: 179.77703475952148\n",
      "  timers:\n",
      "    learn_throughput: 55744.915\n",
      "    learn_time_ms: 4.592\n",
      "    update_time_ms: 2.283\n",
      "  timestamp: 1626227858\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 11500\n",
      "  training_iteration: 11\n",
      "  trial_id: 6b1dd_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/12 CPUs, 0/0 GPUs, 0.0/3.81 GiB heap, 0.0/1.32 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_Pendulum-v0_6b1dd_00000</td><td>RUNNING </td><td>192.168.0.23:17553</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         179.777</td><td style=\"text-align: right;\">11500</td><td style=\"text-align: right;\">-1336.25</td><td style=\"text-align: right;\">            -11.7224</td><td style=\"text-align: right;\">            -1796.95</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_Pendulum-v0_6b1dd_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-13_20-57-57\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_reward_max: -11.722382280782014\n",
      "  episode_reward_mean: -1288.7165801323201\n",
      "  episode_reward_min: -1796.9534032278605\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 62\n",
      "  experiment_id: 43107a19371e49529eb221515032f0aa\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 12500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: 1.1449592113494873\n",
      "        mean_q: -62.394222259521484\n",
      "        min_q: -90.33869934082031\n",
      "        model: {}\n",
      "    num_steps_sampled: 12500\n",
      "    num_steps_trained: 1408256\n",
      "    num_target_updates: 5501\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.78846153846154\n",
      "    ram_util_percent: 61.83461538461539\n",
      "  pid: 17553\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09998511732421295\n",
      "    mean_env_wait_ms: 0.14538881263770745\n",
      "    mean_inference_ms: 0.7394246003424146\n",
      "    mean_raw_obs_processing_ms: 0.22156465250994076\n",
      "  time_since_restore: 198.1817545890808\n",
      "  time_this_iter_s: 18.404719829559326\n",
      "  time_total_s: 198.1817545890808\n",
      "  timers:\n",
      "    learn_throughput: 54282.297\n",
      "    learn_time_ms: 4.716\n",
      "    update_time_ms: 2.672\n",
      "  timestamp: 1626227877\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12500\n",
      "  training_iteration: 12\n",
      "  trial_id: 6b1dd_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/12 CPUs, 0/0 GPUs, 0.0/3.81 GiB heap, 0.0/1.32 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_Pendulum-v0_6b1dd_00000</td><td>RUNNING </td><td>192.168.0.23:17553</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         198.182</td><td style=\"text-align: right;\">12500</td><td style=\"text-align: right;\">-1288.72</td><td style=\"text-align: right;\">            -11.7224</td><td style=\"text-align: right;\">            -1796.95</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_Pendulum-v0_6b1dd_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-13_20-58-14\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_reward_max: -11.722382280782014\n",
      "  episode_reward_mean: -1269.864064156319\n",
      "  episode_reward_min: -1796.9534032278605\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 66\n",
      "  experiment_id: 43107a19371e49529eb221515032f0aa\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 13500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: 0.9233301877975464\n",
      "        mean_q: -62.598182678222656\n",
      "        min_q: -89.0087890625\n",
      "        model: {}\n",
      "    num_steps_sampled: 13500\n",
      "    num_steps_trained: 1536256\n",
      "    num_target_updates: 6001\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.896\n",
      "    ram_util_percent: 62.232\n",
      "  pid: 17553\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10005915854217143\n",
      "    mean_env_wait_ms: 0.14545422921286064\n",
      "    mean_inference_ms: 0.739005902172205\n",
      "    mean_raw_obs_processing_ms: 0.22161749281606014\n",
      "  time_since_restore: 216.09442567825317\n",
      "  time_this_iter_s: 17.912671089172363\n",
      "  time_total_s: 216.09442567825317\n",
      "  timers:\n",
      "    learn_throughput: 66939.841\n",
      "    learn_time_ms: 3.824\n",
      "    update_time_ms: 2.219\n",
      "  timestamp: 1626227894\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 13500\n",
      "  training_iteration: 13\n",
      "  trial_id: 6b1dd_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/12 CPUs, 0/0 GPUs, 0.0/3.81 GiB heap, 0.0/1.32 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_Pendulum-v0_6b1dd_00000</td><td>RUNNING </td><td>192.168.0.23:17553</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         216.094</td><td style=\"text-align: right;\">13500</td><td style=\"text-align: right;\">-1269.86</td><td style=\"text-align: right;\">            -11.7224</td><td style=\"text-align: right;\">            -1796.95</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_Pendulum-v0_6b1dd_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-13_20-58-33\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_reward_max: -3.1396012759307346\n",
      "  episode_reward_mean: -1193.5720942077833\n",
      "  episode_reward_min: -1796.9534032278605\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 72\n",
      "  experiment_id: 43107a19371e49529eb221515032f0aa\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 14500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: 2.8817272186279297\n",
      "        mean_q: -63.85460662841797\n",
      "        min_q: -94.13409423828125\n",
      "        model: {}\n",
      "    num_steps_sampled: 14500\n",
      "    num_steps_trained: 1664256\n",
      "    num_target_updates: 6501\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.12\n",
      "    ram_util_percent: 63.36\n",
      "  pid: 17553\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1001575898790833\n",
      "    mean_env_wait_ms: 0.14555066669231087\n",
      "    mean_inference_ms: 0.7384886316517281\n",
      "    mean_raw_obs_processing_ms: 0.22168737744292352\n",
      "  time_since_restore: 234.4201078414917\n",
      "  time_this_iter_s: 18.325682163238525\n",
      "  time_total_s: 234.4201078414917\n",
      "  timers:\n",
      "    learn_throughput: 70082.554\n",
      "    learn_time_ms: 3.653\n",
      "    update_time_ms: 2.176\n",
      "  timestamp: 1626227913\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 14500\n",
      "  training_iteration: 14\n",
      "  trial_id: 6b1dd_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/12 CPUs, 0/0 GPUs, 0.0/3.81 GiB heap, 0.0/1.32 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_Pendulum-v0_6b1dd_00000</td><td>RUNNING </td><td>192.168.0.23:17553</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">          234.42</td><td style=\"text-align: right;\">14500</td><td style=\"text-align: right;\">-1193.57</td><td style=\"text-align: right;\">             -3.1396</td><td style=\"text-align: right;\">            -1796.95</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_Pendulum-v0_6b1dd_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-13_20-58-51\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_reward_max: -3.1396012759307346\n",
      "  episode_reward_mean: -1179.2687786561391\n",
      "  episode_reward_min: -1796.9534032278605\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 76\n",
      "  experiment_id: 43107a19371e49529eb221515032f0aa\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 15500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: 2.4560861587524414\n",
      "        mean_q: -63.485443115234375\n",
      "        min_q: -99.68254089355469\n",
      "        model: {}\n",
      "    num_steps_sampled: 15500\n",
      "    num_steps_trained: 1792256\n",
      "    num_target_updates: 7001\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.6\n",
      "    ram_util_percent: 63.26\n",
      "  pid: 17553\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10020802807056446\n",
      "    mean_env_wait_ms: 0.14559826169718723\n",
      "    mean_inference_ms: 0.7381199811772466\n",
      "    mean_raw_obs_processing_ms: 0.22171206407209648\n",
      "  time_since_restore: 252.2317407131195\n",
      "  time_this_iter_s: 17.811632871627808\n",
      "  time_total_s: 252.2317407131195\n",
      "  timers:\n",
      "    learn_throughput: 72884.011\n",
      "    learn_time_ms: 3.512\n",
      "    update_time_ms: 2.139\n",
      "  timestamp: 1626227931\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 15500\n",
      "  training_iteration: 15\n",
      "  trial_id: 6b1dd_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/12 CPUs, 0/0 GPUs, 0.0/3.81 GiB heap, 0.0/1.32 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_Pendulum-v0_6b1dd_00000</td><td>RUNNING </td><td>192.168.0.23:17553</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         252.232</td><td style=\"text-align: right;\">15500</td><td style=\"text-align: right;\">-1179.27</td><td style=\"text-align: right;\">             -3.1396</td><td style=\"text-align: right;\">            -1796.95</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_Pendulum-v0_6b1dd_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-13_20-59-08\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_reward_max: -1.8323446584304053\n",
      "  episode_reward_mean: -1113.7970069430512\n",
      "  episode_reward_min: -1796.9534032278605\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 82\n",
      "  experiment_id: 43107a19371e49529eb221515032f0aa\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 16500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: 3.410090684890747\n",
      "        mean_q: -70.59851837158203\n",
      "        min_q: -103.12985229492188\n",
      "        model: {}\n",
      "    num_steps_sampled: 16500\n",
      "    num_steps_trained: 1920256\n",
      "    num_target_updates: 7501\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.516\n",
      "    ram_util_percent: 63.868\n",
      "  pid: 17553\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10026239466162451\n",
      "    mean_env_wait_ms: 0.14564343205419253\n",
      "    mean_inference_ms: 0.7375129405851482\n",
      "    mean_raw_obs_processing_ms: 0.22171667486775146\n",
      "  time_since_restore: 269.8548905849457\n",
      "  time_this_iter_s: 17.623149871826172\n",
      "  time_total_s: 269.8548905849457\n",
      "  timers:\n",
      "    learn_throughput: 68509.454\n",
      "    learn_time_ms: 3.737\n",
      "    update_time_ms: 2.178\n",
      "  timestamp: 1626227948\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 16500\n",
      "  training_iteration: 16\n",
      "  trial_id: 6b1dd_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/12 CPUs, 0/0 GPUs, 0.0/3.81 GiB heap, 0.0/1.32 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_Pendulum-v0_6b1dd_00000</td><td>RUNNING </td><td>192.168.0.23:17553</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         269.855</td><td style=\"text-align: right;\">16500</td><td style=\"text-align: right;\"> -1113.8</td><td style=\"text-align: right;\">            -1.83234</td><td style=\"text-align: right;\">            -1796.95</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_Pendulum-v0_6b1dd_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-13_20-59-26\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_reward_max: -1.8323446584304053\n",
      "  episode_reward_mean: -1085.0581441625716\n",
      "  episode_reward_min: -1796.9534032278605\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 86\n",
      "  experiment_id: 43107a19371e49529eb221515032f0aa\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 17500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: 3.329087018966675\n",
      "        mean_q: -69.15478515625\n",
      "        min_q: -105.27165222167969\n",
      "        model: {}\n",
      "    num_steps_sampled: 17500\n",
      "    num_steps_trained: 2048256\n",
      "    num_target_updates: 8001\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.852000000000004\n",
      "    ram_util_percent: 61.42\n",
      "  pid: 17553\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10029038455152726\n",
      "    mean_env_wait_ms: 0.145662359366662\n",
      "    mean_inference_ms: 0.7371023761861941\n",
      "    mean_raw_obs_processing_ms: 0.22170672561481933\n",
      "  time_since_restore: 287.5606265068054\n",
      "  time_this_iter_s: 17.70573592185974\n",
      "  time_total_s: 287.5606265068054\n",
      "  timers:\n",
      "    learn_throughput: 68548.818\n",
      "    learn_time_ms: 3.735\n",
      "    update_time_ms: 2.23\n",
      "  timestamp: 1626227966\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 17500\n",
      "  training_iteration: 17\n",
      "  trial_id: 6b1dd_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/12 CPUs, 0/0 GPUs, 0.0/3.81 GiB heap, 0.0/1.32 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_Pendulum-v0_6b1dd_00000</td><td>RUNNING </td><td>192.168.0.23:17553</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         287.561</td><td style=\"text-align: right;\">17500</td><td style=\"text-align: right;\">-1085.06</td><td style=\"text-align: right;\">            -1.83234</td><td style=\"text-align: right;\">            -1796.95</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_Pendulum-v0_6b1dd_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-13_20-59-44\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_reward_max: -1.1116457547643066\n",
      "  episode_reward_mean: -1034.789899227471\n",
      "  episode_reward_min: -1796.9534032278605\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 92\n",
      "  experiment_id: 43107a19371e49529eb221515032f0aa\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 18500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: 3.6690609455108643\n",
      "        mean_q: -67.78599548339844\n",
      "        min_q: -111.97421264648438\n",
      "        model: {}\n",
      "    num_steps_sampled: 18500\n",
      "    num_steps_trained: 2176256\n",
      "    num_target_updates: 8501\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.984\n",
      "    ram_util_percent: 61.907999999999994\n",
      "  pid: 17553\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10032541318647033\n",
      "    mean_env_wait_ms: 0.14568362066268506\n",
      "    mean_inference_ms: 0.7365277332727878\n",
      "    mean_raw_obs_processing_ms: 0.22168686968758322\n",
      "  time_since_restore: 305.71143412590027\n",
      "  time_this_iter_s: 18.15080761909485\n",
      "  time_total_s: 305.71143412590027\n",
      "  timers:\n",
      "    learn_throughput: 69610.491\n",
      "    learn_time_ms: 3.678\n",
      "    update_time_ms: 2.14\n",
      "  timestamp: 1626227984\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 18500\n",
      "  training_iteration: 18\n",
      "  trial_id: 6b1dd_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/12 CPUs, 0/0 GPUs, 0.0/3.81 GiB heap, 0.0/1.32 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_Pendulum-v0_6b1dd_00000</td><td>RUNNING </td><td>192.168.0.23:17553</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">         305.711</td><td style=\"text-align: right;\">18500</td><td style=\"text-align: right;\">-1034.79</td><td style=\"text-align: right;\">            -1.11165</td><td style=\"text-align: right;\">            -1796.95</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_Pendulum-v0_6b1dd_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-13_21-00-02\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_reward_max: -1.1116457547643066\n",
      "  episode_reward_mean: -997.122177864407\n",
      "  episode_reward_min: -1796.9534032278605\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 96\n",
      "  experiment_id: 43107a19371e49529eb221515032f0aa\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 19500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: 5.729738235473633\n",
      "        mean_q: -72.16944122314453\n",
      "        min_q: -117.57238006591797\n",
      "        model: {}\n",
      "    num_steps_sampled: 19500\n",
      "    num_steps_trained: 2304256\n",
      "    num_target_updates: 9001\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.772\n",
      "    ram_util_percent: 63.343999999999994\n",
      "  pid: 17553\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10034120140496124\n",
      "    mean_env_wait_ms: 0.1456915975727191\n",
      "    mean_inference_ms: 0.7361423802744369\n",
      "    mean_raw_obs_processing_ms: 0.22166756125687728\n",
      "  time_since_restore: 323.59876918792725\n",
      "  time_this_iter_s: 17.887335062026978\n",
      "  time_total_s: 323.59876918792725\n",
      "  timers:\n",
      "    learn_throughput: 71712.349\n",
      "    learn_time_ms: 3.57\n",
      "    update_time_ms: 2.143\n",
      "  timestamp: 1626228002\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 19500\n",
      "  training_iteration: 19\n",
      "  trial_id: 6b1dd_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/12 CPUs, 0/0 GPUs, 0.0/3.81 GiB heap, 0.0/1.32 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_Pendulum-v0_6b1dd_00000</td><td>RUNNING </td><td>192.168.0.23:17553</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         323.599</td><td style=\"text-align: right;\">19500</td><td style=\"text-align: right;\">-997.122</td><td style=\"text-align: right;\">            -1.11165</td><td style=\"text-align: right;\">            -1796.95</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_Pendulum-v0_6b1dd_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-13_21-00-20\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_reward_max: -1.1116457547643066\n",
      "  episode_reward_mean: -962.5109391533188\n",
      "  episode_reward_min: -1796.9534032278605\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 102\n",
      "  experiment_id: 43107a19371e49529eb221515032f0aa\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 20500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: 5.6856279373168945\n",
      "        mean_q: -70.61603546142578\n",
      "        min_q: -114.73045349121094\n",
      "        model: {}\n",
      "    num_steps_sampled: 20500\n",
      "    num_steps_trained: 2432256\n",
      "    num_target_updates: 9501\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.34\n",
      "    ram_util_percent: 63.876\n",
      "  pid: 17553\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10039843358964308\n",
      "    mean_env_wait_ms: 0.14570528559035256\n",
      "    mean_inference_ms: 0.7345797772574815\n",
      "    mean_raw_obs_processing_ms: 0.22154705123683371\n",
      "  time_since_restore: 341.51777505874634\n",
      "  time_this_iter_s: 17.919005870819092\n",
      "  time_total_s: 341.51777505874634\n",
      "  timers:\n",
      "    learn_throughput: 74080.281\n",
      "    learn_time_ms: 3.456\n",
      "    update_time_ms: 2.081\n",
      "  timestamp: 1626228020\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 20500\n",
      "  training_iteration: 20\n",
      "  trial_id: 6b1dd_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/12 CPUs, 0/0 GPUs, 0.0/3.81 GiB heap, 0.0/1.32 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_Pendulum-v0_6b1dd_00000</td><td>RUNNING </td><td>192.168.0.23:17553</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         341.518</td><td style=\"text-align: right;\">20500</td><td style=\"text-align: right;\">-962.511</td><td style=\"text-align: right;\">            -1.11165</td><td style=\"text-align: right;\">            -1796.95</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_Pendulum-v0_6b1dd_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-13_21-00-38\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_reward_max: -1.1116457547643066\n",
      "  episode_reward_mean: -926.8117919657061\n",
      "  episode_reward_min: -1796.9534032278605\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 106\n",
      "  experiment_id: 43107a19371e49529eb221515032f0aa\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 21500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: 4.623827934265137\n",
      "        mean_q: -73.03113555908203\n",
      "        min_q: -121.44898986816406\n",
      "        model: {}\n",
      "    num_steps_sampled: 21500\n",
      "    num_steps_trained: 2560256\n",
      "    num_target_updates: 10001\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.892\n",
      "    ram_util_percent: 64.264\n",
      "  pid: 17553\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10054363674887985\n",
      "    mean_env_wait_ms: 0.14575424884632088\n",
      "    mean_inference_ms: 0.7328909911016142\n",
      "    mean_raw_obs_processing_ms: 0.22151258708781435\n",
      "  time_since_restore: 359.28413009643555\n",
      "  time_this_iter_s: 17.76635503768921\n",
      "  time_total_s: 359.28413009643555\n",
      "  timers:\n",
      "    learn_throughput: 71191.236\n",
      "    learn_time_ms: 3.596\n",
      "    update_time_ms: 2.169\n",
      "  timestamp: 1626228038\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 21500\n",
      "  training_iteration: 21\n",
      "  trial_id: 6b1dd_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/12 CPUs, 0/0 GPUs, 0.0/3.81 GiB heap, 0.0/1.32 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_Pendulum-v0_6b1dd_00000</td><td>RUNNING </td><td>192.168.0.23:17553</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">         359.284</td><td style=\"text-align: right;\">21500</td><td style=\"text-align: right;\">-926.812</td><td style=\"text-align: right;\">            -1.11165</td><td style=\"text-align: right;\">            -1796.95</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_Pendulum-v0_6b1dd_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-13_21-00-56\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_reward_max: -1.1116457547643066\n",
      "  episode_reward_mean: -845.3818124377199\n",
      "  episode_reward_min: -1702.4878189939052\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 112\n",
      "  experiment_id: 43107a19371e49529eb221515032f0aa\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 22500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: 5.559916973114014\n",
      "        mean_q: -77.15824890136719\n",
      "        min_q: -124.76898956298828\n",
      "        model: {}\n",
      "    num_steps_sampled: 22500\n",
      "    num_steps_trained: 2688256\n",
      "    num_target_updates: 10501\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.011538461538464\n",
      "    ram_util_percent: 62.280769230769245\n",
      "  pid: 17553\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10065893392522074\n",
      "    mean_env_wait_ms: 0.14581588006225654\n",
      "    mean_inference_ms: 0.731451783940171\n",
      "    mean_raw_obs_processing_ms: 0.2214408051532333\n",
      "  time_since_restore: 377.5080909729004\n",
      "  time_this_iter_s: 18.223960876464844\n",
      "  time_total_s: 377.5080909729004\n",
      "  timers:\n",
      "    learn_throughput: 70330.438\n",
      "    learn_time_ms: 3.64\n",
      "    update_time_ms: 2.2\n",
      "  timestamp: 1626228056\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 22500\n",
      "  training_iteration: 22\n",
      "  trial_id: 6b1dd_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/12 CPUs, 0/0 GPUs, 0.0/3.81 GiB heap, 0.0/1.32 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_Pendulum-v0_6b1dd_00000</td><td>RUNNING </td><td>192.168.0.23:17553</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    22</td><td style=\"text-align: right;\">         377.508</td><td style=\"text-align: right;\">22500</td><td style=\"text-align: right;\">-845.382</td><td style=\"text-align: right;\">            -1.11165</td><td style=\"text-align: right;\">            -1702.49</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_Pendulum-v0_6b1dd_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-13_21-01-14\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_reward_max: -1.1116457547643066\n",
      "  episode_reward_mean: -789.2500149746359\n",
      "  episode_reward_min: -1702.4878189939052\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 116\n",
      "  experiment_id: 43107a19371e49529eb221515032f0aa\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 23500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: 5.940964221954346\n",
      "        mean_q: -75.83016967773438\n",
      "        min_q: -128.67138671875\n",
      "        model: {}\n",
      "    num_steps_sampled: 23500\n",
      "    num_steps_trained: 2816256\n",
      "    num_target_updates: 11001\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.7\n",
      "    ram_util_percent: 61.724\n",
      "  pid: 17553\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1007344092037897\n",
      "    mean_env_wait_ms: 0.14590177367580778\n",
      "    mean_inference_ms: 0.7310117294745901\n",
      "    mean_raw_obs_processing_ms: 0.2214846485799835\n",
      "  time_since_restore: 395.6538829803467\n",
      "  time_this_iter_s: 18.14579200744629\n",
      "  time_total_s: 395.6538829803467\n",
      "  timers:\n",
      "    learn_throughput: 64673.113\n",
      "    learn_time_ms: 3.958\n",
      "    update_time_ms: 2.452\n",
      "  timestamp: 1626228074\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 23500\n",
      "  training_iteration: 23\n",
      "  trial_id: 6b1dd_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/12 CPUs, 0/0 GPUs, 0.0/3.81 GiB heap, 0.0/1.32 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_Pendulum-v0_6b1dd_00000</td><td>RUNNING </td><td>192.168.0.23:17553</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    23</td><td style=\"text-align: right;\">         395.654</td><td style=\"text-align: right;\">23500</td><td style=\"text-align: right;\"> -789.25</td><td style=\"text-align: right;\">            -1.11165</td><td style=\"text-align: right;\">            -1702.49</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_Pendulum-v0_6b1dd_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-13_21-01-33\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_reward_max: -1.1116457547643066\n",
      "  episode_reward_mean: -711.1807312858703\n",
      "  episode_reward_min: -1702.4878189939052\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 122\n",
      "  experiment_id: 43107a19371e49529eb221515032f0aa\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 24500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: 5.302407264709473\n",
      "        mean_q: -76.78132629394531\n",
      "        min_q: -129.2058563232422\n",
      "        model: {}\n",
      "    num_steps_sampled: 24500\n",
      "    num_steps_trained: 2944256\n",
      "    num_target_updates: 11501\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.71153846153846\n",
      "    ram_util_percent: 63.11538461538461\n",
      "  pid: 17553\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10082041446917875\n",
      "    mean_env_wait_ms: 0.1460076409820115\n",
      "    mean_inference_ms: 0.7305336008668726\n",
      "    mean_raw_obs_processing_ms: 0.22157216103520583\n",
      "  time_since_restore: 414.03240990638733\n",
      "  time_this_iter_s: 18.37852692604065\n",
      "  time_total_s: 414.03240990638733\n",
      "  timers:\n",
      "    learn_throughput: 69906.887\n",
      "    learn_time_ms: 3.662\n",
      "    update_time_ms: 2.684\n",
      "  timestamp: 1626228093\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 24500\n",
      "  training_iteration: 24\n",
      "  trial_id: 6b1dd_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/12 CPUs, 0/0 GPUs, 0.0/3.81 GiB heap, 0.0/1.32 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_Pendulum-v0_6b1dd_00000</td><td>RUNNING </td><td>192.168.0.23:17553</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    24</td><td style=\"text-align: right;\">         414.032</td><td style=\"text-align: right;\">24500</td><td style=\"text-align: right;\">-711.181</td><td style=\"text-align: right;\">            -1.11165</td><td style=\"text-align: right;\">            -1702.49</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_Pendulum-v0_6b1dd_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-13_21-01-50\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_reward_max: -1.1116457547643066\n",
      "  episode_reward_mean: -654.9789331537673\n",
      "  episode_reward_min: -1543.9091369921614\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 126\n",
      "  experiment_id: 43107a19371e49529eb221515032f0aa\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 25500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: 8.509936332702637\n",
      "        mean_q: -74.9183349609375\n",
      "        min_q: -133.55616760253906\n",
      "        model: {}\n",
      "    num_steps_sampled: 25500\n",
      "    num_steps_trained: 3072256\n",
      "    num_target_updates: 12001\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.025000000000002\n",
      "    ram_util_percent: 63.50416666666666\n",
      "  pid: 17553\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10083697897637284\n",
      "    mean_env_wait_ms: 0.14602871486581848\n",
      "    mean_inference_ms: 0.7300545922472584\n",
      "    mean_raw_obs_processing_ms: 0.22157011281195912\n",
      "  time_since_restore: 431.3214418888092\n",
      "  time_this_iter_s: 17.289031982421875\n",
      "  time_total_s: 431.3214418888092\n",
      "  timers:\n",
      "    learn_throughput: 75198.324\n",
      "    learn_time_ms: 3.404\n",
      "    update_time_ms: 1.789\n",
      "  timestamp: 1626228110\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 25500\n",
      "  training_iteration: 25\n",
      "  trial_id: 6b1dd_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/12 CPUs, 0/0 GPUs, 0.0/3.81 GiB heap, 0.0/1.32 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_Pendulum-v0_6b1dd_00000</td><td>RUNNING </td><td>192.168.0.23:17553</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         431.321</td><td style=\"text-align: right;\">25500</td><td style=\"text-align: right;\">-654.979</td><td style=\"text-align: right;\">            -1.11165</td><td style=\"text-align: right;\">            -1543.91</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_Pendulum-v0_6b1dd_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-13_21-02-09\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_reward_max: -1.1116457547643066\n",
      "  episode_reward_mean: -581.5873670837315\n",
      "  episode_reward_min: -1519.854635629977\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 132\n",
      "  experiment_id: 43107a19371e49529eb221515032f0aa\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 26500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: 5.026442050933838\n",
      "        mean_q: -76.33003234863281\n",
      "        min_q: -136.049560546875\n",
      "        model: {}\n",
      "    num_steps_sampled: 26500\n",
      "    num_steps_trained: 3200256\n",
      "    num_target_updates: 12501\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.35\n",
      "    ram_util_percent: 62.79615384615385\n",
      "  pid: 17553\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10085566443925359\n",
      "    mean_env_wait_ms: 0.14606064681301018\n",
      "    mean_inference_ms: 0.729549470409917\n",
      "    mean_raw_obs_processing_ms: 0.22157791304227836\n",
      "  time_since_restore: 450.27890610694885\n",
      "  time_this_iter_s: 18.95746421813965\n",
      "  time_total_s: 450.27890610694885\n",
      "  timers:\n",
      "    learn_throughput: 64951.657\n",
      "    learn_time_ms: 3.941\n",
      "    update_time_ms: 2.259\n",
      "  timestamp: 1626228129\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 26500\n",
      "  training_iteration: 26\n",
      "  trial_id: 6b1dd_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/12 CPUs, 0/0 GPUs, 0.0/3.81 GiB heap, 0.0/1.32 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_Pendulum-v0_6b1dd_00000</td><td>RUNNING </td><td>192.168.0.23:17553</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    26</td><td style=\"text-align: right;\">         450.279</td><td style=\"text-align: right;\">26500</td><td style=\"text-align: right;\">-581.587</td><td style=\"text-align: right;\">            -1.11165</td><td style=\"text-align: right;\">            -1519.85</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_Pendulum-v0_6b1dd_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-13_21-02-28\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_reward_max: -1.1116457547643066\n",
      "  episode_reward_mean: -543.6693148351875\n",
      "  episode_reward_min: -1519.854635629977\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 136\n",
      "  experiment_id: 43107a19371e49529eb221515032f0aa\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 27500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: 4.031546592712402\n",
      "        mean_q: -76.26606750488281\n",
      "        min_q: -138.93124389648438\n",
      "        model: {}\n",
      "    num_steps_sampled: 27500\n",
      "    num_steps_trained: 3328256\n",
      "    num_target_updates: 13001\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.355555555555558\n",
      "    ram_util_percent: 62.829629629629636\n",
      "  pid: 17553\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10084317243518033\n",
      "    mean_env_wait_ms: 0.14605039801312864\n",
      "    mean_inference_ms: 0.7291718178633722\n",
      "    mean_raw_obs_processing_ms: 0.2215448771838519\n",
      "  time_since_restore: 469.2216773033142\n",
      "  time_this_iter_s: 18.942771196365356\n",
      "  time_total_s: 469.2216773033142\n",
      "  timers:\n",
      "    learn_throughput: 66035.376\n",
      "    learn_time_ms: 3.877\n",
      "    update_time_ms: 2.228\n",
      "  timestamp: 1626228148\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 27500\n",
      "  training_iteration: 27\n",
      "  trial_id: 6b1dd_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/12 CPUs, 0/0 GPUs, 0.0/3.81 GiB heap, 0.0/1.32 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_Pendulum-v0_6b1dd_00000</td><td>RUNNING </td><td>192.168.0.23:17553</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    27</td><td style=\"text-align: right;\">         469.222</td><td style=\"text-align: right;\">27500</td><td style=\"text-align: right;\">-543.669</td><td style=\"text-align: right;\">            -1.11165</td><td style=\"text-align: right;\">            -1519.85</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_Pendulum-v0_6b1dd_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-13_21-02-47\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_reward_max: -1.1116457547643066\n",
      "  episode_reward_mean: -503.3479170812064\n",
      "  episode_reward_min: -1519.854635629977\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 142\n",
      "  experiment_id: 43107a19371e49529eb221515032f0aa\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 28500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: 8.385159492492676\n",
      "        mean_q: -79.76459503173828\n",
      "        min_q: -143.51950073242188\n",
      "        model: {}\n",
      "    num_steps_sampled: 28500\n",
      "    num_steps_trained: 3456256\n",
      "    num_target_updates: 13501\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.71153846153846\n",
      "    ram_util_percent: 62.20384615384616\n",
      "  pid: 17553\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10081609962945773\n",
      "    mean_env_wait_ms: 0.14602355893903582\n",
      "    mean_inference_ms: 0.7286173247160681\n",
      "    mean_raw_obs_processing_ms: 0.2214595482753488\n",
      "  time_since_restore: 488.01311016082764\n",
      "  time_this_iter_s: 18.791432857513428\n",
      "  time_total_s: 488.01311016082764\n",
      "  timers:\n",
      "    learn_throughput: 64560.344\n",
      "    learn_time_ms: 3.965\n",
      "    update_time_ms: 2.323\n",
      "  timestamp: 1626228167\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 28500\n",
      "  training_iteration: 28\n",
      "  trial_id: 6b1dd_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/12 CPUs, 0/0 GPUs, 0.0/3.81 GiB heap, 0.0/1.32 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_Pendulum-v0_6b1dd_00000</td><td>RUNNING </td><td>192.168.0.23:17553</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    28</td><td style=\"text-align: right;\">         488.013</td><td style=\"text-align: right;\">28500</td><td style=\"text-align: right;\">-503.348</td><td style=\"text-align: right;\">            -1.11165</td><td style=\"text-align: right;\">            -1519.85</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_Pendulum-v0_6b1dd_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-13_21-03-06\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_reward_max: -1.1116457547643066\n",
      "  episode_reward_mean: -461.2358136139652\n",
      "  episode_reward_min: -1519.854635629977\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 146\n",
      "  experiment_id: 43107a19371e49529eb221515032f0aa\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 29500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: 6.932671070098877\n",
      "        mean_q: -74.86430358886719\n",
      "        min_q: -146.92617797851562\n",
      "        model: {}\n",
      "    num_steps_sampled: 29500\n",
      "    num_steps_trained: 3584256\n",
      "    num_target_updates: 14001\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.81851851851852\n",
      "    ram_util_percent: 62.31481481481482\n",
      "  pid: 17553\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10080769151659849\n",
      "    mean_env_wait_ms: 0.1460119840536429\n",
      "    mean_inference_ms: 0.7283629026092139\n",
      "    mean_raw_obs_processing_ms: 0.2214165201194093\n",
      "  time_since_restore: 507.24014711380005\n",
      "  time_this_iter_s: 19.227036952972412\n",
      "  time_total_s: 507.24014711380005\n",
      "  timers:\n",
      "    learn_throughput: 52636.467\n",
      "    learn_time_ms: 4.864\n",
      "    update_time_ms: 2.394\n",
      "  timestamp: 1626228186\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 29500\n",
      "  training_iteration: 29\n",
      "  trial_id: 6b1dd_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/12 CPUs, 0/0 GPUs, 0.0/3.81 GiB heap, 0.0/1.32 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_Pendulum-v0_6b1dd_00000</td><td>RUNNING </td><td>192.168.0.23:17553</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    29</td><td style=\"text-align: right;\">          507.24</td><td style=\"text-align: right;\">29500</td><td style=\"text-align: right;\">-461.236</td><td style=\"text-align: right;\">            -1.11165</td><td style=\"text-align: right;\">            -1519.85</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_Pendulum-v0_6b1dd_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-13_21-03-25\n",
      "  done: true\n",
      "  episode_len_mean: 200.0\n",
      "  episode_reward_max: -1.1116457547643066\n",
      "  episode_reward_mean: -444.3119269581459\n",
      "  episode_reward_min: -1524.5879513507703\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 152\n",
      "  experiment_id: 43107a19371e49529eb221515032f0aa\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 30500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: 5.001977920532227\n",
      "        mean_q: -80.31802368164062\n",
      "        min_q: -147.42247009277344\n",
      "        model: {}\n",
      "    num_steps_sampled: 30500\n",
      "    num_steps_trained: 3712256\n",
      "    num_target_updates: 14501\n",
      "  iterations_since_restore: 30\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.751851851851853\n",
      "    ram_util_percent: 63.059259259259264\n",
      "  pid: 17553\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10079419357209049\n",
      "    mean_env_wait_ms: 0.14599417996490968\n",
      "    mean_inference_ms: 0.7280488494211115\n",
      "    mean_raw_obs_processing_ms: 0.22135616529126742\n",
      "  time_since_restore: 526.4707138538361\n",
      "  time_this_iter_s: 19.23056674003601\n",
      "  time_total_s: 526.4707138538361\n",
      "  timers:\n",
      "    learn_throughput: 65008.284\n",
      "    learn_time_ms: 3.938\n",
      "    update_time_ms: 2.475\n",
      "  timestamp: 1626228205\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 30500\n",
      "  training_iteration: 30\n",
      "  trial_id: 6b1dd_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/12 CPUs, 0/0 GPUs, 0.0/3.81 GiB heap, 0.0/1.32 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 1/1 (1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_Pendulum-v0_6b1dd_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    30</td><td style=\"text-align: right;\">         526.471</td><td style=\"text-align: right;\">30500</td><td style=\"text-align: right;\">-444.312</td><td style=\"text-align: right;\">            -1.11165</td><td style=\"text-align: right;\">            -1524.59</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/12 CPUs, 0/0 GPUs, 0.0/3.81 GiB heap, 0.0/1.32 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 1/1 (1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_Pendulum-v0_6b1dd_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    30</td><td style=\"text-align: right;\">         526.471</td><td style=\"text-align: right;\">30500</td><td style=\"text-align: right;\">-444.312</td><td style=\"text-align: right;\">            -1.11165</td><td style=\"text-align: right;\">            -1524.59</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-13 21:03:26,318\tINFO tune.py:448 -- Total run time: 540.24 seconds (539.55 seconds for the tuning loop).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7fadfcd0d490>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alg = 'DDPG'\n",
    "tune.run(alg,\n",
    "    stop={\"training_iteration\": 30},\n",
    "    config={\n",
    "        'env':'Pendulum-v0',\n",
    "        'num_gpus':0,\n",
    "        'num_workers':2,\n",
    "        'lr':tune.grid_search([.001,])     \n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: RLlib Environments\n",
    "\n",
    "1: RLlib works with several different types of environments, including OpenAI Gym, user-defined, multi-agent, and also batched environments.\n",
    "\n",
    "2: RLlib uses Gym as its environment interface for single-agent training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1: Configuring Environments\n",
    "\n",
    "    https://github.com/ray-project/ray/blob/master/rllib/examples/custom_env.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-7352217a18a1>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-7352217a18a1>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    self.action_space = <gym.Space>\u001b[0m\n\u001b[0m                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import gym, ray\n",
    "from ray.rllib.agents import ppo\n",
    "\n",
    "class MyEnv(gym.Env):\n",
    "    def __init__(self, env_config):\n",
    "        self.action_space = <gym.Space>\n",
    "        self.observation_space = <gym.Space>\n",
    "    def reset(self):\n",
    "        return <obs>\n",
    "    def step(self, action):\n",
    "        return <obs>, <reward: float>, <done: bool>, <info: dict>\n",
    "\n",
    "ray.init()\n",
    "trainer = ppo.PPOTrainer(env=MyEnv, config={\n",
    "    \"env_config\": {},  # config to pass to env class\n",
    "})\n",
    "\n",
    "while True:\n",
    "    print(trainer.train())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
