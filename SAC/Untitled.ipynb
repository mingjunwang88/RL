{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f4779ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: __main__.py [-h] [--run RUN] [--framework {tf,tf2,tfe,torch}]\n",
      "                   [--as-test] [--stop-iters STOP_ITERS]\n",
      "                   [--stop-timesteps STOP_TIMESTEPS]\n",
      "                   [--stop-reward STOP_REWARD] [--no-tune] [--local-mode]\n",
      "__main__.py: error: unrecognized arguments: -f /home/ec2-user/.local/share/jupyter/runtime/kernel-0ec54e07-d7ed-4724-a60d-f951c54b915e.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3351: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Example of a custom gym environment and model. Run this for a demo.\n",
    "This example shows:\n",
    "  - using a custom environment\n",
    "  - using a custom model\n",
    "  - using Tune for grid search to try different learning rates\n",
    "You can visualize experiment results in ~/ray_results using TensorBoard.\n",
    "Run example with defaults:\n",
    "$ python custom_env.py\n",
    "For CLI options:\n",
    "$ python custom_env.py --help\n",
    "\"\"\"\n",
    "import argparse\n",
    "import gym\n",
    "from gym.spaces import Discrete, Box\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.agents import ppo\n",
    "from ray.rllib.env.env_context import EnvContext\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.models.tf.tf_modelv2 import TFModelV2\n",
    "from ray.rllib.models.tf.fcnet import FullyConnectedNetwork\n",
    "from ray.rllib.models.torch.torch_modelv2 import TorchModelV2\n",
    "from ray.rllib.models.torch.fcnet import FullyConnectedNetwork as TorchFC\n",
    "from ray.rllib.utils.framework import try_import_tf, try_import_torch\n",
    "from ray.rllib.utils.test_utils import check_learning_achieved\n",
    "from ray.tune.logger import pretty_print\n",
    "\n",
    "tf1, tf, tfv = try_import_tf()\n",
    "torch, nn = try_import_torch()\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    \"--run\",\n",
    "    type=str,\n",
    "    default=\"PPO\",\n",
    "    help=\"The RLlib-registered algorithm to use.\")\n",
    "parser.add_argument(\n",
    "    \"--framework\",\n",
    "    choices=[\"tf\", \"tf2\", \"tfe\", \"torch\"],\n",
    "    default=\"tf\",\n",
    "    help=\"The DL framework specifier.\")\n",
    "parser.add_argument(\n",
    "    \"--as-test\",\n",
    "    action=\"store_true\",\n",
    "    help=\"Whether this script should be run as a test: --stop-reward must \"\n",
    "    \"be achieved within --stop-timesteps AND --stop-iters.\")\n",
    "parser.add_argument(\n",
    "    \"--stop-iters\",\n",
    "    type=int,\n",
    "    default=50,\n",
    "    help=\"Number of iterations to train.\")\n",
    "parser.add_argument(\n",
    "    \"--stop-timesteps\",\n",
    "    type=int,\n",
    "    default=100000,\n",
    "    help=\"Number of timesteps to train.\")\n",
    "parser.add_argument(\n",
    "    \"--stop-reward\",\n",
    "    type=float,\n",
    "    default=0.1,\n",
    "    help=\"Reward at which we stop training.\")\n",
    "parser.add_argument(\n",
    "    \"--no-tune\",\n",
    "    action=\"store_true\",\n",
    "    help=\"Run without Tune using a manual train loop instead. In this case,\"\n",
    "    \"use PPO without grid search and no TensorBoard.\")\n",
    "parser.add_argument(\n",
    "    \"--local-mode\",\n",
    "    action=\"store_true\",\n",
    "    help=\"Init Ray in local mode for easier debugging.\")\n",
    "\n",
    "\n",
    "class SimpleCorridor(gym.Env):\n",
    "    \"\"\"Example of a custom env in which you have to walk down a corridor.\n",
    "    You can configure the length of the corridor via the env config.\"\"\"\n",
    "\n",
    "    def __init__(self, config: EnvContext):\n",
    "        self.end_pos = config[\"corridor_length\"]\n",
    "        self.cur_pos = 0\n",
    "        self.action_space = Discrete(2)\n",
    "        self.observation_space = Box(\n",
    "            0.0, self.end_pos, shape=(1, ), dtype=np.float32)\n",
    "        # Set the seed. This is only used for the final (reach goal) reward.\n",
    "        self.seed(config.worker_index * config.num_workers)\n",
    "\n",
    "    def reset(self):\n",
    "        self.cur_pos = 0\n",
    "        return [self.cur_pos]\n",
    "\n",
    "    def step(self, action):\n",
    "        assert action in [0, 1], action\n",
    "        if action == 0 and self.cur_pos > 0:\n",
    "            self.cur_pos -= 1\n",
    "        elif action == 1:\n",
    "            self.cur_pos += 1\n",
    "        done = self.cur_pos >= self.end_pos\n",
    "        # Produce a random reward when we reach the goal.\n",
    "        return [self.cur_pos], \\\n",
    "            random.random() * 2 if done else -0.1, done, {}\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        random.seed(seed)\n",
    "\n",
    "\n",
    "class CustomModel(TFModelV2):\n",
    "    \"\"\"Example of a keras custom model that just delegates to an fc-net.\"\"\"\n",
    "\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config,\n",
    "                 name):\n",
    "        super(CustomModel, self).__init__(obs_space, action_space, num_outputs,\n",
    "                                          model_config, name)\n",
    "        self.model = FullyConnectedNetwork(obs_space, action_space,\n",
    "                                           num_outputs, model_config, name)\n",
    "\n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        return self.model.forward(input_dict, state, seq_lens)\n",
    "\n",
    "    def value_function(self):\n",
    "        return self.model.value_function()\n",
    "\n",
    "\n",
    "class TorchCustomModel(TorchModelV2, nn.Module):\n",
    "    \"\"\"Example of a PyTorch custom model that just delegates to a fc-net.\"\"\"\n",
    "\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config,\n",
    "                 name):\n",
    "        TorchModelV2.__init__(self, obs_space, action_space, num_outputs,\n",
    "                              model_config, name)\n",
    "        nn.Module.__init__(self)\n",
    "\n",
    "        self.torch_sub_model = TorchFC(obs_space, action_space, num_outputs,\n",
    "                                       model_config, name)\n",
    "\n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        input_dict[\"obs\"] = input_dict[\"obs\"].float()\n",
    "        fc_out, _ = self.torch_sub_model(input_dict, state, seq_lens)\n",
    "        return fc_out, []\n",
    "\n",
    "    def value_function(self):\n",
    "        return torch.reshape(self.torch_sub_model.value_function(), [-1])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = parser.parse_args()\n",
    "    print(f\"Running with following CLI options: {args}\")\n",
    "\n",
    "    ray.init(local_mode=args.local_mode)\n",
    "\n",
    "    # Can also register the env creator function explicitly with:\n",
    "    # register_env(\"corridor\", lambda config: SimpleCorridor(config))\n",
    "    ModelCatalog.register_custom_model(\n",
    "        \"my_model\", TorchCustomModel\n",
    "        if args.framework == \"torch\" else CustomModel)\n",
    "\n",
    "    config = {\n",
    "        \"env\": SimpleCorridor,  # or \"corridor\" if registered above\n",
    "        \"env_config\": {\n",
    "            \"corridor_length\": 5,\n",
    "        },\n",
    "        # Use GPUs iff `RLLIB_NUM_GPUS` env var set to > 0.\n",
    "        \"num_gpus\": int(os.environ.get(\"RLLIB_NUM_GPUS\", \"0\")),\n",
    "        \"model\": {\n",
    "            \"custom_model\": \"my_model\",\n",
    "            \"vf_share_layers\": True,\n",
    "        },\n",
    "        \"num_workers\": 1,  # parallelism\n",
    "        \"framework\": args.framework,\n",
    "    }\n",
    "\n",
    "    stop = {\n",
    "        \"training_iteration\": args.stop_iters,\n",
    "        \"timesteps_total\": args.stop_timesteps,\n",
    "        \"episode_reward_mean\": args.stop_reward,\n",
    "    }\n",
    "\n",
    "    if args.no_tune:\n",
    "        # manual training with train loop using PPO and fixed learning rate\n",
    "        if args.run != \"PPO\":\n",
    "            raise ValueError(\"Only support --run PPO with --no-tune.\")\n",
    "        print(\"Running manual train loop without Ray Tune.\")\n",
    "        ppo_config = ppo.DEFAULT_CONFIG.copy()\n",
    "        ppo_config.update(config)\n",
    "        # use fixed learning rate instead of grid search (needs tune)\n",
    "        ppo_config[\"lr\"] = 1e-3\n",
    "        trainer = ppo.PPOTrainer(config=ppo_config, env=SimpleCorridor)\n",
    "        # run manual training loop and print results after each iteration\n",
    "        for _ in range(args.stop_iters):\n",
    "            result = trainer.train()\n",
    "            print(pretty_print(result))\n",
    "            # stop training of the target train steps or reward are reached\n",
    "            if result[\"timesteps_total\"] >= args.stop_timesteps or \\\n",
    "                    result[\"episode_reward_mean\"] >= args.stop_reward:\n",
    "                break\n",
    "    else:\n",
    "        # automated run with Tune and grid search and TensorBoard\n",
    "        print(\"Training automatically with Ray Tune\")\n",
    "        results = tune.run(args.run, config=config, stop=stop)\n",
    "\n",
    "        if args.as_test:\n",
    "            print(\"Checking if learning goals were achieved\")\n",
    "            check_learning_achieved(results, args.stop_reward)\n",
    "\n",
    "    ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
