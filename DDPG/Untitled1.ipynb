{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3aeeb1ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: __main__.py [-h] [--mode MODE] [--env_name ENV_NAME] [--tau TAU]\n",
      "                   [--target_update_interval TARGET_UPDATE_INTERVAL]\n",
      "                   [--test_iteration TEST_ITERATION]\n",
      "                   [--learning_rate LEARNING_RATE] [--gamma GAMMA]\n",
      "                   [--capacity CAPACITY] [--batch_size BATCH_SIZE]\n",
      "                   [--seed SEED] [--random_seed RANDOM_SEED]\n",
      "                   [--sample_frequency SAMPLE_FREQUENCY] [--render RENDER]\n",
      "                   [--log_interval LOG_INTERVAL] [--load LOAD]\n",
      "                   [--render_interval RENDER_INTERVAL]\n",
      "                   [--exploration_noise EXPLORATION_NOISE]\n",
      "                   [--max_episode MAX_EPISODE] [--print_log PRINT_LOG]\n",
      "                   [--update_iteration UPDATE_ITERATION]\n",
      "__main__.py: error: unrecognized arguments: -f /home/ec2-user/.local/share/jupyter/runtime/kernel-e873691c-ce84-44b5-a399-bb0fd63693da.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3351: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from itertools import count\n",
    "\n",
    "import os, sys, random\n",
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "'''\n",
    "Implementation of Deep Deterministic Policy Gradients (DDPG) with pytorch \n",
    "riginal paper: https://arxiv.org/abs/1509.02971\n",
    "Not the author's implementation !\n",
    "'''\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--mode', default='train', type=str) # mode = 'train' or 'test'\n",
    "# OpenAI gym environment name, # ['BipedalWalker-v2', 'Pendulum-v0'] or any continuous environment\n",
    "# Note that DDPG is feasible about hyper-parameters.\n",
    "# You should fine-tuning if you change to another environment.\n",
    "parser.add_argument(\"--env_name\", default=\"Pendulum-v1\")\n",
    "parser.add_argument('--tau',  default=0.005, type=float) # target smoothing coefficient\n",
    "parser.add_argument('--target_update_interval', default=1, type=int)\n",
    "parser.add_argument('--test_iteration', default=10, type=int)\n",
    "\n",
    "parser.add_argument('--learning_rate', default=1e-4, type=float)\n",
    "parser.add_argument('--gamma', default=0.99, type=int) # discounted factor\n",
    "parser.add_argument('--capacity', default=1000000, type=int) # replay buffer size\n",
    "parser.add_argument('--batch_size', default=100, type=int) # mini batch size\n",
    "parser.add_argument('--seed', default=False, type=bool)\n",
    "parser.add_argument('--random_seed', default=9527, type=int)\n",
    "# optional parameters\n",
    "\n",
    "parser.add_argument('--sample_frequency', default=2000, type=int)\n",
    "parser.add_argument('--render', default=False, type=bool) # show UI or not\n",
    "parser.add_argument('--log_interval', default=50, type=int) #\n",
    "parser.add_argument('--load', default=False, type=bool) # load model\n",
    "parser.add_argument('--render_interval', default=100, type=int) # after render_interval, the env.render() will work\n",
    "parser.add_argument('--exploration_noise', default=0.1, type=float)\n",
    "parser.add_argument('--max_episode', default=100000, type=int) # num of games\n",
    "parser.add_argument('--print_log', default=5, type=int)\n",
    "parser.add_argument('--update_iteration', default=200, type=int)\n",
    "args = parser.parse_args()\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "script_name = os.path.basename(__file__)\n",
    "env = gym.make(args.env_name)\n",
    "\n",
    "if args.seed:\n",
    "    env.seed(args.random_seed)\n",
    "    torch.manual_seed(args.random_seed)\n",
    "    np.random.seed(args.random_seed)\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "max_action = float(env.action_space.high[0])\n",
    "min_Val = torch.tensor(1e-7).float().to(device) # min value\n",
    "\n",
    "directory = './exp' + script_name + args.env_name +'./'\n",
    "\n",
    "class Replay_buffer():\n",
    "    '''\n",
    "    Code based on:\n",
    "    https://github.com/openai/baselines/blob/master/baselines/deepq/replay_buffer.py\n",
    "    Expects tuples of (state, next_state, action, reward, done)\n",
    "    '''\n",
    "    def __init__(self, max_size=args.capacity):\n",
    "        self.storage = []\n",
    "        self.max_size = max_size\n",
    "        self.ptr = 0\n",
    "\n",
    "    def push(self, data):\n",
    "        if len(self.storage) == self.max_size:\n",
    "            self.storage[int(self.ptr)] = data\n",
    "            self.ptr = (self.ptr + 1) % self.max_size\n",
    "        else:\n",
    "            self.storage.append(data)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        ind = np.random.randint(0, len(self.storage), size=batch_size)\n",
    "        x, y, u, r, d = [], [], [], [], []\n",
    "\n",
    "        for i in ind:\n",
    "            X, Y, U, R, D = self.storage[i]\n",
    "            x.append(np.array(X, copy=False))\n",
    "            y.append(np.array(Y, copy=False))\n",
    "            u.append(np.array(U, copy=False))\n",
    "            r.append(np.array(R, copy=False))\n",
    "            d.append(np.array(D, copy=False))\n",
    "\n",
    "        return np.array(x), np.array(y), np.array(u), np.array(r).reshape(-1, 1), np.array(d).reshape(-1, 1)\n",
    "\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        super(Actor, self).__init__()\n",
    "\n",
    "        self.l1 = nn.Linear(state_dim, 400)\n",
    "        self.l2 = nn.Linear(400, 300)\n",
    "        self.l3 = nn.Linear(300, action_dim)\n",
    "\n",
    "        self.max_action = max_action\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = F.relu(self.l2(x))\n",
    "        x = self.max_action * torch.tanh(self.l3(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        self.l1 = nn.Linear(state_dim + action_dim, 400)\n",
    "        self.l2 = nn.Linear(400 , 300)\n",
    "        self.l3 = nn.Linear(300, 1)\n",
    "\n",
    "    def forward(self, x, u):\n",
    "        x = F.relu(self.l1(torch.cat([x, u], 1)))\n",
    "        x = F.relu(self.l2(x))\n",
    "        x = self.l3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DDPG(object):\n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
    "        self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=1e-4)\n",
    "\n",
    "        self.critic = Critic(state_dim, action_dim).to(device)\n",
    "        self.critic_target = Critic(state_dim, action_dim).to(device)\n",
    "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=1e-3)\n",
    "        self.replay_buffer = Replay_buffer()\n",
    "        self.writer = SummaryWriter(directory)\n",
    "\n",
    "        self.num_critic_update_iteration = 0\n",
    "        self.num_actor_update_iteration = 0\n",
    "        self.num_training = 0\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.FloatTensor(state.reshape(1, -1)).to(device)\n",
    "        return self.actor(state).cpu().data.numpy().flatten()\n",
    "\n",
    "    def update(self):\n",
    "\n",
    "        for it in range(args.update_iteration):\n",
    "            # Sample replay buffer\n",
    "            x, y, u, r, d = self.replay_buffer.sample(args.batch_size)\n",
    "            state = torch.FloatTensor(x).to(device)\n",
    "            action = torch.FloatTensor(u).to(device)\n",
    "            next_state = torch.FloatTensor(y).to(device)\n",
    "            done = torch.FloatTensor(1-d).to(device)\n",
    "            reward = torch.FloatTensor(r).to(device)\n",
    "\n",
    "            # Compute the target Q value\n",
    "            target_Q = self.critic_target(next_state, self.actor_target(next_state))\n",
    "            target_Q = reward + (done * args.gamma * target_Q).detach()\n",
    "\n",
    "            # Get current Q estimate\n",
    "            current_Q = self.critic(state, action)\n",
    "\n",
    "            # Compute critic loss\n",
    "            critic_loss = F.mse_loss(current_Q, target_Q)\n",
    "            self.writer.add_scalar('Loss/critic_loss', critic_loss, global_step=self.num_critic_update_iteration)\n",
    "            # Optimize the critic\n",
    "            self.critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            self.critic_optimizer.step()\n",
    "\n",
    "            # Compute actor loss\n",
    "            actor_loss = -self.critic(state, self.actor(state)).mean()\n",
    "            self.writer.add_scalar('Loss/actor_loss', actor_loss, global_step=self.num_actor_update_iteration)\n",
    "\n",
    "            # Optimize the actor\n",
    "            self.actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            self.actor_optimizer.step()\n",
    "\n",
    "            # Update the frozen target models\n",
    "            for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "                target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)\n",
    "\n",
    "            for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "                target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)\n",
    "\n",
    "            self.num_actor_update_iteration += 1\n",
    "            self.num_critic_update_iteration += 1\n",
    "\n",
    "    def save(self):\n",
    "        torch.save(self.actor.state_dict(), directory + 'actor.pth')\n",
    "        torch.save(self.critic.state_dict(), directory + 'critic.pth')\n",
    "        # print(\"====================================\")\n",
    "        # print(\"Model has been saved...\")\n",
    "        # print(\"====================================\")\n",
    "\n",
    "    def load(self):\n",
    "        self.actor.load_state_dict(torch.load(directory + 'actor.pth'))\n",
    "        self.critic.load_state_dict(torch.load(directory + 'critic.pth'))\n",
    "        print(\"====================================\")\n",
    "        print(\"model has been loaded...\")\n",
    "        print(\"====================================\")\n",
    "\n",
    "def main():\n",
    "    agent = DDPG(state_dim, action_dim, max_action)\n",
    "    ep_r = 0\n",
    "    if args.mode == 'test':\n",
    "        agent.load()\n",
    "        for i in range(args.test_iteration):\n",
    "            state = env.reset()\n",
    "            for t in count():\n",
    "                action = agent.select_action(state)\n",
    "                next_state, reward, done, info = env.step(np.float32(action))\n",
    "                ep_r += reward\n",
    "                env.render()\n",
    "                if done or t >= args.max_length_of_trajectory:\n",
    "                    print(\"Ep_i \\t{}, the ep_r is \\t{:0.2f}, the step is \\t{}\".format(i, ep_r, t))\n",
    "                    ep_r = 0\n",
    "                    break\n",
    "                state = next_state\n",
    "\n",
    "    elif args.mode == 'train':\n",
    "        if args.load: agent.load()\n",
    "        total_step = 0\n",
    "        for i in range(args.max_episode):\n",
    "            total_reward = 0\n",
    "            step =0\n",
    "            state = env.reset()\n",
    "            for t in count():\n",
    "                action = agent.select_action(state)\n",
    "                action = (action + np.random.normal(0, args.exploration_noise, size=env.action_space.shape[0])).clip(\n",
    "                    env.action_space.low, env.action_space.high)\n",
    "\n",
    "                next_state, reward, done, info = env.step(action)\n",
    "                if args.render and i >= args.render_interval : env.render()\n",
    "                agent.replay_buffer.push((state, next_state, action, reward, np.float(done)))\n",
    "\n",
    "                state = next_state\n",
    "                if done:\n",
    "                    break\n",
    "                step += 1\n",
    "                total_reward += reward\n",
    "            total_step += step+1\n",
    "            print(\"Total T:{} Episode: \\t{} Total Reward: \\t{:0.2f}\".format(total_step, i, total_reward))\n",
    "            agent.update()\n",
    "           # \"Total T: %d Episode Num: %d Episode T: %d Reward: %f\n",
    "\n",
    "            if i % args.log_interval == 0:\n",
    "                agent.save()\n",
    "    else:\n",
    "        raise NameError(\"mode wrong!!!\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54cbd3e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.8882656], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim_state = env.observation_space.shape[0]\n",
    "dim_action = env.action_space\n",
    "dim_action.sample()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
