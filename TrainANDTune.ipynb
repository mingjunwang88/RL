{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To view the tensorboard: \n",
    "    1: tensorboard --logdir ray_results \n",
    "    2: see http://localhost:6006/ in browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/gpu_cuda10.0/lib/python3.6/site-packages/tensorflow_core/__init__.py:1473: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "import ray.rllib.agents.ppo as ppo\n",
    "from ray.tune.logger import pretty_print\n",
    "from ray import tune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0: RLlib Training APIs: \n",
    "1: At a high level, RLlib provides an Trainer class which holds a policy for environment interaction. Through the trainer interface, the policy can be trained, checkpointed, or an action computed. In multi-agent training, the trainer manages the querying and optimization of multiple policies at once.\n",
    "\n",
    "2: rllib train --run DQN --env CartPole-v0  --config '{\"num_workers\": 8}'\n",
    "    To see the tensorboard: tensorboard --logdir=~/ray_results\n",
    "\n",
    "3: rllib rollout ~/ray_results/default/DQN_CartPole-v0_0upjmdgr0/checkpoint_1/checkpoint-1 \\\n",
    "    --run DQN --env CartPole-v0 --steps 10000\n",
    "\n",
    "4: Loading and restoring a trained agent from a checkpoint is simple:\n",
    "    \n",
    "    agent = ppo.PPOTrainer(config=config, env=env_class)\n",
    "    agent.restore(checkpoint_path)\n",
    "    \n",
    "5: Computing Actions\n",
    "\n",
    "The simplest way to programmatically compute actions from a trained agent is to use trainer.compute_action(). This method preprocesses and filters the observation before passing it to the agent policy. Here is a simple example of testing a trained agent for one episode:\n",
    "\n",
    "    # instantiate env class\n",
    "    env = env_class(env_config)\n",
    "\n",
    "    # run until episode ends\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "    while not done:\n",
    "        action = agent.compute_action(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        episode_reward += reward\n",
    "6: Itâ€™s recommended that you run RLlib trainers with Tune, for easy experiment management and visualization of results. Just set \"run\": ALG_NAME, \"env\": ENV_NAME in the experiment . config. All RLlib trainers are compatible with the Tune API. This enables them to be easily used in experiments with Tune/\n",
    "\n",
    "7: tune.run() returns an ExperimentAnalysis object that allows further analysis of the training results and retrieving the checkpoint(s) of the trained agent. It also simplifies saving the trained agent. For example:\n",
    "\n",
    "a: tune.run() allows setting a custom log directory (other than ``~/ray-results``) and automatically saving the trained agent\n",
    "\n",
    "    analysis = ray.tune.run(\n",
    "        ppo.PPOTrainer,\n",
    "        config=config,\n",
    "        local_dir=log_dir,\n",
    "        stop=stop_criteria,\n",
    "        checkpoint_at_end=True)\n",
    "\n",
    "b: list of lists: one list per checkpoint; each checkpoint list contains 1st the path, 2nd the metric value\n",
    "\n",
    "        checkpoints = analysis.get_trial_checkpoints_paths(\n",
    "            trial=analysis.get_best_trial(\"episode_reward_mean\"),\n",
    "            metric=\"episode_reward_mean\")\n",
    "\n",
    "c: or simply get the last checkpoint (with highest \"training_iteration\")\n",
    "\n",
    "        last_checkpoint = analysis.get_last_checkpoint()\n",
    "    \n",
    "d: if there are multiple trials, select a specific trial or automatically choose the best one according to a given metric\n",
    "\n",
    "        last_checkpoint = analysis.get_last_checkpoint(\n",
    "            metric=\"episode_reward_mean\", mode=\"max\"\n",
    "        )\n",
    "\n",
    "e: Loading and restoring a trained agent from a checkpoint is simple:\n",
    "\n",
    "    agent = ppo.PPOTrainer(config=config, env=env_class)\n",
    "    agent.restore(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 Example of Traing a PPO Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ppo.DEFAULT_CONFIG.copy()\n",
    "config['num_gpus'] = 1\n",
    "config['num_workers'] = 2\n",
    "trainer = ppo.PPOTrainer(config = config, env='CartPole-v1') \n",
    "\n",
    "for i in range(30):\n",
    "    result = trainer.train()\n",
    "    if i % 10 ==0:\n",
    "        checkpoint = trainer.save()\n",
    "        print('checkpoint saved at:', checkpoint) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 Example of Using Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alg = 'PPO'\n",
    "tune.run(alg,\n",
    "    stop={'episode_reward_mean':200},\n",
    "    config={\n",
    "        'env':'CartPole-v0',\n",
    "        'num_gpus':1,\n",
    "        'num_workers':2,\n",
    "        'lr':tune.grid_search([.01,.001,.0001])     \n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alg = 'DDPG'\n",
    "tune.run(alg,\n",
    "    stop={\"training_iteration\": 30},\n",
    "    config={\n",
    "        'env':'Pendulum-v0',\n",
    "        'num_gpus':0,\n",
    "        'num_workers':2,\n",
    "        'lr':tune.grid_search([.001,])     \n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: RLlib Environments\n",
    "\n",
    "1: RLlib works with several different types of environments, including OpenAI Gym, user-defined, multi-agent, and also batched environments.\n",
    "\n",
    "2: RLlib uses Gym as its environment interface for single-agent training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1: Configuring Environments\n",
    "\n",
    "    https://github.com/ray-project/ray/blob/master/rllib/examples/custom_env.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-7352217a18a1>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-7352217a18a1>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    self.action_space = <gym.Space>\u001b[0m\n\u001b[0m                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import gym, ray\n",
    "from ray.rllib.agents import ppo\n",
    "\n",
    "class MyEnv(gym.Env):\n",
    "    def __init__(self, env_config):\n",
    "        self.action_space = <gym.Space>\n",
    "        self.observation_space = <gym.Space>\n",
    "    def reset(self):\n",
    "        return <obs>\n",
    "    def step(self, action):\n",
    "        return <obs>, <reward: float>, <done: bool>, <info: dict>\n",
    "\n",
    "ray.init()\n",
    "trainer = ppo.PPOTrainer(env=MyEnv, config={\n",
    "    \"env_config\": {},  # config to pass to env class\n",
    "})\n",
    "\n",
    "while True:\n",
    "    print(trainer.train())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
