{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To view the tensorboard: \n",
    "    1: tensorboard --logdir ray_results \n",
    "    2: see http://localhost:6006/ in browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/gpu_cuda10.0/lib/python3.6/site-packages/tensorflow_core/__init__.py:1473: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "import ray.rllib.agents.ppo as ppo\n",
    "from ray.tune.logger import pretty_print\n",
    "from ray import tune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0: RLlib Training APIs: \n",
    "1: At a high level, RLlib provides an Trainer class which holds a policy for environment interaction. Through the trainer interface, the policy can be trained, checkpointed, or an action computed. In multi-agent training, the trainer manages the querying and optimization of multiple policies at once.\n",
    "\n",
    "2: rllib train --run DQN --env CartPole-v0  --config '{\"num_workers\": 8}'\n",
    "    To see the tensorboard: tensorboard --logdir=~/ray_results\n",
    "\n",
    "3: rllib rollout ~/ray_results/default/DQN_CartPole-v0_0upjmdgr0/checkpoint_1/checkpoint-1 \\\n",
    "    --run DQN --env CartPole-v0 --steps 10000\n",
    "\n",
    "4: Loading and restoring a trained agent from a checkpoint is simple:\n",
    "    \n",
    "    agent = ppo.PPOTrainer(config=config, env=env_class)\n",
    "    agent.restore(checkpoint_path)\n",
    "    \n",
    "5: Computing Actions\n",
    "\n",
    "The simplest way to programmatically compute actions from a trained agent is to use trainer.compute_action(). This method preprocesses and filters the observation before passing it to the agent policy. Here is a simple example of testing a trained agent for one episode:\n",
    "\n",
    "    # instantiate env class\n",
    "    env = env_class(env_config)\n",
    "\n",
    "    # run until episode ends\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "    while not done:\n",
    "        action = agent.compute_action(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        episode_reward += reward\n",
    "6: Itâ€™s recommended that you run RLlib trainers with Tune, for easy experiment management and visualization of results. Just set \"run\": ALG_NAME, \"env\": ENV_NAME in the experiment . config. All RLlib trainers are compatible with the Tune API. This enables them to be easily used in experiments with Tune/\n",
    "\n",
    "7: tune.run() returns an ExperimentAnalysis object that allows further analysis of the training results and retrieving the checkpoint(s) of the trained agent. It also simplifies saving the trained agent. For example:\n",
    "\n",
    "a: tune.run() allows setting a custom log directory (other than ``~/ray-results``) and automatically saving the trained agent\n",
    "\n",
    "    analysis = ray.tune.run(\n",
    "        ppo.PPOTrainer,\n",
    "        config=config,\n",
    "        local_dir=log_dir,\n",
    "        stop=stop_criteria,\n",
    "        checkpoint_at_end=True)\n",
    "\n",
    "b: list of lists: one list per checkpoint; each checkpoint list contains 1st the path, 2nd the metric value\n",
    "\n",
    "        checkpoints = analysis.get_trial_checkpoints_paths(\n",
    "            trial=analysis.get_best_trial(\"episode_reward_mean\"),\n",
    "            metric=\"episode_reward_mean\")\n",
    "\n",
    "c: or simply get the last checkpoint (with highest \"training_iteration\")\n",
    "\n",
    "        last_checkpoint = analysis.get_last_checkpoint()\n",
    "    \n",
    "d: if there are multiple trials, select a specific trial or automatically choose the best one according to a given metric\n",
    "\n",
    "        last_checkpoint = analysis.get_last_checkpoint(\n",
    "            metric=\"episode_reward_mean\", mode=\"max\"\n",
    "        )\n",
    "\n",
    "e: Loading and restoring a trained agent from a checkpoint is simple:\n",
    "\n",
    "    agent = ppo.PPOTrainer(config=config, env=env_class)\n",
    "    agent.restore(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-16 02:47:55,173\tINFO services.py:1252 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8267\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '172.16.21.53',\n",
       " 'raylet_ip_address': '172.16.21.53',\n",
       " 'redis_address': '172.16.21.53:48500',\n",
       " 'object_store_address': '/tmp/ray/session_2021-10-16_02-47-53_348974_46171/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2021-10-16_02-47-53_348974_46171/sockets/raylet',\n",
       " 'webui_url': '127.0.0.1:8267',\n",
       " 'session_dir': '/tmp/ray/session_2021-10-16_02-47-53_348974_46171',\n",
       " 'metrics_export_port': 64532,\n",
       " 'node_id': 'fc79172c023b86b61972689708962114fb1c722f304004b0b5893f8a'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.shutdown()\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 Example of Traing a PPO Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=48648)\u001b[0m WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/gpu_cuda10.0/lib/python3.6/site-packages/tensorflow_core/__init__.py:1473: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\u001b[2m\u001b[36m(pid=48648)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=48647)\u001b[0m WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/gpu_cuda10.0/lib/python3.6/site-packages/tensorflow_core/__init__.py:1473: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\u001b[2m\u001b[36m(pid=48647)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=48648)\u001b[0m 2021-10-16 16:08:08.679896: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "\u001b[2m\u001b[36m(pid=48648)\u001b[0m WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/gpu_cuda10.0/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=48648)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=48648)\u001b[0m If using Keras pass *_constraint arguments to layers.\n",
      "\u001b[2m\u001b[36m(pid=48647)\u001b[0m 2021-10-16 16:08:08.682076: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "\u001b[2m\u001b[36m(pid=48647)\u001b[0m WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/gpu_cuda10.0/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=48647)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=48647)\u001b[0m If using Keras pass *_constraint arguments to layers.\n",
      "\u001b[2m\u001b[36m(pid=48648)\u001b[0m WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/gpu_cuda10.0/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=48648)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=48648)\u001b[0m Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "\u001b[2m\u001b[36m(pid=48647)\u001b[0m WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/gpu_cuda10.0/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=48647)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=48647)\u001b[0m Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "\u001b[2m\u001b[36m(pid=48648)\u001b[0m WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/horovod/tensorflow/__init__.py:163: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\u001b[2m\u001b[36m(pid=48648)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=48648)\u001b[0m WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/horovod/tensorflow/__init__.py:189: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\u001b[2m\u001b[36m(pid=48648)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=48647)\u001b[0m WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/horovod/tensorflow/__init__.py:163: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\u001b[2m\u001b[36m(pid=48647)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=48647)\u001b[0m WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/horovod/tensorflow/__init__.py:189: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\u001b[2m\u001b[36m(pid=48647)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=48648)\u001b[0m [2021-10-16 16:08:09.558 ip-172-16-19-112:48648 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=48648)\u001b[0m [2021-10-16 16:08:09.592 ip-172-16-19-112:48648 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=48647)\u001b[0m [2021-10-16 16:08:09.611 ip-172-16-19-112:48647 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=48647)\u001b[0m [2021-10-16 16:08:09.646 ip-172-16-19-112:48647 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-16 16:08:13,441\tWARNING trainer_template.py:186 -- `execution_plan` functions should accept `trainer`, `workers`, and `config` as args!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint saved at: /home/ec2-user/ray_results/PPO_CartPole-v1_2021-10-16_16-08-04wkiy6cxa/checkpoint_000001/checkpoint-1\n",
      "checkpoint saved at: /home/ec2-user/ray_results/PPO_CartPole-v1_2021-10-16_16-08-04wkiy6cxa/checkpoint_000011/checkpoint-11\n",
      "checkpoint saved at: /home/ec2-user/ray_results/PPO_CartPole-v1_2021-10-16_16-08-04wkiy6cxa/checkpoint_000021/checkpoint-21\n"
     ]
    }
   ],
   "source": [
    "config = ppo.DEFAULT_CONFIG.copy()\n",
    "config['num_gpus'] = 1\n",
    "config['num_workers'] = 2\n",
    "trainer = ppo.PPOTrainer(config = config, env='CartPole-v1') \n",
    "\n",
    "for i in range(30):\n",
    "    result = trainer.train()\n",
    "    if i % 10 ==0:\n",
    "        checkpoint = trainer.save()\n",
    "        print('checkpoint saved at:', checkpoint) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 Example of Using Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/32 CPUs, 0/8 GPUs, 0.0/323.51 GiB heap, 0.0/142.64 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 3/3 (3 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">    lr</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00000</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">0.01  </td></tr>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00001</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">0.001 </td></tr>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00002</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">0.0001</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=41261)\u001b[0m WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/gpu_cuda10.0/lib/python3.6/site-packages/tensorflow_core/__init__.py:1473: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\u001b[2m\u001b[36m(pid=41261)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=41259)\u001b[0m WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/gpu_cuda10.0/lib/python3.6/site-packages/tensorflow_core/__init__.py:1473: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\u001b[2m\u001b[36m(pid=41259)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=41260)\u001b[0m WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/gpu_cuda10.0/lib/python3.6/site-packages/tensorflow_core/__init__.py:1473: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\u001b[2m\u001b[36m(pid=41260)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=41259)\u001b[0m 2021-10-16 15:44:24,968\tINFO trainer.py:741 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=41259)\u001b[0m 2021-10-16 15:44:24,968\tINFO ppo.py:165 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(pid=41259)\u001b[0m 2021-10-16 15:44:24,968\tINFO trainer.py:760 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=41261)\u001b[0m 2021-10-16 15:44:25,001\tINFO trainer.py:741 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=41261)\u001b[0m 2021-10-16 15:44:25,001\tINFO ppo.py:165 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(pid=41261)\u001b[0m 2021-10-16 15:44:25,001\tINFO trainer.py:760 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=41260)\u001b[0m 2021-10-16 15:44:25,004\tINFO trainer.py:741 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=41260)\u001b[0m 2021-10-16 15:44:25,004\tINFO ppo.py:165 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(pid=41260)\u001b[0m 2021-10-16 15:44:25,004\tINFO trainer.py:760 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.3/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 9.0/32 CPUs, 3.0/8 GPUs, 0.0/323.51 GiB heap, 0.0/142.64 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 3/3 (3 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">    lr</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00000</td><td>RUNNING </td><td>     </td><td style=\"text-align: right;\">0.01  </td></tr>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00001</td><td>RUNNING </td><td>     </td><td style=\"text-align: right;\">0.001 </td></tr>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00002</td><td>RUNNING </td><td>     </td><td style=\"text-align: right;\">0.0001</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=41366)\u001b[0m WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/gpu_cuda10.0/lib/python3.6/site-packages/tensorflow_core/__init__.py:1473: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\u001b[2m\u001b[36m(pid=41366)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=41367)\u001b[0m WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/gpu_cuda10.0/lib/python3.6/site-packages/tensorflow_core/__init__.py:1473: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\u001b[2m\u001b[36m(pid=41367)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=41370)\u001b[0m WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/gpu_cuda10.0/lib/python3.6/site-packages/tensorflow_core/__init__.py:1473: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\u001b[2m\u001b[36m(pid=41370)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=41373)\u001b[0m WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/gpu_cuda10.0/lib/python3.6/site-packages/tensorflow_core/__init__.py:1473: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\u001b[2m\u001b[36m(pid=41373)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=41368)\u001b[0m WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/gpu_cuda10.0/lib/python3.6/site-packages/tensorflow_core/__init__.py:1473: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\u001b[2m\u001b[36m(pid=41368)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=41369)\u001b[0m WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/gpu_cuda10.0/lib/python3.6/site-packages/tensorflow_core/__init__.py:1473: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\u001b[2m\u001b[36m(pid=41369)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=41373)\u001b[0m 2021-10-16 15:44:30.430709: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "\u001b[2m\u001b[36m(pid=41366)\u001b[0m 2021-10-16 15:44:30.466143: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "\u001b[2m\u001b[36m(pid=41366)\u001b[0m WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/gpu_cuda10.0/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=41366)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=41366)\u001b[0m If using Keras pass *_constraint arguments to layers.\n",
      "\u001b[2m\u001b[36m(pid=41367)\u001b[0m 2021-10-16 15:44:30.490393: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "\u001b[2m\u001b[36m(pid=41367)\u001b[0m WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/gpu_cuda10.0/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=41367)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=41367)\u001b[0m If using Keras pass *_constraint arguments to layers.\n",
      "\u001b[2m\u001b[36m(pid=41370)\u001b[0m 2021-10-16 15:44:30.449668: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "\u001b[2m\u001b[36m(pid=41370)\u001b[0m WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/gpu_cuda10.0/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=41370)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=41370)\u001b[0m If using Keras pass *_constraint arguments to layers.\n",
      "\u001b[2m\u001b[36m(pid=41373)\u001b[0m WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/gpu_cuda10.0/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=41373)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=41373)\u001b[0m If using Keras pass *_constraint arguments to layers.\n",
      "\u001b[2m\u001b[36m(pid=41368)\u001b[0m 2021-10-16 15:44:30.494134: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "\u001b[2m\u001b[36m(pid=41368)\u001b[0m WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/gpu_cuda10.0/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=41368)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=41368)\u001b[0m If using Keras pass *_constraint arguments to layers.\n",
      "\u001b[2m\u001b[36m(pid=41369)\u001b[0m 2021-10-16 15:44:30.498516: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "\u001b[2m\u001b[36m(pid=41369)\u001b[0m WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/gpu_cuda10.0/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=41369)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=41369)\u001b[0m If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.9/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 9.0/32 CPUs, 3.0/8 GPUs, 0.0/323.51 GiB heap, 0.0/142.64 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 3/3 (3 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">    lr</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00000</td><td>RUNNING </td><td>     </td><td style=\"text-align: right;\">0.01  </td></tr>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00001</td><td>RUNNING </td><td>     </td><td style=\"text-align: right;\">0.001 </td></tr>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00002</td><td>RUNNING </td><td>     </td><td style=\"text-align: right;\">0.0001</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=41366)\u001b[0m WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/gpu_cuda10.0/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=41366)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=41366)\u001b[0m Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "\u001b[2m\u001b[36m(pid=41367)\u001b[0m WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/gpu_cuda10.0/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=41367)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=41367)\u001b[0m Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "\u001b[2m\u001b[36m(pid=41370)\u001b[0m WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/gpu_cuda10.0/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=41370)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=41370)\u001b[0m Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "\u001b[2m\u001b[36m(pid=41373)\u001b[0m WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/gpu_cuda10.0/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=41373)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=41373)\u001b[0m Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "\u001b[2m\u001b[36m(pid=41368)\u001b[0m WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/gpu_cuda10.0/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=41368)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=41368)\u001b[0m Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "\u001b[2m\u001b[36m(pid=41369)\u001b[0m WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/gpu_cuda10.0/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=41369)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=41369)\u001b[0m Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "\u001b[2m\u001b[36m(pid=41366)\u001b[0m WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/horovod/tensorflow/__init__.py:163: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\u001b[2m\u001b[36m(pid=41366)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=41366)\u001b[0m WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/horovod/tensorflow/__init__.py:189: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\u001b[2m\u001b[36m(pid=41366)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=41370)\u001b[0m WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/horovod/tensorflow/__init__.py:163: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\u001b[2m\u001b[36m(pid=41370)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=41370)\u001b[0m WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/horovod/tensorflow/__init__.py:189: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\u001b[2m\u001b[36m(pid=41370)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=41373)\u001b[0m WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/horovod/tensorflow/__init__.py:163: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\u001b[2m\u001b[36m(pid=41373)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=41373)\u001b[0m WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/horovod/tensorflow/__init__.py:189: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\u001b[2m\u001b[36m(pid=41373)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=41367)\u001b[0m WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/horovod/tensorflow/__init__.py:163: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\u001b[2m\u001b[36m(pid=41367)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=41367)\u001b[0m WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/horovod/tensorflow/__init__.py:189: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\u001b[2m\u001b[36m(pid=41367)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=41368)\u001b[0m WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/horovod/tensorflow/__init__.py:163: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\u001b[2m\u001b[36m(pid=41368)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=41368)\u001b[0m WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/horovod/tensorflow/__init__.py:189: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\u001b[2m\u001b[36m(pid=41368)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=41369)\u001b[0m WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/horovod/tensorflow/__init__.py:163: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\u001b[2m\u001b[36m(pid=41369)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=41369)\u001b[0m WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/horovod/tensorflow/__init__.py:189: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\u001b[2m\u001b[36m(pid=41369)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=41373)\u001b[0m [2021-10-16 15:44:31.333 ip-172-16-19-112:41373 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=41366)\u001b[0m [2021-10-16 15:44:31.357 ip-172-16-19-112:41366 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=41366)\u001b[0m [2021-10-16 15:44:31.391 ip-172-16-19-112:41366 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=41373)\u001b[0m [2021-10-16 15:44:31.367 ip-172-16-19-112:41373 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=41367)\u001b[0m [2021-10-16 15:44:31.403 ip-172-16-19-112:41367 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=41367)\u001b[0m [2021-10-16 15:44:31.437 ip-172-16-19-112:41367 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=41368)\u001b[0m [2021-10-16 15:44:31.381 ip-172-16-19-112:41368 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=41368)\u001b[0m [2021-10-16 15:44:31.415 ip-172-16-19-112:41368 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=41369)\u001b[0m [2021-10-16 15:44:31.395 ip-172-16-19-112:41369 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=41369)\u001b[0m [2021-10-16 15:44:31.429 ip-172-16-19-112:41369 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=41370)\u001b[0m [2021-10-16 15:44:31.351 ip-172-16-19-112:41370 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=41370)\u001b[0m [2021-10-16 15:44:31.385 ip-172-16-19-112:41370 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=41261)\u001b[0m WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/gpu_cuda10.0/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=41261)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=41261)\u001b[0m If using Keras pass *_constraint arguments to layers.\n",
      "\u001b[2m\u001b[36m(pid=41259)\u001b[0m WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/gpu_cuda10.0/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=41259)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=41259)\u001b[0m If using Keras pass *_constraint arguments to layers.\n",
      "\u001b[2m\u001b[36m(pid=41260)\u001b[0m WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/gpu_cuda10.0/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=41260)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=41260)\u001b[0m If using Keras pass *_constraint arguments to layers.\n",
      "\u001b[2m\u001b[36m(pid=41259)\u001b[0m 2021-10-16 15:44:32.666603: F tensorflow/stream_executor/cuda/cuda_driver.cc:175] Check failed: err == cudaSuccess || err == cudaErrorInvalidValue Unexpected CUDA error: out of memory\n",
      "\u001b[2m\u001b[36m(pid=41259)\u001b[0m *** SIGABRT received at time=1634399072 on cpu 5 ***\n",
      "\u001b[2m\u001b[36m(pid=41259)\u001b[0m PC: @     0x7fef6733a3b7  (unknown)  raise\n",
      "\u001b[2m\u001b[36m(pid=41259)\u001b[0m     @     0x7fef67ff2600  (unknown)  (unknown)\n",
      "\u001b[2m\u001b[36m(pid=41261)\u001b[0m WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/gpu_cuda10.0/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=41261)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=41261)\u001b[0m Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "\u001b[2m\u001b[36m(pid=41260)\u001b[0m WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/gpu_cuda10.0/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=41260)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=41260)\u001b[0m Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "\u001b[2m\u001b[36m(pid=41259)\u001b[0m     @     0x7fcb3e813f7c        640  stream_executor::gpu::(anonymous namespace)::CheckPointerIsValid<>()\n",
      "\u001b[2m\u001b[36m(pid=41259)\u001b[0m     @     0x7fcb3e81c49b        560  stream_executor::gpu::GpuDriver::AsynchronousMemcpyH2D()\n",
      "\u001b[2m\u001b[36m(pid=41259)\u001b[0m     @     0x7fcb3e8db8e5        608  stream_executor::Stream::ThenMemcpy()\n",
      "\u001b[2m\u001b[36m(pid=41260)\u001b[0m WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/horovod/tensorflow/__init__.py:163: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\u001b[2m\u001b[36m(pid=41260)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=41260)\u001b[0m WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/horovod/tensorflow/__init__.py:189: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\u001b[2m\u001b[36m(pid=41260)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPO pid=41260)\u001b[0m [2021-10-16 15:44:34.095 ip-172-16-19-112:41260 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "\u001b[2m\u001b[36m(PPO pid=41260)\u001b[0m [2021-10-16 15:44:34.126 ip-172-16-19-112:41260 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "\u001b[2m\u001b[36m(PPO pid=41261)\u001b[0m [2021-10-16 15:44:34.128 ip-172-16-19-112:41261 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "\u001b[2m\u001b[36m(PPO pid=41261)\u001b[0m [2021-10-16 15:44:34.160 ip-172-16-19-112:41261 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=41261)\u001b[0m WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/horovod/tensorflow/__init__.py:163: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\u001b[2m\u001b[36m(pid=41261)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=41261)\u001b[0m WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/horovod/tensorflow/__init__.py:189: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\u001b[2m\u001b[36m(pid=41261)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=41259)\u001b[0m     @     0x7fcb32dc61aa        720  tensorflow::GPUUtil::CopyCPUTensorToGPU()\n",
      "\u001b[2m\u001b[36m(pid=41259)\u001b[0m     @     0x7fcb32dc79f9        128  tensorflow::GPUDeviceContext::CopyCPUTensorToDevice()\n",
      "\u001b[2m\u001b[36m(pid=41259)\u001b[0m     @     0x7fcb32dafe82        400  tensorflow::BaseGPUDevice::MaybeCopyTensorToGPU()\n",
      "\u001b[2m\u001b[36m(pid=41259)\u001b[0m     @     0x7fcb32db2bb0        352  tensorflow::BaseGPUDevice::MakeTensorFromProto()\n",
      "\u001b[2m\u001b[36m(pid=41259)\u001b[0m     @     0x7fcb3a10f211        192  tensorflow::ConstantOp::ConstantOp()\n",
      "\u001b[2m\u001b[36m(pid=41259)\u001b[0m     @     0x7fcb3a10f792         32  tensorflow::{lambda()#15}::_FUN()\n",
      "\u001b[2m\u001b[36m(pid=41259)\u001b[0m     @     0x7fcb32b63d65       1040  tensorflow::CreateOpKernel()\n",
      "\u001b[2m\u001b[36m(pid=41259)\u001b[0m     @     0x7fcb32e0bde1        144  tensorflow::CreateNonCachedKernel()\n",
      "\u001b[2m\u001b[36m(pid=41259)\u001b[0m     @     0x7fcb32e28cc1       1200  tensorflow::FunctionLibraryRuntimeImpl::CreateKernel()\n",
      "\u001b[2m\u001b[36m(pid=41259)\u001b[0m     @     0x7fcb32e29497         32  tensorflow::FunctionLibraryRuntimeImpl::CreateKernel()\n",
      "\u001b[2m\u001b[36m(pid=41259)\u001b[0m     @     0x7fcb390d7a73        112  std::_Function_handler<>::_M_invoke()\n",
      "\u001b[2m\u001b[36m(pid=41259)\u001b[0m     @     0x7fcb32e1a239        816  tensorflow::(anonymous namespace)::ExecutorImpl::Initialize()\n",
      "\u001b[2m\u001b[36m(pid=41259)\u001b[0m     @     0x7fcb32e1b266         64  tensorflow::NewLocalExecutor()\n",
      "\u001b[2m\u001b[36m(pid=41259)\u001b[0m     @     0x7fcb32e1b2ed         80  tensorflow::(anonymous namespace)::DefaultExecutorRegistrar::Factory::NewExecutor()\n",
      "\u001b[2m\u001b[36m(pid=41259)\u001b[0m     @     0x7fcb32e1bb36        112  tensorflow::NewExecutor()\n",
      "\u001b[2m\u001b[36m(pid=41259)\u001b[0m     @     0x7fcb390e9280       1008  tensorflow::DirectSession::CreateExecutors()\n",
      "\u001b[2m\u001b[36m(pid=41259)\u001b[0m     @     0x7fcb390eaf43        976  tensorflow::DirectSession::GetOrCreateExecutors()\n",
      "\u001b[2m\u001b[36m(pid=41259)\u001b[0m     @     0x7fcb390ec8f7        896  tensorflow::DirectSession::Run()\n",
      "\u001b[2m\u001b[36m(pid=41259)\u001b[0m     @     0x7fcb35f155dd        176  tensorflow::SessionRef::Run()\n",
      "\u001b[2m\u001b[36m(pid=41259)\u001b[0m     @     0x7fcb365bcaf3        624  TF_Run_Helper()\n",
      "\u001b[2m\u001b[36m(pid=41259)\u001b[0m     @     0x7fcb365bd80a        256  TF_SessionRun\n",
      "\u001b[2m\u001b[36m(pid=41259)\u001b[0m     @     0x7fcb35f125c4        496  tensorflow::TF_SessionRun_wrapper_helper()\n",
      "\u001b[2m\u001b[36m(pid=41259)\u001b[0m     @     0x7fcb35f126b2         48  tensorflow::TF_SessionRun_wrapper()\n",
      "\u001b[2m\u001b[36m(pid=41259)\u001b[0m     @     0x7fcb35ecb67e        400  _wrap_TF_SessionRun_wrapper\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 9.0/32 CPUs, 3.0/8 GPUs, 0.0/323.51 GiB heap, 0.0/142.64 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 3/3 (3 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">    lr</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00000</td><td>RUNNING </td><td>     </td><td style=\"text-align: right;\">0.01  </td></tr>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00001</td><td>RUNNING </td><td>     </td><td style=\"text-align: right;\">0.001 </td></tr>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00002</td><td>RUNNING </td><td>     </td><td style=\"text-align: right;\">0.0001</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=41259)\u001b[0m     @     0x5608068faef4  (unknown)  call_function\n",
      "\u001b[2m\u001b[36m(pid=41259)\u001b[0m     @     0x560806ae06e0  (unknown)  (unknown)\n",
      "\u001b[2m\u001b[36m(pid=41259)\u001b[0m [2021-10-16 15:44:35,585 E 41259 41259] logging.cc:315: *** SIGABRT received at time=1634399075 on cpu 5 ***\n",
      "\u001b[2m\u001b[36m(pid=41259)\u001b[0m [2021-10-16 15:44:35,586 E 41259 41259] logging.cc:315: PC: @     0x7fef6733a3b7  (unknown)  raise\n",
      "\u001b[2m\u001b[36m(pid=41259)\u001b[0m [2021-10-16 15:44:35,589 E 41259 41259] logging.cc:315:     @     0x7fef67ff2600  (unknown)  (unknown)\n",
      "\u001b[2m\u001b[36m(pid=41259)\u001b[0m [2021-10-16 15:44:35,589 E 41259 41259] logging.cc:315:     @     0x7fcb3e813f7c        640  stream_executor::gpu::(anonymous namespace)::CheckPointerIsValid<>()\n",
      "\u001b[2m\u001b[36m(pid=41259)\u001b[0m [2021-10-16 15:44:35,589 E 41259 41259] logging.cc:315:     @     0x7fcb3e81c49b        560  stream_executor::gpu::GpuDriver::AsynchronousMemcpyH2D()\n",
      "\u001b[2m\u001b[36m(pid=41259)\u001b[0m [2021-10-16 15:44:35,589 E 41259 41259] logging.cc:315:     @     0x7fcb3e8db8e5        608  stream_executor::Stream::ThenMemcpy()\n",
      "\u001b[2m\u001b[36m(pid=41259)\u001b[0m [2021-10-16 15:44:35,589 E 41259 41259] logging.cc:315:     @     0x7fcb32dc61aa        720  tensorflow::GPUUtil::CopyCPUTensorToGPU()\n",
      "\u001b[2m\u001b[36m(pid=41259)\u001b[0m [2021-10-16 15:44:35,589 E 41259 41259] logging.cc:315:     @     0x7fcb32dc79f9        128  tensorflow::GPUDeviceContext::CopyCPUTensorToDevice()\n",
      "\u001b[2m\u001b[36m(pid=41259)\u001b[0m [2021-10-16 15:44:35,589 E 41259 41259] logging.cc:315:     @     0x7fcb32dafe82        400  tensorflow::BaseGPUDevice::MaybeCopyTensorToGPU()\n",
      "\u001b[2m\u001b[36m(pid=41259)\u001b[0m [2021-10-16 15:44:35,589 E 41259 41259] logging.cc:315:     @     0x7fcb32db2bb0        352  tensorflow::BaseGPUDevice::MakeTensorFromProto()\n",
      "\u001b[2m\u001b[36m(pid=41259)\u001b[0m [2021-10-16 15:44:35,589 E 41259 41259] logging.cc:315:     @     0x7fcb3a10f211        192  tensorflow::ConstantOp::ConstantOp()\n",
      "\u001b[2m\u001b[36m(pid=41259)\u001b[0m [2021-10-16 15:44:35,589 E 41259 41259] logging.cc:315:     @     0x7fcb3a10f792         32  tensorflow::{lambda()#15}::_FUN()\n",
      "\u001b[2m\u001b[36m(pid=41259)\u001b[0m [2021-10-16 15:44:35,589 E 41259 41259] logging.cc:315:     @     0x7fcb32b63d65       1040  tensorflow::CreateOpKernel()\n",
      "\u001b[2m\u001b[36m(pid=41259)\u001b[0m [2021-10-16 15:44:35,589 E 41259 41259] logging.cc:315:     @     0x7fcb32e0bde1        144  tensorflow::CreateNonCachedKernel()\n",
      "\u001b[2m\u001b[36m(pid=41259)\u001b[0m [2021-10-16 15:44:35,590 E 41259 41259] logging.cc:315:     @     0x7fcb32e28cc1       1200  tensorflow::FunctionLibraryRuntimeImpl::CreateKernel()\n",
      "\u001b[2m\u001b[36m(pid=41259)\u001b[0m [2021-10-16 15:44:35,590 E 41259 41259] logging.cc:315:     @     0x7fcb32e29497         32  tensorflow::FunctionLibraryRuntimeImpl::CreateKernel()\n",
      "\u001b[2m\u001b[36m(pid=41259)\u001b[0m [2021-10-16 15:44:35,590 E 41259 41259] logging.cc:315:     @     0x7fcb390d7a73        112  std::_Function_handler<>::_M_invoke()\n",
      "\u001b[2m\u001b[36m(pid=41259)\u001b[0m [2021-10-16 15:44:35,590 E 41259 41259] logging.cc:315:     @     0x7fcb32e1a239        816  tensorflow::(anonymous namespace)::ExecutorImpl::Initialize()\n",
      "\u001b[2m\u001b[36m(pid=41259)\u001b[0m [2021-10-16 15:44:35,590 E 41259 41259] logging.cc:315:     @     0x7fcb32e1b266         64  tensorflow::NewLocalExecutor()\n",
      "\u001b[2m\u001b[36m(pid=41259)\u001b[0m [2021-10-16 15:44:35,590 E 41259 41259] logging.cc:315:     @     0x7fcb32e1b2ed         80  tensorflow::(anonymous namespace)::DefaultExecutorRegistrar::Factory::NewExecutor()\n",
      "\u001b[2m\u001b[36m(pid=41259)\u001b[0m [2021-10-16 15:44:35,590 E 41259 41259] logging.cc:315:     @     0x7fcb32e1bb36        112  tensorflow::NewExecutor()\n",
      "\u001b[2m\u001b[36m(pid=41259)\u001b[0m [2021-10-16 15:44:35,590 E 41259 41259] logging.cc:315:     @     0x7fcb390e9280       1008  tensorflow::DirectSession::CreateExecutors()\n",
      "\u001b[2m\u001b[36m(pid=41259)\u001b[0m [2021-10-16 15:44:35,590 E 41259 41259] logging.cc:315:     @     0x7fcb390eaf43        976  tensorflow::DirectSession::GetOrCreateExecutors()\n",
      "\u001b[2m\u001b[36m(pid=41259)\u001b[0m [2021-10-16 15:44:35,590 E 41259 41259] logging.cc:315:     @     0x7fcb390ec8f7        896  tensorflow::DirectSession::Run()\n",
      "\u001b[2m\u001b[36m(pid=41259)\u001b[0m [2021-10-16 15:44:35,590 E 41259 41259] logging.cc:315:     @     0x7fcb35f155dd        176  tensorflow::SessionRef::Run()\n",
      "\u001b[2m\u001b[36m(pid=41259)\u001b[0m [2021-10-16 15:44:35,590 E 41259 41259] logging.cc:315:     @     0x7fcb365bcaf3        624  TF_Run_Helper()\n",
      "\u001b[2m\u001b[36m(pid=41259)\u001b[0m [2021-10-16 15:44:35,590 E 41259 41259] logging.cc:315:     @     0x7fcb365bd80a        256  TF_SessionRun\n",
      "\u001b[2m\u001b[36m(pid=41259)\u001b[0m [2021-10-16 15:44:35,590 E 41259 41259] logging.cc:315:     @     0x7fcb35f125c4        496  tensorflow::TF_SessionRun_wrapper_helper()\n",
      "\u001b[2m\u001b[36m(pid=41259)\u001b[0m [2021-10-16 15:44:35,590 E 41259 41259] logging.cc:315:     @     0x7fcb35f126b2         48  tensorflow::TF_SessionRun_wrapper()\n",
      "\u001b[2m\u001b[36m(pid=41259)\u001b[0m [2021-10-16 15:44:35,590 E 41259 41259] logging.cc:315:     @     0x7fcb35ecb67e        400  _wrap_TF_SessionRun_wrapper\n",
      "\u001b[2m\u001b[36m(pid=41259)\u001b[0m [2021-10-16 15:44:35,590 E 41259 41259] logging.cc:315:     @     0x5608068faef4  (unknown)  call_function\n",
      "\u001b[2m\u001b[36m(pid=41259)\u001b[0m [2021-10-16 15:44:35,595 E 41259 41259] logging.cc:315:     @     0x560806ae06e0  (unknown)  (unknown)\n",
      "2021-10-16 15:44:35,671\tWARNING worker.py:1227 -- A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffff950b438c7b5d71c92c8177f201000000 Worker ID: 995bcf77866c8ed0e87df8a13afa0f333bef29e0a6c7b520841c051b Node ID: 3200506232bdb108316c51449689d52a925bef627a537751fa44b299 Worker IP address: 172.16.19.112 Worker port: 45425 Worker PID: 41259\n",
      "2021-10-16 15:44:35,672\tERROR trial_runner.py:846 -- Trial PPO_CartPole-v0_eda9d_00001: Error processing event.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ray/tune/trial_runner.py\", line 812, in _process_trial\n",
      "    results = self.trial_executor.fetch_result(trial)\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ray/tune/ray_trial_executor.py\", line 767, in fetch_result\n",
      "    result = ray.get(trial_future[0], timeout=DEFAULT_GET_TIMEOUT)\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ray/_private/client_mode_hook.py\", line 89, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ray/worker.py\", line 1623, in get\n",
      "    raise value\n",
      "ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_eda9d_00001:\n",
      "  {}\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=41260)\u001b[0m 2021-10-16 15:44:36,295\tWARNING trainer_template.py:186 -- `execution_plan` functions should accept `trainer`, `workers`, and `config` as args!\n",
      "\u001b[2m\u001b[36m(pid=41260)\u001b[0m 2021-10-16 15:44:36,295\tINFO trainable.py:112 -- Trainable.setup took 11.291 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=41261)\u001b[0m 2021-10-16 15:44:36,463\tWARNING trainer_template.py:186 -- `execution_plan` functions should accept `trainer`, `workers`, and `config` as args!\n",
      "\u001b[2m\u001b[36m(pid=41261)\u001b[0m 2021-10-16 15:44:36,464\tINFO trainable.py:112 -- Trainable.setup took 11.463 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.7/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/32 CPUs, 2.0/8 GPUs, 0.0/323.51 GiB heap, 0.0/142.64 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 3/3 (1 ERROR, 2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">    lr</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00000</td><td>RUNNING </td><td>     </td><td style=\"text-align: right;\">0.01  </td></tr>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00002</td><td>RUNNING </td><td>     </td><td style=\"text-align: right;\">0.0001</td></tr>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00001</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">0.001 </td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                         </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00001</td><td style=\"text-align: right;\">           1</td><td>/home/ec2-user/ray_results/PPO/PPO_CartPole-v0_eda9d_00001_1_lr=0.001_2021-10-16_15-44-21/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_eda9d_00000:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-16_15-44-44\n",
      "  done: false\n",
      "  episode_len_mean: 22.92485549132948\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 78.0\n",
      "  episode_reward_mean: 22.92485549132948\n",
      "  episode_reward_min: 8.0\n",
      "  episodes_this_iter: 173\n",
      "  episodes_total: 173\n",
      "  experiment_id: fee4be6e696a4767b3e6c9df50a77f99\n",
      "  hostname: ip-172-16-19-112\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 0.009999999776482582\n",
      "          entropy: 0.6559051871299744\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.039926424622535706\n",
      "          model: {}\n",
      "          policy_loss: -0.04267517849802971\n",
      "          total_loss: 92.9194107055664\n",
      "          vf_explained_var: 0.3246573805809021\n",
      "          vf_loss: 92.9541015625\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 172.16.19.112\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.681818181818182\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.1927272727272727\n",
      "    gpu_util_percent2: 0.17545454545454547\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.8909090909090907\n",
      "    vram_util_percent0: 0.9886373568743991\n",
      "    vram_util_percent1: 0.9591818896949568\n",
      "    vram_util_percent2: 0.9591818896949568\n",
      "    vram_util_percent3: 0.011362643125600909\n",
      "    vram_util_percent4: 0.011362643125600909\n",
      "    vram_util_percent5: 0.011362643125600909\n",
      "    vram_util_percent6: 0.011362643125600909\n",
      "    vram_util_percent7: 0.011362643125600909\n",
      "  pid: 41260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07025987799270743\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07913613227589707\n",
      "    mean_inference_ms: 1.1474377202483113\n",
      "    mean_raw_obs_processing_ms: 0.09996007874401035\n",
      "  time_since_restore: 8.666732549667358\n",
      "  time_this_iter_s: 8.666732549667358\n",
      "  time_total_s: 8.666732549667358\n",
      "  timers:\n",
      "    learn_throughput: 713.06\n",
      "    learn_time_ms: 5609.624\n",
      "    load_throughput: 59590.455\n",
      "    load_time_ms: 67.125\n",
      "    sample_throughput: 1356.426\n",
      "    sample_time_ms: 2948.926\n",
      "    update_time_ms: 3.443\n",
      "  timestamp: 1634399084\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: eda9d_00000\n",
      "  \n",
      "Result for PPO_CartPole-v0_eda9d_00002:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-16_15-44-45\n",
      "  done: false\n",
      "  episode_len_mean: 22.982658959537574\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 73.0\n",
      "  episode_reward_mean: 22.982658959537574\n",
      "  episode_reward_min: 9.0\n",
      "  episodes_this_iter: 173\n",
      "  episodes_total: 173\n",
      "  experiment_id: 36a8061aa2d24b2a90c2e8a7584a9cbe\n",
      "  hostname: ip-172-16-19-112\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.6627019047737122\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.030518757179379463\n",
      "          model: {}\n",
      "          policy_loss: -0.04533430561423302\n",
      "          total_loss: 153.71444702148438\n",
      "          vf_explained_var: 0.05318145081400871\n",
      "          vf_loss: 153.75369262695312\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 172.16.19.112\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.636363636363635\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.1827272727272727\n",
      "    gpu_util_percent2: 0.1909090909090909\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.8999999999999995\n",
      "    vram_util_percent0: 0.9886373568743991\n",
      "    vram_util_percent1: 0.9591818896949568\n",
      "    vram_util_percent2: 0.9591818896949568\n",
      "    vram_util_percent3: 0.011362643125600909\n",
      "    vram_util_percent4: 0.011362643125600909\n",
      "    vram_util_percent5: 0.011362643125600909\n",
      "    vram_util_percent6: 0.011362643125600909\n",
      "    vram_util_percent7: 0.011362643125600909\n",
      "  pid: 41261\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07135971801634218\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07904829551946108\n",
      "    mean_inference_ms: 1.1457480140657363\n",
      "    mean_raw_obs_processing_ms: 0.10102079564889849\n",
      "  time_since_restore: 8.5901460647583\n",
      "  time_this_iter_s: 8.5901460647583\n",
      "  time_total_s: 8.5901460647583\n",
      "  timers:\n",
      "    learn_throughput: 725.297\n",
      "    learn_time_ms: 5514.985\n",
      "    load_throughput: 61881.83\n",
      "    load_time_ms: 64.639\n",
      "    sample_throughput: 1346.374\n",
      "    sample_time_ms: 2970.943\n",
      "    update_time_ms: 3.479\n",
      "  timestamp: 1634399085\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: eda9d_00002\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.7/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/32 CPUs, 2.0/8 GPUs, 0.0/323.51 GiB heap, 0.0/142.64 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 3/3 (1 ERROR, 2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00000</td><td>RUNNING </td><td>172.16.19.112:41260</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         8.66673</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\"> 22.9249</td><td style=\"text-align: right;\">                  78</td><td style=\"text-align: right;\">                   8</td><td style=\"text-align: right;\">           22.9249</td></tr>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00002</td><td>RUNNING </td><td>172.16.19.112:41261</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         8.59015</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\"> 22.9827</td><td style=\"text-align: right;\">                  73</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">           22.9827</td></tr>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00001</td><td>ERROR   </td><td>                   </td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                         </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00001</td><td style=\"text-align: right;\">           1</td><td>/home/ec2-user/ray_results/PPO/PPO_CartPole-v0_eda9d_00001_1_lr=0.001_2021-10-16_15-44-21/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.7/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/32 CPUs, 2.0/8 GPUs, 0.0/323.51 GiB heap, 0.0/142.64 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 3/3 (1 ERROR, 2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00000</td><td>RUNNING </td><td>172.16.19.112:41260</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         8.66673</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\"> 22.9249</td><td style=\"text-align: right;\">                  78</td><td style=\"text-align: right;\">                   8</td><td style=\"text-align: right;\">           22.9249</td></tr>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00002</td><td>RUNNING </td><td>172.16.19.112:41261</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         8.59015</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\"> 22.9827</td><td style=\"text-align: right;\">                  73</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">           22.9827</td></tr>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00001</td><td>ERROR   </td><td>                   </td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                         </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00001</td><td style=\"text-align: right;\">           1</td><td>/home/ec2-user/ray_results/PPO/PPO_CartPole-v0_eda9d_00001_1_lr=0.001_2021-10-16_15-44-21/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_eda9d_00000:\n",
      "  agent_timesteps_total: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-16_15-44-52\n",
      "  done: false\n",
      "  episode_len_mean: 50.89\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 177.0\n",
      "  episode_reward_mean: 50.89\n",
      "  episode_reward_min: 8.0\n",
      "  episodes_this_iter: 51\n",
      "  episodes_total: 224\n",
      "  experiment_id: fee4be6e696a4767b3e6c9df50a77f99\n",
      "  hostname: ip-172-16-19-112\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 0.009999999776482582\n",
      "          entropy: 0.5796728134155273\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02672765962779522\n",
      "          model: {}\n",
      "          policy_loss: -0.015455672517418861\n",
      "          total_loss: 441.8940124511719\n",
      "          vf_explained_var: 0.24405714869499207\n",
      "          vf_loss: 441.9041442871094\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 8000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 172.16.19.112\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.27\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.21799999999999997\n",
      "    gpu_util_percent2: 0.202\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.8999999999999995\n",
      "    vram_util_percent0: 0.9886373568743991\n",
      "    vram_util_percent1: 0.9591818896949567\n",
      "    vram_util_percent2: 0.9591818896949567\n",
      "    vram_util_percent3: 0.011362643125600909\n",
      "    vram_util_percent4: 0.011362643125600909\n",
      "    vram_util_percent5: 0.011362643125600909\n",
      "    vram_util_percent6: 0.011362643125600909\n",
      "    vram_util_percent7: 0.011362643125600909\n",
      "  pid: 41260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06985132392282202\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07884686725610347\n",
      "    mean_inference_ms: 1.1373584746055596\n",
      "    mean_raw_obs_processing_ms: 0.09580201825728726\n",
      "  time_since_restore: 16.554983615875244\n",
      "  time_this_iter_s: 7.888251066207886\n",
      "  time_total_s: 16.554983615875244\n",
      "  timers:\n",
      "    learn_throughput: 740.734\n",
      "    learn_time_ms: 5400.051\n",
      "    load_throughput: 114508.131\n",
      "    load_time_ms: 34.932\n",
      "    sample_throughput: 700.293\n",
      "    sample_time_ms: 5711.899\n",
      "    update_time_ms: 3.377\n",
      "  timestamp: 1634399092\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 2\n",
      "  trial_id: eda9d_00000\n",
      "  \n",
      "Result for PPO_CartPole-v0_eda9d_00002:\n",
      "  agent_timesteps_total: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-16_15-44-52\n",
      "  done: false\n",
      "  episode_len_mean: 42.62\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 181.0\n",
      "  episode_reward_mean: 42.62\n",
      "  episode_reward_min: 10.0\n",
      "  episodes_this_iter: 89\n",
      "  episodes_total: 262\n",
      "  experiment_id: 36a8061aa2d24b2a90c2e8a7584a9cbe\n",
      "  hostname: ip-172-16-19-112\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.5963361859321594\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.020783523097634315\n",
      "          model: {}\n",
      "          policy_loss: -0.032203998416662216\n",
      "          total_loss: 289.6868896484375\n",
      "          vf_explained_var: 0.10902044922113419\n",
      "          vf_loss: 289.7149658203125\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 8000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 172.16.19.112\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.239999999999998\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.217\n",
      "    gpu_util_percent2: 0.24\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.8999999999999995\n",
      "    vram_util_percent0: 0.9886373568743991\n",
      "    vram_util_percent1: 0.9591818896949567\n",
      "    vram_util_percent2: 0.9591818896949567\n",
      "    vram_util_percent3: 0.011362643125600909\n",
      "    vram_util_percent4: 0.011362643125600909\n",
      "    vram_util_percent5: 0.011362643125600909\n",
      "    vram_util_percent6: 0.011362643125600909\n",
      "    vram_util_percent7: 0.011362643125600909\n",
      "  pid: 41261\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07016447363955387\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07819705547518643\n",
      "    mean_inference_ms: 1.1137383272533763\n",
      "    mean_raw_obs_processing_ms: 0.09585112427614659\n",
      "  time_since_restore: 16.44606876373291\n",
      "  time_this_iter_s: 7.855922698974609\n",
      "  time_total_s: 16.44606876373291\n",
      "  timers:\n",
      "    learn_throughput: 750.516\n",
      "    learn_time_ms: 5329.67\n",
      "    load_throughput: 117301.451\n",
      "    load_time_ms: 34.1\n",
      "    sample_throughput: 704.445\n",
      "    sample_time_ms: 5678.232\n",
      "    update_time_ms: 3.286\n",
      "  timestamp: 1634399092\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 2\n",
      "  trial_id: eda9d_00002\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.7/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/32 CPUs, 2.0/8 GPUs, 0.0/323.51 GiB heap, 0.0/142.64 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 3/3 (1 ERROR, 2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00000</td><td>RUNNING </td><td>172.16.19.112:41260</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         16.555 </td><td style=\"text-align: right;\">8000</td><td style=\"text-align: right;\">   50.89</td><td style=\"text-align: right;\">                 177</td><td style=\"text-align: right;\">                   8</td><td style=\"text-align: right;\">             50.89</td></tr>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00002</td><td>RUNNING </td><td>172.16.19.112:41261</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         16.4461</td><td style=\"text-align: right;\">8000</td><td style=\"text-align: right;\">   42.62</td><td style=\"text-align: right;\">                 181</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">             42.62</td></tr>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00001</td><td>ERROR   </td><td>                   </td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                         </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00001</td><td style=\"text-align: right;\">           1</td><td>/home/ec2-user/ray_results/PPO/PPO_CartPole-v0_eda9d_00001_1_lr=0.001_2021-10-16_15-44-21/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_eda9d_00002:\n",
      "  agent_timesteps_total: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-16_15-45-00\n",
      "  done: false\n",
      "  episode_len_mean: 65.2\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 194.0\n",
      "  episode_reward_mean: 65.2\n",
      "  episode_reward_min: 10.0\n",
      "  episodes_this_iter: 41\n",
      "  episodes_total: 303\n",
      "  experiment_id: 36a8061aa2d24b2a90c2e8a7584a9cbe\n",
      "  hostname: ip-172-16-19-112\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.566329836845398\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010319194756448269\n",
      "          model: {}\n",
      "          policy_loss: -0.02498255856335163\n",
      "          total_loss: 481.6850891113281\n",
      "          vf_explained_var: 0.19725137948989868\n",
      "          vf_loss: 481.7080078125\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 172.16.19.112\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.566666666666668\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.23666666666666666\n",
      "    gpu_util_percent2: 0.2288888888888889\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.9\n",
      "    vram_util_percent0: 0.9886373568743991\n",
      "    vram_util_percent1: 0.9591818896949567\n",
      "    vram_util_percent2: 0.9591818896949567\n",
      "    vram_util_percent3: 0.011362643125600909\n",
      "    vram_util_percent4: 0.011362643125600909\n",
      "    vram_util_percent5: 0.011362643125600909\n",
      "    vram_util_percent6: 0.011362643125600909\n",
      "    vram_util_percent7: 0.011362643125600909\n",
      "  pid: 41261\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07006932651066117\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07793253640051385\n",
      "    mean_inference_ms: 1.1061050049298344\n",
      "    mean_raw_obs_processing_ms: 0.09385931400283087\n",
      "  time_since_restore: 24.27835512161255\n",
      "  time_this_iter_s: 7.832286357879639\n",
      "  time_total_s: 24.27835512161255\n",
      "  timers:\n",
      "    learn_throughput: 761.735\n",
      "    learn_time_ms: 5251.171\n",
      "    load_throughput: 168933.862\n",
      "    load_time_ms: 23.678\n",
      "    sample_throughput: 622.188\n",
      "    sample_time_ms: 6428.928\n",
      "    update_time_ms: 3.287\n",
      "  timestamp: 1634399100\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: eda9d_00002\n",
      "  \n",
      "Result for PPO_CartPole-v0_eda9d_00000:\n",
      "  agent_timesteps_total: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-16_15-45-00\n",
      "  done: false\n",
      "  episode_len_mean: 83.84\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 83.84\n",
      "  episode_reward_min: 9.0\n",
      "  episodes_this_iter: 31\n",
      "  episodes_total: 255\n",
      "  experiment_id: fee4be6e696a4767b3e6c9df50a77f99\n",
      "  hostname: ip-172-16-19-112\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 0.009999999776482582\n",
      "          entropy: 0.47230264544487\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.16511328518390656\n",
      "          model: {}\n",
      "          policy_loss: 0.030759310349822044\n",
      "          total_loss: 421.8064880371094\n",
      "          vf_explained_var: 0.38217154145240784\n",
      "          vf_loss: 421.74273681640625\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 172.16.19.112\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.390000000000002\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.22199999999999998\n",
      "    gpu_util_percent2: 0.223\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.8999999999999995\n",
      "    vram_util_percent0: 0.9886373568743991\n",
      "    vram_util_percent1: 0.9591818896949567\n",
      "    vram_util_percent2: 0.9591818896949567\n",
      "    vram_util_percent3: 0.011362643125600909\n",
      "    vram_util_percent4: 0.011362643125600909\n",
      "    vram_util_percent5: 0.011362643125600909\n",
      "    vram_util_percent6: 0.011362643125600909\n",
      "    vram_util_percent7: 0.011362643125600909\n",
      "  pid: 41260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06929746506194263\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07843126411254234\n",
      "    mean_inference_ms: 1.1173691334989118\n",
      "    mean_raw_obs_processing_ms: 0.0926054192568194\n",
      "  time_since_restore: 24.452184915542603\n",
      "  time_this_iter_s: 7.897201299667358\n",
      "  time_total_s: 24.452184915542603\n",
      "  timers:\n",
      "    learn_throughput: 751.237\n",
      "    learn_time_ms: 5324.554\n",
      "    load_throughput: 162637.688\n",
      "    load_time_ms: 24.595\n",
      "    sample_throughput: 619.056\n",
      "    sample_time_ms: 6461.449\n",
      "    update_time_ms: 3.369\n",
      "  timestamp: 1634399100\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: eda9d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.7/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/32 CPUs, 2.0/8 GPUs, 0.0/323.51 GiB heap, 0.0/142.64 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 3/3 (1 ERROR, 2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00000</td><td>RUNNING </td><td>172.16.19.112:41260</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         24.4522</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">   83.84</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">             83.84</td></tr>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00002</td><td>RUNNING </td><td>172.16.19.112:41261</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         24.2784</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">   65.2 </td><td style=\"text-align: right;\">                 194</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">             65.2 </td></tr>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00001</td><td>ERROR   </td><td>                   </td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                         </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00001</td><td style=\"text-align: right;\">           1</td><td>/home/ec2-user/ray_results/PPO/PPO_CartPole-v0_eda9d_00001_1_lr=0.001_2021-10-16_15-44-21/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.7/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/32 CPUs, 2.0/8 GPUs, 0.0/323.51 GiB heap, 0.0/142.64 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 3/3 (1 ERROR, 2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00000</td><td>RUNNING </td><td>172.16.19.112:41260</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         24.4522</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">   83.84</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">             83.84</td></tr>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00002</td><td>RUNNING </td><td>172.16.19.112:41261</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         24.2784</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">   65.2 </td><td style=\"text-align: right;\">                 194</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">             65.2 </td></tr>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00001</td><td>ERROR   </td><td>                   </td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                         </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00001</td><td style=\"text-align: right;\">           1</td><td>/home/ec2-user/ray_results/PPO/PPO_CartPole-v0_eda9d_00001_1_lr=0.001_2021-10-16_15-44-21/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_eda9d_00002:\n",
      "  agent_timesteps_total: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-16_15-45-08\n",
      "  done: false\n",
      "  episode_len_mean: 94.56\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 94.56\n",
      "  episode_reward_min: 10.0\n",
      "  episodes_this_iter: 26\n",
      "  episodes_total: 329\n",
      "  experiment_id: 36a8061aa2d24b2a90c2e8a7584a9cbe\n",
      "  hostname: ip-172-16-19-112\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.5372269153594971\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006838078144937754\n",
      "          model: {}\n",
      "          policy_loss: -0.016161803156137466\n",
      "          total_loss: 417.2235107421875\n",
      "          vf_explained_var: 0.33221960067749023\n",
      "          vf_loss: 417.2383117675781\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 16000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 172.16.19.112\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.459999999999999\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.21000000000000002\n",
      "    gpu_util_percent2: 0.219\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.8999999999999995\n",
      "    vram_util_percent0: 0.9886373568743991\n",
      "    vram_util_percent1: 0.9591818896949567\n",
      "    vram_util_percent2: 0.9591818896949567\n",
      "    vram_util_percent3: 0.011362643125600909\n",
      "    vram_util_percent4: 0.011362643125600909\n",
      "    vram_util_percent5: 0.011362643125600909\n",
      "    vram_util_percent6: 0.011362643125600909\n",
      "    vram_util_percent7: 0.011362643125600909\n",
      "  pid: 41261\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0699348598838749\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0778421273791679\n",
      "    mean_inference_ms: 1.1019601563173331\n",
      "    mean_raw_obs_processing_ms: 0.09253020031285025\n",
      "  time_since_restore: 32.05593967437744\n",
      "  time_this_iter_s: 7.777584552764893\n",
      "  time_total_s: 32.05593967437744\n",
      "  timers:\n",
      "    learn_throughput: 767.248\n",
      "    learn_time_ms: 5213.437\n",
      "    load_throughput: 216254.186\n",
      "    load_time_ms: 18.497\n",
      "    sample_throughput: 590.325\n",
      "    sample_time_ms: 6775.927\n",
      "    update_time_ms: 3.298\n",
      "  timestamp: 1634399108\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 4\n",
      "  trial_id: eda9d_00002\n",
      "  \n",
      "Result for PPO_CartPole-v0_eda9d_00000:\n",
      "  agent_timesteps_total: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-16_15-45-08\n",
      "  done: false\n",
      "  episode_len_mean: 117.39\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 117.39\n",
      "  episode_reward_min: 13.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 275\n",
      "  experiment_id: fee4be6e696a4767b3e6c9df50a77f99\n",
      "  hostname: ip-172-16-19-112\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 0.009999999776482582\n",
      "          entropy: 0.4653274416923523\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.07469668984413147\n",
      "          model: {}\n",
      "          policy_loss: 0.016748130321502686\n",
      "          total_loss: 513.47998046875\n",
      "          vf_explained_var: 0.3167491555213928\n",
      "          vf_loss: 513.4483032226562\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 16000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 172.16.19.112\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.459999999999999\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.21799999999999997\n",
      "    gpu_util_percent2: 0.22800000000000004\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.8999999999999995\n",
      "    vram_util_percent0: 0.9886373568743991\n",
      "    vram_util_percent1: 0.9591818896949567\n",
      "    vram_util_percent2: 0.9591818896949567\n",
      "    vram_util_percent3: 0.011362643125600909\n",
      "    vram_util_percent4: 0.011362643125600909\n",
      "    vram_util_percent5: 0.011362643125600909\n",
      "    vram_util_percent6: 0.011362643125600909\n",
      "    vram_util_percent7: 0.011362643125600909\n",
      "  pid: 41260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06896301464927289\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07820045006161008\n",
      "    mean_inference_ms: 1.1048654278497496\n",
      "    mean_raw_obs_processing_ms: 0.09036403162515218\n",
      "  time_since_restore: 32.44402480125427\n",
      "  time_this_iter_s: 7.99183988571167\n",
      "  time_total_s: 32.44402480125427\n",
      "  timers:\n",
      "    learn_throughput: 753.572\n",
      "    learn_time_ms: 5308.051\n",
      "    load_throughput: 209065.789\n",
      "    load_time_ms: 19.133\n",
      "    sample_throughput: 584.574\n",
      "    sample_time_ms: 6842.595\n",
      "    update_time_ms: 3.378\n",
      "  timestamp: 1634399108\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 4\n",
      "  trial_id: eda9d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.7/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/32 CPUs, 2.0/8 GPUs, 0.0/323.51 GiB heap, 0.0/142.64 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 3/3 (1 ERROR, 2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00000</td><td>RUNNING </td><td>172.16.19.112:41260</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         32.444 </td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\">  117.39</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">            117.39</td></tr>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00002</td><td>RUNNING </td><td>172.16.19.112:41261</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         32.0559</td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\">   94.56</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">             94.56</td></tr>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00001</td><td>ERROR   </td><td>                   </td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                         </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00001</td><td style=\"text-align: right;\">           1</td><td>/home/ec2-user/ray_results/PPO/PPO_CartPole-v0_eda9d_00001_1_lr=0.001_2021-10-16_15-44-21/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_eda9d_00002:\n",
      "  agent_timesteps_total: 20000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-16_15-45-16\n",
      "  done: false\n",
      "  episode_len_mean: 124.1\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 124.1\n",
      "  episode_reward_min: 10.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 350\n",
      "  experiment_id: 36a8061aa2d24b2a90c2e8a7584a9cbe\n",
      "  hostname: ip-172-16-19-112\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.5300061702728271\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00630270317196846\n",
      "          model: {}\n",
      "          policy_loss: -0.008943001739680767\n",
      "          total_loss: 356.69757080078125\n",
      "          vf_explained_var: 0.4037415683269501\n",
      "          vf_loss: 356.705322265625\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 20000\n",
      "    num_agent_steps_trained: 20000\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 172.16.19.112\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 13.88\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.2\n",
      "    gpu_util_percent2: 0.199\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.8999999999999995\n",
      "    vram_util_percent0: 0.9886373568743991\n",
      "    vram_util_percent1: 0.9591818896949567\n",
      "    vram_util_percent2: 0.9591818896949567\n",
      "    vram_util_percent3: 0.011362643125600909\n",
      "    vram_util_percent4: 0.011362643125600909\n",
      "    vram_util_percent5: 0.011362643125600909\n",
      "    vram_util_percent6: 0.011362643125600909\n",
      "    vram_util_percent7: 0.011362643125600909\n",
      "  pid: 41261\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06959450180295852\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07774947621862063\n",
      "    mean_inference_ms: 1.0975571938618809\n",
      "    mean_raw_obs_processing_ms: 0.0911498699355257\n",
      "  time_since_restore: 40.01454448699951\n",
      "  time_this_iter_s: 7.95860481262207\n",
      "  time_total_s: 40.01454448699951\n",
      "  timers:\n",
      "    learn_throughput: 766.998\n",
      "    learn_time_ms: 5215.14\n",
      "    load_throughput: 260017.544\n",
      "    load_time_ms: 15.384\n",
      "    sample_throughput: 571.574\n",
      "    sample_time_ms: 6998.224\n",
      "    update_time_ms: 3.26\n",
      "  timestamp: 1634399116\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: eda9d_00002\n",
      "  \n",
      "Result for PPO_CartPole-v0_eda9d_00000:\n",
      "  agent_timesteps_total: 20000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-16_15-45-16\n",
      "  done: false\n",
      "  episode_len_mean: 139.13\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 139.13\n",
      "  episode_reward_min: 13.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 296\n",
      "  experiment_id: fee4be6e696a4767b3e6c9df50a77f99\n",
      "  hostname: ip-172-16-19-112\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 0.009999999776482582\n",
      "          entropy: 0.42148369550704956\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.045561783015728\n",
      "          model: {}\n",
      "          policy_loss: 0.005295531824231148\n",
      "          total_loss: 456.5293884277344\n",
      "          vf_explained_var: 0.24893420934677124\n",
      "          vf_loss: 456.5149841308594\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 20000\n",
      "    num_agent_steps_trained: 20000\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 172.16.19.112\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 14.266666666666667\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.23333333333333334\n",
      "    gpu_util_percent2: 0.23333333333333334\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.9\n",
      "    vram_util_percent0: 0.9886373568743991\n",
      "    vram_util_percent1: 0.9591818896949567\n",
      "    vram_util_percent2: 0.9591818896949567\n",
      "    vram_util_percent3: 0.011362643125600909\n",
      "    vram_util_percent4: 0.011362643125600909\n",
      "    vram_util_percent5: 0.011362643125600909\n",
      "    vram_util_percent6: 0.011362643125600909\n",
      "    vram_util_percent7: 0.011362643125600909\n",
      "  pid: 41260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06880144511373232\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07803969505857018\n",
      "    mean_inference_ms: 1.1023150551932388\n",
      "    mean_raw_obs_processing_ms: 0.08903394214167515\n",
      "  time_since_restore: 40.395798683166504\n",
      "  time_this_iter_s: 7.9517738819122314\n",
      "  time_total_s: 40.395798683166504\n",
      "  timers:\n",
      "    learn_throughput: 754.786\n",
      "    learn_time_ms: 5299.513\n",
      "    load_throughput: 251619.239\n",
      "    load_time_ms: 15.897\n",
      "    sample_throughput: 565.119\n",
      "    sample_time_ms: 7078.16\n",
      "    update_time_ms: 3.34\n",
      "  timestamp: 1634399116\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: eda9d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.7/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/32 CPUs, 2.0/8 GPUs, 0.0/323.51 GiB heap, 0.0/142.64 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 3/3 (1 ERROR, 2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00000</td><td>RUNNING </td><td>172.16.19.112:41260</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         40.3958</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">  139.13</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">            139.13</td></tr>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00002</td><td>RUNNING </td><td>172.16.19.112:41261</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         40.0145</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">  124.1 </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">            124.1 </td></tr>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00001</td><td>ERROR   </td><td>                   </td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                         </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00001</td><td style=\"text-align: right;\">           1</td><td>/home/ec2-user/ray_results/PPO/PPO_CartPole-v0_eda9d_00001_1_lr=0.001_2021-10-16_15-44-21/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.7/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/32 CPUs, 2.0/8 GPUs, 0.0/323.51 GiB heap, 0.0/142.64 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 3/3 (1 ERROR, 2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00000</td><td>RUNNING </td><td>172.16.19.112:41260</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         40.3958</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">  139.13</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">            139.13</td></tr>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00002</td><td>RUNNING </td><td>172.16.19.112:41261</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         40.0145</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">  124.1 </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">            124.1 </td></tr>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00001</td><td>ERROR   </td><td>                   </td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                         </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00001</td><td style=\"text-align: right;\">           1</td><td>/home/ec2-user/ray_results/PPO/PPO_CartPole-v0_eda9d_00001_1_lr=0.001_2021-10-16_15-44-21/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_eda9d_00002:\n",
      "  agent_timesteps_total: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-16_15-45-24\n",
      "  done: false\n",
      "  episode_len_mean: 148.77\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 148.77\n",
      "  episode_reward_min: 15.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 370\n",
      "  experiment_id: 36a8061aa2d24b2a90c2e8a7584a9cbe\n",
      "  hostname: ip-172-16-19-112\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.5496629476547241\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005510628689080477\n",
      "          model: {}\n",
      "          policy_loss: -0.00802704319357872\n",
      "          total_loss: 316.805419921875\n",
      "          vf_explained_var: 0.47900041937828064\n",
      "          vf_loss: 316.81231689453125\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_steps_sampled: 24000\n",
      "    num_steps_trained: 24000\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 172.16.19.112\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.655555555555557\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.22666666666666668\n",
      "    gpu_util_percent2: 0.19444444444444445\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.9\n",
      "    vram_util_percent0: 0.9886373568743991\n",
      "    vram_util_percent1: 0.9591818896949567\n",
      "    vram_util_percent2: 0.9591818896949567\n",
      "    vram_util_percent3: 0.011362643125600909\n",
      "    vram_util_percent4: 0.011362643125600909\n",
      "    vram_util_percent5: 0.011362643125600909\n",
      "    vram_util_percent6: 0.011362643125600909\n",
      "    vram_util_percent7: 0.011362643125600909\n",
      "  pid: 41261\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06941729563819553\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07764892743529023\n",
      "    mean_inference_ms: 1.0940547222653418\n",
      "    mean_raw_obs_processing_ms: 0.08991265378745678\n",
      "  time_since_restore: 47.81762194633484\n",
      "  time_this_iter_s: 7.803077459335327\n",
      "  time_total_s: 47.81762194633484\n",
      "  timers:\n",
      "    learn_throughput: 769.861\n",
      "    learn_time_ms: 5195.741\n",
      "    load_throughput: 301070.719\n",
      "    load_time_ms: 13.286\n",
      "    sample_throughput: 558.593\n",
      "    sample_time_ms: 7160.847\n",
      "    update_time_ms: 3.263\n",
      "  timestamp: 1634399124\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 6\n",
      "  trial_id: eda9d_00002\n",
      "  \n",
      "Result for PPO_CartPole-v0_eda9d_00000:\n",
      "  agent_timesteps_total: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-16_15-45-24\n",
      "  done: false\n",
      "  episode_len_mean: 160.57\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 160.57\n",
      "  episode_reward_min: 13.0\n",
      "  episodes_this_iter: 24\n",
      "  episodes_total: 320\n",
      "  experiment_id: fee4be6e696a4767b3e6c9df50a77f99\n",
      "  hostname: ip-172-16-19-112\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 0.009999999776482582\n",
      "          entropy: 0.4125935733318329\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.10932371765375137\n",
      "          model: {}\n",
      "          policy_loss: 0.030703246593475342\n",
      "          total_loss: 282.7218017578125\n",
      "          vf_explained_var: 0.5843014717102051\n",
      "          vf_loss: 282.6692199707031\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_steps_sampled: 24000\n",
      "    num_steps_trained: 24000\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 172.16.19.112\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.54\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.21600000000000003\n",
      "    gpu_util_percent2: 0.20500000000000002\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.8999999999999995\n",
      "    vram_util_percent0: 0.9886373568743991\n",
      "    vram_util_percent1: 0.9591818896949567\n",
      "    vram_util_percent2: 0.9591818896949567\n",
      "    vram_util_percent3: 0.011362643125600909\n",
      "    vram_util_percent4: 0.011362643125600909\n",
      "    vram_util_percent5: 0.011362643125600909\n",
      "    vram_util_percent6: 0.011362643125600909\n",
      "    vram_util_percent7: 0.011362643125600909\n",
      "  pid: 41260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06876631168889193\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07808200436698443\n",
      "    mean_inference_ms: 1.096013020247298\n",
      "    mean_raw_obs_processing_ms: 0.08759261507638298\n",
      "  time_since_restore: 48.294018030166626\n",
      "  time_this_iter_s: 7.898219347000122\n",
      "  time_total_s: 48.294018030166626\n",
      "  timers:\n",
      "    learn_throughput: 757.873\n",
      "    learn_time_ms: 5277.929\n",
      "    load_throughput: 291946.914\n",
      "    load_time_ms: 13.701\n",
      "    sample_throughput: 552.557\n",
      "    sample_time_ms: 7239.07\n",
      "    update_time_ms: 3.323\n",
      "  timestamp: 1634399124\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 6\n",
      "  trial_id: eda9d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.7/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/32 CPUs, 2.0/8 GPUs, 0.0/323.51 GiB heap, 0.0/142.64 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 3/3 (1 ERROR, 2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00000</td><td>RUNNING </td><td>172.16.19.112:41260</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         48.294 </td><td style=\"text-align: right;\">24000</td><td style=\"text-align: right;\">  160.57</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">            160.57</td></tr>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00002</td><td>RUNNING </td><td>172.16.19.112:41261</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         47.8176</td><td style=\"text-align: right;\">24000</td><td style=\"text-align: right;\">  148.77</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  15</td><td style=\"text-align: right;\">            148.77</td></tr>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00001</td><td>ERROR   </td><td>                   </td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                         </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00001</td><td style=\"text-align: right;\">           1</td><td>/home/ec2-user/ray_results/PPO/PPO_CartPole-v0_eda9d_00001_1_lr=0.001_2021-10-16_15-44-21/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_eda9d_00002:\n",
      "  agent_timesteps_total: 28000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-16_15-45-32\n",
      "  done: false\n",
      "  episode_len_mean: 171.24\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 171.24\n",
      "  episode_reward_min: 15.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 390\n",
      "  experiment_id: 36a8061aa2d24b2a90c2e8a7584a9cbe\n",
      "  hostname: ip-172-16-19-112\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.5375571250915527\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0032317060977220535\n",
      "          model: {}\n",
      "          policy_loss: -0.0022186448331922293\n",
      "          total_loss: 318.508056640625\n",
      "          vf_explained_var: 0.4775434136390686\n",
      "          vf_loss: 318.5096740722656\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 28000\n",
      "    num_agent_steps_trained: 28000\n",
      "    num_steps_sampled: 28000\n",
      "    num_steps_trained: 28000\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 172.16.19.112\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.370000000000001\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.21199999999999997\n",
      "    gpu_util_percent2: 0.24699999999999997\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.8999999999999995\n",
      "    vram_util_percent0: 0.9886373568743991\n",
      "    vram_util_percent1: 0.9591818896949567\n",
      "    vram_util_percent2: 0.9591818896949567\n",
      "    vram_util_percent3: 0.011362643125600909\n",
      "    vram_util_percent4: 0.011362643125600909\n",
      "    vram_util_percent5: 0.011362643125600909\n",
      "    vram_util_percent6: 0.011362643125600909\n",
      "    vram_util_percent7: 0.011362643125600909\n",
      "  pid: 41261\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06929586978615233\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07758176728418183\n",
      "    mean_inference_ms: 1.0909871014526917\n",
      "    mean_raw_obs_processing_ms: 0.08890125817511287\n",
      "  time_since_restore: 55.65130925178528\n",
      "  time_this_iter_s: 7.8336873054504395\n",
      "  time_total_s: 55.65130925178528\n",
      "  timers:\n",
      "    learn_throughput: 771.942\n",
      "    learn_time_ms: 5181.736\n",
      "    load_throughput: 339295.964\n",
      "    load_time_ms: 11.789\n",
      "    sample_throughput: 550.547\n",
      "    sample_time_ms: 7265.502\n",
      "    update_time_ms: 3.25\n",
      "  timestamp: 1634399132\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 7\n",
      "  trial_id: eda9d_00002\n",
      "  \n",
      "Result for PPO_CartPole-v0_eda9d_00000:\n",
      "  agent_timesteps_total: 28000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-16_15-45-32\n",
      "  done: false\n",
      "  episode_len_mean: 174.66\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 174.66\n",
      "  episode_reward_min: 44.0\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 343\n",
      "  experiment_id: fee4be6e696a4767b3e6c9df50a77f99\n",
      "  hostname: ip-172-16-19-112\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 0.009999999776482582\n",
      "          entropy: 0.4162995517253876\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.07591450959444046\n",
      "          model: {}\n",
      "          policy_loss: 0.02121187187731266\n",
      "          total_loss: 196.13148498535156\n",
      "          vf_explained_var: 0.7488142251968384\n",
      "          vf_loss: 196.0950927734375\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 28000\n",
      "    num_agent_steps_trained: 28000\n",
      "    num_steps_sampled: 28000\n",
      "    num_steps_trained: 28000\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 172.16.19.112\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.319999999999997\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.23500000000000001\n",
      "    gpu_util_percent2: 0.203\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.8999999999999995\n",
      "    vram_util_percent0: 0.9886373568743991\n",
      "    vram_util_percent1: 0.9591818896949567\n",
      "    vram_util_percent2: 0.9591818896949567\n",
      "    vram_util_percent3: 0.011362643125600909\n",
      "    vram_util_percent4: 0.011362643125600909\n",
      "    vram_util_percent5: 0.011362643125600909\n",
      "    vram_util_percent6: 0.011362643125600909\n",
      "    vram_util_percent7: 0.011362643125600909\n",
      "  pid: 41260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06869690284428193\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07804104142199768\n",
      "    mean_inference_ms: 1.0937695524780275\n",
      "    mean_raw_obs_processing_ms: 0.08659359696547636\n",
      "  time_since_restore: 56.18887686729431\n",
      "  time_this_iter_s: 7.8948588371276855\n",
      "  time_total_s: 56.18887686729431\n",
      "  timers:\n",
      "    learn_throughput: 759.749\n",
      "    learn_time_ms: 5264.897\n",
      "    load_throughput: 329736.251\n",
      "    load_time_ms: 12.131\n",
      "    sample_throughput: 545.166\n",
      "    sample_time_ms: 7337.211\n",
      "    update_time_ms: 3.32\n",
      "  timestamp: 1634399132\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 7\n",
      "  trial_id: eda9d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.7/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/32 CPUs, 2.0/8 GPUs, 0.0/323.51 GiB heap, 0.0/142.64 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 3/3 (1 ERROR, 2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00000</td><td>RUNNING </td><td>172.16.19.112:41260</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         56.1889</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">  174.66</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  44</td><td style=\"text-align: right;\">            174.66</td></tr>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00002</td><td>RUNNING </td><td>172.16.19.112:41261</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         55.6513</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">  171.24</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  15</td><td style=\"text-align: right;\">            171.24</td></tr>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00001</td><td>ERROR   </td><td>                   </td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                         </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00001</td><td style=\"text-align: right;\">           1</td><td>/home/ec2-user/ray_results/PPO/PPO_CartPole-v0_eda9d_00001_1_lr=0.001_2021-10-16_15-44-21/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_eda9d_00002:\n",
      "  agent_timesteps_total: 32000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-16_15-45-40\n",
      "  done: false\n",
      "  episode_len_mean: 187.34\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 187.34\n",
      "  episode_reward_min: 57.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 411\n",
      "  experiment_id: 36a8061aa2d24b2a90c2e8a7584a9cbe\n",
      "  hostname: ip-172-16-19-112\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.5155300498008728\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0047530303709208965\n",
      "          model: {}\n",
      "          policy_loss: -0.0044758738949894905\n",
      "          total_loss: 369.3384094238281\n",
      "          vf_explained_var: 0.38220083713531494\n",
      "          vf_loss: 369.3419494628906\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_steps_sampled: 32000\n",
      "    num_steps_trained: 32000\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 172.16.19.112\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.500000000000002\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.202\n",
      "    gpu_util_percent2: 0.244\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.8999999999999995\n",
      "    vram_util_percent0: 0.9886373568743991\n",
      "    vram_util_percent1: 0.9591818896949567\n",
      "    vram_util_percent2: 0.9591818896949567\n",
      "    vram_util_percent3: 0.011362643125600909\n",
      "    vram_util_percent4: 0.011362643125600909\n",
      "    vram_util_percent5: 0.011362643125600909\n",
      "    vram_util_percent6: 0.011362643125600909\n",
      "    vram_util_percent7: 0.011362643125600909\n",
      "  pid: 41261\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06916961646451344\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07755570371420947\n",
      "    mean_inference_ms: 1.0882622761358942\n",
      "    mean_raw_obs_processing_ms: 0.08791217835468412\n",
      "  time_since_restore: 63.54485321044922\n",
      "  time_this_iter_s: 7.89354395866394\n",
      "  time_total_s: 63.54485321044922\n",
      "  timers:\n",
      "    learn_throughput: 773.244\n",
      "    learn_time_ms: 5173.013\n",
      "    load_throughput: 375160.311\n",
      "    load_time_ms: 10.662\n",
      "    sample_throughput: 544.348\n",
      "    sample_time_ms: 7348.245\n",
      "    update_time_ms: 3.249\n",
      "  timestamp: 1634399140\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 32000\n",
      "  training_iteration: 8\n",
      "  trial_id: eda9d_00002\n",
      "  \n",
      "Result for PPO_CartPole-v0_eda9d_00000:\n",
      "  agent_timesteps_total: 32000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-16_15-45-40\n",
      "  done: false\n",
      "  episode_len_mean: 183.9\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 183.9\n",
      "  episode_reward_min: 48.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 363\n",
      "  experiment_id: fee4be6e696a4767b3e6c9df50a77f99\n",
      "  hostname: ip-172-16-19-112\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 0.009999999776482582\n",
      "          entropy: 0.3061726689338684\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.04321584478020668\n",
      "          model: {}\n",
      "          policy_loss: 0.015996141359210014\n",
      "          total_loss: 340.1735534667969\n",
      "          vf_explained_var: 0.5695023536682129\n",
      "          vf_loss: 340.14892578125\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_steps_sampled: 32000\n",
      "    num_steps_trained: 32000\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 172.16.19.112\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.52\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.215\n",
      "    gpu_util_percent2: 0.194\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.8999999999999995\n",
      "    vram_util_percent0: 0.9886373568743991\n",
      "    vram_util_percent1: 0.9591818896949567\n",
      "    vram_util_percent2: 0.9591818896949567\n",
      "    vram_util_percent3: 0.011362643125600909\n",
      "    vram_util_percent4: 0.011362643125600909\n",
      "    vram_util_percent5: 0.011362643125600909\n",
      "    vram_util_percent6: 0.011362643125600909\n",
      "    vram_util_percent7: 0.011362643125600909\n",
      "  pid: 41260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06871155540848557\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07808774092941059\n",
      "    mean_inference_ms: 1.09282999642236\n",
      "    mean_raw_obs_processing_ms: 0.08595585607246033\n",
      "  time_since_restore: 64.25789308547974\n",
      "  time_this_iter_s: 8.069016218185425\n",
      "  time_total_s: 64.25789308547974\n",
      "  timers:\n",
      "    learn_throughput: 760.514\n",
      "    learn_time_ms: 5259.602\n",
      "    load_throughput: 364843.04\n",
      "    load_time_ms: 10.964\n",
      "    sample_throughput: 538.335\n",
      "    sample_time_ms: 7430.323\n",
      "    update_time_ms: 3.309\n",
      "  timestamp: 1634399140\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 32000\n",
      "  training_iteration: 8\n",
      "  trial_id: eda9d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.7/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/32 CPUs, 2.0/8 GPUs, 0.0/323.51 GiB heap, 0.0/142.64 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 3/3 (1 ERROR, 2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00000</td><td>RUNNING </td><td>172.16.19.112:41260</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         64.2579</td><td style=\"text-align: right;\">32000</td><td style=\"text-align: right;\">  183.9 </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  48</td><td style=\"text-align: right;\">            183.9 </td></tr>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00002</td><td>RUNNING </td><td>172.16.19.112:41261</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         63.5449</td><td style=\"text-align: right;\">32000</td><td style=\"text-align: right;\">  187.34</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  57</td><td style=\"text-align: right;\">            187.34</td></tr>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00001</td><td>ERROR   </td><td>                   </td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                         </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00001</td><td style=\"text-align: right;\">           1</td><td>/home/ec2-user/ray_results/PPO/PPO_CartPole-v0_eda9d_00001_1_lr=0.001_2021-10-16_15-44-21/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.7/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/32 CPUs, 2.0/8 GPUs, 0.0/323.51 GiB heap, 0.0/142.64 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 3/3 (1 ERROR, 2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00000</td><td>RUNNING </td><td>172.16.19.112:41260</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         64.2579</td><td style=\"text-align: right;\">32000</td><td style=\"text-align: right;\">  183.9 </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  48</td><td style=\"text-align: right;\">            183.9 </td></tr>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00002</td><td>RUNNING </td><td>172.16.19.112:41261</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         63.5449</td><td style=\"text-align: right;\">32000</td><td style=\"text-align: right;\">  187.34</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  57</td><td style=\"text-align: right;\">            187.34</td></tr>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00001</td><td>ERROR   </td><td>                   </td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                         </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00001</td><td style=\"text-align: right;\">           1</td><td>/home/ec2-user/ray_results/PPO/PPO_CartPole-v0_eda9d_00001_1_lr=0.001_2021-10-16_15-44-21/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_eda9d_00002:\n",
      "  agent_timesteps_total: 36000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-16_15-45-48\n",
      "  done: false\n",
      "  episode_len_mean: 195.29\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 195.29\n",
      "  episode_reward_min: 76.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 431\n",
      "  experiment_id: 36a8061aa2d24b2a90c2e8a7584a9cbe\n",
      "  hostname: ip-172-16-19-112\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.506653368473053\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0024979093577712774\n",
      "          model: {}\n",
      "          policy_loss: 0.0024217134341597557\n",
      "          total_loss: 437.5098571777344\n",
      "          vf_explained_var: 0.47176593542099\n",
      "          vf_loss: 437.5069274902344\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 36000\n",
      "    num_agent_steps_trained: 36000\n",
      "    num_steps_sampled: 36000\n",
      "    num_steps_trained: 36000\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 172.16.19.112\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.45\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.23200000000000004\n",
      "    gpu_util_percent2: 0.252\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.8999999999999995\n",
      "    vram_util_percent0: 0.9886373568743991\n",
      "    vram_util_percent1: 0.9591818896949567\n",
      "    vram_util_percent2: 0.9591818896949567\n",
      "    vram_util_percent3: 0.011362643125600909\n",
      "    vram_util_percent4: 0.011362643125600909\n",
      "    vram_util_percent5: 0.011362643125600909\n",
      "    vram_util_percent6: 0.011362643125600909\n",
      "    vram_util_percent7: 0.011362643125600909\n",
      "  pid: 41261\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06911312506482727\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07756422542133044\n",
      "    mean_inference_ms: 1.0865510776921614\n",
      "    mean_raw_obs_processing_ms: 0.08721048069066475\n",
      "  time_since_restore: 71.43147993087769\n",
      "  time_this_iter_s: 7.886626720428467\n",
      "  time_total_s: 71.43147993087769\n",
      "  timers:\n",
      "    learn_throughput: 772.818\n",
      "    learn_time_ms: 5175.862\n",
      "    load_throughput: 408753.973\n",
      "    load_time_ms: 9.786\n",
      "    sample_throughput: 540.256\n",
      "    sample_time_ms: 7403.895\n",
      "    update_time_ms: 3.264\n",
      "  timestamp: 1634399148\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 9\n",
      "  trial_id: eda9d_00002\n",
      "  \n",
      "Result for PPO_CartPole-v0_eda9d_00000:\n",
      "  agent_timesteps_total: 36000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-16_15-45-48\n",
      "  done: false\n",
      "  episode_len_mean: 183.93\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 183.93\n",
      "  episode_reward_min: 48.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 383\n",
      "  experiment_id: fee4be6e696a4767b3e6c9df50a77f99\n",
      "  hostname: ip-172-16-19-112\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 0.009999999776482582\n",
      "          entropy: 0.29322493076324463\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0570620596408844\n",
      "          model: {}\n",
      "          policy_loss: 0.013830691576004028\n",
      "          total_loss: 464.1732482910156\n",
      "          vf_explained_var: 0.30839505791664124\n",
      "          vf_loss: 464.1479797363281\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 36000\n",
      "    num_agent_steps_trained: 36000\n",
      "    num_steps_sampled: 36000\n",
      "    num_steps_trained: 36000\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 172.16.19.112\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.459999999999999\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.217\n",
      "    gpu_util_percent2: 0.23399999999999999\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.8999999999999995\n",
      "    vram_util_percent0: 0.9886373568743991\n",
      "    vram_util_percent1: 0.9591818896949567\n",
      "    vram_util_percent2: 0.9591818896949567\n",
      "    vram_util_percent3: 0.011362643125600909\n",
      "    vram_util_percent4: 0.011362643125600909\n",
      "    vram_util_percent5: 0.011362643125600909\n",
      "    vram_util_percent6: 0.011362643125600909\n",
      "    vram_util_percent7: 0.011362643125600909\n",
      "  pid: 41260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06872041450065162\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07810370184259288\n",
      "    mean_inference_ms: 1.0926144558847763\n",
      "    mean_raw_obs_processing_ms: 0.08550749607394176\n",
      "  time_since_restore: 72.22573900222778\n",
      "  time_this_iter_s: 7.967845916748047\n",
      "  time_total_s: 72.22573900222778\n",
      "  timers:\n",
      "    learn_throughput: 761.067\n",
      "    learn_time_ms: 5255.781\n",
      "    load_throughput: 395985.838\n",
      "    load_time_ms: 10.101\n",
      "    sample_throughput: 533.681\n",
      "    sample_time_ms: 7495.116\n",
      "    update_time_ms: 3.301\n",
      "  timestamp: 1634399148\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 9\n",
      "  trial_id: eda9d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.7/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/32 CPUs, 2.0/8 GPUs, 0.0/323.51 GiB heap, 0.0/142.64 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 3/3 (1 ERROR, 2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00000</td><td>RUNNING </td><td>172.16.19.112:41260</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         72.2257</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\">  183.93</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  48</td><td style=\"text-align: right;\">            183.93</td></tr>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00002</td><td>RUNNING </td><td>172.16.19.112:41261</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         71.4315</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\">  195.29</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  76</td><td style=\"text-align: right;\">            195.29</td></tr>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00001</td><td>ERROR   </td><td>                   </td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                         </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00001</td><td style=\"text-align: right;\">           1</td><td>/home/ec2-user/ray_results/PPO/PPO_CartPole-v0_eda9d_00001_1_lr=0.001_2021-10-16_15-44-21/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.7/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/32 CPUs, 2.0/8 GPUs, 0.0/323.51 GiB heap, 0.0/142.64 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 3/3 (1 ERROR, 2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00000</td><td>RUNNING </td><td>172.16.19.112:41260</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         72.2257</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\">  183.93</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  48</td><td style=\"text-align: right;\">            183.93</td></tr>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00002</td><td>RUNNING </td><td>172.16.19.112:41261</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         71.4315</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\">  195.29</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  76</td><td style=\"text-align: right;\">            195.29</td></tr>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00001</td><td>ERROR   </td><td>                   </td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                         </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00001</td><td style=\"text-align: right;\">           1</td><td>/home/ec2-user/ray_results/PPO/PPO_CartPole-v0_eda9d_00001_1_lr=0.001_2021-10-16_15-44-21/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_eda9d_00002:\n",
      "  agent_timesteps_total: 40000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-16_15-45-56\n",
      "  done: false\n",
      "  episode_len_mean: 198.74\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 198.74\n",
      "  episode_reward_min: 158.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 451\n",
      "  experiment_id: 36a8061aa2d24b2a90c2e8a7584a9cbe\n",
      "  hostname: ip-172-16-19-112\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.5164790153503418\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.002633011667057872\n",
      "          model: {}\n",
      "          policy_loss: 0.0010574019979685545\n",
      "          total_loss: 471.9617004394531\n",
      "          vf_explained_var: 0.29917800426483154\n",
      "          vf_loss: 471.9600830078125\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_steps_sampled: 40000\n",
      "    num_steps_trained: 40000\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 172.16.19.112\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.555555555555555\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.2222222222222222\n",
      "    gpu_util_percent2: 0.20777777777777778\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.9\n",
      "    vram_util_percent0: 0.9886373568743991\n",
      "    vram_util_percent1: 0.9591818896949567\n",
      "    vram_util_percent2: 0.9591818896949567\n",
      "    vram_util_percent3: 0.011362643125600909\n",
      "    vram_util_percent4: 0.011362643125600909\n",
      "    vram_util_percent5: 0.011362643125600909\n",
      "    vram_util_percent6: 0.011362643125600909\n",
      "    vram_util_percent7: 0.011362643125600909\n",
      "  pid: 41261\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06908147587084347\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07756881859622877\n",
      "    mean_inference_ms: 1.0854967421911086\n",
      "    mean_raw_obs_processing_ms: 0.08668883136140115\n",
      "  time_since_restore: 79.34370040893555\n",
      "  time_this_iter_s: 7.912220478057861\n",
      "  time_total_s: 79.34370040893555\n",
      "  timers:\n",
      "    learn_throughput: 773.405\n",
      "    learn_time_ms: 5171.932\n",
      "    load_throughput: 440689.249\n",
      "    load_time_ms: 9.077\n",
      "    sample_throughput: 535.596\n",
      "    sample_time_ms: 7468.319\n",
      "    update_time_ms: 3.24\n",
      "  timestamp: 1634399156\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 10\n",
      "  trial_id: eda9d_00002\n",
      "  \n",
      "Result for PPO_CartPole-v0_eda9d_00000:\n",
      "  agent_timesteps_total: 40000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-16_15-45-57\n",
      "  done: false\n",
      "  episode_len_mean: 189.66\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 189.66\n",
      "  episode_reward_min: 48.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 403\n",
      "  experiment_id: fee4be6e696a4767b3e6c9df50a77f99\n",
      "  hostname: ip-172-16-19-112\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 0.009999999776482582\n",
      "          entropy: 0.318561851978302\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.1206667348742485\n",
      "          model: {}\n",
      "          policy_loss: 0.023825356736779213\n",
      "          total_loss: 264.2132568359375\n",
      "          vf_explained_var: 0.5402150750160217\n",
      "          vf_loss: 264.165283203125\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_steps_sampled: 40000\n",
      "    num_steps_trained: 40000\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 172.16.19.112\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.489999999999998\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.24\n",
      "    gpu_util_percent2: 0.22999999999999998\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.8999999999999995\n",
      "    vram_util_percent0: 0.9886373568743991\n",
      "    vram_util_percent1: 0.9591818896949567\n",
      "    vram_util_percent2: 0.9591818896949567\n",
      "    vram_util_percent3: 0.011362643125600909\n",
      "    vram_util_percent4: 0.011362643125600909\n",
      "    vram_util_percent5: 0.011362643125600909\n",
      "    vram_util_percent6: 0.011362643125600909\n",
      "    vram_util_percent7: 0.011362643125600909\n",
      "  pid: 41260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0687376289241064\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07812181217901622\n",
      "    mean_inference_ms: 1.0925654782340593\n",
      "    mean_raw_obs_processing_ms: 0.08516945452510828\n",
      "  time_since_restore: 80.19675993919373\n",
      "  time_this_iter_s: 7.971020936965942\n",
      "  time_total_s: 80.19675993919373\n",
      "  timers:\n",
      "    learn_throughput: 761.413\n",
      "    learn_time_ms: 5253.393\n",
      "    load_throughput: 426453.015\n",
      "    load_time_ms: 9.38\n",
      "    sample_throughput: 530.033\n",
      "    sample_time_ms: 7546.7\n",
      "    update_time_ms: 3.299\n",
      "  timestamp: 1634399157\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 10\n",
      "  trial_id: eda9d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.7/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/32 CPUs, 2.0/8 GPUs, 0.0/323.51 GiB heap, 0.0/142.64 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 3/3 (1 ERROR, 2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00000</td><td>RUNNING </td><td>172.16.19.112:41260</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         80.1968</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\">  189.66</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  48</td><td style=\"text-align: right;\">            189.66</td></tr>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00002</td><td>RUNNING </td><td>172.16.19.112:41261</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         79.3437</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\">  198.74</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 158</td><td style=\"text-align: right;\">            198.74</td></tr>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00001</td><td>ERROR   </td><td>                   </td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                         </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00001</td><td style=\"text-align: right;\">           1</td><td>/home/ec2-user/ray_results/PPO/PPO_CartPole-v0_eda9d_00001_1_lr=0.001_2021-10-16_15-44-21/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_eda9d_00002:\n",
      "  agent_timesteps_total: 44000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-16_15-46-04\n",
      "  done: false\n",
      "  episode_len_mean: 198.87\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 198.87\n",
      "  episode_reward_min: 158.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 471\n",
      "  experiment_id: 36a8061aa2d24b2a90c2e8a7584a9cbe\n",
      "  hostname: ip-172-16-19-112\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.5321366786956787\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0026774462312459946\n",
      "          model: {}\n",
      "          policy_loss: 0.001468151924200356\n",
      "          total_loss: 388.29180908203125\n",
      "          vf_explained_var: 0.45292162895202637\n",
      "          vf_loss: 388.289794921875\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 44000\n",
      "    num_agent_steps_trained: 44000\n",
      "    num_steps_sampled: 44000\n",
      "    num_steps_trained: 44000\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 172.16.19.112\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.41\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.2\n",
      "    gpu_util_percent2: 0.215\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.8999999999999995\n",
      "    vram_util_percent0: 0.9886373568743991\n",
      "    vram_util_percent1: 0.9591818896949567\n",
      "    vram_util_percent2: 0.9591818896949567\n",
      "    vram_util_percent3: 0.011362643125600909\n",
      "    vram_util_percent4: 0.011362643125600909\n",
      "    vram_util_percent5: 0.011362643125600909\n",
      "    vram_util_percent6: 0.011362643125600909\n",
      "    vram_util_percent7: 0.011362643125600909\n",
      "  pid: 41261\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06908244250736628\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07757908991152773\n",
      "    mean_inference_ms: 1.0849883092064958\n",
      "    mean_raw_obs_processing_ms: 0.0863112922749478\n",
      "  time_since_restore: 87.1435272693634\n",
      "  time_this_iter_s: 7.7998268604278564\n",
      "  time_total_s: 87.1435272693634\n",
      "  timers:\n",
      "    learn_throughput: 779.398\n",
      "    learn_time_ms: 5132.168\n",
      "    load_throughput: 1395751.818\n",
      "    load_time_ms: 2.866\n",
      "    sample_throughput: 502.636\n",
      "    sample_time_ms: 7958.048\n",
      "    update_time_ms: 3.219\n",
      "  timestamp: 1634399164\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 44000\n",
      "  training_iteration: 11\n",
      "  trial_id: eda9d_00002\n",
      "  \n",
      "Result for PPO_CartPole-v0_eda9d_00000:\n",
      "  agent_timesteps_total: 44000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-16_15-46-04\n",
      "  done: false\n",
      "  episode_len_mean: 195.54\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 195.54\n",
      "  episode_reward_min: 122.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 423\n",
      "  experiment_id: fee4be6e696a4767b3e6c9df50a77f99\n",
      "  hostname: ip-172-16-19-112\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 0.009999999776482582\n",
      "          entropy: 0.2851118743419647\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.07937058806419373\n",
      "          model: {}\n",
      "          policy_loss: 0.013714062049984932\n",
      "          total_loss: 377.63165283203125\n",
      "          vf_explained_var: 0.3015579581260681\n",
      "          vf_loss: 377.6020812988281\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 44000\n",
      "    num_agent_steps_trained: 44000\n",
      "    num_steps_sampled: 44000\n",
      "    num_steps_trained: 44000\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 172.16.19.112\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.466666666666669\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.2411111111111111\n",
      "    gpu_util_percent2: 0.21444444444444447\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.9\n",
      "    vram_util_percent0: 0.9886373568743991\n",
      "    vram_util_percent1: 0.9591818896949567\n",
      "    vram_util_percent2: 0.9591818896949567\n",
      "    vram_util_percent3: 0.011362643125600909\n",
      "    vram_util_percent4: 0.011362643125600909\n",
      "    vram_util_percent5: 0.011362643125600909\n",
      "    vram_util_percent6: 0.011362643125600909\n",
      "    vram_util_percent7: 0.011362643125600909\n",
      "  pid: 41260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06872254428269871\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07811545455918462\n",
      "    mean_inference_ms: 1.0922217722946563\n",
      "    mean_raw_obs_processing_ms: 0.08485553268108406\n",
      "  time_since_restore: 88.09347677230835\n",
      "  time_this_iter_s: 7.896716833114624\n",
      "  time_total_s: 88.09347677230835\n",
      "  timers:\n",
      "    learn_throughput: 767.59\n",
      "    learn_time_ms: 5211.113\n",
      "    load_throughput: 1354738.415\n",
      "    load_time_ms: 2.953\n",
      "    sample_throughput: 496.559\n",
      "    sample_time_ms: 8055.435\n",
      "    update_time_ms: 3.271\n",
      "  timestamp: 1634399164\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 44000\n",
      "  training_iteration: 11\n",
      "  trial_id: eda9d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.7/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/32 CPUs, 2.0/8 GPUs, 0.0/323.51 GiB heap, 0.0/142.64 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 3/3 (1 ERROR, 2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00000</td><td>RUNNING </td><td>172.16.19.112:41260</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         88.0935</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\">  195.54</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 122</td><td style=\"text-align: right;\">            195.54</td></tr>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00002</td><td>RUNNING </td><td>172.16.19.112:41261</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         87.1435</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\">  198.87</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 158</td><td style=\"text-align: right;\">            198.87</td></tr>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00001</td><td>ERROR   </td><td>                   </td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                         </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00001</td><td style=\"text-align: right;\">           1</td><td>/home/ec2-user/ray_results/PPO/PPO_CartPole-v0_eda9d_00001_1_lr=0.001_2021-10-16_15-44-21/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.7/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/32 CPUs, 2.0/8 GPUs, 0.0/323.51 GiB heap, 0.0/142.64 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 3/3 (1 ERROR, 2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00000</td><td>RUNNING </td><td>172.16.19.112:41260</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         88.0935</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\">  195.54</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 122</td><td style=\"text-align: right;\">            195.54</td></tr>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00002</td><td>RUNNING </td><td>172.16.19.112:41261</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         87.1435</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\">  198.87</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 158</td><td style=\"text-align: right;\">            198.87</td></tr>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00001</td><td>ERROR   </td><td>                   </td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                         </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00001</td><td style=\"text-align: right;\">           1</td><td>/home/ec2-user/ray_results/PPO/PPO_CartPole-v0_eda9d_00001_1_lr=0.001_2021-10-16_15-44-21/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_eda9d_00002:\n",
      "  agent_timesteps_total: 48000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-16_15-46-12\n",
      "  done: false\n",
      "  episode_len_mean: 199.01\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 199.01\n",
      "  episode_reward_min: 158.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 491\n",
      "  experiment_id: 36a8061aa2d24b2a90c2e8a7584a9cbe\n",
      "  hostname: ip-172-16-19-112\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.4930127263069153\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0037080482579767704\n",
      "          model: {}\n",
      "          policy_loss: -0.005831567104905844\n",
      "          total_loss: 409.0596923828125\n",
      "          vf_explained_var: 0.4206683933734894\n",
      "          vf_loss: 409.0647888183594\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 48000\n",
      "    num_agent_steps_trained: 48000\n",
      "    num_steps_sampled: 48000\n",
      "    num_steps_trained: 48000\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 172.16.19.112\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.34\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.219\n",
      "    gpu_util_percent2: 0.24299999999999997\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.8999999999999995\n",
      "    vram_util_percent0: 0.9886373568743991\n",
      "    vram_util_percent1: 0.9591818896949567\n",
      "    vram_util_percent2: 0.9591818896949567\n",
      "    vram_util_percent3: 0.011362643125600909\n",
      "    vram_util_percent4: 0.011362643125600909\n",
      "    vram_util_percent5: 0.011362643125600909\n",
      "    vram_util_percent6: 0.011362643125600909\n",
      "    vram_util_percent7: 0.011362643125600909\n",
      "  pid: 41261\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06907639952291239\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07757566328162141\n",
      "    mean_inference_ms: 1.0846962521151\n",
      "    mean_raw_obs_processing_ms: 0.08600140863218997\n",
      "  time_since_restore: 95.0503716468811\n",
      "  time_this_iter_s: 7.9068443775177\n",
      "  time_total_s: 95.0503716468811\n",
      "  timers:\n",
      "    learn_throughput: 779.183\n",
      "    learn_time_ms: 5133.581\n",
      "    load_throughput: 1437008.334\n",
      "    load_time_ms: 2.784\n",
      "    sample_throughput: 505.636\n",
      "    sample_time_ms: 7910.833\n",
      "    update_time_ms: 3.234\n",
      "  timestamp: 1634399172\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 48000\n",
      "  training_iteration: 12\n",
      "  trial_id: eda9d_00002\n",
      "  \n",
      "Result for PPO_CartPole-v0_eda9d_00000:\n",
      "  agent_timesteps_total: 48000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-16_15-46-12\n",
      "  done: false\n",
      "  episode_len_mean: 198.89\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 198.89\n",
      "  episode_reward_min: 137.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 443\n",
      "  experiment_id: fee4be6e696a4767b3e6c9df50a77f99\n",
      "  hostname: ip-172-16-19-112\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 0.009999999776482582\n",
      "          entropy: 0.23004505038261414\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.1081719845533371\n",
      "          model: {}\n",
      "          policy_loss: 0.018322331830859184\n",
      "          total_loss: 349.71282958984375\n",
      "          vf_explained_var: 0.4229765236377716\n",
      "          vf_loss: 349.67291259765625\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 48000\n",
      "    num_agent_steps_trained: 48000\n",
      "    num_steps_sampled: 48000\n",
      "    num_steps_trained: 48000\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 172.16.19.112\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.440000000000001\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.213\n",
      "    gpu_util_percent2: 0.229\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.8999999999999995\n",
      "    vram_util_percent0: 0.9886373568743991\n",
      "    vram_util_percent1: 0.9591818896949567\n",
      "    vram_util_percent2: 0.9591818896949567\n",
      "    vram_util_percent3: 0.011362643125600909\n",
      "    vram_util_percent4: 0.011362643125600909\n",
      "    vram_util_percent5: 0.011362643125600909\n",
      "    vram_util_percent6: 0.011362643125600909\n",
      "    vram_util_percent7: 0.011362643125600909\n",
      "  pid: 41260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06871090734598098\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07810967927500607\n",
      "    mean_inference_ms: 1.09193804923096\n",
      "    mean_raw_obs_processing_ms: 0.08458231491979686\n",
      "  time_since_restore: 95.99134969711304\n",
      "  time_this_iter_s: 7.8978729248046875\n",
      "  time_total_s: 95.99134969711304\n",
      "  timers:\n",
      "    learn_throughput: 767.884\n",
      "    learn_time_ms: 5209.121\n",
      "    load_throughput: 1317554.816\n",
      "    load_time_ms: 3.036\n",
      "    sample_throughput: 499.73\n",
      "    sample_time_ms: 8004.329\n",
      "    update_time_ms: 3.258\n",
      "  timestamp: 1634399172\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 48000\n",
      "  training_iteration: 12\n",
      "  trial_id: eda9d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.7/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/32 CPUs, 2.0/8 GPUs, 0.0/323.51 GiB heap, 0.0/142.64 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 3/3 (1 ERROR, 2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00000</td><td>RUNNING </td><td>172.16.19.112:41260</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         95.9913</td><td style=\"text-align: right;\">48000</td><td style=\"text-align: right;\">  198.89</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 137</td><td style=\"text-align: right;\">            198.89</td></tr>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00002</td><td>RUNNING </td><td>172.16.19.112:41261</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         95.0504</td><td style=\"text-align: right;\">48000</td><td style=\"text-align: right;\">  199.01</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 158</td><td style=\"text-align: right;\">            199.01</td></tr>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00001</td><td>ERROR   </td><td>                   </td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                         </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00001</td><td style=\"text-align: right;\">           1</td><td>/home/ec2-user/ray_results/PPO/PPO_CartPole-v0_eda9d_00001_1_lr=0.001_2021-10-16_15-44-21/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_eda9d_00002:\n",
      "  agent_timesteps_total: 52000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-16_15-46-20\n",
      "  done: false\n",
      "  episode_len_mean: 198.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 198.79\n",
      "  episode_reward_min: 115.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 512\n",
      "  experiment_id: 36a8061aa2d24b2a90c2e8a7584a9cbe\n",
      "  hostname: ip-172-16-19-112\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.47625434398651123\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0026906304992735386\n",
      "          model: {}\n",
      "          policy_loss: 0.0019425859209150076\n",
      "          total_loss: 370.4873046875\n",
      "          vf_explained_var: 0.4837858974933624\n",
      "          vf_loss: 370.48480224609375\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 52000\n",
      "    num_agent_steps_trained: 52000\n",
      "    num_steps_sampled: 52000\n",
      "    num_steps_trained: 52000\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 172.16.19.112\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.389999999999999\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.225\n",
      "    gpu_util_percent2: 0.253\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.8999999999999995\n",
      "    vram_util_percent0: 0.9886373568743991\n",
      "    vram_util_percent1: 0.9591818896949567\n",
      "    vram_util_percent2: 0.9591818896949567\n",
      "    vram_util_percent3: 0.011362643125600909\n",
      "    vram_util_percent4: 0.011362643125600909\n",
      "    vram_util_percent5: 0.011362643125600909\n",
      "    vram_util_percent6: 0.011362643125600909\n",
      "    vram_util_percent7: 0.011362643125600909\n",
      "  pid: 41261\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06906236854424737\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07753496495200392\n",
      "    mean_inference_ms: 1.0842333050405792\n",
      "    mean_raw_obs_processing_ms: 0.08569702294772867\n",
      "  time_since_restore: 102.98734664916992\n",
      "  time_this_iter_s: 7.936975002288818\n",
      "  time_total_s: 102.98734664916992\n",
      "  timers:\n",
      "    learn_throughput: 777.897\n",
      "    learn_time_ms: 5142.071\n",
      "    load_throughput: 1383790.632\n",
      "    load_time_ms: 2.891\n",
      "    sample_throughput: 505.44\n",
      "    sample_time_ms: 7913.899\n",
      "    update_time_ms: 3.287\n",
      "  timestamp: 1634399180\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 52000\n",
      "  training_iteration: 13\n",
      "  trial_id: eda9d_00002\n",
      "  \n",
      "Result for PPO_CartPole-v0_eda9d_00000:\n",
      "  agent_timesteps_total: 52000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-16_15-46-20\n",
      "  done: false\n",
      "  episode_len_mean: 198.37\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 198.37\n",
      "  episode_reward_min: 137.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 464\n",
      "  experiment_id: fee4be6e696a4767b3e6c9df50a77f99\n",
      "  hostname: ip-172-16-19-112\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 0.009999999776482582\n",
      "          entropy: 0.16059443354606628\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.08764877915382385\n",
      "          model: {}\n",
      "          policy_loss: 0.016448497772216797\n",
      "          total_loss: 459.92822265625\n",
      "          vf_explained_var: 0.2854905128479004\n",
      "          vf_loss: 459.894287109375\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 52000\n",
      "    num_agent_steps_trained: 52000\n",
      "    num_steps_sampled: 52000\n",
      "    num_steps_trained: 52000\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 172.16.19.112\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.33\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.221\n",
      "    gpu_util_percent2: 0.215\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.8999999999999995\n",
      "    vram_util_percent0: 0.9886373568743991\n",
      "    vram_util_percent1: 0.9591818896949567\n",
      "    vram_util_percent2: 0.9591818896949567\n",
      "    vram_util_percent3: 0.011362643125600909\n",
      "    vram_util_percent4: 0.011362643125600909\n",
      "    vram_util_percent5: 0.011362643125600909\n",
      "    vram_util_percent6: 0.011362643125600909\n",
      "    vram_util_percent7: 0.011362643125600909\n",
      "  pid: 41260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06862873750479329\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07802292629716476\n",
      "    mean_inference_ms: 1.0907151663748578\n",
      "    mean_raw_obs_processing_ms: 0.08428536066396243\n",
      "  time_since_restore: 103.91320848464966\n",
      "  time_this_iter_s: 7.921858787536621\n",
      "  time_total_s: 103.91320848464966\n",
      "  timers:\n",
      "    learn_throughput: 767.754\n",
      "    learn_time_ms: 5210.0\n",
      "    load_throughput: 1372998.347\n",
      "    load_time_ms: 2.913\n",
      "    sample_throughput: 499.747\n",
      "    sample_time_ms: 8004.056\n",
      "    update_time_ms: 3.258\n",
      "  timestamp: 1634399180\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 52000\n",
      "  training_iteration: 13\n",
      "  trial_id: eda9d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.7/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/32 CPUs, 2.0/8 GPUs, 0.0/323.51 GiB heap, 0.0/142.64 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 3/3 (1 ERROR, 2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00000</td><td>RUNNING </td><td>172.16.19.112:41260</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         103.913</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\">  198.37</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 137</td><td style=\"text-align: right;\">            198.37</td></tr>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00002</td><td>RUNNING </td><td>172.16.19.112:41261</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         102.987</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\">  198.79</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 115</td><td style=\"text-align: right;\">            198.79</td></tr>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00001</td><td>ERROR   </td><td>                   </td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                         </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00001</td><td style=\"text-align: right;\">           1</td><td>/home/ec2-user/ray_results/PPO/PPO_CartPole-v0_eda9d_00001_1_lr=0.001_2021-10-16_15-44-21/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_eda9d_00002:\n",
      "  agent_timesteps_total: 56000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-16_15-46-28\n",
      "  done: false\n",
      "  episode_len_mean: 198.79\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 198.79\n",
      "  episode_reward_min: 115.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 532\n",
      "  experiment_id: 36a8061aa2d24b2a90c2e8a7584a9cbe\n",
      "  hostname: ip-172-16-19-112\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.4743359088897705\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.002090497175231576\n",
      "          model: {}\n",
      "          policy_loss: -0.0012050993973389268\n",
      "          total_loss: 273.14276123046875\n",
      "          vf_explained_var: 0.6138594150543213\n",
      "          vf_loss: 273.1435852050781\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 56000\n",
      "    num_agent_steps_trained: 56000\n",
      "    num_steps_sampled: 56000\n",
      "    num_steps_trained: 56000\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 172.16.19.112\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.61111111111111\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.21333333333333332\n",
      "    gpu_util_percent2: 0.23888888888888887\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.9\n",
      "    vram_util_percent0: 0.9886373568743991\n",
      "    vram_util_percent1: 0.9591818896949567\n",
      "    vram_util_percent2: 0.9591818896949567\n",
      "    vram_util_percent3: 0.011362643125600909\n",
      "    vram_util_percent4: 0.011362643125600909\n",
      "    vram_util_percent5: 0.011362643125600909\n",
      "    vram_util_percent6: 0.011362643125600909\n",
      "    vram_util_percent7: 0.011362643125600909\n",
      "  pid: 41261\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0690712185837306\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0775314768903631\n",
      "    mean_inference_ms: 1.084261278073572\n",
      "    mean_raw_obs_processing_ms: 0.08549229865509307\n",
      "  time_since_restore: 110.89968013763428\n",
      "  time_this_iter_s: 7.9123334884643555\n",
      "  time_total_s: 110.89968013763428\n",
      "  timers:\n",
      "    learn_throughput: 776.904\n",
      "    learn_time_ms: 5148.645\n",
      "    load_throughput: 1393190.337\n",
      "    load_time_ms: 2.871\n",
      "    sample_throughput: 504.016\n",
      "    sample_time_ms: 7936.264\n",
      "    update_time_ms: 3.272\n",
      "  timestamp: 1634399188\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 56000\n",
      "  training_iteration: 14\n",
      "  trial_id: eda9d_00002\n",
      "  \n",
      "Result for PPO_CartPole-v0_eda9d_00000:\n",
      "  agent_timesteps_total: 56000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-16_15-46-28\n",
      "  done: false\n",
      "  episode_len_mean: 198.37\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 198.37\n",
      "  episode_reward_min: 137.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 484\n",
      "  experiment_id: fee4be6e696a4767b3e6c9df50a77f99\n",
      "  hostname: ip-172-16-19-112\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 0.009999999776482582\n",
      "          entropy: 0.12913906574249268\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.06097273528575897\n",
      "          model: {}\n",
      "          policy_loss: 0.014025657437741756\n",
      "          total_loss: 354.6938781738281\n",
      "          vf_explained_var: 0.4772065281867981\n",
      "          vf_loss: 354.66766357421875\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 56000\n",
      "    num_agent_steps_trained: 56000\n",
      "    num_steps_sampled: 56000\n",
      "    num_steps_trained: 56000\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 172.16.19.112\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.459999999999999\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.253\n",
      "    gpu_util_percent2: 0.22999999999999998\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.8999999999999995\n",
      "    vram_util_percent0: 0.9886373568743991\n",
      "    vram_util_percent1: 0.9591818896949567\n",
      "    vram_util_percent2: 0.9591818896949567\n",
      "    vram_util_percent3: 0.011362643125600909\n",
      "    vram_util_percent4: 0.011362643125600909\n",
      "    vram_util_percent5: 0.011362643125600909\n",
      "    vram_util_percent6: 0.011362643125600909\n",
      "    vram_util_percent7: 0.011362643125600909\n",
      "  pid: 41260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06855525855147457\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07794769724411178\n",
      "    mean_inference_ms: 1.0896987798380742\n",
      "    mean_raw_obs_processing_ms: 0.0840348432933696\n",
      "  time_since_restore: 111.90194988250732\n",
      "  time_this_iter_s: 7.988741397857666\n",
      "  time_total_s: 111.90194988250732\n",
      "  timers:\n",
      "    learn_throughput: 767.541\n",
      "    learn_time_ms: 5211.445\n",
      "    load_throughput: 1375001.311\n",
      "    load_time_ms: 2.909\n",
      "    sample_throughput: 499.995\n",
      "    sample_time_ms: 8000.073\n",
      "    update_time_ms: 3.229\n",
      "  timestamp: 1634399188\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 56000\n",
      "  training_iteration: 14\n",
      "  trial_id: eda9d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.7/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/32 CPUs, 2.0/8 GPUs, 0.0/323.51 GiB heap, 0.0/142.64 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 3/3 (1 ERROR, 2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00000</td><td>RUNNING </td><td>172.16.19.112:41260</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">         111.902</td><td style=\"text-align: right;\">56000</td><td style=\"text-align: right;\">  198.37</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 137</td><td style=\"text-align: right;\">            198.37</td></tr>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00002</td><td>RUNNING </td><td>172.16.19.112:41261</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">         110.9  </td><td style=\"text-align: right;\">56000</td><td style=\"text-align: right;\">  198.79</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 115</td><td style=\"text-align: right;\">            198.79</td></tr>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00001</td><td>ERROR   </td><td>                   </td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                         </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00001</td><td style=\"text-align: right;\">           1</td><td>/home/ec2-user/ray_results/PPO/PPO_CartPole-v0_eda9d_00001_1_lr=0.001_2021-10-16_15-44-21/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.7/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/32 CPUs, 2.0/8 GPUs, 0.0/323.51 GiB heap, 0.0/142.64 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 3/3 (1 ERROR, 2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00000</td><td>RUNNING </td><td>172.16.19.112:41260</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">         111.902</td><td style=\"text-align: right;\">56000</td><td style=\"text-align: right;\">  198.37</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 137</td><td style=\"text-align: right;\">            198.37</td></tr>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00002</td><td>RUNNING </td><td>172.16.19.112:41261</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">         110.9  </td><td style=\"text-align: right;\">56000</td><td style=\"text-align: right;\">  198.79</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 115</td><td style=\"text-align: right;\">            198.79</td></tr>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00001</td><td>ERROR   </td><td>                   </td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                         </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00001</td><td style=\"text-align: right;\">           1</td><td>/home/ec2-user/ray_results/PPO/PPO_CartPole-v0_eda9d_00001_1_lr=0.001_2021-10-16_15-44-21/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_eda9d_00002:\n",
      "  agent_timesteps_total: 60000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-16_15-46-36\n",
      "  done: false\n",
      "  episode_len_mean: 198.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 198.99\n",
      "  episode_reward_min: 115.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 552\n",
      "  experiment_id: 36a8061aa2d24b2a90c2e8a7584a9cbe\n",
      "  hostname: ip-172-16-19-112\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.4306418001651764\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.002831242047250271\n",
      "          model: {}\n",
      "          policy_loss: 0.0013957979390397668\n",
      "          total_loss: 332.7289733886719\n",
      "          vf_explained_var: 0.4252530336380005\n",
      "          vf_loss: 332.7269592285156\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 60000\n",
      "    num_agent_steps_trained: 60000\n",
      "    num_steps_sampled: 60000\n",
      "    num_steps_trained: 60000\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 172.16.19.112\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.530000000000001\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.227\n",
      "    gpu_util_percent2: 0.23199999999999998\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.8999999999999995\n",
      "    vram_util_percent0: 0.9886373568743991\n",
      "    vram_util_percent1: 0.9591818896949567\n",
      "    vram_util_percent2: 0.9591818896949567\n",
      "    vram_util_percent3: 0.011362643125600909\n",
      "    vram_util_percent4: 0.011362643125600909\n",
      "    vram_util_percent5: 0.011362643125600909\n",
      "    vram_util_percent6: 0.011362643125600909\n",
      "    vram_util_percent7: 0.011362643125600909\n",
      "  pid: 41261\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06908463943054033\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.077532077489656\n",
      "    mean_inference_ms: 1.084285618583134\n",
      "    mean_raw_obs_processing_ms: 0.08532075373372422\n",
      "  time_since_restore: 118.84991240501404\n",
      "  time_this_iter_s: 7.950232267379761\n",
      "  time_total_s: 118.84991240501404\n",
      "  timers:\n",
      "    learn_throughput: 777.717\n",
      "    learn_time_ms: 5143.262\n",
      "    load_throughput: 1401066.925\n",
      "    load_time_ms: 2.855\n",
      "    sample_throughput: 503.326\n",
      "    sample_time_ms: 7947.137\n",
      "    update_time_ms: 3.287\n",
      "  timestamp: 1634399196\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 60000\n",
      "  training_iteration: 15\n",
      "  trial_id: eda9d_00002\n",
      "  \n",
      "Result for PPO_CartPole-v0_eda9d_00000:\n",
      "  agent_timesteps_total: 60000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-16_15-46-37\n",
      "  done: false\n",
      "  episode_len_mean: 198.37\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 198.37\n",
      "  episode_reward_min: 137.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 504\n",
      "  experiment_id: fee4be6e696a4767b3e6c9df50a77f99\n",
      "  hostname: ip-172-16-19-112\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 0.009999999776482582\n",
      "          entropy: 0.09818951040506363\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.09712763875722885\n",
      "          model: {}\n",
      "          policy_loss: 0.019388163462281227\n",
      "          total_loss: 314.5386962890625\n",
      "          vf_explained_var: 0.44819507002830505\n",
      "          vf_loss: 314.4998779296875\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 60000\n",
      "    num_agent_steps_trained: 60000\n",
      "    num_steps_sampled: 60000\n",
      "    num_steps_trained: 60000\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 172.16.19.112\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.56\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.252\n",
      "    gpu_util_percent2: 0.233\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.8999999999999995\n",
      "    vram_util_percent0: 0.9886373568743991\n",
      "    vram_util_percent1: 0.9591818896949567\n",
      "    vram_util_percent2: 0.9591818896949567\n",
      "    vram_util_percent3: 0.011362643125600909\n",
      "    vram_util_percent4: 0.011362643125600909\n",
      "    vram_util_percent5: 0.011362643125600909\n",
      "    vram_util_percent6: 0.011362643125600909\n",
      "    vram_util_percent7: 0.011362643125600909\n",
      "  pid: 41260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06850238941321099\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07789666184148214\n",
      "    mean_inference_ms: 1.0891079217899642\n",
      "    mean_raw_obs_processing_ms: 0.08384576517850359\n",
      "  time_since_restore: 119.90837287902832\n",
      "  time_this_iter_s: 8.006422996520996\n",
      "  time_total_s: 119.90837287902832\n",
      "  timers:\n",
      "    learn_throughput: 767.876\n",
      "    learn_time_ms: 5209.176\n",
      "    load_throughput: 1378322.407\n",
      "    load_time_ms: 2.902\n",
      "    sample_throughput: 499.604\n",
      "    sample_time_ms: 8006.333\n",
      "    update_time_ms: 3.243\n",
      "  timestamp: 1634399197\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 60000\n",
      "  training_iteration: 15\n",
      "  trial_id: eda9d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.7/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/32 CPUs, 2.0/8 GPUs, 0.0/323.51 GiB heap, 0.0/142.64 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 3/3 (1 ERROR, 2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00000</td><td>RUNNING </td><td>172.16.19.112:41260</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         119.908</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\">  198.37</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 137</td><td style=\"text-align: right;\">            198.37</td></tr>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00002</td><td>RUNNING </td><td>172.16.19.112:41261</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         118.85 </td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\">  198.99</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 115</td><td style=\"text-align: right;\">            198.99</td></tr>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00001</td><td>ERROR   </td><td>                   </td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                         </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00001</td><td style=\"text-align: right;\">           1</td><td>/home/ec2-user/ray_results/PPO/PPO_CartPole-v0_eda9d_00001_1_lr=0.001_2021-10-16_15-44-21/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_eda9d_00002:\n",
      "  agent_timesteps_total: 64000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-16_15-46-44\n",
      "  done: false\n",
      "  episode_len_mean: 198.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 198.99\n",
      "  episode_reward_min: 115.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 572\n",
      "  experiment_id: 36a8061aa2d24b2a90c2e8a7584a9cbe\n",
      "  hostname: ip-172-16-19-112\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.44470614194869995\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0044144997373223305\n",
      "          model: {}\n",
      "          policy_loss: -0.002049433533102274\n",
      "          total_loss: 271.29095458984375\n",
      "          vf_explained_var: 0.46868187189102173\n",
      "          vf_loss: 271.2921142578125\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 64000\n",
      "    num_agent_steps_trained: 64000\n",
      "    num_steps_sampled: 64000\n",
      "    num_steps_trained: 64000\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 172.16.19.112\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.45\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.2\n",
      "    gpu_util_percent2: 0.217\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.8999999999999995\n",
      "    vram_util_percent0: 0.9886373568743991\n",
      "    vram_util_percent1: 0.9591818896949567\n",
      "    vram_util_percent2: 0.9591818896949567\n",
      "    vram_util_percent3: 0.011362643125600909\n",
      "    vram_util_percent4: 0.011362643125600909\n",
      "    vram_util_percent5: 0.011362643125600909\n",
      "    vram_util_percent6: 0.011362643125600909\n",
      "    vram_util_percent7: 0.011362643125600909\n",
      "  pid: 41261\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06911583283777295\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07755165473570416\n",
      "    mean_inference_ms: 1.0845942456445246\n",
      "    mean_raw_obs_processing_ms: 0.08519698421439788\n",
      "  time_since_restore: 126.71211099624634\n",
      "  time_this_iter_s: 7.8621985912323\n",
      "  time_total_s: 126.71211099624634\n",
      "  timers:\n",
      "    learn_throughput: 777.177\n",
      "    learn_time_ms: 5146.831\n",
      "    load_throughput: 1402448.925\n",
      "    load_time_ms: 2.852\n",
      "    sample_throughput: 503.501\n",
      "    sample_time_ms: 7944.381\n",
      "    update_time_ms: 3.274\n",
      "  timestamp: 1634399204\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 64000\n",
      "  training_iteration: 16\n",
      "  trial_id: eda9d_00002\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.7/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/32 CPUs, 2.0/8 GPUs, 0.0/323.51 GiB heap, 0.0/142.64 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 3/3 (1 ERROR, 2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00000</td><td>RUNNING </td><td>172.16.19.112:41260</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         119.908</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\">  198.37</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 137</td><td style=\"text-align: right;\">            198.37</td></tr>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00002</td><td>RUNNING </td><td>172.16.19.112:41261</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         126.712</td><td style=\"text-align: right;\">64000</td><td style=\"text-align: right;\">  198.99</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 115</td><td style=\"text-align: right;\">            198.99</td></tr>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00001</td><td>ERROR   </td><td>                   </td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                         </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00001</td><td style=\"text-align: right;\">           1</td><td>/home/ec2-user/ray_results/PPO/PPO_CartPole-v0_eda9d_00001_1_lr=0.001_2021-10-16_15-44-21/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_eda9d_00000:\n",
      "  agent_timesteps_total: 64000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-16_15-46-44\n",
      "  done: false\n",
      "  episode_len_mean: 198.59\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 198.59\n",
      "  episode_reward_min: 137.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 524\n",
      "  experiment_id: fee4be6e696a4767b3e6c9df50a77f99\n",
      "  hostname: ip-172-16-19-112\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 0.009999999776482582\n",
      "          entropy: 0.13560442626476288\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.09108387678861618\n",
      "          model: {}\n",
      "          policy_loss: 0.015016925521194935\n",
      "          total_loss: 341.0106201171875\n",
      "          vf_explained_var: 0.4172094464302063\n",
      "          vf_loss: 340.9773864746094\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 64000\n",
      "    num_agent_steps_trained: 64000\n",
      "    num_steps_sampled: 64000\n",
      "    num_steps_trained: 64000\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 172.16.19.112\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.533333333333335\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.2411111111111111\n",
      "    gpu_util_percent2: 0.2211111111111111\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.9\n",
      "    vram_util_percent0: 0.9886373568743991\n",
      "    vram_util_percent1: 0.9591818896949567\n",
      "    vram_util_percent2: 0.9591818896949567\n",
      "    vram_util_percent3: 0.011362643125600909\n",
      "    vram_util_percent4: 0.011362643125600909\n",
      "    vram_util_percent5: 0.011362643125600909\n",
      "    vram_util_percent6: 0.011362643125600909\n",
      "    vram_util_percent7: 0.011362643125600909\n",
      "  pid: 41260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06844843614259313\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0778447663984063\n",
      "    mean_inference_ms: 1.0885590423315405\n",
      "    mean_raw_obs_processing_ms: 0.08367598380789801\n",
      "  time_since_restore: 127.77822184562683\n",
      "  time_this_iter_s: 7.869848966598511\n",
      "  time_total_s: 127.77822184562683\n",
      "  timers:\n",
      "    learn_throughput: 767.27\n",
      "    learn_time_ms: 5213.288\n",
      "    load_throughput: 1376083.99\n",
      "    load_time_ms: 2.907\n",
      "    sample_throughput: 500.187\n",
      "    sample_time_ms: 7997.013\n",
      "    update_time_ms: 3.253\n",
      "  timestamp: 1634399204\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 64000\n",
      "  training_iteration: 16\n",
      "  trial_id: eda9d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.7/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/32 CPUs, 2.0/8 GPUs, 0.0/323.51 GiB heap, 0.0/142.64 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 3/3 (1 ERROR, 2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00000</td><td>RUNNING </td><td>172.16.19.112:41260</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         127.778</td><td style=\"text-align: right;\">64000</td><td style=\"text-align: right;\">  198.59</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 137</td><td style=\"text-align: right;\">            198.59</td></tr>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00002</td><td>RUNNING </td><td>172.16.19.112:41261</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         126.712</td><td style=\"text-align: right;\">64000</td><td style=\"text-align: right;\">  198.99</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 115</td><td style=\"text-align: right;\">            198.99</td></tr>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00001</td><td>ERROR   </td><td>                   </td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                         </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00001</td><td style=\"text-align: right;\">           1</td><td>/home/ec2-user/ray_results/PPO/PPO_CartPole-v0_eda9d_00001_1_lr=0.001_2021-10-16_15-44-21/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_eda9d_00002:\n",
      "  agent_timesteps_total: 68000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-16_15-46-52\n",
      "  done: false\n",
      "  episode_len_mean: 198.99\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 198.99\n",
      "  episode_reward_min: 115.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 592\n",
      "  experiment_id: 36a8061aa2d24b2a90c2e8a7584a9cbe\n",
      "  hostname: ip-172-16-19-112\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.4091958701610565\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0030504337046295404\n",
      "          model: {}\n",
      "          policy_loss: 0.0008771381690166891\n",
      "          total_loss: 283.1387023925781\n",
      "          vf_explained_var: 0.48283061385154724\n",
      "          vf_loss: 283.1372375488281\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 68000\n",
      "    num_agent_steps_trained: 68000\n",
      "    num_steps_sampled: 68000\n",
      "    num_steps_trained: 68000\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 172.16.19.112\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.670000000000002\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.233\n",
      "    gpu_util_percent2: 0.24100000000000002\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.8999999999999995\n",
      "    vram_util_percent0: 0.9886373568743991\n",
      "    vram_util_percent1: 0.9591818896949567\n",
      "    vram_util_percent2: 0.9591818896949567\n",
      "    vram_util_percent3: 0.011362643125600909\n",
      "    vram_util_percent4: 0.011362643125600909\n",
      "    vram_util_percent5: 0.011362643125600909\n",
      "    vram_util_percent6: 0.011362643125600909\n",
      "    vram_util_percent7: 0.011362643125600909\n",
      "  pid: 41261\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06914492253426466\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07757338477318014\n",
      "    mean_inference_ms: 1.084951033175855\n",
      "    mean_raw_obs_processing_ms: 0.08510469920097005\n",
      "  time_since_restore: 134.8091139793396\n",
      "  time_this_iter_s: 8.097002983093262\n",
      "  time_total_s: 134.8091139793396\n",
      "  timers:\n",
      "    learn_throughput: 773.139\n",
      "    learn_time_ms: 5173.714\n",
      "    load_throughput: 1397390.993\n",
      "    load_time_ms: 2.862\n",
      "    sample_throughput: 503.375\n",
      "    sample_time_ms: 7946.365\n",
      "    update_time_ms: 3.304\n",
      "  timestamp: 1634399212\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 68000\n",
      "  training_iteration: 17\n",
      "  trial_id: eda9d_00002\n",
      "  \n",
      "Result for PPO_CartPole-v0_eda9d_00000:\n",
      "  agent_timesteps_total: 68000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-16_15-46-53\n",
      "  done: false\n",
      "  episode_len_mean: 199.48\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 199.48\n",
      "  episode_reward_min: 148.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 544\n",
      "  experiment_id: fee4be6e696a4767b3e6c9df50a77f99\n",
      "  hostname: ip-172-16-19-112\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 0.009999999776482582\n",
      "          entropy: 0.1039978489279747\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.1146194115281105\n",
      "          model: {}\n",
      "          policy_loss: 0.018585963174700737\n",
      "          total_loss: 441.64739990234375\n",
      "          vf_explained_var: 0.4098968505859375\n",
      "          vf_loss: 441.6059265136719\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 68000\n",
      "    num_agent_steps_trained: 68000\n",
      "    num_steps_sampled: 68000\n",
      "    num_steps_trained: 68000\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 172.16.19.112\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.72\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.21599999999999997\n",
      "    gpu_util_percent2: 0.225\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.8999999999999995\n",
      "    vram_util_percent0: 0.9886373568743991\n",
      "    vram_util_percent1: 0.9591818896949567\n",
      "    vram_util_percent2: 0.9591818896949567\n",
      "    vram_util_percent3: 0.011362643125600909\n",
      "    vram_util_percent4: 0.011362643125600909\n",
      "    vram_util_percent5: 0.011362643125600909\n",
      "    vram_util_percent6: 0.011362643125600909\n",
      "    vram_util_percent7: 0.011362643125600909\n",
      "  pid: 41260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06841381647369131\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07780934704578674\n",
      "    mean_inference_ms: 1.088088005212805\n",
      "    mean_raw_obs_processing_ms: 0.08354505683091591\n",
      "  time_since_restore: 135.91559839248657\n",
      "  time_this_iter_s: 8.137376546859741\n",
      "  time_total_s: 135.91559839248657\n",
      "  timers:\n",
      "    learn_throughput: 763.859\n",
      "    learn_time_ms: 5236.571\n",
      "    load_throughput: 1367949.448\n",
      "    load_time_ms: 2.924\n",
      "    sample_throughput: 499.882\n",
      "    sample_time_ms: 8001.887\n",
      "    update_time_ms: 3.249\n",
      "  timestamp: 1634399213\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 68000\n",
      "  training_iteration: 17\n",
      "  trial_id: eda9d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.7/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/32 CPUs, 2.0/8 GPUs, 0.0/323.51 GiB heap, 0.0/142.64 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 3/3 (1 ERROR, 2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00000</td><td>RUNNING </td><td>172.16.19.112:41260</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         135.916</td><td style=\"text-align: right;\">68000</td><td style=\"text-align: right;\">  199.48</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 148</td><td style=\"text-align: right;\">            199.48</td></tr>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00002</td><td>RUNNING </td><td>172.16.19.112:41261</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         134.809</td><td style=\"text-align: right;\">68000</td><td style=\"text-align: right;\">  198.99</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 115</td><td style=\"text-align: right;\">            198.99</td></tr>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00001</td><td>ERROR   </td><td>                   </td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                         </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00001</td><td style=\"text-align: right;\">           1</td><td>/home/ec2-user/ray_results/PPO/PPO_CartPole-v0_eda9d_00001_1_lr=0.001_2021-10-16_15-44-21/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_eda9d_00002:\n",
      "  agent_timesteps_total: 72000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-16_15-47-00\n",
      "  done: false\n",
      "  episode_len_mean: 199.95\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 199.95\n",
      "  episode_reward_min: 195.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 612\n",
      "  experiment_id: 36a8061aa2d24b2a90c2e8a7584a9cbe\n",
      "  hostname: ip-172-16-19-112\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.419415146112442\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0035245378967374563\n",
      "          model: {}\n",
      "          policy_loss: -0.002441737800836563\n",
      "          total_loss: 200.3439178466797\n",
      "          vf_explained_var: 0.6449847221374512\n",
      "          vf_loss: 200.34564208984375\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 72000\n",
      "    num_agent_steps_trained: 72000\n",
      "    num_steps_sampled: 72000\n",
      "    num_steps_trained: 72000\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 172.16.19.112\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.488888888888889\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.21333333333333335\n",
      "    gpu_util_percent2: 0.23333333333333334\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.9\n",
      "    vram_util_percent0: 0.9886373568743991\n",
      "    vram_util_percent1: 0.9591818896949567\n",
      "    vram_util_percent2: 0.9591818896949567\n",
      "    vram_util_percent3: 0.011362643125600909\n",
      "    vram_util_percent4: 0.011362643125600909\n",
      "    vram_util_percent5: 0.011362643125600909\n",
      "    vram_util_percent6: 0.011362643125600909\n",
      "    vram_util_percent7: 0.011362643125600909\n",
      "  pid: 41261\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06915187310464954\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07759277985482015\n",
      "    mean_inference_ms: 1.0853316636169632\n",
      "    mean_raw_obs_processing_ms: 0.08502432946060332\n",
      "  time_since_restore: 142.72182297706604\n",
      "  time_this_iter_s: 7.91270899772644\n",
      "  time_total_s: 142.72182297706604\n",
      "  timers:\n",
      "    learn_throughput: 772.806\n",
      "    learn_time_ms: 5175.941\n",
      "    load_throughput: 1395148.269\n",
      "    load_time_ms: 2.867\n",
      "    sample_throughput: 501.546\n",
      "    sample_time_ms: 7975.34\n",
      "    update_time_ms: 3.299\n",
      "  timestamp: 1634399220\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 72000\n",
      "  training_iteration: 18\n",
      "  trial_id: eda9d_00002\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.7/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/32 CPUs, 2.0/8 GPUs, 0.0/323.51 GiB heap, 0.0/142.64 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 3/3 (1 ERROR, 2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00000</td><td>RUNNING </td><td>172.16.19.112:41260</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         135.916</td><td style=\"text-align: right;\">68000</td><td style=\"text-align: right;\">  199.48</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 148</td><td style=\"text-align: right;\">            199.48</td></tr>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00002</td><td>RUNNING </td><td>172.16.19.112:41261</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">         142.722</td><td style=\"text-align: right;\">72000</td><td style=\"text-align: right;\">  199.95</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 195</td><td style=\"text-align: right;\">            199.95</td></tr>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00001</td><td>ERROR   </td><td>                   </td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                         </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00001</td><td style=\"text-align: right;\">           1</td><td>/home/ec2-user/ray_results/PPO/PPO_CartPole-v0_eda9d_00001_1_lr=0.001_2021-10-16_15-44-21/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_eda9d_00000:\n",
      "  agent_timesteps_total: 72000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-16_15-47-01\n",
      "  done: true\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 200.0\n",
      "  episode_reward_min: 200.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 564\n",
      "  experiment_id: fee4be6e696a4767b3e6c9df50a77f99\n",
      "  hostname: ip-172-16-19-112\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 0.009999999776482582\n",
      "          entropy: 0.1091565415263176\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.07244950532913208\n",
      "          model: {}\n",
      "          policy_loss: 0.005926945246756077\n",
      "          total_loss: 325.9423522949219\n",
      "          vf_explained_var: 0.4758451282978058\n",
      "          vf_loss: 325.92193603515625\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 72000\n",
      "    num_agent_steps_trained: 72000\n",
      "    num_steps_sampled: 72000\n",
      "    num_steps_trained: 72000\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 172.16.19.112\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 11.38\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.213\n",
      "    gpu_util_percent2: 0.221\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.8999999999999995\n",
      "    vram_util_percent0: 0.9886373568743991\n",
      "    vram_util_percent1: 0.9591818896949567\n",
      "    vram_util_percent2: 0.9591818896949567\n",
      "    vram_util_percent3: 0.011362643125600909\n",
      "    vram_util_percent4: 0.011362643125600909\n",
      "    vram_util_percent5: 0.011362643125600909\n",
      "    vram_util_percent6: 0.011362643125600909\n",
      "    vram_util_percent7: 0.011362643125600909\n",
      "  pid: 41260\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06840017542974852\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07779753361252871\n",
      "    mean_inference_ms: 1.0876039795939247\n",
      "    mean_raw_obs_processing_ms: 0.08343840605786206\n",
      "  time_since_restore: 143.92099571228027\n",
      "  time_this_iter_s: 8.005397319793701\n",
      "  time_total_s: 143.92099571228027\n",
      "  timers:\n",
      "    learn_throughput: 763.624\n",
      "    learn_time_ms: 5238.179\n",
      "    load_throughput: 1362527.998\n",
      "    load_time_ms: 2.936\n",
      "    sample_throughput: 498.919\n",
      "    sample_time_ms: 8017.335\n",
      "    update_time_ms: 3.263\n",
      "  timestamp: 1634399221\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 72000\n",
      "  training_iteration: 18\n",
      "  trial_id: eda9d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.0/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 1.0/8 GPUs, 0.0/323.51 GiB heap, 0.0/142.64 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 3/3 (1 ERROR, 1 RUNNING, 1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00002</td><td>RUNNING   </td><td>172.16.19.112:41261</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">         142.722</td><td style=\"text-align: right;\">72000</td><td style=\"text-align: right;\">  199.95</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 195</td><td style=\"text-align: right;\">            199.95</td></tr>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">         143.921</td><td style=\"text-align: right;\">72000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00001</td><td>ERROR     </td><td>                   </td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                         </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00001</td><td style=\"text-align: right;\">           1</td><td>/home/ec2-user/ray_results/PPO/PPO_CartPole-v0_eda9d_00001_1_lr=0.001_2021-10-16_15-44-21/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_eda9d_00002:\n",
      "  agent_timesteps_total: 76000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-16_15-47-07\n",
      "  done: false\n",
      "  episode_len_mean: 199.95\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 199.95\n",
      "  episode_reward_min: 195.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 632\n",
      "  experiment_id: 36a8061aa2d24b2a90c2e8a7584a9cbe\n",
      "  hostname: ip-172-16-19-112\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.4171346127986908\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0031355975661426783\n",
      "          model: {}\n",
      "          policy_loss: 0.0022702447604388\n",
      "          total_loss: 273.60052490234375\n",
      "          vf_explained_var: 0.536511242389679\n",
      "          vf_loss: 273.5976257324219\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 76000\n",
      "    num_agent_steps_trained: 76000\n",
      "    num_steps_sampled: 76000\n",
      "    num_steps_trained: 76000\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 172.16.19.112\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 7.589999999999999\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.238\n",
      "    gpu_util_percent2: 0.178\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.6100000000000003\n",
      "    vram_util_percent0: 0.9886373568743991\n",
      "    vram_util_percent1: 0.9591818896949567\n",
      "    vram_util_percent2: 0.20092649243947208\n",
      "    vram_util_percent3: 0.011362643125600909\n",
      "    vram_util_percent4: 0.011362643125600909\n",
      "    vram_util_percent5: 0.011362643125600909\n",
      "    vram_util_percent6: 0.011362643125600909\n",
      "    vram_util_percent7: 0.011362643125600909\n",
      "  pid: 41261\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0691446763365774\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07758470514888031\n",
      "    mean_inference_ms: 1.0853925257669308\n",
      "    mean_raw_obs_processing_ms: 0.08492429155577319\n",
      "  time_since_restore: 150.30449056625366\n",
      "  time_this_iter_s: 7.582667589187622\n",
      "  time_total_s: 150.30449056625366\n",
      "  timers:\n",
      "    learn_throughput: 777.136\n",
      "    learn_time_ms: 5147.103\n",
      "    load_throughput: 1396599.988\n",
      "    load_time_ms: 2.864\n",
      "    sample_throughput: 501.492\n",
      "    sample_time_ms: 7976.203\n",
      "    update_time_ms: 3.283\n",
      "  timestamp: 1634399227\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 76000\n",
      "  training_iteration: 19\n",
      "  trial_id: eda9d_00002\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.0/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/32 CPUs, 1.0/8 GPUs, 0.0/323.51 GiB heap, 0.0/142.64 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 3/3 (1 ERROR, 1 RUNNING, 1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00002</td><td>RUNNING   </td><td>172.16.19.112:41261</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         150.304</td><td style=\"text-align: right;\">76000</td><td style=\"text-align: right;\">  199.95</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 195</td><td style=\"text-align: right;\">            199.95</td></tr>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00000</td><td>TERMINATED</td><td>                   </td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">         143.921</td><td style=\"text-align: right;\">72000</td><td style=\"text-align: right;\">  200   </td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">            200   </td></tr>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00001</td><td>ERROR     </td><td>                   </td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                         </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00001</td><td style=\"text-align: right;\">           1</td><td>/home/ec2-user/ray_results/PPO/PPO_CartPole-v0_eda9d_00001_1_lr=0.001_2021-10-16_15-44-21/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CartPole-v0_eda9d_00002:\n",
      "  agent_timesteps_total: 80000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-10-16_15-47-15\n",
      "  done: true\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 200.0\n",
      "  episode_reward_mean: 200.0\n",
      "  episode_reward_min: 200.0\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 652\n",
      "  experiment_id: 36a8061aa2d24b2a90c2e8a7584a9cbe\n",
      "  hostname: ip-172-16-19-112\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.39377790689468384\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.002887232694774866\n",
      "          model: {}\n",
      "          policy_loss: -0.0012949386145919561\n",
      "          total_loss: 222.5024871826172\n",
      "          vf_explained_var: 0.6581946015357971\n",
      "          vf_loss: 222.503173828125\n",
      "        train: null\n",
      "    num_agent_steps_sampled: 80000\n",
      "    num_agent_steps_trained: 80000\n",
      "    num_steps_sampled: 80000\n",
      "    num_steps_trained: 80000\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 172.16.19.112\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 6.011111111111111\n",
      "    gpu_util_percent0: 0.0\n",
      "    gpu_util_percent1: 0.23333333333333328\n",
      "    gpu_util_percent2: 0.0\n",
      "    gpu_util_percent3: 0.0\n",
      "    gpu_util_percent4: 0.0\n",
      "    gpu_util_percent5: 0.0\n",
      "    gpu_util_percent6: 0.0\n",
      "    gpu_util_percent7: 0.0\n",
      "    ram_util_percent: 2.5\n",
      "    vram_util_percent0: 0.9886373568743991\n",
      "    vram_util_percent1: 0.9591818896949567\n",
      "    vram_util_percent2: 0.011362643125600909\n",
      "    vram_util_percent3: 0.011362643125600909\n",
      "    vram_util_percent4: 0.011362643125600909\n",
      "    vram_util_percent5: 0.011362643125600909\n",
      "    vram_util_percent6: 0.011362643125600909\n",
      "    vram_util_percent7: 0.011362643125600909\n",
      "  pid: 41261\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06908996468518072\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07753115592737354\n",
      "    mean_inference_ms: 1.0846983235077443\n",
      "    mean_raw_obs_processing_ms: 0.08477871091453788\n",
      "  time_since_restore: 157.82178831100464\n",
      "  time_this_iter_s: 7.517297744750977\n",
      "  time_total_s: 157.82178831100464\n",
      "  timers:\n",
      "    learn_throughput: 780.251\n",
      "    learn_time_ms: 5126.553\n",
      "    load_throughput: 1401160.534\n",
      "    load_time_ms: 2.855\n",
      "    sample_throughput: 504.692\n",
      "    sample_time_ms: 7925.62\n",
      "    update_time_ms: 3.292\n",
      "  timestamp: 1634399235\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 80000\n",
      "  training_iteration: 20\n",
      "  trial_id: eda9d_00002\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.0/480.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/32 CPUs, 0/8 GPUs, 0.0/323.51 GiB heap, 0.0/142.64 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 3/3 (1 ERROR, 2 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">         143.921</td><td style=\"text-align: right;\">72000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00002</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         157.822</td><td style=\"text-align: right;\">80000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00001</td><td>ERROR     </td><td>     </td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                         </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CartPole-v0_eda9d_00001</td><td style=\"text-align: right;\">           1</td><td>/home/ec2-user/ray_results/PPO/PPO_CartPole-v0_eda9d_00001_1_lr=0.001_2021-10-16_15-44-21/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=41370)\u001b[0m [2021-10-16 15:47:15,698 E 41370 41370] raylet_client.cc:159: IOError: Broken pipe [RayletClient] Failed to disconnect from raylet.\n",
      "\u001b[2m\u001b[36m(pid=41373)\u001b[0m 2021-10-16 15:47:15,697\tERROR worker.py:428 -- SystemExit was raised from the worker\n",
      "\u001b[2m\u001b[36m(pid=41373)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=41373)\u001b[0m   File \"python/ray/_raylet.pyx\", line 561, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=41373)\u001b[0m   File \"python/ray/_raylet.pyx\", line 568, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=41373)\u001b[0m   File \"python/ray/_raylet.pyx\", line 572, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=41373)\u001b[0m   File \"python/ray/_raylet.pyx\", line 522, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(pid=41373)\u001b[0m   File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ray/_private/function_manager.py\", line 579, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(pid=41373)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=41373)\u001b[0m   File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ray/util/tracing/tracing_helper.py\", line 449, in _resume_span\n",
      "\u001b[2m\u001b[36m(pid=41373)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(pid=41373)\u001b[0m   File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ray/actor.py\", line 1052, in __ray_terminate__\n",
      "\u001b[2m\u001b[36m(pid=41373)\u001b[0m     ray.actor.exit_actor()\n",
      "\u001b[2m\u001b[36m(pid=41373)\u001b[0m   File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ray/actor.py\", line 1128, in exit_actor\n",
      "\u001b[2m\u001b[36m(pid=41373)\u001b[0m     raise exit\n",
      "\u001b[2m\u001b[36m(pid=41373)\u001b[0m SystemExit: 0\n",
      "\u001b[2m\u001b[36m(pid=41373)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=41373)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(pid=41373)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=41373)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=41373)\u001b[0m   File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/linecache.py\", line 95, in updatecache\n",
      "\u001b[2m\u001b[36m(pid=41373)\u001b[0m     stat = os.stat(fullname)\n",
      "\u001b[2m\u001b[36m(pid=41373)\u001b[0m FileNotFoundError: [Errno 2] No such file or directory: 'python/ray/_raylet.pyx'\n",
      "\u001b[2m\u001b[36m(pid=41373)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=41373)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(pid=41373)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=41373)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(pid=41373)\u001b[0m   File \"python/ray/_raylet.pyx\", line 684, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(pid=41373)\u001b[0m   File \"python/ray/_raylet.pyx\", line 524, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(pid=41373)\u001b[0m   File \"python/ray/includes/libcoreworker.pxi\", line 33, in ray._raylet.ProfileEvent.__exit__\n",
      "\u001b[2m\u001b[36m(pid=41373)\u001b[0m   File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/traceback.py\", line 167, in format_exc\n",
      "\u001b[2m\u001b[36m(pid=41373)\u001b[0m     return \"\".join(format_exception(*sys.exc_info(), limit=limit, chain=chain))\n",
      "\u001b[2m\u001b[36m(pid=41373)\u001b[0m   File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/traceback.py\", line 121, in format_exception\n",
      "\u001b[2m\u001b[36m(pid=41373)\u001b[0m     type(value), value, tb, limit=limit).format(chain=chain))\n",
      "\u001b[2m\u001b[36m(pid=41373)\u001b[0m   File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/traceback.py\", line 509, in __init__\n",
      "\u001b[2m\u001b[36m(pid=41373)\u001b[0m     capture_locals=capture_locals)\n",
      "\u001b[2m\u001b[36m(pid=41373)\u001b[0m   File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/traceback.py\", line 364, in extract\n",
      "\u001b[2m\u001b[36m(pid=41373)\u001b[0m     f.line\n",
      "\u001b[2m\u001b[36m(pid=41373)\u001b[0m   File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/traceback.py\", line 286, in line\n",
      "\u001b[2m\u001b[36m(pid=41373)\u001b[0m     self._line = linecache.getline(self.filename, self.lineno).strip()\n",
      "\u001b[2m\u001b[36m(pid=41373)\u001b[0m   File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/linecache.py\", line 16, in getline\n",
      "\u001b[2m\u001b[36m(pid=41373)\u001b[0m     lines = getlines(filename, module_globals)\n",
      "\u001b[2m\u001b[36m(pid=41373)\u001b[0m   File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/linecache.py\", line 47, in getlines\n",
      "\u001b[2m\u001b[36m(pid=41373)\u001b[0m     return updatecache(filename, module_globals)\n",
      "\u001b[2m\u001b[36m(pid=41373)\u001b[0m   File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/linecache.py\", line 95, in updatecache\n",
      "\u001b[2m\u001b[36m(pid=41373)\u001b[0m     stat = os.stat(fullname)\n",
      "\u001b[2m\u001b[36m(pid=41373)\u001b[0m   File \"/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ray/worker.py\", line 425, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(pid=41373)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(pid=41373)\u001b[0m SystemExit: 1\n"
     ]
    },
    {
     "ename": "TuneError",
     "evalue": "('Trials did not complete', [PPO_CartPole-v0_eda9d_00001])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTuneError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-fd8ddcb53d40>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;34m'num_gpus'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;34m'num_workers'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0;34m'lr'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtune\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m.01\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m.001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m.0001\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     }\n\u001b[1;32m     10\u001b[0m )\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ray/tune/tune.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, local_dir, search_alg, scheduler, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, sync_config, export_formats, max_failures, fail_fast, restore, server_port, resume, queue_trials, reuse_actors, trial_executor, raise_on_failed_trial, callbacks, max_concurrent_trials, loggers, ray_auto_init, run_errored_only, global_checkpoint_period, with_server, upload_dir, sync_to_cloud, sync_to_driver, sync_on_checkpoint, _remote)\u001b[0m\n\u001b[1;32m    609\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mincomplete_trials\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mraise_on_failed_trial\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSIGINT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTuneError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trials did not complete\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincomplete_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trials did not complete: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincomplete_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTuneError\u001b[0m: ('Trials did not complete', [PPO_CartPole-v0_eda9d_00001])"
     ]
    }
   ],
   "source": [
    "alg = 'PPO'\n",
    "tune.run(alg,\n",
    "    stop={'episode_reward_mean':200},\n",
    "    config={\n",
    "        'env':'CartPole-v0',\n",
    "        'num_gpus':1,\n",
    "        'num_workers':2,\n",
    "        'lr':tune.grid_search([.01,.001,.0001])     \n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/12 CPUs, 0/0 GPUs, 0.0/3.81 GiB heap, 0.0/1.32 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">   lr</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_Pendulum-v0_6b1dd_00000</td><td>RUNNING </td><td>     </td><td style=\"text-align: right;\">0.001</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17553)\u001b[0m WARNING:tensorflow:From /Users/mingjunwang/opt/miniconda3/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=17553)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=17553)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=17553)\u001b[0m 2021-07-13 20:54:31,552\tINFO trainer.py:591 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=17553)\u001b[0m 2021-07-13 20:54:31,552\tINFO trainer.py:616 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=17556)\u001b[0m WARNING:tensorflow:From /Users/mingjunwang/opt/miniconda3/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=17556)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=17556)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=17555)\u001b[0m WARNING:tensorflow:From /Users/mingjunwang/opt/miniconda3/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=17555)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=17555)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=17555)\u001b[0m 2021-07-13 20:54:38,692\tWARNING deprecation.py:29 -- DeprecationWarning: `env_index` has been deprecated. Use `episode.env_id` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_Pendulum-v0_6b1dd_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-13_20-54-41\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_reward_max: -781.1207639597676\n",
      "  episode_reward_mean: -1131.8902231793475\n",
      "  episode_reward_min: -1410.9375467664963\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 6\n",
      "  experiment_id: 43107a19371e49529eb221515032f0aa\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 1500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: 0.1796478033065796\n",
      "        mean_q: -0.10519210994243622\n",
      "        min_q: -0.6069953441619873\n",
      "        model: {}\n",
      "    num_steps_sampled: 1500\n",
      "    num_steps_trained: 256\n",
      "    num_target_updates: 1\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.799999999999997\n",
      "    ram_util_percent: 63.55\n",
      "  pid: 17553\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0973585918644932\n",
      "    mean_env_wait_ms: 0.14473166510204818\n",
      "    mean_inference_ms: 0.7737403544541523\n",
      "    mean_raw_obs_processing_ms: 0.22314836118573675\n",
      "  time_since_restore: 2.5313608646392822\n",
      "  time_this_iter_s: 2.5313608646392822\n",
      "  time_total_s: 2.5313608646392822\n",
      "  timers:\n",
      "    learn_throughput: 1305.163\n",
      "    learn_time_ms: 196.144\n",
      "    update_time_ms: 4.681\n",
      "  timestamp: 1626227681\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1500\n",
      "  training_iteration: 1\n",
      "  trial_id: 6b1dd_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/12 CPUs, 0/0 GPUs, 0.0/3.81 GiB heap, 0.0/1.32 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_Pendulum-v0_6b1dd_00000</td><td>RUNNING </td><td>192.168.0.23:17553</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.53136</td><td style=\"text-align: right;\">1500</td><td style=\"text-align: right;\">-1131.89</td><td style=\"text-align: right;\">            -781.121</td><td style=\"text-align: right;\">            -1410.94</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_Pendulum-v0_6b1dd_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-13_20-54-58\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_reward_max: -781.1207639597676\n",
      "  episode_reward_mean: -1331.3202113249129\n",
      "  episode_reward_min: -1796.9534032278605\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 12\n",
      "  experiment_id: 43107a19371e49529eb221515032f0aa\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 2500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: -0.50157630443573\n",
      "        mean_q: -11.473435401916504\n",
      "        min_q: -24.651611328125\n",
      "        model: {}\n",
      "    num_steps_sampled: 2500\n",
      "    num_steps_trained: 128256\n",
      "    num_target_updates: 501\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.545833333333334\n",
      "    ram_util_percent: 63.48333333333334\n",
      "  pid: 17553\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09797157483190518\n",
      "    mean_env_wait_ms: 0.14469709336634642\n",
      "    mean_inference_ms: 0.7615721799224051\n",
      "    mean_raw_obs_processing_ms: 0.22257846912676293\n",
      "  time_since_restore: 19.607420921325684\n",
      "  time_this_iter_s: 17.0760600566864\n",
      "  time_total_s: 19.607420921325684\n",
      "  timers:\n",
      "    learn_throughput: 58299.452\n",
      "    learn_time_ms: 4.391\n",
      "    update_time_ms: 2.605\n",
      "  timestamp: 1626227698\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2500\n",
      "  training_iteration: 2\n",
      "  trial_id: 6b1dd_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/12 CPUs, 0/0 GPUs, 0.0/3.81 GiB heap, 0.0/1.32 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_Pendulum-v0_6b1dd_00000</td><td>RUNNING </td><td>192.168.0.23:17553</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         19.6074</td><td style=\"text-align: right;\">2500</td><td style=\"text-align: right;\">-1331.32</td><td style=\"text-align: right;\">            -781.121</td><td style=\"text-align: right;\">            -1796.95</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_Pendulum-v0_6b1dd_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-13_20-55-15\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_reward_max: -781.1207639597676\n",
      "  episode_reward_mean: -1398.030697460801\n",
      "  episode_reward_min: -1796.9534032278605\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 16\n",
      "  experiment_id: 43107a19371e49529eb221515032f0aa\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 3500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: -0.1660407930612564\n",
      "        mean_q: -18.283344268798828\n",
      "        min_q: -33.41071319580078\n",
      "        model: {}\n",
      "    num_steps_sampled: 3500\n",
      "    num_steps_trained: 256256\n",
      "    num_target_updates: 1001\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.1125\n",
      "    ram_util_percent: 64.00416666666666\n",
      "  pid: 17553\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09813306505833767\n",
      "    mean_env_wait_ms: 0.1444267892224933\n",
      "    mean_inference_ms: 0.7552609488782389\n",
      "    mean_raw_obs_processing_ms: 0.22187214395356114\n",
      "  time_since_restore: 36.444782733917236\n",
      "  time_this_iter_s: 16.837361812591553\n",
      "  time_total_s: 36.444782733917236\n",
      "  timers:\n",
      "    learn_throughput: 67208.837\n",
      "    learn_time_ms: 3.809\n",
      "    update_time_ms: 2.164\n",
      "  timestamp: 1626227715\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3500\n",
      "  training_iteration: 3\n",
      "  trial_id: 6b1dd_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/12 CPUs, 0/0 GPUs, 0.0/3.81 GiB heap, 0.0/1.32 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_Pendulum-v0_6b1dd_00000</td><td>RUNNING </td><td>192.168.0.23:17553</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         36.4448</td><td style=\"text-align: right;\">3500</td><td style=\"text-align: right;\">-1398.03</td><td style=\"text-align: right;\">            -781.121</td><td style=\"text-align: right;\">            -1796.95</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_Pendulum-v0_6b1dd_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-13_20-55-32\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_reward_max: -781.1207639597676\n",
      "  episode_reward_mean: -1425.8668538197078\n",
      "  episode_reward_min: -1796.9534032278605\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 22\n",
      "  experiment_id: 43107a19371e49529eb221515032f0aa\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 4500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: -0.6811314225196838\n",
      "        mean_q: -23.862667083740234\n",
      "        min_q: -44.24247360229492\n",
      "        model: {}\n",
      "    num_steps_sampled: 4500\n",
      "    num_steps_trained: 384256\n",
      "    num_target_updates: 1501\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.125\n",
      "    ram_util_percent: 64.6375\n",
      "  pid: 17553\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09838679174479754\n",
      "    mean_env_wait_ms: 0.1443084430306832\n",
      "    mean_inference_ms: 0.7492579731789636\n",
      "    mean_raw_obs_processing_ms: 0.22119187406960872\n",
      "  time_since_restore: 53.642000675201416\n",
      "  time_this_iter_s: 17.19721794128418\n",
      "  time_total_s: 53.642000675201416\n",
      "  timers:\n",
      "    learn_throughput: 69144.745\n",
      "    learn_time_ms: 3.702\n",
      "    update_time_ms: 2.206\n",
      "  timestamp: 1626227732\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4500\n",
      "  training_iteration: 4\n",
      "  trial_id: 6b1dd_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/12 CPUs, 0/0 GPUs, 0.0/3.81 GiB heap, 0.0/1.32 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_Pendulum-v0_6b1dd_00000</td><td>RUNNING </td><td>192.168.0.23:17553</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">          53.642</td><td style=\"text-align: right;\">4500</td><td style=\"text-align: right;\">-1425.87</td><td style=\"text-align: right;\">            -781.121</td><td style=\"text-align: right;\">            -1796.95</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_Pendulum-v0_6b1dd_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-13_20-55-50\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_reward_max: -781.1207639597676\n",
      "  episode_reward_mean: -1444.8272991160627\n",
      "  episode_reward_min: -1796.9534032278605\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 26\n",
      "  experiment_id: 43107a19371e49529eb221515032f0aa\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 5500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: 0.3510587811470032\n",
      "        mean_q: -29.40019416809082\n",
      "        min_q: -47.306602478027344\n",
      "        model: {}\n",
      "    num_steps_sampled: 5500\n",
      "    num_steps_trained: 512256\n",
      "    num_target_updates: 2001\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.332000000000004\n",
      "    ram_util_percent: 64.232\n",
      "  pid: 17553\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09862273063120153\n",
      "    mean_env_wait_ms: 0.14439809792260078\n",
      "    mean_inference_ms: 0.7471738745456697\n",
      "    mean_raw_obs_processing_ms: 0.22106400627231673\n",
      "  time_since_restore: 71.59774255752563\n",
      "  time_this_iter_s: 17.95574188232422\n",
      "  time_total_s: 71.59774255752563\n",
      "  timers:\n",
      "    learn_throughput: 66434.142\n",
      "    learn_time_ms: 3.853\n",
      "    update_time_ms: 2.271\n",
      "  timestamp: 1626227750\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 5500\n",
      "  training_iteration: 5\n",
      "  trial_id: 6b1dd_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/12 CPUs, 0/0 GPUs, 0.0/3.81 GiB heap, 0.0/1.32 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_Pendulum-v0_6b1dd_00000</td><td>RUNNING </td><td>192.168.0.23:17553</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         71.5977</td><td style=\"text-align: right;\">5500</td><td style=\"text-align: right;\">-1444.83</td><td style=\"text-align: right;\">            -781.121</td><td style=\"text-align: right;\">            -1796.95</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_Pendulum-v0_6b1dd_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-13_20-56-07\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_reward_max: -781.1207639597676\n",
      "  episode_reward_mean: -1436.744265061477\n",
      "  episode_reward_min: -1796.9534032278605\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 32\n",
      "  experiment_id: 43107a19371e49529eb221515032f0aa\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 6500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: -0.09803511202335358\n",
      "        mean_q: -34.90961456298828\n",
      "        min_q: -63.530277252197266\n",
      "        model: {}\n",
      "    num_steps_sampled: 6500\n",
      "    num_steps_trained: 640256\n",
      "    num_target_updates: 2501\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.962500000000002\n",
      "    ram_util_percent: 63.50416666666667\n",
      "  pid: 17553\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09891302519653711\n",
      "    mean_env_wait_ms: 0.1445202853097181\n",
      "    mean_inference_ms: 0.7445685789103453\n",
      "    mean_raw_obs_processing_ms: 0.22094713838037758\n",
      "  time_since_restore: 89.20183849334717\n",
      "  time_this_iter_s: 17.604095935821533\n",
      "  time_total_s: 89.20183849334717\n",
      "  timers:\n",
      "    learn_throughput: 66701.153\n",
      "    learn_time_ms: 3.838\n",
      "    update_time_ms: 2.108\n",
      "  timestamp: 1626227767\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 6500\n",
      "  training_iteration: 6\n",
      "  trial_id: 6b1dd_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/12 CPUs, 0/0 GPUs, 0.0/3.81 GiB heap, 0.0/1.32 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_Pendulum-v0_6b1dd_00000</td><td>RUNNING </td><td>192.168.0.23:17553</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         89.2018</td><td style=\"text-align: right;\">6500</td><td style=\"text-align: right;\">-1436.74</td><td style=\"text-align: right;\">            -781.121</td><td style=\"text-align: right;\">            -1796.95</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_Pendulum-v0_6b1dd_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-13_20-56-26\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_reward_max: -781.1207639597676\n",
      "  episode_reward_mean: -1438.6196838828807\n",
      "  episode_reward_min: -1796.9534032278605\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 36\n",
      "  experiment_id: 43107a19371e49529eb221515032f0aa\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 7500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: 0.008759599179029465\n",
      "        mean_q: -40.90607452392578\n",
      "        min_q: -63.068790435791016\n",
      "        model: {}\n",
      "    num_steps_sampled: 7500\n",
      "    num_steps_trained: 768256\n",
      "    num_target_updates: 3001\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.47307692307692\n",
      "    ram_util_percent: 65.36153846153846\n",
      "  pid: 17553\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09913483847547423\n",
      "    mean_env_wait_ms: 0.14468110819074503\n",
      "    mean_inference_ms: 0.7435151127284532\n",
      "    mean_raw_obs_processing_ms: 0.2210145012650037\n",
      "  time_since_restore: 107.37493371963501\n",
      "  time_this_iter_s: 18.173095226287842\n",
      "  time_total_s: 107.37493371963501\n",
      "  timers:\n",
      "    learn_throughput: 55955.82\n",
      "    learn_time_ms: 4.575\n",
      "    update_time_ms: 2.636\n",
      "  timestamp: 1626227786\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 7500\n",
      "  training_iteration: 7\n",
      "  trial_id: 6b1dd_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/12 CPUs, 0/0 GPUs, 0.0/3.81 GiB heap, 0.0/1.32 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_Pendulum-v0_6b1dd_00000</td><td>RUNNING </td><td>192.168.0.23:17553</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         107.375</td><td style=\"text-align: right;\">7500</td><td style=\"text-align: right;\">-1438.62</td><td style=\"text-align: right;\">            -781.121</td><td style=\"text-align: right;\">            -1796.95</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_Pendulum-v0_6b1dd_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-13_20-56-44\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_reward_max: -781.1207639597676\n",
      "  episode_reward_mean: -1412.6190355939557\n",
      "  episode_reward_min: -1796.9534032278605\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 42\n",
      "  experiment_id: 43107a19371e49529eb221515032f0aa\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 8500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: 1.5346838235855103\n",
      "        mean_q: -44.83782958984375\n",
      "        min_q: -71.63545227050781\n",
      "        model: {}\n",
      "    num_steps_sampled: 8500\n",
      "    num_steps_trained: 896256\n",
      "    num_target_updates: 3501\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.483999999999995\n",
      "    ram_util_percent: 60.803999999999995\n",
      "  pid: 17553\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09941605964921256\n",
      "    mean_env_wait_ms: 0.14489890198444585\n",
      "    mean_inference_ms: 0.742334410942045\n",
      "    mean_raw_obs_processing_ms: 0.2211894070115637\n",
      "  time_since_restore: 125.71777677536011\n",
      "  time_this_iter_s: 18.342843055725098\n",
      "  time_total_s: 125.71777677536011\n",
      "  timers:\n",
      "    learn_throughput: 66089.841\n",
      "    learn_time_ms: 3.874\n",
      "    update_time_ms: 2.326\n",
      "  timestamp: 1626227804\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 8500\n",
      "  training_iteration: 8\n",
      "  trial_id: 6b1dd_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.7/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/12 CPUs, 0/0 GPUs, 0.0/3.81 GiB heap, 0.0/1.32 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_Pendulum-v0_6b1dd_00000</td><td>RUNNING </td><td>192.168.0.23:17553</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         125.718</td><td style=\"text-align: right;\">8500</td><td style=\"text-align: right;\">-1412.62</td><td style=\"text-align: right;\">            -781.121</td><td style=\"text-align: right;\">            -1796.95</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_Pendulum-v0_6b1dd_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-13_20-57-02\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_reward_max: -781.1207639597676\n",
      "  episode_reward_mean: -1398.6154951909177\n",
      "  episode_reward_min: -1796.9534032278605\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 46\n",
      "  experiment_id: 43107a19371e49529eb221515032f0aa\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 9500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: -0.022161278873682022\n",
      "        mean_q: -50.20433044433594\n",
      "        min_q: -73.2963638305664\n",
      "        model: {}\n",
      "    num_steps_sampled: 9500\n",
      "    num_steps_trained: 1024256\n",
      "    num_target_updates: 4001\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.676\n",
      "    ram_util_percent: 60.964\n",
      "  pid: 17553\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09955409264438625\n",
      "    mean_env_wait_ms: 0.14501209496502157\n",
      "    mean_inference_ms: 0.7415564006076368\n",
      "    mean_raw_obs_processing_ms: 0.22126649260221518\n",
      "  time_since_restore: 143.43251085281372\n",
      "  time_this_iter_s: 17.714734077453613\n",
      "  time_total_s: 143.43251085281372\n",
      "  timers:\n",
      "    learn_throughput: 63325.184\n",
      "    learn_time_ms: 4.043\n",
      "    update_time_ms: 2.284\n",
      "  timestamp: 1626227822\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 9500\n",
      "  training_iteration: 9\n",
      "  trial_id: 6b1dd_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/12 CPUs, 0/0 GPUs, 0.0/3.81 GiB heap, 0.0/1.32 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_Pendulum-v0_6b1dd_00000</td><td>RUNNING </td><td>192.168.0.23:17553</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         143.433</td><td style=\"text-align: right;\">9500</td><td style=\"text-align: right;\">-1398.62</td><td style=\"text-align: right;\">            -781.121</td><td style=\"text-align: right;\">            -1796.95</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_Pendulum-v0_6b1dd_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-13_20-57-20\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_reward_max: -11.722382280782014\n",
      "  episode_reward_mean: -1348.1319338104424\n",
      "  episode_reward_min: -1796.9534032278605\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 52\n",
      "  experiment_id: 43107a19371e49529eb221515032f0aa\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 10500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: 0.9547820687294006\n",
      "        mean_q: -53.53691864013672\n",
      "        min_q: -75.73155975341797\n",
      "        model: {}\n",
      "    num_steps_sampled: 10500\n",
      "    num_steps_trained: 1152256\n",
      "    num_target_updates: 4501\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.58076923076923\n",
      "    ram_util_percent: 62.80384615384616\n",
      "  pid: 17553\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09973805000463255\n",
      "    mean_env_wait_ms: 0.14517130740979722\n",
      "    mean_inference_ms: 0.7405815724945874\n",
      "    mean_raw_obs_processing_ms: 0.221378329083068\n",
      "  time_since_restore: 161.5978705883026\n",
      "  time_this_iter_s: 18.16535973548889\n",
      "  time_total_s: 161.5978705883026\n",
      "  timers:\n",
      "    learn_throughput: 61002.512\n",
      "    learn_time_ms: 4.197\n",
      "    update_time_ms: 2.488\n",
      "  timestamp: 1626227840\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10500\n",
      "  training_iteration: 10\n",
      "  trial_id: 6b1dd_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/12 CPUs, 0/0 GPUs, 0.0/3.81 GiB heap, 0.0/1.32 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_Pendulum-v0_6b1dd_00000</td><td>RUNNING </td><td>192.168.0.23:17553</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         161.598</td><td style=\"text-align: right;\">10500</td><td style=\"text-align: right;\">-1348.13</td><td style=\"text-align: right;\">            -11.7224</td><td style=\"text-align: right;\">            -1796.95</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_Pendulum-v0_6b1dd_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-13_20-57-38\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_reward_max: -11.722382280782014\n",
      "  episode_reward_mean: -1336.2461581544399\n",
      "  episode_reward_min: -1796.9534032278605\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 56\n",
      "  experiment_id: 43107a19371e49529eb221515032f0aa\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 11500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: -1.291930079460144\n",
      "        mean_q: -56.992698669433594\n",
      "        min_q: -83.67649841308594\n",
      "        model: {}\n",
      "    num_steps_sampled: 11500\n",
      "    num_steps_trained: 1280256\n",
      "    num_target_updates: 5001\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.019999999999996\n",
      "    ram_util_percent: 63.53200000000001\n",
      "  pid: 17553\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09984162821424539\n",
      "    mean_env_wait_ms: 0.1452610650848398\n",
      "    mean_inference_ms: 0.7400414020223257\n",
      "    mean_raw_obs_processing_ms: 0.22145066812292358\n",
      "  time_since_restore: 179.77703475952148\n",
      "  time_this_iter_s: 18.179164171218872\n",
      "  time_total_s: 179.77703475952148\n",
      "  timers:\n",
      "    learn_throughput: 55744.915\n",
      "    learn_time_ms: 4.592\n",
      "    update_time_ms: 2.283\n",
      "  timestamp: 1626227858\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 11500\n",
      "  training_iteration: 11\n",
      "  trial_id: 6b1dd_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/12 CPUs, 0/0 GPUs, 0.0/3.81 GiB heap, 0.0/1.32 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_Pendulum-v0_6b1dd_00000</td><td>RUNNING </td><td>192.168.0.23:17553</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         179.777</td><td style=\"text-align: right;\">11500</td><td style=\"text-align: right;\">-1336.25</td><td style=\"text-align: right;\">            -11.7224</td><td style=\"text-align: right;\">            -1796.95</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_Pendulum-v0_6b1dd_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-13_20-57-57\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_reward_max: -11.722382280782014\n",
      "  episode_reward_mean: -1288.7165801323201\n",
      "  episode_reward_min: -1796.9534032278605\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 62\n",
      "  experiment_id: 43107a19371e49529eb221515032f0aa\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 12500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: 1.1449592113494873\n",
      "        mean_q: -62.394222259521484\n",
      "        min_q: -90.33869934082031\n",
      "        model: {}\n",
      "    num_steps_sampled: 12500\n",
      "    num_steps_trained: 1408256\n",
      "    num_target_updates: 5501\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.78846153846154\n",
      "    ram_util_percent: 61.83461538461539\n",
      "  pid: 17553\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.09998511732421295\n",
      "    mean_env_wait_ms: 0.14538881263770745\n",
      "    mean_inference_ms: 0.7394246003424146\n",
      "    mean_raw_obs_processing_ms: 0.22156465250994076\n",
      "  time_since_restore: 198.1817545890808\n",
      "  time_this_iter_s: 18.404719829559326\n",
      "  time_total_s: 198.1817545890808\n",
      "  timers:\n",
      "    learn_throughput: 54282.297\n",
      "    learn_time_ms: 4.716\n",
      "    update_time_ms: 2.672\n",
      "  timestamp: 1626227877\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12500\n",
      "  training_iteration: 12\n",
      "  trial_id: 6b1dd_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/12 CPUs, 0/0 GPUs, 0.0/3.81 GiB heap, 0.0/1.32 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_Pendulum-v0_6b1dd_00000</td><td>RUNNING </td><td>192.168.0.23:17553</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         198.182</td><td style=\"text-align: right;\">12500</td><td style=\"text-align: right;\">-1288.72</td><td style=\"text-align: right;\">            -11.7224</td><td style=\"text-align: right;\">            -1796.95</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_Pendulum-v0_6b1dd_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-13_20-58-14\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_reward_max: -11.722382280782014\n",
      "  episode_reward_mean: -1269.864064156319\n",
      "  episode_reward_min: -1796.9534032278605\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 66\n",
      "  experiment_id: 43107a19371e49529eb221515032f0aa\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 13500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: 0.9233301877975464\n",
      "        mean_q: -62.598182678222656\n",
      "        min_q: -89.0087890625\n",
      "        model: {}\n",
      "    num_steps_sampled: 13500\n",
      "    num_steps_trained: 1536256\n",
      "    num_target_updates: 6001\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.896\n",
      "    ram_util_percent: 62.232\n",
      "  pid: 17553\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10005915854217143\n",
      "    mean_env_wait_ms: 0.14545422921286064\n",
      "    mean_inference_ms: 0.739005902172205\n",
      "    mean_raw_obs_processing_ms: 0.22161749281606014\n",
      "  time_since_restore: 216.09442567825317\n",
      "  time_this_iter_s: 17.912671089172363\n",
      "  time_total_s: 216.09442567825317\n",
      "  timers:\n",
      "    learn_throughput: 66939.841\n",
      "    learn_time_ms: 3.824\n",
      "    update_time_ms: 2.219\n",
      "  timestamp: 1626227894\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 13500\n",
      "  training_iteration: 13\n",
      "  trial_id: 6b1dd_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/12 CPUs, 0/0 GPUs, 0.0/3.81 GiB heap, 0.0/1.32 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_Pendulum-v0_6b1dd_00000</td><td>RUNNING </td><td>192.168.0.23:17553</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         216.094</td><td style=\"text-align: right;\">13500</td><td style=\"text-align: right;\">-1269.86</td><td style=\"text-align: right;\">            -11.7224</td><td style=\"text-align: right;\">            -1796.95</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_Pendulum-v0_6b1dd_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-13_20-58-33\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_reward_max: -3.1396012759307346\n",
      "  episode_reward_mean: -1193.5720942077833\n",
      "  episode_reward_min: -1796.9534032278605\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 72\n",
      "  experiment_id: 43107a19371e49529eb221515032f0aa\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 14500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: 2.8817272186279297\n",
      "        mean_q: -63.85460662841797\n",
      "        min_q: -94.13409423828125\n",
      "        model: {}\n",
      "    num_steps_sampled: 14500\n",
      "    num_steps_trained: 1664256\n",
      "    num_target_updates: 6501\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.12\n",
      "    ram_util_percent: 63.36\n",
      "  pid: 17553\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1001575898790833\n",
      "    mean_env_wait_ms: 0.14555066669231087\n",
      "    mean_inference_ms: 0.7384886316517281\n",
      "    mean_raw_obs_processing_ms: 0.22168737744292352\n",
      "  time_since_restore: 234.4201078414917\n",
      "  time_this_iter_s: 18.325682163238525\n",
      "  time_total_s: 234.4201078414917\n",
      "  timers:\n",
      "    learn_throughput: 70082.554\n",
      "    learn_time_ms: 3.653\n",
      "    update_time_ms: 2.176\n",
      "  timestamp: 1626227913\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 14500\n",
      "  training_iteration: 14\n",
      "  trial_id: 6b1dd_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/12 CPUs, 0/0 GPUs, 0.0/3.81 GiB heap, 0.0/1.32 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_Pendulum-v0_6b1dd_00000</td><td>RUNNING </td><td>192.168.0.23:17553</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">          234.42</td><td style=\"text-align: right;\">14500</td><td style=\"text-align: right;\">-1193.57</td><td style=\"text-align: right;\">             -3.1396</td><td style=\"text-align: right;\">            -1796.95</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_Pendulum-v0_6b1dd_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-13_20-58-51\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_reward_max: -3.1396012759307346\n",
      "  episode_reward_mean: -1179.2687786561391\n",
      "  episode_reward_min: -1796.9534032278605\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 76\n",
      "  experiment_id: 43107a19371e49529eb221515032f0aa\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 15500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: 2.4560861587524414\n",
      "        mean_q: -63.485443115234375\n",
      "        min_q: -99.68254089355469\n",
      "        model: {}\n",
      "    num_steps_sampled: 15500\n",
      "    num_steps_trained: 1792256\n",
      "    num_target_updates: 7001\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.6\n",
      "    ram_util_percent: 63.26\n",
      "  pid: 17553\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10020802807056446\n",
      "    mean_env_wait_ms: 0.14559826169718723\n",
      "    mean_inference_ms: 0.7381199811772466\n",
      "    mean_raw_obs_processing_ms: 0.22171206407209648\n",
      "  time_since_restore: 252.2317407131195\n",
      "  time_this_iter_s: 17.811632871627808\n",
      "  time_total_s: 252.2317407131195\n",
      "  timers:\n",
      "    learn_throughput: 72884.011\n",
      "    learn_time_ms: 3.512\n",
      "    update_time_ms: 2.139\n",
      "  timestamp: 1626227931\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 15500\n",
      "  training_iteration: 15\n",
      "  trial_id: 6b1dd_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/12 CPUs, 0/0 GPUs, 0.0/3.81 GiB heap, 0.0/1.32 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_Pendulum-v0_6b1dd_00000</td><td>RUNNING </td><td>192.168.0.23:17553</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         252.232</td><td style=\"text-align: right;\">15500</td><td style=\"text-align: right;\">-1179.27</td><td style=\"text-align: right;\">             -3.1396</td><td style=\"text-align: right;\">            -1796.95</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_Pendulum-v0_6b1dd_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-13_20-59-08\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_reward_max: -1.8323446584304053\n",
      "  episode_reward_mean: -1113.7970069430512\n",
      "  episode_reward_min: -1796.9534032278605\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 82\n",
      "  experiment_id: 43107a19371e49529eb221515032f0aa\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 16500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: 3.410090684890747\n",
      "        mean_q: -70.59851837158203\n",
      "        min_q: -103.12985229492188\n",
      "        model: {}\n",
      "    num_steps_sampled: 16500\n",
      "    num_steps_trained: 1920256\n",
      "    num_target_updates: 7501\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.516\n",
      "    ram_util_percent: 63.868\n",
      "  pid: 17553\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10026239466162451\n",
      "    mean_env_wait_ms: 0.14564343205419253\n",
      "    mean_inference_ms: 0.7375129405851482\n",
      "    mean_raw_obs_processing_ms: 0.22171667486775146\n",
      "  time_since_restore: 269.8548905849457\n",
      "  time_this_iter_s: 17.623149871826172\n",
      "  time_total_s: 269.8548905849457\n",
      "  timers:\n",
      "    learn_throughput: 68509.454\n",
      "    learn_time_ms: 3.737\n",
      "    update_time_ms: 2.178\n",
      "  timestamp: 1626227948\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 16500\n",
      "  training_iteration: 16\n",
      "  trial_id: 6b1dd_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/12 CPUs, 0/0 GPUs, 0.0/3.81 GiB heap, 0.0/1.32 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_Pendulum-v0_6b1dd_00000</td><td>RUNNING </td><td>192.168.0.23:17553</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         269.855</td><td style=\"text-align: right;\">16500</td><td style=\"text-align: right;\"> -1113.8</td><td style=\"text-align: right;\">            -1.83234</td><td style=\"text-align: right;\">            -1796.95</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_Pendulum-v0_6b1dd_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-13_20-59-26\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_reward_max: -1.8323446584304053\n",
      "  episode_reward_mean: -1085.0581441625716\n",
      "  episode_reward_min: -1796.9534032278605\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 86\n",
      "  experiment_id: 43107a19371e49529eb221515032f0aa\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 17500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: 3.329087018966675\n",
      "        mean_q: -69.15478515625\n",
      "        min_q: -105.27165222167969\n",
      "        model: {}\n",
      "    num_steps_sampled: 17500\n",
      "    num_steps_trained: 2048256\n",
      "    num_target_updates: 8001\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.852000000000004\n",
      "    ram_util_percent: 61.42\n",
      "  pid: 17553\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10029038455152726\n",
      "    mean_env_wait_ms: 0.145662359366662\n",
      "    mean_inference_ms: 0.7371023761861941\n",
      "    mean_raw_obs_processing_ms: 0.22170672561481933\n",
      "  time_since_restore: 287.5606265068054\n",
      "  time_this_iter_s: 17.70573592185974\n",
      "  time_total_s: 287.5606265068054\n",
      "  timers:\n",
      "    learn_throughput: 68548.818\n",
      "    learn_time_ms: 3.735\n",
      "    update_time_ms: 2.23\n",
      "  timestamp: 1626227966\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 17500\n",
      "  training_iteration: 17\n",
      "  trial_id: 6b1dd_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/12 CPUs, 0/0 GPUs, 0.0/3.81 GiB heap, 0.0/1.32 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_Pendulum-v0_6b1dd_00000</td><td>RUNNING </td><td>192.168.0.23:17553</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         287.561</td><td style=\"text-align: right;\">17500</td><td style=\"text-align: right;\">-1085.06</td><td style=\"text-align: right;\">            -1.83234</td><td style=\"text-align: right;\">            -1796.95</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_Pendulum-v0_6b1dd_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-13_20-59-44\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_reward_max: -1.1116457547643066\n",
      "  episode_reward_mean: -1034.789899227471\n",
      "  episode_reward_min: -1796.9534032278605\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 92\n",
      "  experiment_id: 43107a19371e49529eb221515032f0aa\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 18500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: 3.6690609455108643\n",
      "        mean_q: -67.78599548339844\n",
      "        min_q: -111.97421264648438\n",
      "        model: {}\n",
      "    num_steps_sampled: 18500\n",
      "    num_steps_trained: 2176256\n",
      "    num_target_updates: 8501\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.984\n",
      "    ram_util_percent: 61.907999999999994\n",
      "  pid: 17553\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10032541318647033\n",
      "    mean_env_wait_ms: 0.14568362066268506\n",
      "    mean_inference_ms: 0.7365277332727878\n",
      "    mean_raw_obs_processing_ms: 0.22168686968758322\n",
      "  time_since_restore: 305.71143412590027\n",
      "  time_this_iter_s: 18.15080761909485\n",
      "  time_total_s: 305.71143412590027\n",
      "  timers:\n",
      "    learn_throughput: 69610.491\n",
      "    learn_time_ms: 3.678\n",
      "    update_time_ms: 2.14\n",
      "  timestamp: 1626227984\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 18500\n",
      "  training_iteration: 18\n",
      "  trial_id: 6b1dd_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/12 CPUs, 0/0 GPUs, 0.0/3.81 GiB heap, 0.0/1.32 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_Pendulum-v0_6b1dd_00000</td><td>RUNNING </td><td>192.168.0.23:17553</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">         305.711</td><td style=\"text-align: right;\">18500</td><td style=\"text-align: right;\">-1034.79</td><td style=\"text-align: right;\">            -1.11165</td><td style=\"text-align: right;\">            -1796.95</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_Pendulum-v0_6b1dd_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-13_21-00-02\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_reward_max: -1.1116457547643066\n",
      "  episode_reward_mean: -997.122177864407\n",
      "  episode_reward_min: -1796.9534032278605\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 96\n",
      "  experiment_id: 43107a19371e49529eb221515032f0aa\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 19500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: 5.729738235473633\n",
      "        mean_q: -72.16944122314453\n",
      "        min_q: -117.57238006591797\n",
      "        model: {}\n",
      "    num_steps_sampled: 19500\n",
      "    num_steps_trained: 2304256\n",
      "    num_target_updates: 9001\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.772\n",
      "    ram_util_percent: 63.343999999999994\n",
      "  pid: 17553\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10034120140496124\n",
      "    mean_env_wait_ms: 0.1456915975727191\n",
      "    mean_inference_ms: 0.7361423802744369\n",
      "    mean_raw_obs_processing_ms: 0.22166756125687728\n",
      "  time_since_restore: 323.59876918792725\n",
      "  time_this_iter_s: 17.887335062026978\n",
      "  time_total_s: 323.59876918792725\n",
      "  timers:\n",
      "    learn_throughput: 71712.349\n",
      "    learn_time_ms: 3.57\n",
      "    update_time_ms: 2.143\n",
      "  timestamp: 1626228002\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 19500\n",
      "  training_iteration: 19\n",
      "  trial_id: 6b1dd_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/12 CPUs, 0/0 GPUs, 0.0/3.81 GiB heap, 0.0/1.32 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_Pendulum-v0_6b1dd_00000</td><td>RUNNING </td><td>192.168.0.23:17553</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         323.599</td><td style=\"text-align: right;\">19500</td><td style=\"text-align: right;\">-997.122</td><td style=\"text-align: right;\">            -1.11165</td><td style=\"text-align: right;\">            -1796.95</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_Pendulum-v0_6b1dd_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-13_21-00-20\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_reward_max: -1.1116457547643066\n",
      "  episode_reward_mean: -962.5109391533188\n",
      "  episode_reward_min: -1796.9534032278605\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 102\n",
      "  experiment_id: 43107a19371e49529eb221515032f0aa\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 20500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: 5.6856279373168945\n",
      "        mean_q: -70.61603546142578\n",
      "        min_q: -114.73045349121094\n",
      "        model: {}\n",
      "    num_steps_sampled: 20500\n",
      "    num_steps_trained: 2432256\n",
      "    num_target_updates: 9501\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.34\n",
      "    ram_util_percent: 63.876\n",
      "  pid: 17553\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10039843358964308\n",
      "    mean_env_wait_ms: 0.14570528559035256\n",
      "    mean_inference_ms: 0.7345797772574815\n",
      "    mean_raw_obs_processing_ms: 0.22154705123683371\n",
      "  time_since_restore: 341.51777505874634\n",
      "  time_this_iter_s: 17.919005870819092\n",
      "  time_total_s: 341.51777505874634\n",
      "  timers:\n",
      "    learn_throughput: 74080.281\n",
      "    learn_time_ms: 3.456\n",
      "    update_time_ms: 2.081\n",
      "  timestamp: 1626228020\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 20500\n",
      "  training_iteration: 20\n",
      "  trial_id: 6b1dd_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/12 CPUs, 0/0 GPUs, 0.0/3.81 GiB heap, 0.0/1.32 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_Pendulum-v0_6b1dd_00000</td><td>RUNNING </td><td>192.168.0.23:17553</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         341.518</td><td style=\"text-align: right;\">20500</td><td style=\"text-align: right;\">-962.511</td><td style=\"text-align: right;\">            -1.11165</td><td style=\"text-align: right;\">            -1796.95</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_Pendulum-v0_6b1dd_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-13_21-00-38\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_reward_max: -1.1116457547643066\n",
      "  episode_reward_mean: -926.8117919657061\n",
      "  episode_reward_min: -1796.9534032278605\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 106\n",
      "  experiment_id: 43107a19371e49529eb221515032f0aa\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 21500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: 4.623827934265137\n",
      "        mean_q: -73.03113555908203\n",
      "        min_q: -121.44898986816406\n",
      "        model: {}\n",
      "    num_steps_sampled: 21500\n",
      "    num_steps_trained: 2560256\n",
      "    num_target_updates: 10001\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.892\n",
      "    ram_util_percent: 64.264\n",
      "  pid: 17553\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10054363674887985\n",
      "    mean_env_wait_ms: 0.14575424884632088\n",
      "    mean_inference_ms: 0.7328909911016142\n",
      "    mean_raw_obs_processing_ms: 0.22151258708781435\n",
      "  time_since_restore: 359.28413009643555\n",
      "  time_this_iter_s: 17.76635503768921\n",
      "  time_total_s: 359.28413009643555\n",
      "  timers:\n",
      "    learn_throughput: 71191.236\n",
      "    learn_time_ms: 3.596\n",
      "    update_time_ms: 2.169\n",
      "  timestamp: 1626228038\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 21500\n",
      "  training_iteration: 21\n",
      "  trial_id: 6b1dd_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/12 CPUs, 0/0 GPUs, 0.0/3.81 GiB heap, 0.0/1.32 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_Pendulum-v0_6b1dd_00000</td><td>RUNNING </td><td>192.168.0.23:17553</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">         359.284</td><td style=\"text-align: right;\">21500</td><td style=\"text-align: right;\">-926.812</td><td style=\"text-align: right;\">            -1.11165</td><td style=\"text-align: right;\">            -1796.95</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_Pendulum-v0_6b1dd_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-13_21-00-56\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_reward_max: -1.1116457547643066\n",
      "  episode_reward_mean: -845.3818124377199\n",
      "  episode_reward_min: -1702.4878189939052\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 112\n",
      "  experiment_id: 43107a19371e49529eb221515032f0aa\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 22500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: 5.559916973114014\n",
      "        mean_q: -77.15824890136719\n",
      "        min_q: -124.76898956298828\n",
      "        model: {}\n",
      "    num_steps_sampled: 22500\n",
      "    num_steps_trained: 2688256\n",
      "    num_target_updates: 10501\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.011538461538464\n",
      "    ram_util_percent: 62.280769230769245\n",
      "  pid: 17553\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10065893392522074\n",
      "    mean_env_wait_ms: 0.14581588006225654\n",
      "    mean_inference_ms: 0.731451783940171\n",
      "    mean_raw_obs_processing_ms: 0.2214408051532333\n",
      "  time_since_restore: 377.5080909729004\n",
      "  time_this_iter_s: 18.223960876464844\n",
      "  time_total_s: 377.5080909729004\n",
      "  timers:\n",
      "    learn_throughput: 70330.438\n",
      "    learn_time_ms: 3.64\n",
      "    update_time_ms: 2.2\n",
      "  timestamp: 1626228056\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 22500\n",
      "  training_iteration: 22\n",
      "  trial_id: 6b1dd_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/12 CPUs, 0/0 GPUs, 0.0/3.81 GiB heap, 0.0/1.32 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_Pendulum-v0_6b1dd_00000</td><td>RUNNING </td><td>192.168.0.23:17553</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    22</td><td style=\"text-align: right;\">         377.508</td><td style=\"text-align: right;\">22500</td><td style=\"text-align: right;\">-845.382</td><td style=\"text-align: right;\">            -1.11165</td><td style=\"text-align: right;\">            -1702.49</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_Pendulum-v0_6b1dd_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-13_21-01-14\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_reward_max: -1.1116457547643066\n",
      "  episode_reward_mean: -789.2500149746359\n",
      "  episode_reward_min: -1702.4878189939052\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 116\n",
      "  experiment_id: 43107a19371e49529eb221515032f0aa\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 23500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: 5.940964221954346\n",
      "        mean_q: -75.83016967773438\n",
      "        min_q: -128.67138671875\n",
      "        model: {}\n",
      "    num_steps_sampled: 23500\n",
      "    num_steps_trained: 2816256\n",
      "    num_target_updates: 11001\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.7\n",
      "    ram_util_percent: 61.724\n",
      "  pid: 17553\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1007344092037897\n",
      "    mean_env_wait_ms: 0.14590177367580778\n",
      "    mean_inference_ms: 0.7310117294745901\n",
      "    mean_raw_obs_processing_ms: 0.2214846485799835\n",
      "  time_since_restore: 395.6538829803467\n",
      "  time_this_iter_s: 18.14579200744629\n",
      "  time_total_s: 395.6538829803467\n",
      "  timers:\n",
      "    learn_throughput: 64673.113\n",
      "    learn_time_ms: 3.958\n",
      "    update_time_ms: 2.452\n",
      "  timestamp: 1626228074\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 23500\n",
      "  training_iteration: 23\n",
      "  trial_id: 6b1dd_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/12 CPUs, 0/0 GPUs, 0.0/3.81 GiB heap, 0.0/1.32 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_Pendulum-v0_6b1dd_00000</td><td>RUNNING </td><td>192.168.0.23:17553</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    23</td><td style=\"text-align: right;\">         395.654</td><td style=\"text-align: right;\">23500</td><td style=\"text-align: right;\"> -789.25</td><td style=\"text-align: right;\">            -1.11165</td><td style=\"text-align: right;\">            -1702.49</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_Pendulum-v0_6b1dd_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-13_21-01-33\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_reward_max: -1.1116457547643066\n",
      "  episode_reward_mean: -711.1807312858703\n",
      "  episode_reward_min: -1702.4878189939052\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 122\n",
      "  experiment_id: 43107a19371e49529eb221515032f0aa\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 24500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: 5.302407264709473\n",
      "        mean_q: -76.78132629394531\n",
      "        min_q: -129.2058563232422\n",
      "        model: {}\n",
      "    num_steps_sampled: 24500\n",
      "    num_steps_trained: 2944256\n",
      "    num_target_updates: 11501\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.71153846153846\n",
      "    ram_util_percent: 63.11538461538461\n",
      "  pid: 17553\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10082041446917875\n",
      "    mean_env_wait_ms: 0.1460076409820115\n",
      "    mean_inference_ms: 0.7305336008668726\n",
      "    mean_raw_obs_processing_ms: 0.22157216103520583\n",
      "  time_since_restore: 414.03240990638733\n",
      "  time_this_iter_s: 18.37852692604065\n",
      "  time_total_s: 414.03240990638733\n",
      "  timers:\n",
      "    learn_throughput: 69906.887\n",
      "    learn_time_ms: 3.662\n",
      "    update_time_ms: 2.684\n",
      "  timestamp: 1626228093\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 24500\n",
      "  training_iteration: 24\n",
      "  trial_id: 6b1dd_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/12 CPUs, 0/0 GPUs, 0.0/3.81 GiB heap, 0.0/1.32 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_Pendulum-v0_6b1dd_00000</td><td>RUNNING </td><td>192.168.0.23:17553</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    24</td><td style=\"text-align: right;\">         414.032</td><td style=\"text-align: right;\">24500</td><td style=\"text-align: right;\">-711.181</td><td style=\"text-align: right;\">            -1.11165</td><td style=\"text-align: right;\">            -1702.49</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_Pendulum-v0_6b1dd_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-13_21-01-50\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_reward_max: -1.1116457547643066\n",
      "  episode_reward_mean: -654.9789331537673\n",
      "  episode_reward_min: -1543.9091369921614\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 126\n",
      "  experiment_id: 43107a19371e49529eb221515032f0aa\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 25500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: 8.509936332702637\n",
      "        mean_q: -74.9183349609375\n",
      "        min_q: -133.55616760253906\n",
      "        model: {}\n",
      "    num_steps_sampled: 25500\n",
      "    num_steps_trained: 3072256\n",
      "    num_target_updates: 12001\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.025000000000002\n",
      "    ram_util_percent: 63.50416666666666\n",
      "  pid: 17553\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10083697897637284\n",
      "    mean_env_wait_ms: 0.14602871486581848\n",
      "    mean_inference_ms: 0.7300545922472584\n",
      "    mean_raw_obs_processing_ms: 0.22157011281195912\n",
      "  time_since_restore: 431.3214418888092\n",
      "  time_this_iter_s: 17.289031982421875\n",
      "  time_total_s: 431.3214418888092\n",
      "  timers:\n",
      "    learn_throughput: 75198.324\n",
      "    learn_time_ms: 3.404\n",
      "    update_time_ms: 1.789\n",
      "  timestamp: 1626228110\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 25500\n",
      "  training_iteration: 25\n",
      "  trial_id: 6b1dd_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/12 CPUs, 0/0 GPUs, 0.0/3.81 GiB heap, 0.0/1.32 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_Pendulum-v0_6b1dd_00000</td><td>RUNNING </td><td>192.168.0.23:17553</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         431.321</td><td style=\"text-align: right;\">25500</td><td style=\"text-align: right;\">-654.979</td><td style=\"text-align: right;\">            -1.11165</td><td style=\"text-align: right;\">            -1543.91</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_Pendulum-v0_6b1dd_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-13_21-02-09\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_reward_max: -1.1116457547643066\n",
      "  episode_reward_mean: -581.5873670837315\n",
      "  episode_reward_min: -1519.854635629977\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 132\n",
      "  experiment_id: 43107a19371e49529eb221515032f0aa\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 26500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: 5.026442050933838\n",
      "        mean_q: -76.33003234863281\n",
      "        min_q: -136.049560546875\n",
      "        model: {}\n",
      "    num_steps_sampled: 26500\n",
      "    num_steps_trained: 3200256\n",
      "    num_target_updates: 12501\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.35\n",
      "    ram_util_percent: 62.79615384615385\n",
      "  pid: 17553\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10085566443925359\n",
      "    mean_env_wait_ms: 0.14606064681301018\n",
      "    mean_inference_ms: 0.729549470409917\n",
      "    mean_raw_obs_processing_ms: 0.22157791304227836\n",
      "  time_since_restore: 450.27890610694885\n",
      "  time_this_iter_s: 18.95746421813965\n",
      "  time_total_s: 450.27890610694885\n",
      "  timers:\n",
      "    learn_throughput: 64951.657\n",
      "    learn_time_ms: 3.941\n",
      "    update_time_ms: 2.259\n",
      "  timestamp: 1626228129\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 26500\n",
      "  training_iteration: 26\n",
      "  trial_id: 6b1dd_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/12 CPUs, 0/0 GPUs, 0.0/3.81 GiB heap, 0.0/1.32 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_Pendulum-v0_6b1dd_00000</td><td>RUNNING </td><td>192.168.0.23:17553</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    26</td><td style=\"text-align: right;\">         450.279</td><td style=\"text-align: right;\">26500</td><td style=\"text-align: right;\">-581.587</td><td style=\"text-align: right;\">            -1.11165</td><td style=\"text-align: right;\">            -1519.85</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_Pendulum-v0_6b1dd_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-13_21-02-28\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_reward_max: -1.1116457547643066\n",
      "  episode_reward_mean: -543.6693148351875\n",
      "  episode_reward_min: -1519.854635629977\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 136\n",
      "  experiment_id: 43107a19371e49529eb221515032f0aa\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 27500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: 4.031546592712402\n",
      "        mean_q: -76.26606750488281\n",
      "        min_q: -138.93124389648438\n",
      "        model: {}\n",
      "    num_steps_sampled: 27500\n",
      "    num_steps_trained: 3328256\n",
      "    num_target_updates: 13001\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.355555555555558\n",
      "    ram_util_percent: 62.829629629629636\n",
      "  pid: 17553\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10084317243518033\n",
      "    mean_env_wait_ms: 0.14605039801312864\n",
      "    mean_inference_ms: 0.7291718178633722\n",
      "    mean_raw_obs_processing_ms: 0.2215448771838519\n",
      "  time_since_restore: 469.2216773033142\n",
      "  time_this_iter_s: 18.942771196365356\n",
      "  time_total_s: 469.2216773033142\n",
      "  timers:\n",
      "    learn_throughput: 66035.376\n",
      "    learn_time_ms: 3.877\n",
      "    update_time_ms: 2.228\n",
      "  timestamp: 1626228148\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 27500\n",
      "  training_iteration: 27\n",
      "  trial_id: 6b1dd_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/12 CPUs, 0/0 GPUs, 0.0/3.81 GiB heap, 0.0/1.32 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_Pendulum-v0_6b1dd_00000</td><td>RUNNING </td><td>192.168.0.23:17553</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    27</td><td style=\"text-align: right;\">         469.222</td><td style=\"text-align: right;\">27500</td><td style=\"text-align: right;\">-543.669</td><td style=\"text-align: right;\">            -1.11165</td><td style=\"text-align: right;\">            -1519.85</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_Pendulum-v0_6b1dd_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-13_21-02-47\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_reward_max: -1.1116457547643066\n",
      "  episode_reward_mean: -503.3479170812064\n",
      "  episode_reward_min: -1519.854635629977\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 142\n",
      "  experiment_id: 43107a19371e49529eb221515032f0aa\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 28500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: 8.385159492492676\n",
      "        mean_q: -79.76459503173828\n",
      "        min_q: -143.51950073242188\n",
      "        model: {}\n",
      "    num_steps_sampled: 28500\n",
      "    num_steps_trained: 3456256\n",
      "    num_target_updates: 13501\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.71153846153846\n",
      "    ram_util_percent: 62.20384615384616\n",
      "  pid: 17553\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10081609962945773\n",
      "    mean_env_wait_ms: 0.14602355893903582\n",
      "    mean_inference_ms: 0.7286173247160681\n",
      "    mean_raw_obs_processing_ms: 0.2214595482753488\n",
      "  time_since_restore: 488.01311016082764\n",
      "  time_this_iter_s: 18.791432857513428\n",
      "  time_total_s: 488.01311016082764\n",
      "  timers:\n",
      "    learn_throughput: 64560.344\n",
      "    learn_time_ms: 3.965\n",
      "    update_time_ms: 2.323\n",
      "  timestamp: 1626228167\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 28500\n",
      "  training_iteration: 28\n",
      "  trial_id: 6b1dd_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/12 CPUs, 0/0 GPUs, 0.0/3.81 GiB heap, 0.0/1.32 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_Pendulum-v0_6b1dd_00000</td><td>RUNNING </td><td>192.168.0.23:17553</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    28</td><td style=\"text-align: right;\">         488.013</td><td style=\"text-align: right;\">28500</td><td style=\"text-align: right;\">-503.348</td><td style=\"text-align: right;\">            -1.11165</td><td style=\"text-align: right;\">            -1519.85</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_Pendulum-v0_6b1dd_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-13_21-03-06\n",
      "  done: false\n",
      "  episode_len_mean: 200.0\n",
      "  episode_reward_max: -1.1116457547643066\n",
      "  episode_reward_mean: -461.2358136139652\n",
      "  episode_reward_min: -1519.854635629977\n",
      "  episodes_this_iter: 4\n",
      "  episodes_total: 146\n",
      "  experiment_id: 43107a19371e49529eb221515032f0aa\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 29500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: 6.932671070098877\n",
      "        mean_q: -74.86430358886719\n",
      "        min_q: -146.92617797851562\n",
      "        model: {}\n",
      "    num_steps_sampled: 29500\n",
      "    num_steps_trained: 3584256\n",
      "    num_target_updates: 14001\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.81851851851852\n",
      "    ram_util_percent: 62.31481481481482\n",
      "  pid: 17553\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10080769151659849\n",
      "    mean_env_wait_ms: 0.1460119840536429\n",
      "    mean_inference_ms: 0.7283629026092139\n",
      "    mean_raw_obs_processing_ms: 0.2214165201194093\n",
      "  time_since_restore: 507.24014711380005\n",
      "  time_this_iter_s: 19.227036952972412\n",
      "  time_total_s: 507.24014711380005\n",
      "  timers:\n",
      "    learn_throughput: 52636.467\n",
      "    learn_time_ms: 4.864\n",
      "    update_time_ms: 2.394\n",
      "  timestamp: 1626228186\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 29500\n",
      "  training_iteration: 29\n",
      "  trial_id: 6b1dd_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3/12 CPUs, 0/0 GPUs, 0.0/3.81 GiB heap, 0.0/1.32 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_Pendulum-v0_6b1dd_00000</td><td>RUNNING </td><td>192.168.0.23:17553</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    29</td><td style=\"text-align: right;\">          507.24</td><td style=\"text-align: right;\">29500</td><td style=\"text-align: right;\">-461.236</td><td style=\"text-align: right;\">            -1.11165</td><td style=\"text-align: right;\">            -1519.85</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DDPG_Pendulum-v0_6b1dd_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-13_21-03-25\n",
      "  done: true\n",
      "  episode_len_mean: 200.0\n",
      "  episode_reward_max: -1.1116457547643066\n",
      "  episode_reward_mean: -444.3119269581459\n",
      "  episode_reward_min: -1524.5879513507703\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 152\n",
      "  experiment_id: 43107a19371e49529eb221515032f0aa\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    last_target_update_ts: 30500\n",
      "    learner:\n",
      "      default_policy:\n",
      "        max_q: 5.001977920532227\n",
      "        mean_q: -80.31802368164062\n",
      "        min_q: -147.42247009277344\n",
      "        model: {}\n",
      "    num_steps_sampled: 30500\n",
      "    num_steps_trained: 3712256\n",
      "    num_target_updates: 14501\n",
      "  iterations_since_restore: 30\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.751851851851853\n",
      "    ram_util_percent: 63.059259259259264\n",
      "  pid: 17553\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10079419357209049\n",
      "    mean_env_wait_ms: 0.14599417996490968\n",
      "    mean_inference_ms: 0.7280488494211115\n",
      "    mean_raw_obs_processing_ms: 0.22135616529126742\n",
      "  time_since_restore: 526.4707138538361\n",
      "  time_this_iter_s: 19.23056674003601\n",
      "  time_total_s: 526.4707138538361\n",
      "  timers:\n",
      "    learn_throughput: 65008.284\n",
      "    learn_time_ms: 3.938\n",
      "    update_time_ms: 2.475\n",
      "  timestamp: 1626228205\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 30500\n",
      "  training_iteration: 30\n",
      "  trial_id: 6b1dd_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/12 CPUs, 0/0 GPUs, 0.0/3.81 GiB heap, 0.0/1.32 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 1/1 (1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_Pendulum-v0_6b1dd_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    30</td><td style=\"text-align: right;\">         526.471</td><td style=\"text-align: right;\">30500</td><td style=\"text-align: right;\">-444.312</td><td style=\"text-align: right;\">            -1.11165</td><td style=\"text-align: right;\">            -1524.59</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/12 CPUs, 0/0 GPUs, 0.0/3.81 GiB heap, 0.0/1.32 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/DDPG<br>Number of trials: 1/1 (1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DDPG_Pendulum-v0_6b1dd_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    30</td><td style=\"text-align: right;\">         526.471</td><td style=\"text-align: right;\">30500</td><td style=\"text-align: right;\">-444.312</td><td style=\"text-align: right;\">            -1.11165</td><td style=\"text-align: right;\">            -1524.59</td><td style=\"text-align: right;\">               200</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-13 21:03:26,318\tINFO tune.py:448 -- Total run time: 540.24 seconds (539.55 seconds for the tuning loop).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7fadfcd0d490>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alg = 'DDPG'\n",
    "tune.run(alg,\n",
    "    stop={\"training_iteration\": 30},\n",
    "    config={\n",
    "        'env':'Pendulum-v0',\n",
    "        'num_gpus':0,\n",
    "        'num_workers':2,\n",
    "        'lr':tune.grid_search([.001,])     \n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: RLlib Environments\n",
    "\n",
    "1: RLlib works with several different types of environments, including OpenAI Gym, user-defined, multi-agent, and also batched environments.\n",
    "\n",
    "2: RLlib uses Gym as its environment interface for single-agent training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1: Configuring Environments\n",
    "\n",
    "    https://github.com/ray-project/ray/blob/master/rllib/examples/custom_env.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-7352217a18a1>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-7352217a18a1>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    self.action_space = <gym.Space>\u001b[0m\n\u001b[0m                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import gym, ray\n",
    "from ray.rllib.agents import ppo\n",
    "\n",
    "class MyEnv(gym.Env):\n",
    "    def __init__(self, env_config):\n",
    "        self.action_space = <gym.Space>\n",
    "        self.observation_space = <gym.Space>\n",
    "    def reset(self):\n",
    "        return <obs>\n",
    "    def step(self, action):\n",
    "        return <obs>, <reward: float>, <done: bool>, <info: dict>\n",
    "\n",
    "ray.init()\n",
    "trainer = ppo.PPOTrainer(env=MyEnv, config={\n",
    "    \"env_config\": {},  # config to pass to env class\n",
    "})\n",
    "\n",
    "while True:\n",
    "    print(trainer.train())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
