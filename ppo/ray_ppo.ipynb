{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfe325d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray.rllib.agents import ppo, ddpg, sac\n",
    "from ray import tune \n",
    "from supplychain import SimpleSupplyChain\n",
    "import importlib\n",
    "#importlib.reload(supplychain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9391dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-21 13:35:55,138\tINFO services.py:1462 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RayContext(dashboard_url='127.0.0.1:8265', python_version='3.6.13', ray_version='1.12.0', ray_commit='f18fc31c7562990955556899090f8e8656b48d2d', address_info={'node_ip_address': '172.16.30.231', 'raylet_ip_address': '172.16.30.231', 'redis_address': None, 'object_store_address': '/tmp/ray/session_2022-04-21_13-35-52_535355_83246/sockets/plasma_store', 'raylet_socket_name': '/tmp/ray/session_2022-04-21_13-35-52_535355_83246/sockets/raylet', 'webui_url': '127.0.0.1:8265', 'session_dir': '/tmp/ray/session_2022-04-21_13-35-52_535355_83246', 'metrics_export_port': 55386, 'gcs_address': '172.16.30.231:65303', 'address': '172.16.30.231:65303', 'node_id': '35ed623f560a9a9d5460970abc112ccaf25270d342ce32c2af65c6d4'})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.shutdown()\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26cf9c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-21 13:36:00,319\tINFO trial_runner.py:803 -- starting PPO_SimpleSupplyChain_faf0d_00000\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:36:06,475\tINFO trainer.py:2296 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:36:06,476\tINFO ppo.py:269 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:36:06,476\tINFO trainer.py:867 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=86918)\u001b[0m 2022-04-21 13:36:12,639\tWARNING rollout_worker.py:499 -- We've added a module for checking environments that are used in experiments. It will cause your environment to fail if your environment is not set upcorrectly. You can disable check env by setting `disable_env_checking` to True in your experiment config dictionary. You can run the environment checking module standalone by calling ray.rllib.utils.check_env(env).\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=86918)\u001b[0m 2022-04-21 13:36:12,639\tWARNING env.py:121 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=86919)\u001b[0m 2022-04-21 13:36:12,622\tWARNING rollout_worker.py:499 -- We've added a module for checking environments that are used in experiments. It will cause your environment to fail if your environment is not set upcorrectly. You can disable check env by setting `disable_env_checking` to True in your experiment config dictionary. You can run the environment checking module standalone by calling ray.rllib.utils.check_env(env).\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=86919)\u001b[0m 2022-04-21 13:36:12,622\tWARNING env.py:121 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:36:15 (running for 00:00:15.64)<br>Memory usage on this node: 17.5/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:36:15,652\tWARNING util.py:60 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:36:18,702\tWARNING deprecation.py:47 -- DeprecationWarning: `slice` has been deprecated. Use `SampleBatch[start:stop]` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:36:20 (running for 00:00:20.66)<br>Memory usage on this node: 17.5/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-36-21\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -193503.1946754939\n",
      "  episode_reward_mean: -608365.011875949\n",
      "  episode_reward_min: -847644.6801263577\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 160\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 5.742432117462158\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.047365326434373856\n",
      "          model: {}\n",
      "          policy_loss: -0.03163792937994003\n",
      "          total_loss: 9.977835655212402\n",
      "          vf_explained_var: -2.6918225870531387e-08\n",
      "          vf_loss: 10.0\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.466666666666667\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1514266575055978\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.28396790650771414\n",
      "    mean_inference_ms: 0.9117039485552023\n",
      "    mean_raw_obs_processing_ms: 0.13088792756103027\n",
      "  time_since_restore: 5.76857590675354\n",
      "  time_this_iter_s: 5.76857590675354\n",
      "  time_total_s: 5.76857590675354\n",
      "  timers:\n",
      "    learn_throughput: 1477.655\n",
      "    learn_time_ms: 2706.992\n",
      "    load_throughput: 13357656.051\n",
      "    load_time_ms: 0.299\n",
      "    sample_throughput: 1311.147\n",
      "    sample_time_ms: 3050.765\n",
      "    update_time_ms: 2.432\n",
      "  timestamp: 1650548181\n",
      "  timesteps_since_restore: 4000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:36:21,429\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 60837.0x the scale of `vf_clip_param`. This means that it will take more than 60837.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:36:26 (running for 00:00:26.45)<br>Memory usage on this node: 17.5/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         5.76858</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\"> -608365</td><td style=\"text-align: right;\">             -193503</td><td style=\"text-align: right;\">             -847645</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-36-27\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -281709.626340676\n",
      "  episode_reward_mean: -546910.2196914337\n",
      "  episode_reward_min: -782705.1014653926\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 320\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 5.803913593292236\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.05201631039381027\n",
      "          model: {}\n",
      "          policy_loss: -0.041059184819459915\n",
      "          total_loss: 9.974546432495117\n",
      "          vf_explained_var: -3.076368670917873e-08\n",
      "          vf_loss: 10.0\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 8000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.2375\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.15459645244366704\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.28756864962950857\n",
      "    mean_inference_ms: 0.9077103011043809\n",
      "    mean_raw_obs_processing_ms: 0.1306159887812013\n",
      "  time_since_restore: 11.32939887046814\n",
      "  time_this_iter_s: 5.5608229637146\n",
      "  time_total_s: 11.32939887046814\n",
      "  timers:\n",
      "    learn_throughput: 1535.054\n",
      "    learn_time_ms: 2605.771\n",
      "    load_throughput: 13612345.639\n",
      "    load_time_ms: 0.294\n",
      "    sample_throughput: 902.507\n",
      "    sample_time_ms: 4432.099\n",
      "    update_time_ms: 2.361\n",
      "  timestamp: 1650548187\n",
      "  timesteps_since_restore: 8000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 2\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:36:27,031\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 54691.0x the scale of `vf_clip_param`. This means that it will take more than 54691.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:36:32 (running for 00:00:32.09)<br>Memory usage on this node: 17.5/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         11.3294</td><td style=\"text-align: right;\">8000</td><td style=\"text-align: right;\"> -546910</td><td style=\"text-align: right;\">             -281710</td><td style=\"text-align: right;\">             -782705</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-36-32\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -294587.81568389\n",
      "  episode_reward_mean: -489298.7742206796\n",
      "  episode_reward_min: -745764.1249535566\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 480\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 5.842227458953857\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.046054862439632416\n",
      "          model: {}\n",
      "          policy_loss: -0.038190994411706924\n",
      "          total_loss: 9.98253345489502\n",
      "          vf_explained_var: -2.6918225870531387e-08\n",
      "          vf_loss: 10.0\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.6875\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.15311282071604806\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.28692877425727914\n",
      "    mean_inference_ms: 0.9053538310847626\n",
      "    mean_raw_obs_processing_ms: 0.1298598181583587\n",
      "  time_since_restore: 16.779393196105957\n",
      "  time_this_iter_s: 5.449994325637817\n",
      "  time_total_s: 16.779393196105957\n",
      "  timers:\n",
      "    learn_throughput: 1570.921\n",
      "    learn_time_ms: 2546.277\n",
      "    load_throughput: 13464860.353\n",
      "    load_time_ms: 0.297\n",
      "    sample_throughput: 831.47\n",
      "    sample_time_ms: 4810.759\n",
      "    update_time_ms: 2.403\n",
      "  timestamp: 1650548192\n",
      "  timesteps_since_restore: 12000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:36:32,522\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 48930.0x the scale of `vf_clip_param`. This means that it will take more than 48930.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:36:37 (running for 00:00:37.54)<br>Memory usage on this node: 17.5/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         16.7794</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\"> -489299</td><td style=\"text-align: right;\">             -294588</td><td style=\"text-align: right;\">             -745764</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:36:38,081\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 45555.0x the scale of `vf_clip_param`. This means that it will take more than 45555.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-36-38\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -195180.672430206\n",
      "  episode_reward_mean: -455551.4615010797\n",
      "  episode_reward_min: -682845.035961557\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 640\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 5.740345478057861\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.03842884302139282\n",
      "          model: {}\n",
      "          policy_loss: -0.04815276339650154\n",
      "          total_loss: 9.977787017822266\n",
      "          vf_explained_var: -3.076368670917873e-08\n",
      "          vf_loss: 10.0\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 16000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.314285714285715\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.15380732373377187\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.28801597575666965\n",
      "    mean_inference_ms: 0.9058593675622703\n",
      "    mean_raw_obs_processing_ms: 0.13010782862347284\n",
      "  time_since_restore: 22.300357580184937\n",
      "  time_this_iter_s: 5.5209643840789795\n",
      "  time_total_s: 22.300357580184937\n",
      "  timers:\n",
      "    learn_throughput: 1581.906\n",
      "    learn_time_ms: 2528.596\n",
      "    load_throughput: 13454062.55\n",
      "    load_time_ms: 0.297\n",
      "    sample_throughput: 802.285\n",
      "    sample_time_ms: 4985.759\n",
      "    update_time_ms: 2.427\n",
      "  timestamp: 1650548198\n",
      "  timesteps_since_restore: 16000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 4\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:36:43 (running for 00:00:43.14)<br>Memory usage on this node: 17.5/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         22.3004</td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\"> -455551</td><td style=\"text-align: right;\">             -195181</td><td style=\"text-align: right;\">             -682845</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 20000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-36-43\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -177900.91044478482\n",
      "  episode_reward_mean: -421164.9203217945\n",
      "  episode_reward_min: -640388.4611486678\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 800\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 5.772097110748291\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.028771821409463882\n",
      "          model: {}\n",
      "          policy_loss: -0.05088400840759277\n",
      "          total_loss: 9.978246688842773\n",
      "          vf_explained_var: -3.653187974350658e-08\n",
      "          vf_loss: 10.0\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 20000\n",
      "    num_agent_steps_trained: 20000\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.677777777777777\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.15343636134280858\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.28778931913632366\n",
      "    mean_inference_ms: 0.9054207894792796\n",
      "    mean_raw_obs_processing_ms: 0.1299272309707506\n",
      "  time_since_restore: 27.98729681968689\n",
      "  time_this_iter_s: 5.686939239501953\n",
      "  time_total_s: 27.98729681968689\n",
      "  timers:\n",
      "    learn_throughput: 1571.602\n",
      "    learn_time_ms: 2545.174\n",
      "    load_throughput: 13279417.445\n",
      "    load_time_ms: 0.301\n",
      "    sample_throughput: 783.286\n",
      "    sample_time_ms: 5106.694\n",
      "    update_time_ms: 2.389\n",
      "  timestamp: 1650548203\n",
      "  timesteps_since_restore: 20000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:36:43,807\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 42116.0x the scale of `vf_clip_param`. This means that it will take more than 42116.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:36:48 (running for 00:00:48.83)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         27.9873</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\"> -421165</td><td style=\"text-align: right;\">             -177901</td><td style=\"text-align: right;\">             -640388</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-36-49\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -168580.6645125634\n",
      "  episode_reward_mean: -404634.2905648655\n",
      "  episode_reward_min: -655884.3621384387\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 960\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5187499523162842\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 5.678738117218018\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.021642908453941345\n",
      "          model: {}\n",
      "          policy_loss: -0.053789395838975906\n",
      "          total_loss: 9.979080200195312\n",
      "          vf_explained_var: -5.7681912579710115e-08\n",
      "          vf_loss: 10.0\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_steps_sampled: 24000\n",
      "    num_steps_trained: 24000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.357142857142857\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1537098148327829\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2879189129780058\n",
      "    mean_inference_ms: 0.9049531589219117\n",
      "    mean_raw_obs_processing_ms: 0.12988303564119494\n",
      "  time_since_restore: 33.46194934844971\n",
      "  time_this_iter_s: 5.474652528762817\n",
      "  time_total_s: 33.46194934844971\n",
      "  timers:\n",
      "    learn_throughput: 1582.775\n",
      "    learn_time_ms: 2527.207\n",
      "    load_throughput: 13414618.337\n",
      "    load_time_ms: 0.298\n",
      "    sample_throughput: 768.714\n",
      "    sample_time_ms: 5203.494\n",
      "    update_time_ms: 2.379\n",
      "  timestamp: 1650548209\n",
      "  timesteps_since_restore: 24000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 6\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:36:49,321\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 40463.0x the scale of `vf_clip_param`. This means that it will take more than 40463.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:36:54 (running for 00:00:54.38)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         33.4619</td><td style=\"text-align: right;\">24000</td><td style=\"text-align: right;\"> -404634</td><td style=\"text-align: right;\">             -168581</td><td style=\"text-align: right;\">             -655884</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 28000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-36-54\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -123330.29285459584\n",
      "  episode_reward_mean: -375206.8757570981\n",
      "  episode_reward_min: -657388.174664188\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 1120\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.278125047683716\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 5.759029388427734\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016154808923602104\n",
      "          model: {}\n",
      "          policy_loss: -0.048159755766391754\n",
      "          total_loss: 9.988642692565918\n",
      "          vf_explained_var: -4.614553006376809e-08\n",
      "          vf_loss: 10.0\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 28000\n",
      "    num_agent_steps_trained: 28000\n",
      "    num_steps_sampled: 28000\n",
      "    num_steps_trained: 28000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.5625\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.15413435891086516\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2884180168962148\n",
      "    mean_inference_ms: 0.907876280220685\n",
      "    mean_raw_obs_processing_ms: 0.13002700681695256\n",
      "  time_since_restore: 39.00662708282471\n",
      "  time_this_iter_s: 5.544677734375\n",
      "  time_total_s: 39.00662708282471\n",
      "  timers:\n",
      "    learn_throughput: 1592.534\n",
      "    learn_time_ms: 2511.72\n",
      "    load_throughput: 13303184.413\n",
      "    load_time_ms: 0.301\n",
      "    sample_throughput: 760.393\n",
      "    sample_time_ms: 5260.44\n",
      "    update_time_ms: 2.363\n",
      "  timestamp: 1650548214\n",
      "  timesteps_since_restore: 28000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 7\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:36:54,904\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 37521.0x the scale of `vf_clip_param`. This means that it will take more than 37521.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:36:59 (running for 00:00:59.92)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         39.0066</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\"> -375207</td><td style=\"text-align: right;\">             -123330</td><td style=\"text-align: right;\">             -657388</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 32000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-37-00\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -102616.83272640771\n",
      "  episode_reward_mean: -360158.52236744825\n",
      "  episode_reward_min: -627049.6997108941\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 1280\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.278125047683716\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 5.821545124053955\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01644718274474144\n",
      "          model: {}\n",
      "          policy_loss: -0.05723543465137482\n",
      "          total_loss: 9.980233192443848\n",
      "          vf_explained_var: -6.921829509565214e-08\n",
      "          vf_loss: 10.0\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_steps_sampled: 32000\n",
      "    num_steps_trained: 32000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.55\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.15387525559067688\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.28851570751389727\n",
      "    mean_inference_ms: 0.9078540903919524\n",
      "    mean_raw_obs_processing_ms: 0.1299939567718973\n",
      "  time_since_restore: 44.497962474823\n",
      "  time_this_iter_s: 5.491335391998291\n",
      "  time_total_s: 44.497962474823\n",
      "  timers:\n",
      "    learn_throughput: 1596.103\n",
      "    learn_time_ms: 2506.104\n",
      "    load_throughput: 13441935.704\n",
      "    load_time_ms: 0.298\n",
      "    sample_throughput: 756.393\n",
      "    sample_time_ms: 5288.255\n",
      "    update_time_ms: 2.344\n",
      "  timestamp: 1650548220\n",
      "  timesteps_since_restore: 32000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 32000\n",
      "  training_iteration: 8\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:37:00,435\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 36016.0x the scale of `vf_clip_param`. This means that it will take more than 36016.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:37:05 (running for 00:01:05.50)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">          44.498</td><td style=\"text-align: right;\">32000</td><td style=\"text-align: right;\"> -360159</td><td style=\"text-align: right;\">             -102617</td><td style=\"text-align: right;\">             -627050</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 36000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-37-05\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -130415.07218163079\n",
      "  episode_reward_mean: -363721.8212213471\n",
      "  episode_reward_min: -726381.5196699386\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 1440\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.278125047683716\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 5.754332542419434\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016300927847623825\n",
      "          model: {}\n",
      "          policy_loss: -0.06210087984800339\n",
      "          total_loss: 9.975034713745117\n",
      "          vf_explained_var: -5.3836451741062774e-08\n",
      "          vf_loss: 10.0\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 36000\n",
      "    num_agent_steps_trained: 36000\n",
      "    num_steps_sampled: 36000\n",
      "    num_steps_trained: 36000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.425000000000001\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.15367699903843438\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2885155462965715\n",
      "    mean_inference_ms: 0.9081101100992199\n",
      "    mean_raw_obs_processing_ms: 0.1300138259263815\n",
      "  time_since_restore: 49.91052722930908\n",
      "  time_this_iter_s: 5.412564754486084\n",
      "  time_total_s: 49.91052722930908\n",
      "  timers:\n",
      "    learn_throughput: 1605.857\n",
      "    learn_time_ms: 2490.883\n",
      "    load_throughput: 13434909.156\n",
      "    load_time_ms: 0.298\n",
      "    sample_throughput: 752.244\n",
      "    sample_time_ms: 5317.422\n",
      "    update_time_ms: 2.362\n",
      "  timestamp: 1650548225\n",
      "  timesteps_since_restore: 36000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 9\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:37:05,889\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 36372.0x the scale of `vf_clip_param`. This means that it will take more than 36372.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:37:10 (running for 00:01:10.91)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         49.9105</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\"> -363722</td><td style=\"text-align: right;\">             -130415</td><td style=\"text-align: right;\">             -726382</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 40000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-37-11\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -118048.90585975713\n",
      "  episode_reward_mean: -361315.5261736548\n",
      "  episode_reward_min: -677445.034607697\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 1600\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.278125047683716\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 5.762753009796143\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01646474190056324\n",
      "          model: {}\n",
      "          policy_loss: -0.05528077110648155\n",
      "          total_loss: 9.98222827911377\n",
      "          vf_explained_var: -4.999099090241543e-08\n",
      "          vf_loss: 10.0\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_steps_sampled: 40000\n",
      "    num_steps_trained: 40000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.85\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.15356239572321617\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2886832317682059\n",
      "    mean_inference_ms: 0.9075689361093164\n",
      "    mean_raw_obs_processing_ms: 0.13006478510798886\n",
      "  time_since_restore: 55.42015528678894\n",
      "  time_this_iter_s: 5.509628057479858\n",
      "  time_total_s: 55.42015528678894\n",
      "  timers:\n",
      "    learn_throughput: 1608.163\n",
      "    learn_time_ms: 2487.31\n",
      "    load_throughput: 13557346.263\n",
      "    load_time_ms: 0.295\n",
      "    sample_throughput: 750.167\n",
      "    sample_time_ms: 5332.149\n",
      "    update_time_ms: 2.351\n",
      "  timestamp: 1650548231\n",
      "  timesteps_since_restore: 40000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 10\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:37:11,439\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 36132.0x the scale of `vf_clip_param`. This means that it will take more than 36132.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:37:16 (running for 00:01:16.51)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         55.4202</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\"> -361316</td><td style=\"text-align: right;\">             -118049</td><td style=\"text-align: right;\">             -677445</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 44000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-37-16\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -152777.58404617375\n",
      "  episode_reward_mean: -341521.60197646834\n",
      "  episode_reward_min: -584824.5988575702\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 1760\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.278125047683716\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 5.838249206542969\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01673905923962593\n",
      "          model: {}\n",
      "          policy_loss: -0.05667697265744209\n",
      "          total_loss: 9.981456756591797\n",
      "          vf_explained_var: -5.3836451741062774e-08\n",
      "          vf_loss: 10.0\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 44000\n",
      "    num_agent_steps_trained: 44000\n",
      "    num_steps_sampled: 44000\n",
      "    num_steps_trained: 44000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.35\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1533588199885096\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.28868476551590466\n",
      "    mean_inference_ms: 0.9074675365500188\n",
      "    mean_raw_obs_processing_ms: 0.1300119453167667\n",
      "  time_since_restore: 60.92633104324341\n",
      "  time_this_iter_s: 5.506175756454468\n",
      "  time_total_s: 60.92633104324341\n",
      "  timers:\n",
      "    learn_throughput: 1622.89\n",
      "    learn_time_ms: 2464.738\n",
      "    load_throughput: 13628932.575\n",
      "    load_time_ms: 0.293\n",
      "    sample_throughput: 716.977\n",
      "    sample_time_ms: 5578.976\n",
      "    update_time_ms: 2.353\n",
      "  timestamp: 1650548236\n",
      "  timesteps_since_restore: 44000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 44000\n",
      "  training_iteration: 11\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:37:16,985\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 34152.0x the scale of `vf_clip_param`. This means that it will take more than 34152.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:37:22 (running for 00:01:22.01)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         60.9263</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\"> -341522</td><td style=\"text-align: right;\">             -152778</td><td style=\"text-align: right;\">             -584825</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 48000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-37-22\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -81784.21298019952\n",
      "  episode_reward_mean: -318215.9359996281\n",
      "  episode_reward_min: -653945.3966076856\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 1920\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.278125047683716\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 5.753024578094482\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01732444390654564\n",
      "          model: {}\n",
      "          policy_loss: -0.05993831530213356\n",
      "          total_loss: 9.979527473449707\n",
      "          vf_explained_var: -5.3836451741062774e-08\n",
      "          vf_loss: 10.0\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 48000\n",
      "    num_agent_steps_trained: 48000\n",
      "    num_steps_sampled: 48000\n",
      "    num_steps_trained: 48000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.875\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.15340982429743122\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.28850196237430176\n",
      "    mean_inference_ms: 0.9076086800623218\n",
      "    mean_raw_obs_processing_ms: 0.1299600343714952\n",
      "  time_since_restore: 66.4393060207367\n",
      "  time_this_iter_s: 5.512974977493286\n",
      "  time_total_s: 66.4393060207367\n",
      "  timers:\n",
      "    learn_throughput: 1624.664\n",
      "    learn_time_ms: 2462.048\n",
      "    load_throughput: 13361911.437\n",
      "    load_time_ms: 0.299\n",
      "    sample_throughput: 720.288\n",
      "    sample_time_ms: 5553.333\n",
      "    update_time_ms: 2.343\n",
      "  timestamp: 1650548242\n",
      "  timesteps_since_restore: 48000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 48000\n",
      "  training_iteration: 12\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:37:22,537\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 31822.0x the scale of `vf_clip_param`. This means that it will take more than 31822.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:37:27 (running for 00:01:27.60)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         66.4393</td><td style=\"text-align: right;\">48000</td><td style=\"text-align: right;\"> -318216</td><td style=\"text-align: right;\">            -81784.2</td><td style=\"text-align: right;\">             -653945</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 52000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-37-28\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -129830.47611265248\n",
      "  episode_reward_mean: -323802.31799113145\n",
      "  episode_reward_min: -564227.6417711263\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 2080\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.278125047683716\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 5.762295246124268\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01730094850063324\n",
      "          model: {}\n",
      "          policy_loss: -0.06975915282964706\n",
      "          total_loss: 9.96965503692627\n",
      "          vf_explained_var: -3.268641890485924e-08\n",
      "          vf_loss: 10.0\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 52000\n",
      "    num_agent_steps_trained: 52000\n",
      "    num_steps_sampled: 52000\n",
      "    num_steps_trained: 52000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.5125\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.15321779579370048\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.28839339100807004\n",
      "    mean_inference_ms: 0.9076341216983504\n",
      "    mean_raw_obs_processing_ms: 0.12995564411715413\n",
      "  time_since_restore: 71.94478178024292\n",
      "  time_this_iter_s: 5.505475759506226\n",
      "  time_total_s: 71.94478178024292\n",
      "  timers:\n",
      "    learn_throughput: 1621.909\n",
      "    learn_time_ms: 2466.229\n",
      "    load_throughput: 13267865.56\n",
      "    load_time_ms: 0.301\n",
      "    sample_throughput: 720.479\n",
      "    sample_time_ms: 5551.865\n",
      "    update_time_ms: 2.349\n",
      "  timestamp: 1650548248\n",
      "  timesteps_since_restore: 52000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 52000\n",
      "  training_iteration: 13\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:37:28,083\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 32380.0x the scale of `vf_clip_param`. This means that it will take more than 32380.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:37:33 (running for 00:01:33.10)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         71.9448</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\"> -323802</td><td style=\"text-align: right;\">             -129830</td><td style=\"text-align: right;\">             -564228</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 56000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-37-33\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -92952.94257764882\n",
      "  episode_reward_mean: -281711.0692783653\n",
      "  episode_reward_min: -547459.6031885629\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 2240\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.278125047683716\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 5.81886625289917\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01786467246711254\n",
      "          model: {}\n",
      "          policy_loss: -0.07313121110200882\n",
      "          total_loss: 9.967567443847656\n",
      "          vf_explained_var: -3.845460838647341e-08\n",
      "          vf_loss: 10.0\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 56000\n",
      "    num_agent_steps_trained: 56000\n",
      "    num_steps_sampled: 56000\n",
      "    num_steps_trained: 56000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.6\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.15332956730623354\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2883684421870492\n",
      "    mean_inference_ms: 0.9088503974262533\n",
      "    mean_raw_obs_processing_ms: 0.12992347190603265\n",
      "  time_since_restore: 77.43999314308167\n",
      "  time_this_iter_s: 5.495211362838745\n",
      "  time_total_s: 77.43999314308167\n",
      "  timers:\n",
      "    learn_throughput: 1626.024\n",
      "    learn_time_ms: 2459.989\n",
      "    load_throughput: 13043003.965\n",
      "    load_time_ms: 0.307\n",
      "    sample_throughput: 719.443\n",
      "    sample_time_ms: 5559.86\n",
      "    update_time_ms: 2.348\n",
      "  timestamp: 1650548253\n",
      "  timesteps_since_restore: 56000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 56000\n",
      "  training_iteration: 14\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:37:33,617\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 28171.0x the scale of `vf_clip_param`. This means that it will take more than 28171.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:37:38 (running for 00:01:38.68)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">           77.44</td><td style=\"text-align: right;\">56000</td><td style=\"text-align: right;\"> -281711</td><td style=\"text-align: right;\">            -92952.9</td><td style=\"text-align: right;\">             -547460</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 60000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-37-39\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -80476.07735507553\n",
      "  episode_reward_mean: -281186.9705309614\n",
      "  episode_reward_min: -533468.375043798\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 2400\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.278125047683716\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 5.813822269439697\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017977023497223854\n",
      "          model: {}\n",
      "          policy_loss: -0.06451277434825897\n",
      "          total_loss: 9.9764404296875\n",
      "          vf_explained_var: -5.191372309809594e-08\n",
      "          vf_loss: 10.0\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 60000\n",
      "    num_agent_steps_trained: 60000\n",
      "    num_steps_sampled: 60000\n",
      "    num_steps_trained: 60000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.414285714285714\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1535859053454627\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.28833182770937055\n",
      "    mean_inference_ms: 0.9118107643800079\n",
      "    mean_raw_obs_processing_ms: 0.1298388037410268\n",
      "  time_since_restore: 82.97311067581177\n",
      "  time_this_iter_s: 5.5331175327301025\n",
      "  time_total_s: 82.97311067581177\n",
      "  timers:\n",
      "    learn_throughput: 1638.71\n",
      "    learn_time_ms: 2440.944\n",
      "    load_throughput: 13126684.923\n",
      "    load_time_ms: 0.305\n",
      "    sample_throughput: 719.772\n",
      "    sample_time_ms: 5557.316\n",
      "    update_time_ms: 2.376\n",
      "  timestamp: 1650548259\n",
      "  timesteps_since_restore: 60000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 60000\n",
      "  training_iteration: 15\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:37:39,190\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 28119.0x the scale of `vf_clip_param`. This means that it will take more than 28119.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:37:44 (running for 00:01:44.21)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         82.9731</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\"> -281187</td><td style=\"text-align: right;\">            -80476.1</td><td style=\"text-align: right;\">             -533468</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 64000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-37-44\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -100666.99335627622\n",
      "  episode_reward_mean: -317667.1277927323\n",
      "  episode_reward_min: -645820.2618662601\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 2560\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.278125047683716\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 5.8083577156066895\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01838693954050541\n",
      "          model: {}\n",
      "          policy_loss: -0.06370516866445541\n",
      "          total_loss: 9.978182792663574\n",
      "          vf_explained_var: -5.7681912579710115e-08\n",
      "          vf_loss: 10.0\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 64000\n",
      "    num_agent_steps_trained: 64000\n",
      "    num_steps_sampled: 64000\n",
      "    num_steps_trained: 64000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.525\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.15355241637293843\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2883378984272426\n",
      "    mean_inference_ms: 0.9103822080810599\n",
      "    mean_raw_obs_processing_ms: 0.12983756826645187\n",
      "  time_since_restore: 88.28047561645508\n",
      "  time_this_iter_s: 5.3073649406433105\n",
      "  time_total_s: 88.28047561645508\n",
      "  timers:\n",
      "    learn_throughput: 1648.443\n",
      "    learn_time_ms: 2426.531\n",
      "    load_throughput: 12998540.327\n",
      "    load_time_ms: 0.308\n",
      "    sample_throughput: 722.535\n",
      "    sample_time_ms: 5536.063\n",
      "    update_time_ms: 2.397\n",
      "  timestamp: 1650548264\n",
      "  timesteps_since_restore: 64000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 64000\n",
      "  training_iteration: 16\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:37:44,537\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 31767.0x the scale of `vf_clip_param`. This means that it will take more than 31767.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:37:49 (running for 00:01:49.60)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         88.2805</td><td style=\"text-align: right;\">64000</td><td style=\"text-align: right;\"> -317667</td><td style=\"text-align: right;\">             -100667</td><td style=\"text-align: right;\">             -645820</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 68000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-37-50\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -90319.17234187192\n",
      "  episode_reward_mean: -275150.0753032974\n",
      "  episode_reward_min: -671259.1998955493\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 2720\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.278125047683716\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 5.7719526290893555\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018250737339258194\n",
      "          model: {}\n",
      "          policy_loss: -0.07435929775238037\n",
      "          total_loss: 9.967218399047852\n",
      "          vf_explained_var: -3.268641890485924e-08\n",
      "          vf_loss: 10.0\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 68000\n",
      "    num_agent_steps_trained: 68000\n",
      "    num_steps_sampled: 68000\n",
      "    num_steps_trained: 68000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.3999999999999995\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.15345724087379103\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2883271025803534\n",
      "    mean_inference_ms: 0.9098114089009929\n",
      "    mean_raw_obs_processing_ms: 0.12978474983961868\n",
      "  time_since_restore: 93.79876661300659\n",
      "  time_this_iter_s: 5.518290996551514\n",
      "  time_total_s: 93.79876661300659\n",
      "  timers:\n",
      "    learn_throughput: 1644.337\n",
      "    learn_time_ms: 2432.592\n",
      "    load_throughput: 12972408.567\n",
      "    load_time_ms: 0.308\n",
      "    sample_throughput: 725.536\n",
      "    sample_time_ms: 5513.162\n",
      "    update_time_ms: 2.399\n",
      "  timestamp: 1650548270\n",
      "  timesteps_since_restore: 68000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 68000\n",
      "  training_iteration: 17\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:37:50,096\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 27515.0x the scale of `vf_clip_param`. This means that it will take more than 27515.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:37:55 (running for 00:01:55.12)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         93.7988</td><td style=\"text-align: right;\">68000</td><td style=\"text-align: right;\"> -275150</td><td style=\"text-align: right;\">            -90319.2</td><td style=\"text-align: right;\">             -671259</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 72000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-37-55\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -63688.40619199339\n",
      "  episode_reward_mean: -246473.75237074337\n",
      "  episode_reward_min: -550536.7259988076\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 2880\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.278125047683716\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 5.741990089416504\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019361106678843498\n",
      "          model: {}\n",
      "          policy_loss: -0.09035976976156235\n",
      "          total_loss: 9.953747749328613\n",
      "          vf_explained_var: -2.8840956289855058e-08\n",
      "          vf_loss: 10.0\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 72000\n",
      "    num_agent_steps_trained: 72000\n",
      "    num_steps_sampled: 72000\n",
      "    num_steps_trained: 72000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.75\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.15333301739369376\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2881801643953308\n",
      "    mean_inference_ms: 0.9108819298232811\n",
      "    mean_raw_obs_processing_ms: 0.12971047199334354\n",
      "  time_since_restore: 99.34817290306091\n",
      "  time_this_iter_s: 5.549406290054321\n",
      "  time_total_s: 99.34817290306091\n",
      "  timers:\n",
      "    learn_throughput: 1642.847\n",
      "    learn_time_ms: 2434.798\n",
      "    load_throughput: 12612551.496\n",
      "    load_time_ms: 0.317\n",
      "    sample_throughput: 724.276\n",
      "    sample_time_ms: 5522.754\n",
      "    update_time_ms: 2.427\n",
      "  timestamp: 1650548275\n",
      "  timesteps_since_restore: 72000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 72000\n",
      "  training_iteration: 18\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:37:55,685\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 24647.0x the scale of `vf_clip_param`. This means that it will take more than 24647.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:38:00 (running for 00:02:00.75)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">         99.3482</td><td style=\"text-align: right;\">72000</td><td style=\"text-align: right;\"> -246474</td><td style=\"text-align: right;\">            -63688.4</td><td style=\"text-align: right;\">             -550537</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 76000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-38-01\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -71716.79132049148\n",
      "  episode_reward_mean: -266981.34215408406\n",
      "  episode_reward_min: -533992.2140008694\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 3040\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.278125047683716\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 5.787504196166992\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0191948264837265\n",
      "          model: {}\n",
      "          policy_loss: -0.0814167857170105\n",
      "          total_loss: 9.962310791015625\n",
      "          vf_explained_var: -6.152737341835746e-08\n",
      "          vf_loss: 10.0\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 76000\n",
      "    num_agent_steps_trained: 76000\n",
      "    num_steps_sampled: 76000\n",
      "    num_steps_trained: 76000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.8374999999999995\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1535010254385283\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.28834392270246073\n",
      "    mean_inference_ms: 0.9117536439898387\n",
      "    mean_raw_obs_processing_ms: 0.12975997201537043\n",
      "  time_since_restore: 104.92607426643372\n",
      "  time_this_iter_s: 5.577901363372803\n",
      "  time_total_s: 104.92607426643372\n",
      "  timers:\n",
      "    learn_throughput: 1636.519\n",
      "    learn_time_ms: 2444.212\n",
      "    load_throughput: 12666829.747\n",
      "    load_time_ms: 0.316\n",
      "    sample_throughput: 723.065\n",
      "    sample_time_ms: 5532.008\n",
      "    update_time_ms: 2.435\n",
      "  timestamp: 1650548281\n",
      "  timesteps_since_restore: 76000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 76000\n",
      "  training_iteration: 19\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:38:01,303\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 26698.0x the scale of `vf_clip_param`. This means that it will take more than 26698.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:38:06 (running for 00:02:06.32)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         104.926</td><td style=\"text-align: right;\">76000</td><td style=\"text-align: right;\"> -266981</td><td style=\"text-align: right;\">            -71716.8</td><td style=\"text-align: right;\">             -533992</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 80000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-38-06\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -101348.79392521447\n",
      "  episode_reward_mean: -225772.26879910743\n",
      "  episode_reward_min: -580740.4016540532\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 3200\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.278125047683716\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 5.726658344268799\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02030693180859089\n",
      "          model: {}\n",
      "          policy_loss: -0.07684469223022461\n",
      "          total_loss: 9.969416618347168\n",
      "          vf_explained_var: -4.614553006376809e-08\n",
      "          vf_loss: 10.0\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 80000\n",
      "    num_agent_steps_trained: 80000\n",
      "    num_steps_sampled: 80000\n",
      "    num_steps_trained: 80000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.375\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1534380223410675\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.28833536287447303\n",
      "    mean_inference_ms: 0.9128383533861818\n",
      "    mean_raw_obs_processing_ms: 0.1297699106070093\n",
      "  time_since_restore: 110.50690770149231\n",
      "  time_this_iter_s: 5.580833435058594\n",
      "  time_total_s: 110.50690770149231\n",
      "  timers:\n",
      "    learn_throughput: 1634.154\n",
      "    learn_time_ms: 2447.75\n",
      "    load_throughput: 12543712.897\n",
      "    load_time_ms: 0.319\n",
      "    sample_throughput: 721.38\n",
      "    sample_time_ms: 5544.926\n",
      "    update_time_ms: 2.446\n",
      "  timestamp: 1650548286\n",
      "  timesteps_since_restore: 80000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 80000\n",
      "  training_iteration: 20\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:38:06,924\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 22577.0x the scale of `vf_clip_param`. This means that it will take more than 22577.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:38:12 (running for 00:02:11.98)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         110.507</td><td style=\"text-align: right;\">80000</td><td style=\"text-align: right;\"> -225772</td><td style=\"text-align: right;\">             -101349</td><td style=\"text-align: right;\">             -580740</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 84000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-38-12\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -82786.16624944276\n",
      "  episode_reward_mean: -225017.29983814945\n",
      "  episode_reward_min: -615950.9053730255\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 3360\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.417187452316284\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 5.763797760009766\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014309454709291458\n",
      "          model: {}\n",
      "          policy_loss: -0.07329795509576797\n",
      "          total_loss: 9.975600242614746\n",
      "          vf_explained_var: -2.3072765031884046e-08\n",
      "          vf_loss: 10.0\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 84000\n",
      "    num_agent_steps_trained: 84000\n",
      "    num_steps_sampled: 84000\n",
      "    num_steps_trained: 84000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 5.0625\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.15334730099951668\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.28831797104098\n",
      "    mean_inference_ms: 0.913317741483414\n",
      "    mean_raw_obs_processing_ms: 0.12975890154906794\n",
      "  time_since_restore: 116.05676746368408\n",
      "  time_this_iter_s: 5.5498597621917725\n",
      "  time_total_s: 116.05676746368408\n",
      "  timers:\n",
      "    learn_throughput: 1633.613\n",
      "    learn_time_ms: 2448.56\n",
      "    load_throughput: 12465425.366\n",
      "    load_time_ms: 0.321\n",
      "    sample_throughput: 720.459\n",
      "    sample_time_ms: 5552.013\n",
      "    update_time_ms: 2.442\n",
      "  timestamp: 1650548292\n",
      "  timesteps_since_restore: 84000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 84000\n",
      "  training_iteration: 21\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:38:12,513\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 22502.0x the scale of `vf_clip_param`. This means that it will take more than 22502.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:38:17 (running for 00:02:17.54)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">         116.057</td><td style=\"text-align: right;\">84000</td><td style=\"text-align: right;\"> -225017</td><td style=\"text-align: right;\">            -82786.2</td><td style=\"text-align: right;\">             -615951</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 88000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-38-18\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -68065.5349994189\n",
      "  episode_reward_mean: -213662.772427301\n",
      "  episode_reward_min: -565609.5291444545\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 3520\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.417187452316284\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 5.745495319366455\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01400835532695055\n",
      "          model: {}\n",
      "          policy_loss: -0.07238240540027618\n",
      "          total_loss: 9.975486755371094\n",
      "          vf_explained_var: -4.230006922512075e-08\n",
      "          vf_loss: 10.0\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 88000\n",
      "    num_agent_steps_trained: 88000\n",
      "    num_steps_sampled: 88000\n",
      "    num_steps_trained: 88000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.375\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.15357634346121177\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2884278635862635\n",
      "    mean_inference_ms: 0.9134941129683798\n",
      "    mean_raw_obs_processing_ms: 0.12975192764114732\n",
      "  time_since_restore: 121.70080733299255\n",
      "  time_this_iter_s: 5.644039869308472\n",
      "  time_total_s: 121.70080733299255\n",
      "  timers:\n",
      "    learn_throughput: 1631.662\n",
      "    learn_time_ms: 2451.488\n",
      "    load_throughput: 12554977.176\n",
      "    load_time_ms: 0.319\n",
      "    sample_throughput: 719.016\n",
      "    sample_time_ms: 5563.16\n",
      "    update_time_ms: 2.454\n",
      "  timestamp: 1650548298\n",
      "  timesteps_since_restore: 88000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 88000\n",
      "  training_iteration: 22\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:38:18,199\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 21366.0x the scale of `vf_clip_param`. This means that it will take more than 21366.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:38:23 (running for 00:02:23.26)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    22</td><td style=\"text-align: right;\">         121.701</td><td style=\"text-align: right;\">88000</td><td style=\"text-align: right;\"> -213663</td><td style=\"text-align: right;\">            -68065.5</td><td style=\"text-align: right;\">             -565610</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 92000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-38-23\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -72553.02488689487\n",
      "  episode_reward_mean: -203179.85221753636\n",
      "  episode_reward_min: -447225.7501438147\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 3680\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.417187452316284\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 5.728304386138916\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01450782734900713\n",
      "          model: {}\n",
      "          policy_loss: -0.07555817812681198\n",
      "          total_loss: 9.974017143249512\n",
      "          vf_explained_var: -4.422280142080126e-08\n",
      "          vf_loss: 10.0\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 92000\n",
      "    num_agent_steps_trained: 92000\n",
      "    num_steps_sampled: 92000\n",
      "    num_steps_trained: 92000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.625\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1536893423753019\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.28851223972548024\n",
      "    mean_inference_ms: 0.9147260207766668\n",
      "    mean_raw_obs_processing_ms: 0.1297828070570761\n",
      "  time_since_restore: 127.28022193908691\n",
      "  time_this_iter_s: 5.57941460609436\n",
      "  time_total_s: 127.28022193908691\n",
      "  timers:\n",
      "    learn_throughput: 1631.28\n",
      "    learn_time_ms: 2452.063\n",
      "    load_throughput: 12666829.747\n",
      "    load_time_ms: 0.316\n",
      "    sample_throughput: 717.731\n",
      "    sample_time_ms: 5573.118\n",
      "    update_time_ms: 2.424\n",
      "  timestamp: 1650548303\n",
      "  timesteps_since_restore: 92000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 92000\n",
      "  training_iteration: 23\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:38:23,819\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 20318.0x the scale of `vf_clip_param`. This means that it will take more than 20318.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:38:28 (running for 00:02:28.84)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    23</td><td style=\"text-align: right;\">          127.28</td><td style=\"text-align: right;\">92000</td><td style=\"text-align: right;\"> -203180</td><td style=\"text-align: right;\">              -72553</td><td style=\"text-align: right;\">             -447226</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 96000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-38-29\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -62954.72839884822\n",
      "  episode_reward_mean: -195418.6445940345\n",
      "  episode_reward_min: -426807.39289693895\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 3840\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.417187452316284\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 5.735327243804932\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014823161996901035\n",
      "          model: {}\n",
      "          policy_loss: -0.07546104490756989\n",
      "          total_loss: 9.97519302368164\n",
      "          vf_explained_var: -3.845460838647341e-08\n",
      "          vf_loss: 10.0\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 96000\n",
      "    num_agent_steps_trained: 96000\n",
      "    num_steps_sampled: 96000\n",
      "    num_steps_trained: 96000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.5625\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.15387593009854797\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2886369892394616\n",
      "    mean_inference_ms: 0.9168620626220271\n",
      "    mean_raw_obs_processing_ms: 0.12980153199491476\n",
      "  time_since_restore: 132.8931884765625\n",
      "  time_this_iter_s: 5.612966537475586\n",
      "  time_total_s: 132.8931884765625\n",
      "  timers:\n",
      "    learn_throughput: 1628.161\n",
      "    learn_time_ms: 2456.76\n",
      "    load_throughput: 12824656.78\n",
      "    load_time_ms: 0.312\n",
      "    sample_throughput: 716.737\n",
      "    sample_time_ms: 5580.849\n",
      "    update_time_ms: 2.405\n",
      "  timestamp: 1650548309\n",
      "  timesteps_since_restore: 96000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 96000\n",
      "  training_iteration: 24\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:38:29,473\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 19542.0x the scale of `vf_clip_param`. This means that it will take more than 19542.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:38:34 (running for 00:02:34.53)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    24</td><td style=\"text-align: right;\">         132.893</td><td style=\"text-align: right;\">96000</td><td style=\"text-align: right;\"> -195419</td><td style=\"text-align: right;\">            -62954.7</td><td style=\"text-align: right;\">             -426807</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 100000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-38-35\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -37202.20152180259\n",
      "  episode_reward_mean: -183065.33213508275\n",
      "  episode_reward_min: -482092.4230411536\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 4000\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.417187452316284\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 5.756886959075928\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014634904451668262\n",
      "          model: {}\n",
      "          policy_loss: -0.08414419740438461\n",
      "          total_loss: 9.965866088867188\n",
      "          vf_explained_var: -2.1150034612560376e-08\n",
      "          vf_loss: 10.0\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 100000\n",
      "    num_agent_steps_trained: 100000\n",
      "    num_steps_sampled: 100000\n",
      "    num_steps_trained: 100000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.574999999999999\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1539398304441138\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2887048272332588\n",
      "    mean_inference_ms: 0.9182961315482248\n",
      "    mean_raw_obs_processing_ms: 0.1298270939145121\n",
      "  time_since_restore: 138.5130295753479\n",
      "  time_this_iter_s: 5.6198410987854\n",
      "  time_total_s: 138.5130295753479\n",
      "  timers:\n",
      "    learn_throughput: 1624.021\n",
      "    learn_time_ms: 2463.023\n",
      "    load_throughput: 12693664.22\n",
      "    load_time_ms: 0.315\n",
      "    sample_throughput: 715.816\n",
      "    sample_time_ms: 5588.029\n",
      "    update_time_ms: 2.393\n",
      "  timestamp: 1650548315\n",
      "  timesteps_since_restore: 100000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 100000\n",
      "  training_iteration: 25\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:38:35,133\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 18307.0x the scale of `vf_clip_param`. This means that it will take more than 18307.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:38:40 (running for 00:02:40.16)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         138.513</td><td style=\"text-align: right;\">100000</td><td style=\"text-align: right;\"> -183065</td><td style=\"text-align: right;\">            -37202.2</td><td style=\"text-align: right;\">             -482092</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 104000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-38-40\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -66123.27788786952\n",
      "  episode_reward_mean: -179242.9459153413\n",
      "  episode_reward_min: -536637.8362377173\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 4160\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.417187452316284\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 5.796573638916016\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014753267168998718\n",
      "          model: {}\n",
      "          policy_loss: -0.08237267285585403\n",
      "          total_loss: 9.968042373657227\n",
      "          vf_explained_var: -4.422280142080126e-08\n",
      "          vf_loss: 10.0\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 104000\n",
      "    num_agent_steps_trained: 104000\n",
      "    num_steps_sampled: 104000\n",
      "    num_steps_trained: 104000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.5375\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1540767493398095\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.28875896269141593\n",
      "    mean_inference_ms: 0.9198156114436188\n",
      "    mean_raw_obs_processing_ms: 0.1298266097569988\n",
      "  time_since_restore: 144.0986647605896\n",
      "  time_this_iter_s: 5.585635185241699\n",
      "  time_total_s: 144.0986647605896\n",
      "  timers:\n",
      "    learn_throughput: 1614.18\n",
      "    learn_time_ms: 2478.039\n",
      "    load_throughput: 12631543.442\n",
      "    load_time_ms: 0.317\n",
      "    sample_throughput: 713.361\n",
      "    sample_time_ms: 5607.26\n",
      "    update_time_ms: 2.37\n",
      "  timestamp: 1650548320\n",
      "  timesteps_since_restore: 104000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 104000\n",
      "  training_iteration: 26\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:38:40,760\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 17924.0x the scale of `vf_clip_param`. This means that it will take more than 17924.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:38:45 (running for 00:02:45.82)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    26</td><td style=\"text-align: right;\">         144.099</td><td style=\"text-align: right;\">104000</td><td style=\"text-align: right;\"> -179243</td><td style=\"text-align: right;\">            -66123.3</td><td style=\"text-align: right;\">             -536638</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 108000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-38-46\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -58495.074396777796\n",
      "  episode_reward_mean: -177164.40825586757\n",
      "  episode_reward_min: -442188.8170898444\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 4320\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.417187452316284\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 5.729790210723877\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015084764920175076\n",
      "          model: {}\n",
      "          policy_loss: -0.08894567936658859\n",
      "          total_loss: 9.962600708007812\n",
      "          vf_explained_var: -5.7681912579710115e-08\n",
      "          vf_loss: 10.0\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 108000\n",
      "    num_agent_steps_trained: 108000\n",
      "    num_steps_sampled: 108000\n",
      "    num_steps_trained: 108000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.5625\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.15407582680006093\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2886967752507915\n",
      "    mean_inference_ms: 0.9205664412378965\n",
      "    mean_raw_obs_processing_ms: 0.1297748737190744\n",
      "  time_since_restore: 149.67048120498657\n",
      "  time_this_iter_s: 5.571816444396973\n",
      "  time_total_s: 149.67048120498657\n",
      "  timers:\n",
      "    learn_throughput: 1614.272\n",
      "    learn_time_ms: 2477.898\n",
      "    load_throughput: 12667786.167\n",
      "    load_time_ms: 0.316\n",
      "    sample_throughput: 710.761\n",
      "    sample_time_ms: 5627.771\n",
      "    update_time_ms: 2.369\n",
      "  timestamp: 1650548326\n",
      "  timesteps_since_restore: 108000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 108000\n",
      "  training_iteration: 27\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:38:46,372\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 17716.0x the scale of `vf_clip_param`. This means that it will take more than 17716.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:38:51 (running for 00:02:51.40)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    27</td><td style=\"text-align: right;\">          149.67</td><td style=\"text-align: right;\">108000</td><td style=\"text-align: right;\"> -177164</td><td style=\"text-align: right;\">            -58495.1</td><td style=\"text-align: right;\">             -442189</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 112000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-38-51\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -69600.18401985233\n",
      "  episode_reward_mean: -164031.73823045634\n",
      "  episode_reward_min: -656461.4232608562\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 4480\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.417187452316284\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 5.730015277862549\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014017540030181408\n",
      "          model: {}\n",
      "          policy_loss: -0.07203990966081619\n",
      "          total_loss: 9.975860595703125\n",
      "          vf_explained_var: -4.614553006376809e-08\n",
      "          vf_loss: 10.0\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 112000\n",
      "    num_agent_steps_trained: 112000\n",
      "    num_steps_sampled: 112000\n",
      "    num_steps_trained: 112000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.7375\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.15412475830278125\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.28864959016812425\n",
      "    mean_inference_ms: 0.9203532923907225\n",
      "    mean_raw_obs_processing_ms: 0.1297533821841227\n",
      "  time_since_restore: 155.18702721595764\n",
      "  time_this_iter_s: 5.516546010971069\n",
      "  time_total_s: 155.18702721595764\n",
      "  timers:\n",
      "    learn_throughput: 1615.977\n",
      "    learn_time_ms: 2475.283\n",
      "    load_throughput: 12941388.46\n",
      "    load_time_ms: 0.309\n",
      "    sample_throughput: 710.835\n",
      "    sample_time_ms: 5627.181\n",
      "    update_time_ms: 2.357\n",
      "  timestamp: 1650548331\n",
      "  timesteps_since_restore: 112000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 112000\n",
      "  training_iteration: 28\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:38:51,931\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 16403.0x the scale of `vf_clip_param`. This means that it will take more than 16403.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:38:57 (running for 00:02:56.99)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    28</td><td style=\"text-align: right;\">         155.187</td><td style=\"text-align: right;\">112000</td><td style=\"text-align: right;\"> -164032</td><td style=\"text-align: right;\">            -69600.2</td><td style=\"text-align: right;\">             -656461</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 116000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-38-57\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -34860.21489398544\n",
      "  episode_reward_mean: -149683.03757537965\n",
      "  episode_reward_min: -429077.696224857\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 4640\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.417187452316284\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 5.682192802429199\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015465385280549526\n",
      "          model: {}\n",
      "          policy_loss: -0.07528166472911835\n",
      "          total_loss: 9.977566719055176\n",
      "          vf_explained_var: -4.230006922512075e-08\n",
      "          vf_loss: 10.0\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 116000\n",
      "    num_agent_steps_trained: 116000\n",
      "    num_steps_sampled: 116000\n",
      "    num_steps_trained: 116000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 7.025\n",
      "    ram_util_percent: 2.4124999999999996\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.15406752575660757\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2886432300047456\n",
      "    mean_inference_ms: 0.9207780483547928\n",
      "    mean_raw_obs_processing_ms: 0.12975478899466375\n",
      "  time_since_restore: 160.7281301021576\n",
      "  time_this_iter_s: 5.541102886199951\n",
      "  time_total_s: 160.7281301021576\n",
      "  timers:\n",
      "    learn_throughput: 1618.155\n",
      "    learn_time_ms: 2471.951\n",
      "    load_throughput: 12769992.388\n",
      "    load_time_ms: 0.313\n",
      "    sample_throughput: 711.167\n",
      "    sample_time_ms: 5624.556\n",
      "    update_time_ms: 2.321\n",
      "  timestamp: 1650548337\n",
      "  timesteps_since_restore: 116000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 116000\n",
      "  training_iteration: 29\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:38:57,514\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 14968.0x the scale of `vf_clip_param`. This means that it will take more than 14968.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:39:02 (running for 00:03:02.54)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    29</td><td style=\"text-align: right;\">         160.728</td><td style=\"text-align: right;\">116000</td><td style=\"text-align: right;\"> -149683</td><td style=\"text-align: right;\">            -34860.2</td><td style=\"text-align: right;\">             -429078</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 120000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-39-03\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -38903.42382125918\n",
      "  episode_reward_mean: -137484.63591683423\n",
      "  episode_reward_min: -365859.8382674939\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 4800\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.417187452316284\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 5.557919502258301\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015005439519882202\n",
      "          model: {}\n",
      "          policy_loss: -0.09219588339328766\n",
      "          total_loss: 9.959080696105957\n",
      "          vf_explained_var: -5.7681912579710115e-08\n",
      "          vf_loss: 10.0\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 120000\n",
      "    num_agent_steps_trained: 120000\n",
      "    num_steps_sampled: 120000\n",
      "    num_steps_trained: 120000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 30\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.6625\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.15412722247272442\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.28888775236187264\n",
      "    mean_inference_ms: 0.9213190961187276\n",
      "    mean_raw_obs_processing_ms: 0.12983779478398158\n",
      "  time_since_restore: 166.23324036598206\n",
      "  time_this_iter_s: 5.505110263824463\n",
      "  time_total_s: 166.23324036598206\n",
      "  timers:\n",
      "    learn_throughput: 1624.691\n",
      "    learn_time_ms: 2462.006\n",
      "    load_throughput: 12787512.195\n",
      "    load_time_ms: 0.313\n",
      "    sample_throughput: 711.259\n",
      "    sample_time_ms: 5623.832\n",
      "    update_time_ms: 2.326\n",
      "  timestamp: 1650548343\n",
      "  timesteps_since_restore: 120000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 120000\n",
      "  training_iteration: 30\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:39:03,063\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 13748.0x the scale of `vf_clip_param`. This means that it will take more than 13748.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:39:08 (running for 00:03:08.12)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    30</td><td style=\"text-align: right;\">         166.233</td><td style=\"text-align: right;\">120000</td><td style=\"text-align: right;\"> -137485</td><td style=\"text-align: right;\">            -38903.4</td><td style=\"text-align: right;\">             -365860</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 124000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-39-08\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -42774.72068576876\n",
      "  episode_reward_mean: -135475.45812333963\n",
      "  episode_reward_min: -318467.33878712717\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 4960\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.417187452316284\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 5.546677589416504\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015881838276982307\n",
      "          model: {}\n",
      "          policy_loss: -0.08745338767766953\n",
      "          total_loss: 9.966817855834961\n",
      "          vf_explained_var: -4.80682622594486e-08\n",
      "          vf_loss: 10.0\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 124000\n",
      "    num_agent_steps_trained: 124000\n",
      "    num_steps_sampled: 124000\n",
      "    num_steps_trained: 124000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 31\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.55\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1540322705308529\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2889107166591208\n",
      "    mean_inference_ms: 0.9208861071206575\n",
      "    mean_raw_obs_processing_ms: 0.12983225793239078\n",
      "  time_since_restore: 171.68375253677368\n",
      "  time_this_iter_s: 5.450512170791626\n",
      "  time_total_s: 171.68375253677368\n",
      "  timers:\n",
      "    learn_throughput: 1629.724\n",
      "    learn_time_ms: 2454.403\n",
      "    load_throughput: 12795314.216\n",
      "    load_time_ms: 0.313\n",
      "    sample_throughput: 712.776\n",
      "    sample_time_ms: 5611.863\n",
      "    update_time_ms: 2.341\n",
      "  timestamp: 1650548348\n",
      "  timesteps_since_restore: 124000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 124000\n",
      "  training_iteration: 31\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:39:08,555\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 13548.0x the scale of `vf_clip_param`. This means that it will take more than 13548.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:39:13 (running for 00:03:13.58)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    31</td><td style=\"text-align: right;\">         171.684</td><td style=\"text-align: right;\">124000</td><td style=\"text-align: right;\"> -135475</td><td style=\"text-align: right;\">            -42774.7</td><td style=\"text-align: right;\">             -318467</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 128000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-39-14\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -44442.6177545077\n",
      "  episode_reward_mean: -139894.38246152241\n",
      "  episode_reward_min: -379601.37172071997\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 5120\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.417187452316284\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 5.536262035369873\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015870552510023117\n",
      "          model: {}\n",
      "          policy_loss: -0.09123126417398453\n",
      "          total_loss: 9.96300220489502\n",
      "          vf_explained_var: -4.614553006376809e-08\n",
      "          vf_loss: 10.0\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 128000\n",
      "    num_agent_steps_trained: 128000\n",
      "    num_steps_sampled: 128000\n",
      "    num_steps_trained: 128000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 32\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.6125\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1539185491160071\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.28882358627794974\n",
      "    mean_inference_ms: 0.9205813977932964\n",
      "    mean_raw_obs_processing_ms: 0.12979637866664592\n",
      "  time_since_restore: 177.1232500076294\n",
      "  time_this_iter_s: 5.439497470855713\n",
      "  time_total_s: 177.1232500076294\n",
      "  timers:\n",
      "    learn_throughput: 1636.511\n",
      "    learn_time_ms: 2444.224\n",
      "    load_throughput: 12931413.596\n",
      "    load_time_ms: 0.309\n",
      "    sample_throughput: 715.072\n",
      "    sample_time_ms: 5593.846\n",
      "    update_time_ms: 2.381\n",
      "  timestamp: 1650548354\n",
      "  timesteps_since_restore: 128000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 128000\n",
      "  training_iteration: 32\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:39:14,036\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 13989.0x the scale of `vf_clip_param`. This means that it will take more than 13989.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:39:19 (running for 00:03:19.10)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">         177.123</td><td style=\"text-align: right;\">128000</td><td style=\"text-align: right;\"> -139894</td><td style=\"text-align: right;\">            -44442.6</td><td style=\"text-align: right;\">             -379601</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 132000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-39-19\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -32960.69886057441\n",
      "  episode_reward_mean: -122002.97832088909\n",
      "  episode_reward_min: -415192.264417339\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 5280\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.417187452316284\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 5.507148742675781\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015389497391879559\n",
      "          model: {}\n",
      "          policy_loss: -0.09490054100751877\n",
      "          total_loss: 9.957688331604004\n",
      "          vf_explained_var: -3.845460838647341e-08\n",
      "          vf_loss: 10.0\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 132000\n",
      "    num_agent_steps_trained: 132000\n",
      "    num_steps_sampled: 132000\n",
      "    num_steps_trained: 132000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 33\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.285714285714286\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.15388079331561677\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.28881508155934077\n",
      "    mean_inference_ms: 0.9202209162485602\n",
      "    mean_raw_obs_processing_ms: 0.1298153346777775\n",
      "  time_since_restore: 182.49903106689453\n",
      "  time_this_iter_s: 5.375781059265137\n",
      "  time_total_s: 182.49903106689453\n",
      "  timers:\n",
      "    learn_throughput: 1646.349\n",
      "    learn_time_ms: 2429.619\n",
      "    load_throughput: 12808012.825\n",
      "    load_time_ms: 0.312\n",
      "    sample_throughput: 717.129\n",
      "    sample_time_ms: 5577.8\n",
      "    update_time_ms: 2.39\n",
      "  timestamp: 1650548359\n",
      "  timesteps_since_restore: 132000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 132000\n",
      "  training_iteration: 33\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:39:19,451\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 12200.0x the scale of `vf_clip_param`. This means that it will take more than 12200.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:39:24 (running for 00:03:24.47)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    33</td><td style=\"text-align: right;\">         182.499</td><td style=\"text-align: right;\">132000</td><td style=\"text-align: right;\"> -122003</td><td style=\"text-align: right;\">            -32960.7</td><td style=\"text-align: right;\">             -415192</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 136000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-39-25\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -45714.73790960376\n",
      "  episode_reward_mean: -118988.62749216621\n",
      "  episode_reward_min: -432948.67127626005\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 5440\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.417187452316284\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 5.37094783782959\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015069661661982536\n",
      "          model: {}\n",
      "          policy_loss: -0.08585506677627563\n",
      "          total_loss: 9.965641975402832\n",
      "          vf_explained_var: -6.729556645268531e-08\n",
      "          vf_loss: 10.0\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 136000\n",
      "    num_agent_steps_trained: 136000\n",
      "    num_steps_sampled: 136000\n",
      "    num_steps_trained: 136000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 34\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.6125\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.15390860319323396\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.28900482313041936\n",
      "    mean_inference_ms: 0.9202966654862191\n",
      "    mean_raw_obs_processing_ms: 0.12992334913498174\n",
      "  time_since_restore: 188.11286544799805\n",
      "  time_this_iter_s: 5.613834381103516\n",
      "  time_total_s: 188.11286544799805\n",
      "  timers:\n",
      "    learn_throughput: 1644.189\n",
      "    learn_time_ms: 2432.81\n",
      "    load_throughput: 12671613.293\n",
      "    load_time_ms: 0.316\n",
      "    sample_throughput: 719.428\n",
      "    sample_time_ms: 5559.97\n",
      "    update_time_ms: 2.399\n",
      "  timestamp: 1650548365\n",
      "  timesteps_since_restore: 136000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 136000\n",
      "  training_iteration: 34\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:39:25,104\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 11899.0x the scale of `vf_clip_param`. This means that it will take more than 11899.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:39:30 (running for 00:03:30.16)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    34</td><td style=\"text-align: right;\">         188.113</td><td style=\"text-align: right;\">136000</td><td style=\"text-align: right;\"> -118989</td><td style=\"text-align: right;\">            -45714.7</td><td style=\"text-align: right;\">             -432949</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 140000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-39-30\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -46040.01961915557\n",
      "  episode_reward_mean: -120835.81465806579\n",
      "  episode_reward_min: -739095.168530274\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 5600\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.417187452316284\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 5.421750545501709\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013545087538659573\n",
      "          model: {}\n",
      "          policy_loss: -0.07509177923202515\n",
      "          total_loss: 9.97119426727295\n",
      "          vf_explained_var: -8.844560284160252e-08\n",
      "          vf_loss: 10.0\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 140000\n",
      "    num_agent_steps_trained: 140000\n",
      "    num_steps_sampled: 140000\n",
      "    num_steps_trained: 140000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 35\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.4625\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1539465011922777\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2890661667708684\n",
      "    mean_inference_ms: 0.9203857306836273\n",
      "    mean_raw_obs_processing_ms: 0.1299214212215399\n",
      "  time_since_restore: 193.61709833145142\n",
      "  time_this_iter_s: 5.504232883453369\n",
      "  time_total_s: 193.61709833145142\n",
      "  timers:\n",
      "    learn_throughput: 1648.318\n",
      "    learn_time_ms: 2426.716\n",
      "    load_throughput: 12775826.988\n",
      "    load_time_ms: 0.313\n",
      "    sample_throughput: 719.734\n",
      "    sample_time_ms: 5557.609\n",
      "    update_time_ms: 2.414\n",
      "  timestamp: 1650548370\n",
      "  timesteps_since_restore: 140000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 140000\n",
      "  training_iteration: 35\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:39:30,649\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 12084.0x the scale of `vf_clip_param`. This means that it will take more than 12084.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:39:35 (running for 00:03:35.67)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    35</td><td style=\"text-align: right;\">         193.617</td><td style=\"text-align: right;\">140000</td><td style=\"text-align: right;\"> -120836</td><td style=\"text-align: right;\">              -46040</td><td style=\"text-align: right;\">             -739095</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 144000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-39-36\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -38050.42864494388\n",
      "  episode_reward_mean: -108036.74648460737\n",
      "  episode_reward_min: -351694.24888293806\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 5760\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.417187452316284\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 5.383917331695557\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016272613778710365\n",
      "          model: {}\n",
      "          policy_loss: -0.09507376700639725\n",
      "          total_loss: 9.960532188415527\n",
      "          vf_explained_var: -7.498648812997999e-08\n",
      "          vf_loss: 10.0\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 144000\n",
      "    num_agent_steps_trained: 144000\n",
      "    num_steps_sampled: 144000\n",
      "    num_steps_trained: 144000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 36\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.575\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.15393626638419816\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.28909978412131154\n",
      "    mean_inference_ms: 0.9208461615484996\n",
      "    mean_raw_obs_processing_ms: 0.12993834250201877\n",
      "  time_since_restore: 199.0738399028778\n",
      "  time_this_iter_s: 5.456741571426392\n",
      "  time_total_s: 199.0738399028778\n",
      "  timers:\n",
      "    learn_throughput: 1655.027\n",
      "    learn_time_ms: 2416.879\n",
      "    load_throughput: 12916480.099\n",
      "    load_time_ms: 0.31\n",
      "    sample_throughput: 720.923\n",
      "    sample_time_ms: 5548.44\n",
      "    update_time_ms: 2.458\n",
      "  timestamp: 1650548376\n",
      "  timesteps_since_restore: 144000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 144000\n",
      "  training_iteration: 36\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:39:36,147\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 10804.0x the scale of `vf_clip_param`. This means that it will take more than 10804.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:39:41 (running for 00:03:41.21)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    36</td><td style=\"text-align: right;\">         199.074</td><td style=\"text-align: right;\">144000</td><td style=\"text-align: right;\"> -108037</td><td style=\"text-align: right;\">            -38050.4</td><td style=\"text-align: right;\">             -351694</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 148000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-39-41\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -46290.990778375315\n",
      "  episode_reward_mean: -112282.35232621944\n",
      "  episode_reward_min: -436074.73184471193\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 5920\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.417187452316284\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 5.333505153656006\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01615605130791664\n",
      "          model: {}\n",
      "          policy_loss: -0.08766152709722519\n",
      "          total_loss: 9.967546463012695\n",
      "          vf_explained_var: -6.152737341835746e-08\n",
      "          vf_loss: 10.0\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 148000\n",
      "    num_agent_steps_trained: 148000\n",
      "    num_steps_sampled: 148000\n",
      "    num_steps_trained: 148000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 37\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.65\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1539853864053709\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2891511199032976\n",
      "    mean_inference_ms: 0.9217539876975316\n",
      "    mean_raw_obs_processing_ms: 0.1299513560517656\n",
      "  time_since_restore: 204.51037573814392\n",
      "  time_this_iter_s: 5.436535835266113\n",
      "  time_total_s: 204.51037573814392\n",
      "  timers:\n",
      "    learn_throughput: 1667.785\n",
      "    learn_time_ms: 2398.391\n",
      "    load_throughput: 13025788.82\n",
      "    load_time_ms: 0.307\n",
      "    sample_throughput: 721.563\n",
      "    sample_time_ms: 5543.521\n",
      "    update_time_ms: 2.455\n",
      "  timestamp: 1650548381\n",
      "  timesteps_since_restore: 148000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 148000\n",
      "  training_iteration: 37\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:39:41,623\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 11228.0x the scale of `vf_clip_param`. This means that it will take more than 11228.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:39:46 (running for 00:03:46.65)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    37</td><td style=\"text-align: right;\">          204.51</td><td style=\"text-align: right;\">148000</td><td style=\"text-align: right;\"> -112282</td><td style=\"text-align: right;\">              -46291</td><td style=\"text-align: right;\">             -436075</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 152000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-39-47\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -39584.56232194964\n",
      "  episode_reward_mean: -110100.28903385093\n",
      "  episode_reward_min: -446170.50266997877\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 6080\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.417187452316284\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 5.322446346282959\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014447622001171112\n",
      "          model: {}\n",
      "          policy_loss: -0.08534391969442368\n",
      "          total_loss: 9.96402645111084\n",
      "          vf_explained_var: -7.114103084404633e-08\n",
      "          vf_loss: 10.0\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 152000\n",
      "    num_agent_steps_trained: 152000\n",
      "    num_steps_sampled: 152000\n",
      "    num_steps_trained: 152000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 38\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.3\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.15399563656193832\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2892201556116168\n",
      "    mean_inference_ms: 0.9213026064847408\n",
      "    mean_raw_obs_processing_ms: 0.12996084722938858\n",
      "  time_since_restore: 209.96704173088074\n",
      "  time_this_iter_s: 5.456665992736816\n",
      "  time_total_s: 209.96704173088074\n",
      "  timers:\n",
      "    learn_throughput: 1671.436\n",
      "    learn_time_ms: 2393.152\n",
      "    load_throughput: 13082669.994\n",
      "    load_time_ms: 0.306\n",
      "    sample_throughput: 724.079\n",
      "    sample_time_ms: 5524.262\n",
      "    update_time_ms: 2.431\n",
      "  timestamp: 1650548387\n",
      "  timesteps_since_restore: 152000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 152000\n",
      "  training_iteration: 38\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:39:47,121\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 11010.0x the scale of `vf_clip_param`. This means that it will take more than 11010.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:39:52 (running for 00:03:52.18)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    38</td><td style=\"text-align: right;\">         209.967</td><td style=\"text-align: right;\">152000</td><td style=\"text-align: right;\"> -110100</td><td style=\"text-align: right;\">            -39584.6</td><td style=\"text-align: right;\">             -446171</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 156000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-39-52\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -32955.61952798431\n",
      "  episode_reward_mean: -112575.49510158812\n",
      "  episode_reward_min: -430574.65363113943\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 6240\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.417187452316284\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 5.290125370025635\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015034439042210579\n",
      "          model: {}\n",
      "          policy_loss: -0.11033423990011215\n",
      "          total_loss: 9.941040992736816\n",
      "          vf_explained_var: -4.999099090241543e-08\n",
      "          vf_loss: 10.0\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 156000\n",
      "    num_agent_steps_trained: 156000\n",
      "    num_steps_sampled: 156000\n",
      "    num_steps_trained: 156000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 39\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.775\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.15396429094583142\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2892210679754774\n",
      "    mean_inference_ms: 0.9209362491992994\n",
      "    mean_raw_obs_processing_ms: 0.12995880051540465\n",
      "  time_since_restore: 215.45318579673767\n",
      "  time_this_iter_s: 5.486144065856934\n",
      "  time_total_s: 215.45318579673767\n",
      "  timers:\n",
      "    learn_throughput: 1670.46\n",
      "    learn_time_ms: 2394.55\n",
      "    load_throughput: 13259476.804\n",
      "    load_time_ms: 0.302\n",
      "    sample_throughput: 725.723\n",
      "    sample_time_ms: 5511.746\n",
      "    update_time_ms: 2.428\n",
      "  timestamp: 1650548392\n",
      "  timesteps_since_restore: 156000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 156000\n",
      "  training_iteration: 39\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:39:52,646\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 11258.0x the scale of `vf_clip_param`. This means that it will take more than 11258.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:39:57 (running for 00:03:57.67)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    39</td><td style=\"text-align: right;\">         215.453</td><td style=\"text-align: right;\">156000</td><td style=\"text-align: right;\"> -112575</td><td style=\"text-align: right;\">            -32955.6</td><td style=\"text-align: right;\">             -430575</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 160000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-39-58\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -20830.979667235064\n",
      "  episode_reward_mean: -94395.76987224584\n",
      "  episode_reward_min: -280843.14198987547\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 6400\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.417187452316284\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 5.165038108825684\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016827523708343506\n",
      "          model: {}\n",
      "          policy_loss: -0.09406569600105286\n",
      "          total_loss: 9.9634370803833\n",
      "          vf_explained_var: -7.114103084404633e-08\n",
      "          vf_loss: 10.0\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 160000\n",
      "    num_agent_steps_trained: 160000\n",
      "    num_steps_sampled: 160000\n",
      "    num_steps_trained: 160000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 40\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.5375\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.15391563603589567\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2891923562864759\n",
      "    mean_inference_ms: 0.9211584892310618\n",
      "    mean_raw_obs_processing_ms: 0.12993529144491955\n",
      "  time_since_restore: 221.0345585346222\n",
      "  time_this_iter_s: 5.5813727378845215\n",
      "  time_total_s: 221.0345585346222\n",
      "  timers:\n",
      "    learn_throughput: 1665.989\n",
      "    learn_time_ms: 2400.977\n",
      "    load_throughput: 13340661.578\n",
      "    load_time_ms: 0.3\n",
      "    sample_throughput: 725.421\n",
      "    sample_time_ms: 5514.039\n",
      "    update_time_ms: 2.451\n",
      "  timestamp: 1650548398\n",
      "  timesteps_since_restore: 160000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 160000\n",
      "  training_iteration: 40\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:39:58,268\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 9440.0x the scale of `vf_clip_param`. This means that it will take more than 9440.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:40:03 (running for 00:04:03.33)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">         221.035</td><td style=\"text-align: right;\">160000</td><td style=\"text-align: right;\">-94395.8</td><td style=\"text-align: right;\">              -20831</td><td style=\"text-align: right;\">             -280843</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 164000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-40-03\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -33930.409614492106\n",
      "  episode_reward_mean: -93972.05763779122\n",
      "  episode_reward_min: -412807.4466506726\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 6560\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.417187452316284\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 5.087296485900879\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015484688803553581\n",
      "          model: {}\n",
      "          policy_loss: -0.09149127453565598\n",
      "          total_loss: 9.961262702941895\n",
      "          vf_explained_var: -4.480827192310244e-05\n",
      "          vf_loss: 9.99984073638916\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 164000\n",
      "    num_agent_steps_trained: 164000\n",
      "    num_steps_sampled: 164000\n",
      "    num_steps_trained: 164000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 41\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.6\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.15391928369165261\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.28921538865490226\n",
      "    mean_inference_ms: 0.9217726553511753\n",
      "    mean_raw_obs_processing_ms: 0.12994314978450416\n",
      "  time_since_restore: 226.63613605499268\n",
      "  time_this_iter_s: 5.601577520370483\n",
      "  time_total_s: 226.63613605499268\n",
      "  timers:\n",
      "    learn_throughput: 1662.103\n",
      "    learn_time_ms: 2406.59\n",
      "    load_throughput: 13313137.597\n",
      "    load_time_ms: 0.3\n",
      "    sample_throughput: 723.339\n",
      "    sample_time_ms: 5529.912\n",
      "    update_time_ms: 2.446\n",
      "  timestamp: 1650548403\n",
      "  timesteps_since_restore: 164000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 164000\n",
      "  training_iteration: 41\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:40:03,911\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 9397.0x the scale of `vf_clip_param`. This means that it will take more than 9397.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:40:08 (running for 00:04:08.93)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    41</td><td style=\"text-align: right;\">         226.636</td><td style=\"text-align: right;\">164000</td><td style=\"text-align: right;\">-93972.1</td><td style=\"text-align: right;\">            -33930.4</td><td style=\"text-align: right;\">             -412807</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 168000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-40-09\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -24545.279656458548\n",
      "  episode_reward_mean: -91696.84885378843\n",
      "  episode_reward_min: -353345.08442263666\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 6720\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.417187452316284\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 5.064886569976807\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01590285636484623\n",
      "          model: {}\n",
      "          policy_loss: -0.09007268399000168\n",
      "          total_loss: 9.96427059173584\n",
      "          vf_explained_var: -5.562843944062479e-05\n",
      "          vf_loss: 10.0\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 168000\n",
      "    num_agent_steps_trained: 168000\n",
      "    num_steps_sampled: 168000\n",
      "    num_steps_trained: 168000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 42\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.4375\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1538549766241259\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.289182722794865\n",
      "    mean_inference_ms: 0.9214924048262565\n",
      "    mean_raw_obs_processing_ms: 0.12993010746192524\n",
      "  time_since_restore: 232.15650844573975\n",
      "  time_this_iter_s: 5.52037239074707\n",
      "  time_total_s: 232.15650844573975\n",
      "  timers:\n",
      "    learn_throughput: 1659.79\n",
      "    learn_time_ms: 2409.944\n",
      "    load_throughput: 13335359.669\n",
      "    load_time_ms: 0.3\n",
      "    sample_throughput: 722.005\n",
      "    sample_time_ms: 5540.13\n",
      "    update_time_ms: 2.432\n",
      "  timestamp: 1650548409\n",
      "  timesteps_since_restore: 168000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 168000\n",
      "  training_iteration: 42\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:40:09,471\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 9170.0x the scale of `vf_clip_param`. This means that it will take more than 9170.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:40:14 (running for 00:04:14.53)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    42</td><td style=\"text-align: right;\">         232.157</td><td style=\"text-align: right;\">168000</td><td style=\"text-align: right;\">-91696.8</td><td style=\"text-align: right;\">            -24545.3</td><td style=\"text-align: right;\">             -353345</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 172000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-40-15\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -22850.809913802794\n",
      "  episode_reward_mean: -82087.86120344241\n",
      "  episode_reward_min: -250822.4715612418\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 6880\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.417187452316284\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 4.901378154754639\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01599583588540554\n",
      "          model: {}\n",
      "          policy_loss: -0.09695639461278915\n",
      "          total_loss: 9.957704544067383\n",
      "          vf_explained_var: -5.785111352452077e-05\n",
      "          vf_loss: 10.0\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 172000\n",
      "    num_agent_steps_trained: 172000\n",
      "    num_steps_sampled: 172000\n",
      "    num_steps_trained: 172000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 43\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.375\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.15387743459540898\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.28929527029794483\n",
      "    mean_inference_ms: 0.9215588159399587\n",
      "    mean_raw_obs_processing_ms: 0.12997047530860548\n",
      "  time_since_restore: 237.7896716594696\n",
      "  time_this_iter_s: 5.633163213729858\n",
      "  time_total_s: 237.7896716594696\n",
      "  timers:\n",
      "    learn_throughput: 1650.009\n",
      "    learn_time_ms: 2424.229\n",
      "    load_throughput: 13485424.001\n",
      "    load_time_ms: 0.297\n",
      "    sample_throughput: 720.069\n",
      "    sample_time_ms: 5555.021\n",
      "    update_time_ms: 2.435\n",
      "  timestamp: 1650548415\n",
      "  timesteps_since_restore: 172000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 172000\n",
      "  training_iteration: 43\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:40:15,145\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 8209.0x the scale of `vf_clip_param`. This means that it will take more than 8209.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:40:20 (running for 00:04:20.17)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    43</td><td style=\"text-align: right;\">          237.79</td><td style=\"text-align: right;\">172000</td><td style=\"text-align: right;\">-82087.9</td><td style=\"text-align: right;\">            -22850.8</td><td style=\"text-align: right;\">             -250822</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 176000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-40-20\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -12852.361436176943\n",
      "  episode_reward_mean: -86677.41515599107\n",
      "  episode_reward_min: -274030.44739716116\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 7040\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.417187452316284\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 4.887613296508789\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015916690230369568\n",
      "          model: {}\n",
      "          policy_loss: -0.11599677801132202\n",
      "          total_loss: 9.938393592834473\n",
      "          vf_explained_var: -5.291738852974959e-05\n",
      "          vf_loss: 10.0\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 176000\n",
      "    num_agent_steps_trained: 176000\n",
      "    num_steps_sampled: 176000\n",
      "    num_steps_trained: 176000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 44\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.271428571428571\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.15382249334102568\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2892839303213193\n",
      "    mean_inference_ms: 0.9210729781603906\n",
      "    mean_raw_obs_processing_ms: 0.12995487567984276\n",
      "  time_since_restore: 243.25571513175964\n",
      "  time_this_iter_s: 5.466043472290039\n",
      "  time_total_s: 243.25571513175964\n",
      "  timers:\n",
      "    learn_throughput: 1653.637\n",
      "    learn_time_ms: 2418.91\n",
      "    load_throughput: 13508225.443\n",
      "    load_time_ms: 0.296\n",
      "    sample_throughput: 719.419\n",
      "    sample_time_ms: 5560.04\n",
      "    update_time_ms: 2.443\n",
      "  timestamp: 1650548420\n",
      "  timesteps_since_restore: 176000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 176000\n",
      "  training_iteration: 44\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:40:20,652\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 8668.0x the scale of `vf_clip_param`. This means that it will take more than 8668.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:40:25 (running for 00:04:25.71)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    44</td><td style=\"text-align: right;\">         243.256</td><td style=\"text-align: right;\">176000</td><td style=\"text-align: right;\">-86677.4</td><td style=\"text-align: right;\">            -12852.4</td><td style=\"text-align: right;\">             -274030</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 180000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-40-26\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -23500.778244424513\n",
      "  episode_reward_mean: -89037.6188686705\n",
      "  episode_reward_min: -480239.14381854597\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 7200\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.417187452316284\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 4.9263200759887695\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01506950706243515\n",
      "          model: {}\n",
      "          policy_loss: -0.08196788281202316\n",
      "          total_loss: 9.969527244567871\n",
      "          vf_explained_var: -5.0114045734517276e-05\n",
      "          vf_loss: 10.0\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 180000\n",
      "    num_agent_steps_trained: 180000\n",
      "    num_steps_sampled: 180000\n",
      "    num_steps_trained: 180000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 45\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.6\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1539440203030371\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.28934324027827035\n",
      "    mean_inference_ms: 0.9208860093332065\n",
      "    mean_raw_obs_processing_ms: 0.1299674136828235\n",
      "  time_since_restore: 248.797758102417\n",
      "  time_this_iter_s: 5.542042970657349\n",
      "  time_total_s: 248.797758102417\n",
      "  timers:\n",
      "    learn_throughput: 1652.061\n",
      "    learn_time_ms: 2421.218\n",
      "    load_throughput: 13457300.072\n",
      "    load_time_ms: 0.297\n",
      "    sample_throughput: 719.918\n",
      "    sample_time_ms: 5556.19\n",
      "    update_time_ms: 2.437\n",
      "  timestamp: 1650548426\n",
      "  timesteps_since_restore: 180000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 180000\n",
      "  training_iteration: 45\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:40:26,234\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 8904.0x the scale of `vf_clip_param`. This means that it will take more than 8904.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:40:31 (running for 00:04:31.26)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         248.798</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">-89037.6</td><td style=\"text-align: right;\">            -23500.8</td><td style=\"text-align: right;\">             -480239</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 184000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-40-31\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -20362.4666772372\n",
      "  episode_reward_mean: -89610.8547752208\n",
      "  episode_reward_min: -383320.68913548056\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 7360\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.417187452316284\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 4.8857035636901855\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015429314225912094\n",
      "          model: {}\n",
      "          policy_loss: -0.08326113969087601\n",
      "          total_loss: 9.969464302062988\n",
      "          vf_explained_var: -5.879709715372883e-05\n",
      "          vf_loss: 10.0\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 184000\n",
      "    num_agent_steps_trained: 184000\n",
      "    num_steps_sampled: 184000\n",
      "    num_steps_trained: 184000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 46\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.7\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.15395508840176567\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.28934385829360015\n",
      "    mean_inference_ms: 0.9204913447200498\n",
      "    mean_raw_obs_processing_ms: 0.1299568062804253\n",
      "  time_since_restore: 254.3131308555603\n",
      "  time_this_iter_s: 5.5153727531433105\n",
      "  time_total_s: 254.3131308555603\n",
      "  timers:\n",
      "    learn_throughput: 1643.235\n",
      "    learn_time_ms: 2434.223\n",
      "    load_throughput: 13322652.267\n",
      "    load_time_ms: 0.3\n",
      "    sample_throughput: 720.541\n",
      "    sample_time_ms: 5551.388\n",
      "    update_time_ms: 2.402\n",
      "  timestamp: 1650548431\n",
      "  timesteps_since_restore: 184000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 184000\n",
      "  training_iteration: 46\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:40:31,790\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 8961.0x the scale of `vf_clip_param`. This means that it will take more than 8961.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:40:36 (running for 00:04:36.85)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    46</td><td style=\"text-align: right;\">         254.313</td><td style=\"text-align: right;\">184000</td><td style=\"text-align: right;\">-89610.9</td><td style=\"text-align: right;\">            -20362.5</td><td style=\"text-align: right;\">             -383321</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:40:37,405\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 8018.0x the scale of `vf_clip_param`. This means that it will take more than 8018.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 188000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-40-37\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -29647.396032739332\n",
      "  episode_reward_mean: -80183.50418695349\n",
      "  episode_reward_min: -234255.9546925313\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 7520\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.417187452316284\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 4.764875411987305\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01737857796251774\n",
      "          model: {}\n",
      "          policy_loss: -0.09987708181142807\n",
      "          total_loss: 9.959507942199707\n",
      "          vf_explained_var: -5.3271171054802835e-05\n",
      "          vf_loss: 10.0\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 188000\n",
      "    num_agent_steps_trained: 188000\n",
      "    num_steps_sampled: 188000\n",
      "    num_steps_trained: 188000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 47\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.3\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.15394466052485167\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.28936444525401595\n",
      "    mean_inference_ms: 0.9209945317759194\n",
      "    mean_raw_obs_processing_ms: 0.1299641763675345\n",
      "  time_since_restore: 259.88844203948975\n",
      "  time_this_iter_s: 5.575311183929443\n",
      "  time_total_s: 259.88844203948975\n",
      "  timers:\n",
      "    learn_throughput: 1632.35\n",
      "    learn_time_ms: 2450.455\n",
      "    load_throughput: 13145197.837\n",
      "    load_time_ms: 0.304\n",
      "    sample_throughput: 719.17\n",
      "    sample_time_ms: 5561.965\n",
      "    update_time_ms: 2.402\n",
      "  timestamp: 1650548437\n",
      "  timesteps_since_restore: 188000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 188000\n",
      "  training_iteration: 47\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:40:42 (running for 00:04:42.42)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    47</td><td style=\"text-align: right;\">         259.888</td><td style=\"text-align: right;\">188000</td><td style=\"text-align: right;\">-80183.5</td><td style=\"text-align: right;\">            -29647.4</td><td style=\"text-align: right;\">             -234256</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 192000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-40-43\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -21769.292390752486\n",
      "  episode_reward_mean: -81418.11174012785\n",
      "  episode_reward_min: -211837.48146061963\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 7680\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.417187452316284\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 4.793586730957031\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016660362482070923\n",
      "          model: {}\n",
      "          policy_loss: -0.09632878750562668\n",
      "          total_loss: 9.960603713989258\n",
      "          vf_explained_var: -5.5224663810804486e-05\n",
      "          vf_loss: 10.0\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 192000\n",
      "    num_agent_steps_trained: 192000\n",
      "    num_steps_sampled: 192000\n",
      "    num_steps_trained: 192000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 48\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.825\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1539206043138942\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2893935682758665\n",
      "    mean_inference_ms: 0.9207593054959226\n",
      "    mean_raw_obs_processing_ms: 0.12997264379069273\n",
      "  time_since_restore: 265.46067214012146\n",
      "  time_this_iter_s: 5.572230100631714\n",
      "  time_total_s: 265.46067214012146\n",
      "  timers:\n",
      "    learn_throughput: 1628.891\n",
      "    learn_time_ms: 2455.659\n",
      "    load_throughput: 13094923.509\n",
      "    load_time_ms: 0.305\n",
      "    sample_throughput: 716.293\n",
      "    sample_time_ms: 5584.311\n",
      "    update_time_ms: 2.441\n",
      "  timestamp: 1650548443\n",
      "  timesteps_since_restore: 192000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 192000\n",
      "  training_iteration: 48\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:40:43,016\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 8142.0x the scale of `vf_clip_param`. This means that it will take more than 8142.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:40:48 (running for 00:04:48.08)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    48</td><td style=\"text-align: right;\">         265.461</td><td style=\"text-align: right;\">192000</td><td style=\"text-align: right;\">-81418.1</td><td style=\"text-align: right;\">            -21769.3</td><td style=\"text-align: right;\">             -211837</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 196000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-40-48\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -26787.47857265537\n",
      "  episode_reward_mean: -78727.64670600805\n",
      "  episode_reward_min: -552243.8219228273\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 7840\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.417187452316284\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 4.755102157592773\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014037265442311764\n",
      "          model: {}\n",
      "          policy_loss: -0.07937218993902206\n",
      "          total_loss: 9.968596458435059\n",
      "          vf_explained_var: -6.479986041085795e-05\n",
      "          vf_loss: 10.0\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 196000\n",
      "    num_agent_steps_trained: 196000\n",
      "    num_steps_sampled: 196000\n",
      "    num_steps_trained: 196000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 49\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.2875\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1540325404203658\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.28946442481986145\n",
      "    mean_inference_ms: 0.9209775088766415\n",
      "    mean_raw_obs_processing_ms: 0.1299819990469034\n",
      "  time_since_restore: 271.0570921897888\n",
      "  time_this_iter_s: 5.596420049667358\n",
      "  time_total_s: 271.0570921897888\n",
      "  timers:\n",
      "    learn_throughput: 1629.855\n",
      "    learn_time_ms: 2454.206\n",
      "    load_throughput: 12993506.815\n",
      "    load_time_ms: 0.308\n",
      "    sample_throughput: 714.006\n",
      "    sample_time_ms: 5602.197\n",
      "    update_time_ms: 2.453\n",
      "  timestamp: 1650548448\n",
      "  timesteps_since_restore: 196000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 196000\n",
      "  training_iteration: 49\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:40:48,653\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 7873.0x the scale of `vf_clip_param`. This means that it will take more than 7873.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:40:53 (running for 00:04:53.67)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    49</td><td style=\"text-align: right;\">         271.057</td><td style=\"text-align: right;\">196000</td><td style=\"text-align: right;\">-78727.6</td><td style=\"text-align: right;\">            -26787.5</td><td style=\"text-align: right;\">             -552244</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 200000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-40-54\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -24113.800618458437\n",
      "  episode_reward_mean: -70601.24734018806\n",
      "  episode_reward_min: -209151.82060782975\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 8000\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.417187452316284\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 4.702231407165527\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017841506749391556\n",
      "          model: {}\n",
      "          policy_loss: -0.10208999365568161\n",
      "          total_loss: 9.958877563476562\n",
      "          vf_explained_var: -6.837613909738138e-05\n",
      "          vf_loss: 10.0\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 200000\n",
      "    num_agent_steps_trained: 200000\n",
      "    num_steps_sampled: 200000\n",
      "    num_steps_trained: 200000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 50\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.55\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.15399698398073697\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.28947337117357586\n",
      "    mean_inference_ms: 0.9208928584942819\n",
      "    mean_raw_obs_processing_ms: 0.1299840207278775\n",
      "  time_since_restore: 276.5565097332001\n",
      "  time_this_iter_s: 5.499417543411255\n",
      "  time_total_s: 276.5565097332001\n",
      "  timers:\n",
      "    learn_throughput: 1634.108\n",
      "    learn_time_ms: 2447.818\n",
      "    load_throughput: 12860045.991\n",
      "    load_time_ms: 0.311\n",
      "    sample_throughput: 714.432\n",
      "    sample_time_ms: 5598.855\n",
      "    update_time_ms: 2.418\n",
      "  timestamp: 1650548454\n",
      "  timesteps_since_restore: 200000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 200000\n",
      "  training_iteration: 50\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:40:54,192\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 7060.0x the scale of `vf_clip_param`. This means that it will take more than 7060.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:40:59 (running for 00:04:59.26)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         276.557</td><td style=\"text-align: right;\">200000</td><td style=\"text-align: right;\">-70601.2</td><td style=\"text-align: right;\">            -24113.8</td><td style=\"text-align: right;\">             -209152</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 204000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-40-59\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -21722.08626513546\n",
      "  episode_reward_mean: -67425.36526032898\n",
      "  episode_reward_min: -257343.92453663415\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 8160\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.417187452316284\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 4.628891944885254\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01657550409436226\n",
      "          model: {}\n",
      "          policy_loss: -0.09763742983341217\n",
      "          total_loss: 9.959003448486328\n",
      "          vf_explained_var: -7.093337626429275e-05\n",
      "          vf_loss: 10.0\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 204000\n",
      "    num_agent_steps_trained: 204000\n",
      "    num_steps_sampled: 204000\n",
      "    num_steps_trained: 204000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 51\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.5125\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.15400582564286033\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.28957981084898654\n",
      "    mean_inference_ms: 0.9208702400391445\n",
      "    mean_raw_obs_processing_ms: 0.1300069215518561\n",
      "  time_since_restore: 282.1361837387085\n",
      "  time_this_iter_s: 5.579674005508423\n",
      "  time_total_s: 282.1361837387085\n",
      "  timers:\n",
      "    learn_throughput: 1634.585\n",
      "    learn_time_ms: 2447.105\n",
      "    load_throughput: 12804102.877\n",
      "    load_time_ms: 0.312\n",
      "    sample_throughput: 715.418\n",
      "    sample_time_ms: 5591.134\n",
      "    update_time_ms: 2.416\n",
      "  timestamp: 1650548459\n",
      "  timesteps_since_restore: 204000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 204000\n",
      "  training_iteration: 51\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:40:59,815\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 6743.0x the scale of `vf_clip_param`. This means that it will take more than 6743.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:41:04 (running for 00:05:04.84)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    51</td><td style=\"text-align: right;\">         282.136</td><td style=\"text-align: right;\">204000</td><td style=\"text-align: right;\">-67425.4</td><td style=\"text-align: right;\">            -21722.1</td><td style=\"text-align: right;\">             -257344</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 208000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-41-05\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -23131.66417460506\n",
      "  episode_reward_mean: -61140.72175504479\n",
      "  episode_reward_min: -129271.16878967351\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 8320\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.417187452316284\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 4.5364603996276855\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017298569902777672\n",
      "          model: {}\n",
      "          policy_loss: -0.10434015840291977\n",
      "          total_loss: 9.95477294921875\n",
      "          vf_explained_var: -7.340985030168667e-05\n",
      "          vf_loss: 10.0\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 208000\n",
      "    num_agent_steps_trained: 208000\n",
      "    num_steps_sampled: 208000\n",
      "    num_steps_trained: 208000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 52\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.475\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.15397552613990767\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.28961904977298947\n",
      "    mean_inference_ms: 0.9206797088325356\n",
      "    mean_raw_obs_processing_ms: 0.13002081021252881\n",
      "  time_since_restore: 287.7041277885437\n",
      "  time_this_iter_s: 5.567944049835205\n",
      "  time_total_s: 287.7041277885437\n",
      "  timers:\n",
      "    learn_throughput: 1629.088\n",
      "    learn_time_ms: 2455.362\n",
      "    load_throughput: 12806057.553\n",
      "    load_time_ms: 0.312\n",
      "    sample_throughput: 715.947\n",
      "    sample_time_ms: 5587.009\n",
      "    update_time_ms: 2.416\n",
      "  timestamp: 1650548465\n",
      "  timesteps_since_restore: 208000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 208000\n",
      "  training_iteration: 52\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:41:05,423\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 6114.0x the scale of `vf_clip_param`. This means that it will take more than 6114.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:41:10 (running for 00:05:10.49)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    52</td><td style=\"text-align: right;\">         287.704</td><td style=\"text-align: right;\">208000</td><td style=\"text-align: right;\">-61140.7</td><td style=\"text-align: right;\">            -23131.7</td><td style=\"text-align: right;\">             -129271</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 212000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-41-11\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -14419.797518301653\n",
      "  episode_reward_mean: -66503.88759494189\n",
      "  episode_reward_min: -487211.88199024263\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 8480\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.417187452316284\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 4.59406042098999\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01319909654557705\n",
      "          model: {}\n",
      "          policy_loss: -0.07541155815124512\n",
      "          total_loss: 9.969693183898926\n",
      "          vf_explained_var: -8.114691445371136e-05\n",
      "          vf_loss: 10.0\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 212000\n",
      "    num_agent_steps_trained: 212000\n",
      "    num_steps_sampled: 212000\n",
      "    num_steps_trained: 212000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 53\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.5\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.15404390477467036\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.28960797425961754\n",
      "    mean_inference_ms: 0.9211684079432529\n",
      "    mean_raw_obs_processing_ms: 0.13000497777417402\n",
      "  time_since_restore: 293.2473120689392\n",
      "  time_this_iter_s: 5.543184280395508\n",
      "  time_total_s: 293.2473120689392\n",
      "  timers:\n",
      "    learn_throughput: 1631.166\n",
      "    learn_time_ms: 2452.234\n",
      "    load_throughput: 12781666.921\n",
      "    load_time_ms: 0.313\n",
      "    sample_throughput: 715.64\n",
      "    sample_time_ms: 5589.4\n",
      "    update_time_ms: 2.418\n",
      "  timestamp: 1650548471\n",
      "  timesteps_since_restore: 212000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 212000\n",
      "  training_iteration: 53\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:41:11,007\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 6650.0x the scale of `vf_clip_param`. This means that it will take more than 6650.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:41:16 (running for 00:05:16.03)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    53</td><td style=\"text-align: right;\">         293.247</td><td style=\"text-align: right;\">212000</td><td style=\"text-align: right;\">-66503.9</td><td style=\"text-align: right;\">            -14419.8</td><td style=\"text-align: right;\">             -487212</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 216000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-41-16\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -22428.609917808226\n",
      "  episode_reward_mean: -63078.50612298089\n",
      "  episode_reward_min: -215213.2252403981\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 8640\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.417187452316284\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 4.544011116027832\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0172144565731287\n",
      "          model: {}\n",
      "          policy_loss: -0.10145111382007599\n",
      "          total_loss: 9.95737361907959\n",
      "          vf_explained_var: -8.821872324915603e-05\n",
      "          vf_loss: 10.0\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 216000\n",
      "    num_agent_steps_trained: 216000\n",
      "    num_steps_sampled: 216000\n",
      "    num_steps_trained: 216000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 54\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.5625\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.15402555459075568\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2896222113158981\n",
      "    mean_inference_ms: 0.9212104747984602\n",
      "    mean_raw_obs_processing_ms: 0.13000973661661513\n",
      "  time_since_restore: 298.85025429725647\n",
      "  time_this_iter_s: 5.602942228317261\n",
      "  time_total_s: 298.85025429725647\n",
      "  timers:\n",
      "    learn_throughput: 1628.215\n",
      "    learn_time_ms: 2456.678\n",
      "    load_throughput: 12968397.619\n",
      "    load_time_ms: 0.308\n",
      "    sample_throughput: 714.848\n",
      "    sample_time_ms: 5595.597\n",
      "    update_time_ms: 2.416\n",
      "  timestamp: 1650548476\n",
      "  timesteps_since_restore: 216000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 216000\n",
      "  training_iteration: 54\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:41:16,652\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 6308.0x the scale of `vf_clip_param`. This means that it will take more than 6308.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:41:21 (running for 00:05:21.71)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    54</td><td style=\"text-align: right;\">          298.85</td><td style=\"text-align: right;\">216000</td><td style=\"text-align: right;\">-63078.5</td><td style=\"text-align: right;\">            -22428.6</td><td style=\"text-align: right;\">             -215213</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 220000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-41-22\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -12377.742278624224\n",
      "  episode_reward_mean: -61175.59958851624\n",
      "  episode_reward_min: -303295.99006192747\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 8800\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.417187452316284\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 4.484002590179443\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015677426010370255\n",
      "          model: {}\n",
      "          policy_loss: -0.08918122202157974\n",
      "          total_loss: 9.96439266204834\n",
      "          vf_explained_var: -7.897423347458243e-05\n",
      "          vf_loss: 10.0\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 220000\n",
      "    num_agent_steps_trained: 220000\n",
      "    num_steps_sampled: 220000\n",
      "    num_steps_trained: 220000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 55\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.7875\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.15401935901638897\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2896531562514914\n",
      "    mean_inference_ms: 0.9211891916719104\n",
      "    mean_raw_obs_processing_ms: 0.13001730586809232\n",
      "  time_since_restore: 304.45155930519104\n",
      "  time_this_iter_s: 5.60130500793457\n",
      "  time_total_s: 304.45155930519104\n",
      "  timers:\n",
      "    learn_throughput: 1626.466\n",
      "    learn_time_ms: 2459.32\n",
      "    load_throughput: 13077571.128\n",
      "    load_time_ms: 0.306\n",
      "    sample_throughput: 713.832\n",
      "    sample_time_ms: 5603.556\n",
      "    update_time_ms: 2.385\n",
      "  timestamp: 1650548482\n",
      "  timesteps_since_restore: 220000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 220000\n",
      "  training_iteration: 55\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:41:22,295\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 6118.0x the scale of `vf_clip_param`. This means that it will take more than 6118.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:41:27 (running for 00:05:27.32)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    55</td><td style=\"text-align: right;\">         304.452</td><td style=\"text-align: right;\">220000</td><td style=\"text-align: right;\">-61175.6</td><td style=\"text-align: right;\">            -12377.7</td><td style=\"text-align: right;\">             -303296</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 224000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-41-27\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -21292.039438057593\n",
      "  episode_reward_mean: -60129.27078238044\n",
      "  episode_reward_min: -298545.39914636675\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 8960\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.417187452316284\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 4.464050769805908\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015399615280330181\n",
      "          model: {}\n",
      "          policy_loss: -0.08770003169775009\n",
      "          total_loss: 9.964923858642578\n",
      "          vf_explained_var: -8.837253699311987e-05\n",
      "          vf_loss: 10.0\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 224000\n",
      "    num_agent_steps_trained: 224000\n",
      "    num_steps_sampled: 224000\n",
      "    num_steps_trained: 224000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 56\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.3125\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1539933556000093\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2896485086553896\n",
      "    mean_inference_ms: 0.921292869784217\n",
      "    mean_raw_obs_processing_ms: 0.130019971636178\n",
      "  time_since_restore: 310.0105526447296\n",
      "  time_this_iter_s: 5.558993339538574\n",
      "  time_total_s: 310.0105526447296\n",
      "  timers:\n",
      "    learn_throughput: 1627.868\n",
      "    learn_time_ms: 2457.202\n",
      "    load_throughput: 13162730.268\n",
      "    load_time_ms: 0.304\n",
      "    sample_throughput: 712.695\n",
      "    sample_time_ms: 5612.496\n",
      "    update_time_ms: 2.382\n",
      "  timestamp: 1650548487\n",
      "  timesteps_since_restore: 224000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 224000\n",
      "  training_iteration: 56\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:41:27,894\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 6013.0x the scale of `vf_clip_param`. This means that it will take more than 6013.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:41:32 (running for 00:05:32.95)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    56</td><td style=\"text-align: right;\">         310.011</td><td style=\"text-align: right;\">224000</td><td style=\"text-align: right;\">-60129.3</td><td style=\"text-align: right;\">              -21292</td><td style=\"text-align: right;\">             -298545</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 228000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-41-33\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -23444.641830969504\n",
      "  episode_reward_mean: -60877.80123239967\n",
      "  episode_reward_min: -190604.23915903634\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 9120\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.417187452316284\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 4.538825035095215\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017101282253861427\n",
      "          model: {}\n",
      "          policy_loss: -0.1021391823887825\n",
      "          total_loss: 9.956174850463867\n",
      "          vf_explained_var: -0.00027034655795432627\n",
      "          vf_loss: 9.999876022338867\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 228000\n",
      "    num_agent_steps_trained: 228000\n",
      "    num_steps_sampled: 228000\n",
      "    num_steps_trained: 228000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 57\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.6875\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1539831980229608\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.28960410087226\n",
      "    mean_inference_ms: 0.9209083495023505\n",
      "    mean_raw_obs_processing_ms: 0.13000685543705748\n",
      "  time_since_restore: 315.5066092014313\n",
      "  time_this_iter_s: 5.49605655670166\n",
      "  time_total_s: 315.5066092014313\n",
      "  timers:\n",
      "    learn_throughput: 1628.425\n",
      "    learn_time_ms: 2456.362\n",
      "    load_throughput: 13370430.347\n",
      "    load_time_ms: 0.299\n",
      "    sample_throughput: 713.867\n",
      "    sample_time_ms: 5603.287\n",
      "    update_time_ms: 2.382\n",
      "  timestamp: 1650548493\n",
      "  timesteps_since_restore: 228000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 228000\n",
      "  training_iteration: 57\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:41:33,429\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 6088.0x the scale of `vf_clip_param`. This means that it will take more than 6088.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:41:38 (running for 00:05:38.45)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    57</td><td style=\"text-align: right;\">         315.507</td><td style=\"text-align: right;\">228000</td><td style=\"text-align: right;\">-60877.8</td><td style=\"text-align: right;\">            -23444.6</td><td style=\"text-align: right;\">             -190604</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 232000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-41-39\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -24276.8699089772\n",
      "  episode_reward_mean: -53326.41354499224\n",
      "  episode_reward_min: -109647.79416148728\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 9280\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.417187452316284\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 4.354489326477051\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017873985692858696\n",
      "          model: {}\n",
      "          policy_loss: -0.10285267233848572\n",
      "          total_loss: 9.958226203918457\n",
      "          vf_explained_var: -0.00026756717124953866\n",
      "          vf_loss: 10.0\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 232000\n",
      "    num_agent_steps_trained: 232000\n",
      "    num_steps_sampled: 232000\n",
      "    num_steps_trained: 232000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 58\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.3875\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.15397416247490037\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.28963000137996403\n",
      "    mean_inference_ms: 0.9209008000268831\n",
      "    mean_raw_obs_processing_ms: 0.13002669015533633\n",
      "  time_since_restore: 321.0447750091553\n",
      "  time_this_iter_s: 5.538165807723999\n",
      "  time_total_s: 321.0447750091553\n",
      "  timers:\n",
      "    learn_throughput: 1629.117\n",
      "    learn_time_ms: 2455.318\n",
      "    load_throughput: 13385364.608\n",
      "    load_time_ms: 0.299\n",
      "    sample_throughput: 714.262\n",
      "    sample_time_ms: 5600.183\n",
      "    update_time_ms: 2.368\n",
      "  timestamp: 1650548499\n",
      "  timesteps_since_restore: 232000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 232000\n",
      "  training_iteration: 58\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:41:39,007\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 5333.0x the scale of `vf_clip_param`. This means that it will take more than 5333.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:41:44 (running for 00:05:44.07)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    58</td><td style=\"text-align: right;\">         321.045</td><td style=\"text-align: right;\">232000</td><td style=\"text-align: right;\">-53326.4</td><td style=\"text-align: right;\">            -24276.9</td><td style=\"text-align: right;\">             -109648</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 236000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-41-44\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -21834.583313513445\n",
      "  episode_reward_mean: -56688.44498434577\n",
      "  episode_reward_min: -224324.83778350422\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 9440\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.417187452316284\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 4.4018120765686035\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01682661660015583\n",
      "          model: {}\n",
      "          policy_loss: -0.09893229603767395\n",
      "          total_loss: 9.95856761932373\n",
      "          vf_explained_var: -0.00026039924705401063\n",
      "          vf_loss: 10.0\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 236000\n",
      "    num_agent_steps_trained: 236000\n",
      "    num_steps_sampled: 236000\n",
      "    num_steps_trained: 236000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 59\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.487500000000001\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.15399194291280058\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.28964077145074135\n",
      "    mean_inference_ms: 0.921592550885228\n",
      "    mean_raw_obs_processing_ms: 0.13002832081700563\n",
      "  time_since_restore: 326.5746269226074\n",
      "  time_this_iter_s: 5.529851913452148\n",
      "  time_total_s: 326.5746269226074\n",
      "  timers:\n",
      "    learn_throughput: 1632.763\n",
      "    learn_time_ms: 2449.836\n",
      "    load_throughput: 13418552.347\n",
      "    load_time_ms: 0.298\n",
      "    sample_throughput: 714.583\n",
      "    sample_time_ms: 5597.672\n",
      "    update_time_ms: 2.382\n",
      "  timestamp: 1650548504\n",
      "  timesteps_since_restore: 236000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 236000\n",
      "  training_iteration: 59\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:41:44,576\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 5669.0x the scale of `vf_clip_param`. This means that it will take more than 5669.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:41:49 (running for 00:05:49.60)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    59</td><td style=\"text-align: right;\">         326.575</td><td style=\"text-align: right;\">236000</td><td style=\"text-align: right;\">-56688.4</td><td style=\"text-align: right;\">            -21834.6</td><td style=\"text-align: right;\">             -224325</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 240000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-41-50\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -20636.72446029251\n",
      "  episode_reward_mean: -57498.25566656727\n",
      "  episode_reward_min: -275959.1029626614\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 9600\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.417187452316284\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 4.3870158195495605\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01573917455971241\n",
      "          model: {}\n",
      "          policy_loss: -0.0938207283616066\n",
      "          total_loss: 9.959962844848633\n",
      "          vf_explained_var: -0.0002621297026053071\n",
      "          vf_loss: 10.0\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 240000\n",
      "    num_agent_steps_trained: 240000\n",
      "    num_steps_sampled: 240000\n",
      "    num_steps_trained: 240000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 60\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.3875\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1540491697608453\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2896777778930217\n",
      "    mean_inference_ms: 0.9214363887223982\n",
      "    mean_raw_obs_processing_ms: 0.1300317607467687\n",
      "  time_since_restore: 332.021625995636\n",
      "  time_this_iter_s: 5.4469990730285645\n",
      "  time_total_s: 332.021625995636\n",
      "  timers:\n",
      "    learn_throughput: 1632.996\n",
      "    learn_time_ms: 2449.485\n",
      "    load_throughput: 13467021.994\n",
      "    load_time_ms: 0.297\n",
      "    sample_throughput: 715.883\n",
      "    sample_time_ms: 5587.505\n",
      "    update_time_ms: 2.38\n",
      "  timestamp: 1650548510\n",
      "  timesteps_since_restore: 240000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 240000\n",
      "  training_iteration: 60\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:41:50,063\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 5750.0x the scale of `vf_clip_param`. This means that it will take more than 5750.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:41:55 (running for 00:05:55.12)<br>Memory usage on this node: 17.7/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    60</td><td style=\"text-align: right;\">         332.022</td><td style=\"text-align: right;\">240000</td><td style=\"text-align: right;\">-57498.3</td><td style=\"text-align: right;\">            -20636.7</td><td style=\"text-align: right;\">             -275959</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 244000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-41-55\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -16827.012020755457\n",
      "  episode_reward_mean: -55596.67965883899\n",
      "  episode_reward_min: -359660.50334065023\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 9760\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.417187452316284\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 4.309603691101074\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013109642080962658\n",
      "          model: {}\n",
      "          policy_loss: -0.07794524729251862\n",
      "          total_loss: 9.96466064453125\n",
      "          vf_explained_var: -0.00011489993630675599\n",
      "          vf_loss: 9.997809410095215\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 244000\n",
      "    num_agent_steps_trained: 244000\n",
      "    num_steps_sampled: 244000\n",
      "    num_steps_trained: 244000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 61\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.975\n",
      "    ram_util_percent: 2.425\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.15400632395247832\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2896632426377256\n",
      "    mean_inference_ms: 0.9210955483898899\n",
      "    mean_raw_obs_processing_ms: 0.13001622366598986\n",
      "  time_since_restore: 337.412880897522\n",
      "  time_this_iter_s: 5.391254901885986\n",
      "  time_total_s: 337.412880897522\n",
      "  timers:\n",
      "    learn_throughput: 1638.395\n",
      "    learn_time_ms: 2441.413\n",
      "    load_throughput: 13573799.353\n",
      "    load_time_ms: 0.295\n",
      "    sample_throughput: 717.367\n",
      "    sample_time_ms: 5575.945\n",
      "    update_time_ms: 2.391\n",
      "  timestamp: 1650548515\n",
      "  timesteps_since_restore: 244000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 244000\n",
      "  training_iteration: 61\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:41:55,494\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 5560.0x the scale of `vf_clip_param`. This means that it will take more than 5560.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:42:00 (running for 00:06:00.51)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    61</td><td style=\"text-align: right;\">         337.413</td><td style=\"text-align: right;\">244000</td><td style=\"text-align: right;\">-55596.7</td><td style=\"text-align: right;\">              -16827</td><td style=\"text-align: right;\">             -359661</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 248000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-42-00\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -15830.402901340174\n",
      "  episode_reward_mean: -53606.48911041382\n",
      "  episode_reward_min: -231749.9101417548\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 9920\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.417187452316284\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 4.295462608337402\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017232464626431465\n",
      "          model: {}\n",
      "          policy_loss: -0.09912294894456863\n",
      "          total_loss: 9.957249641418457\n",
      "          vf_explained_var: -0.0001511102746007964\n",
      "          vf_loss: 9.997486114501953\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 248000\n",
      "    num_agent_steps_trained: 248000\n",
      "    num_steps_sampled: 248000\n",
      "    num_steps_trained: 248000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 62\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 6.942857142857143\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.15402601989632392\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2897630019454839\n",
      "    mean_inference_ms: 0.9211627155802662\n",
      "    mean_raw_obs_processing_ms: 0.13008260322004941\n",
      "  time_since_restore: 342.802294254303\n",
      "  time_this_iter_s: 5.389413356781006\n",
      "  time_total_s: 342.802294254303\n",
      "  timers:\n",
      "    learn_throughput: 1653.98\n",
      "    learn_time_ms: 2418.409\n",
      "    load_throughput: 13569407.959\n",
      "    load_time_ms: 0.295\n",
      "    sample_throughput: 717.76\n",
      "    sample_time_ms: 5572.894\n",
      "    update_time_ms: 2.4\n",
      "  timestamp: 1650548520\n",
      "  timesteps_since_restore: 248000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 248000\n",
      "  training_iteration: 62\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:42:00,922\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 5361.0x the scale of `vf_clip_param`. This means that it will take more than 5361.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:42:06 (running for 00:06:05.99)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    62</td><td style=\"text-align: right;\">         342.802</td><td style=\"text-align: right;\">248000</td><td style=\"text-align: right;\">-53606.5</td><td style=\"text-align: right;\">            -15830.4</td><td style=\"text-align: right;\">             -231750</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 252000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-42-06\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -10571.18180136745\n",
      "  episode_reward_mean: -58363.57022505689\n",
      "  episode_reward_min: -557492.3305796868\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 10080\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.417187452316284\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 4.385322093963623\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01383584551513195\n",
      "          model: {}\n",
      "          policy_loss: -0.05945293605327606\n",
      "          total_loss: 9.987826347351074\n",
      "          vf_explained_var: -0.00014715232828166336\n",
      "          vf_loss: 10.0\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 252000\n",
      "    num_agent_steps_trained: 252000\n",
      "    num_steps_sampled: 252000\n",
      "    num_steps_trained: 252000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 63\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.4125\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.15405844758812787\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2897668331952558\n",
      "    mean_inference_ms: 0.9209910358391384\n",
      "    mean_raw_obs_processing_ms: 0.13008489284139516\n",
      "  time_since_restore: 348.2425391674042\n",
      "  time_this_iter_s: 5.440244913101196\n",
      "  time_total_s: 348.2425391674042\n",
      "  timers:\n",
      "    learn_throughput: 1658.849\n",
      "    learn_time_ms: 2411.311\n",
      "    load_throughput: 13440051.27\n",
      "    load_time_ms: 0.298\n",
      "    sample_throughput: 721.174\n",
      "    sample_time_ms: 5546.515\n",
      "    update_time_ms: 2.402\n",
      "  timestamp: 1650548526\n",
      "  timesteps_since_restore: 252000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 252000\n",
      "  training_iteration: 63\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:42:06,401\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 5836.0x the scale of `vf_clip_param`. This means that it will take more than 5836.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:42:11 (running for 00:06:11.42)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    63</td><td style=\"text-align: right;\">         348.243</td><td style=\"text-align: right;\">252000</td><td style=\"text-align: right;\">-58363.6</td><td style=\"text-align: right;\">            -10571.2</td><td style=\"text-align: right;\">             -557492</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 256000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-42-12\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -15698.233873892475\n",
      "  episode_reward_mean: -54604.59302771229\n",
      "  episode_reward_min: -570165.5593663936\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 10240\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.417187452316284\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 4.310352325439453\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01130640972405672\n",
      "          model: {}\n",
      "          policy_loss: -0.06313637644052505\n",
      "          total_loss: 9.975500106811523\n",
      "          vf_explained_var: -0.00019166161655448377\n",
      "          vf_loss: 10.0\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 256000\n",
      "    num_agent_steps_trained: 256000\n",
      "    num_steps_sampled: 256000\n",
      "    num_steps_trained: 256000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 64\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.6625\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.15410034294485148\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.28981134378001383\n",
      "    mean_inference_ms: 0.9212983460997449\n",
      "    mean_raw_obs_processing_ms: 0.1300968496767115\n",
      "  time_since_restore: 353.8473060131073\n",
      "  time_this_iter_s: 5.604766845703125\n",
      "  time_total_s: 353.8473060131073\n",
      "  timers:\n",
      "    learn_throughput: 1661.325\n",
      "    learn_time_ms: 2407.717\n",
      "    load_throughput: 13441128.024\n",
      "    load_time_ms: 0.298\n",
      "    sample_throughput: 721.626\n",
      "    sample_time_ms: 5543.041\n",
      "    update_time_ms: 2.399\n",
      "  timestamp: 1650548532\n",
      "  timesteps_since_restore: 256000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 256000\n",
      "  training_iteration: 64\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:42:12,047\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 5460.0x the scale of `vf_clip_param`. This means that it will take more than 5460.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:42:17 (running for 00:06:17.11)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">         353.847</td><td style=\"text-align: right;\">256000</td><td style=\"text-align: right;\">-54604.6</td><td style=\"text-align: right;\">            -15698.2</td><td style=\"text-align: right;\">             -570166</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 260000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-42-17\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -16028.231878448176\n",
      "  episode_reward_mean: -53723.06745995422\n",
      "  episode_reward_min: -370699.0361160523\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 10400\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.417187452316284\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 4.227100372314453\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016900306567549706\n",
      "          model: {}\n",
      "          policy_loss: -0.07371385395526886\n",
      "          total_loss: 9.984036445617676\n",
      "          vf_explained_var: -7.550754526164383e-05\n",
      "          vf_loss: 10.0\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 260000\n",
      "    num_agent_steps_trained: 260000\n",
      "    num_steps_sampled: 260000\n",
      "    num_steps_trained: 260000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 65\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.1875\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.15417505473898227\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2898863272091393\n",
      "    mean_inference_ms: 0.921194202590853\n",
      "    mean_raw_obs_processing_ms: 0.13011509319038422\n",
      "  time_since_restore: 359.4074008464813\n",
      "  time_this_iter_s: 5.560094833374023\n",
      "  time_total_s: 359.4074008464813\n",
      "  timers:\n",
      "    learn_throughput: 1661.671\n",
      "    learn_time_ms: 2407.215\n",
      "    load_throughput: 13449748.276\n",
      "    load_time_ms: 0.297\n",
      "    sample_throughput: 722.592\n",
      "    sample_time_ms: 5535.63\n",
      "    update_time_ms: 2.419\n",
      "  timestamp: 1650548537\n",
      "  timesteps_since_restore: 260000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 260000\n",
      "  training_iteration: 65\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:42:17,646\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 5372.0x the scale of `vf_clip_param`. This means that it will take more than 5372.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:42:22 (running for 00:06:22.67)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    65</td><td style=\"text-align: right;\">         359.407</td><td style=\"text-align: right;\">260000</td><td style=\"text-align: right;\">-53723.1</td><td style=\"text-align: right;\">            -16028.2</td><td style=\"text-align: right;\">             -370699</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 264000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-42-23\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -20253.866970110586\n",
      "  episode_reward_mean: -52411.70219807242\n",
      "  episode_reward_min: -236188.03989296025\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 10560\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.417187452316284\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 4.3474650382995605\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016482245177030563\n",
      "          model: {}\n",
      "          policy_loss: -0.09652116149663925\n",
      "          total_loss: 9.95980167388916\n",
      "          vf_explained_var: -0.00011306039959890768\n",
      "          vf_loss: 10.0\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 264000\n",
      "    num_agent_steps_trained: 264000\n",
      "    num_steps_sampled: 264000\n",
      "    num_steps_trained: 264000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 66\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.7125\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1541300570092154\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.28982791733309543\n",
      "    mean_inference_ms: 0.9210932673014017\n",
      "    mean_raw_obs_processing_ms: 0.13009131561169943\n",
      "  time_since_restore: 364.94338965415955\n",
      "  time_this_iter_s: 5.535988807678223\n",
      "  time_total_s: 364.94338965415955\n",
      "  timers:\n",
      "    learn_throughput: 1657.838\n",
      "    learn_time_ms: 2412.781\n",
      "    load_throughput: 13416406.238\n",
      "    load_time_ms: 0.298\n",
      "    sample_throughput: 723.68\n",
      "    sample_time_ms: 5527.306\n",
      "    update_time_ms: 2.435\n",
      "  timestamp: 1650548543\n",
      "  timesteps_since_restore: 264000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 264000\n",
      "  training_iteration: 66\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:42:23,223\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 5241.0x the scale of `vf_clip_param`. This means that it will take more than 5241.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:42:28 (running for 00:06:28.28)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    66</td><td style=\"text-align: right;\">         364.943</td><td style=\"text-align: right;\">264000</td><td style=\"text-align: right;\">-52411.7</td><td style=\"text-align: right;\">            -20253.9</td><td style=\"text-align: right;\">             -236188</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 268000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-42-28\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -15580.318417835879\n",
      "  episode_reward_mean: -47417.78319282968\n",
      "  episode_reward_min: -109278.59931163854\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 10720\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.417187452316284\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 4.1756439208984375\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018339328467845917\n",
      "          model: {}\n",
      "          policy_loss: -0.10457625985145569\n",
      "          total_loss: 9.958025932312012\n",
      "          vf_explained_var: -5.614546171273105e-05\n",
      "          vf_loss: 9.999933242797852\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 268000\n",
      "    num_agent_steps_trained: 268000\n",
      "    num_steps_sampled: 268000\n",
      "    num_steps_trained: 268000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 67\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.2125\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.15412579641334265\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2898544842844172\n",
      "    mean_inference_ms: 0.921548718382217\n",
      "    mean_raw_obs_processing_ms: 0.13009118741663936\n",
      "  time_since_restore: 370.48201417922974\n",
      "  time_this_iter_s: 5.53862452507019\n",
      "  time_total_s: 370.48201417922974\n",
      "  timers:\n",
      "    learn_throughput: 1662.857\n",
      "    learn_time_ms: 2405.499\n",
      "    load_throughput: 13251098.649\n",
      "    load_time_ms: 0.302\n",
      "    sample_throughput: 721.449\n",
      "    sample_time_ms: 5544.399\n",
      "    update_time_ms: 2.432\n",
      "  timestamp: 1650548548\n",
      "  timesteps_since_restore: 268000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 268000\n",
      "  training_iteration: 67\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:42:28,801\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 4742.0x the scale of `vf_clip_param`. This means that it will take more than 4742.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:42:33 (running for 00:06:33.82)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    67</td><td style=\"text-align: right;\">         370.482</td><td style=\"text-align: right;\">268000</td><td style=\"text-align: right;\">-47417.8</td><td style=\"text-align: right;\">            -15580.3</td><td style=\"text-align: right;\">             -109279</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 272000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-42-34\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -20998.534157443693\n",
      "  episode_reward_mean: -49571.56516061713\n",
      "  episode_reward_min: -431931.4493880755\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 10880\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.417187452316284\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 4.158077716827393\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01213175430893898\n",
      "          model: {}\n",
      "          policy_loss: -0.06317263096570969\n",
      "          total_loss: 9.978129386901855\n",
      "          vf_explained_var: -0.00012986749061383307\n",
      "          vf_loss: 9.999845504760742\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 272000\n",
      "    num_agent_steps_trained: 272000\n",
      "    num_steps_sampled: 272000\n",
      "    num_steps_trained: 272000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 68\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.362500000000001\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.15410837815546474\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.28986223871156175\n",
      "    mean_inference_ms: 0.9214341057140067\n",
      "    mean_raw_obs_processing_ms: 0.1300819505606582\n",
      "  time_since_restore: 376.10126543045044\n",
      "  time_this_iter_s: 5.619251251220703\n",
      "  time_total_s: 376.10126543045044\n",
      "  timers:\n",
      "    learn_throughput: 1660.351\n",
      "    learn_time_ms: 2409.13\n",
      "    load_throughput: 13236462.327\n",
      "    load_time_ms: 0.302\n",
      "    sample_throughput: 721.785\n",
      "    sample_time_ms: 5541.816\n",
      "    update_time_ms: 2.433\n",
      "  timestamp: 1650548554\n",
      "  timesteps_since_restore: 272000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 272000\n",
      "  training_iteration: 68\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:42:34,462\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 4957.0x the scale of `vf_clip_param`. This means that it will take more than 4957.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:42:39 (running for 00:06:39.52)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    68</td><td style=\"text-align: right;\">         376.101</td><td style=\"text-align: right;\">272000</td><td style=\"text-align: right;\">-49571.6</td><td style=\"text-align: right;\">            -20998.5</td><td style=\"text-align: right;\">             -431931</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 276000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-42-40\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -21103.48745899265\n",
      "  episode_reward_mean: -55458.38794799362\n",
      "  episode_reward_min: -651437.6995682006\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 11040\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.417187452316284\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 4.221481800079346\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01226605661213398\n",
      "          model: {}\n",
      "          policy_loss: -0.054836206138134\n",
      "          total_loss: 9.987079620361328\n",
      "          vf_explained_var: -0.00015632568101864308\n",
      "          vf_loss: 10.0\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 276000\n",
      "    num_agent_steps_trained: 276000\n",
      "    num_steps_sampled: 276000\n",
      "    num_steps_trained: 276000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 69\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.6625\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.15409983527564397\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.289879598750307\n",
      "    mean_inference_ms: 0.9216639067148634\n",
      "    mean_raw_obs_processing_ms: 0.13008683207463506\n",
      "  time_since_restore: 381.6446225643158\n",
      "  time_this_iter_s: 5.5433571338653564\n",
      "  time_total_s: 381.6446225643158\n",
      "  timers:\n",
      "    learn_throughput: 1657.267\n",
      "    learn_time_ms: 2413.612\n",
      "    load_throughput: 13192746.717\n",
      "    load_time_ms: 0.303\n",
      "    sample_throughput: 721.766\n",
      "    sample_time_ms: 5541.962\n",
      "    update_time_ms: 2.784\n",
      "  timestamp: 1650548560\n",
      "  timesteps_since_restore: 276000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 276000\n",
      "  training_iteration: 69\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:42:40,044\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 5546.0x the scale of `vf_clip_param`. This means that it will take more than 5546.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:42:45 (running for 00:06:45.07)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    69</td><td style=\"text-align: right;\">         381.645</td><td style=\"text-align: right;\">276000</td><td style=\"text-align: right;\">-55458.4</td><td style=\"text-align: right;\">            -21103.5</td><td style=\"text-align: right;\">             -651438</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 280000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-42-45\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -20565.003287482905\n",
      "  episode_reward_mean: -51001.702994011226\n",
      "  episode_reward_min: -347071.0782504565\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 11200\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.417187452316284\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 4.175662517547607\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014967529103159904\n",
      "          model: {}\n",
      "          policy_loss: -0.08136741816997528\n",
      "          total_loss: 9.969779014587402\n",
      "          vf_explained_var: -0.0002185413904953748\n",
      "          vf_loss: 10.0\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 280000\n",
      "    num_agent_steps_trained: 280000\n",
      "    num_steps_sampled: 280000\n",
      "    num_steps_trained: 280000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 70\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.5\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.15407212082775545\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.28984738638051566\n",
      "    mean_inference_ms: 0.9217723672840815\n",
      "    mean_raw_obs_processing_ms: 0.130077040054714\n",
      "  time_since_restore: 387.2291705608368\n",
      "  time_this_iter_s: 5.584547996520996\n",
      "  time_total_s: 387.2291705608368\n",
      "  timers:\n",
      "    learn_throughput: 1649.333\n",
      "    learn_time_ms: 2425.223\n",
      "    load_throughput: 13224985.023\n",
      "    load_time_ms: 0.302\n",
      "    sample_throughput: 720.863\n",
      "    sample_time_ms: 5548.903\n",
      "    update_time_ms: 2.794\n",
      "  timestamp: 1650548565\n",
      "  timesteps_since_restore: 280000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 280000\n",
      "  training_iteration: 70\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:42:45,669\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 5100.0x the scale of `vf_clip_param`. This means that it will take more than 5100.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:42:50 (running for 00:06:50.73)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    70</td><td style=\"text-align: right;\">         387.229</td><td style=\"text-align: right;\">280000</td><td style=\"text-align: right;\">-51001.7</td><td style=\"text-align: right;\">              -20565</td><td style=\"text-align: right;\">             -347071</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 284000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-42-51\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -15464.344230223345\n",
      "  episode_reward_mean: -54694.982478678125\n",
      "  episode_reward_min: -540544.9331125504\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 11360\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.417187452316284\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 4.227253437042236\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011852666735649109\n",
      "          model: {}\n",
      "          policy_loss: -0.0649799257516861\n",
      "          total_loss: 9.965641975402832\n",
      "          vf_explained_var: -0.0001563272235216573\n",
      "          vf_loss: 9.990119934082031\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 284000\n",
      "    num_agent_steps_trained: 284000\n",
      "    num_steps_sampled: 284000\n",
      "    num_steps_trained: 284000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 71\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.612500000000001\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.15402771211165942\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.28981717022150955\n",
      "    mean_inference_ms: 0.921342985071653\n",
      "    mean_raw_obs_processing_ms: 0.13004996328440993\n",
      "  time_since_restore: 392.69608426094055\n",
      "  time_this_iter_s: 5.46691370010376\n",
      "  time_total_s: 392.69608426094055\n",
      "  timers:\n",
      "    learn_throughput: 1643.501\n",
      "    learn_time_ms: 2433.829\n",
      "    load_throughput: 13239595.96\n",
      "    load_time_ms: 0.302\n",
      "    sample_throughput: 719.491\n",
      "    sample_time_ms: 5559.487\n",
      "    update_time_ms: 2.76\n",
      "  timestamp: 1650548571\n",
      "  timesteps_since_restore: 284000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 284000\n",
      "  training_iteration: 71\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:42:51,174\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 5469.0x the scale of `vf_clip_param`. This means that it will take more than 5469.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:42:56 (running for 00:06:56.20)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    71</td><td style=\"text-align: right;\">         392.696</td><td style=\"text-align: right;\">284000</td><td style=\"text-align: right;\">  -54695</td><td style=\"text-align: right;\">            -15464.3</td><td style=\"text-align: right;\">             -540545</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 288000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-42-56\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -14449.095407295872\n",
      "  episode_reward_mean: -49649.85956374918\n",
      "  episode_reward_min: -222314.3397766835\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 11520\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.417187452316284\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 4.18628454208374\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01696471869945526\n",
      "          model: {}\n",
      "          policy_loss: -0.0900624617934227\n",
      "          total_loss: 9.962920188903809\n",
      "          vf_explained_var: -0.0001984189875656739\n",
      "          vf_loss: 9.995010375976562\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 288000\n",
      "    num_agent_steps_trained: 288000\n",
      "    num_steps_sampled: 288000\n",
      "    num_steps_trained: 288000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 72\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.387499999999999\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.15401013201257796\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2898055927078646\n",
      "    mean_inference_ms: 0.9216117461194046\n",
      "    mean_raw_obs_processing_ms: 0.13004836860234104\n",
      "  time_since_restore: 398.26052832603455\n",
      "  time_this_iter_s: 5.564444065093994\n",
      "  time_total_s: 398.26052832603455\n",
      "  timers:\n",
      "    learn_throughput: 1632.626\n",
      "    learn_time_ms: 2450.04\n",
      "    load_throughput: 13103105.28\n",
      "    load_time_ms: 0.305\n",
      "    sample_throughput: 718.191\n",
      "    sample_time_ms: 5569.552\n",
      "    update_time_ms: 2.723\n",
      "  timestamp: 1650548576\n",
      "  timesteps_since_restore: 288000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 288000\n",
      "  training_iteration: 72\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:42:56,779\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 4965.0x the scale of `vf_clip_param`. This means that it will take more than 4965.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:43:01 (running for 00:07:01.84)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    72</td><td style=\"text-align: right;\">         398.261</td><td style=\"text-align: right;\">288000</td><td style=\"text-align: right;\">-49649.9</td><td style=\"text-align: right;\">            -14449.1</td><td style=\"text-align: right;\">             -222314</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 292000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-43-02\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -14529.88958256309\n",
      "  episode_reward_mean: -46169.23793991645\n",
      "  episode_reward_min: -195365.513670016\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 11680\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.417187452316284\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 4.172398567199707\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016780920326709747\n",
      "          model: {}\n",
      "          policy_loss: -0.09725556522607803\n",
      "          total_loss: 9.9575834274292\n",
      "          vf_explained_var: -0.00022810313384979963\n",
      "          vf_loss: 9.9974946975708\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 292000\n",
      "    num_agent_steps_trained: 292000\n",
      "    num_steps_sampled: 292000\n",
      "    num_steps_trained: 292000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 73\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.8\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.15403529867592025\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.28982755807183197\n",
      "    mean_inference_ms: 0.9222658794816759\n",
      "    mean_raw_obs_processing_ms: 0.13005527406301204\n",
      "  time_since_restore: 403.87457251548767\n",
      "  time_this_iter_s: 5.614044189453125\n",
      "  time_total_s: 403.87457251548767\n",
      "  timers:\n",
      "    learn_throughput: 1627.315\n",
      "    learn_time_ms: 2458.036\n",
      "    load_throughput: 13290989.464\n",
      "    load_time_ms: 0.301\n",
      "    sample_throughput: 714.913\n",
      "    sample_time_ms: 5595.083\n",
      "    update_time_ms: 2.707\n",
      "  timestamp: 1650548582\n",
      "  timesteps_since_restore: 292000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 292000\n",
      "  training_iteration: 73\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:43:02,432\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 4617.0x the scale of `vf_clip_param`. This means that it will take more than 4617.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:43:07 (running for 00:07:07.45)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    73</td><td style=\"text-align: right;\">         403.875</td><td style=\"text-align: right;\">292000</td><td style=\"text-align: right;\">-46169.2</td><td style=\"text-align: right;\">            -14529.9</td><td style=\"text-align: right;\">             -195366</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 296000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-43-08\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -12236.632089901614\n",
      "  episode_reward_mean: -46525.69431191374\n",
      "  episode_reward_min: -287502.468160916\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 11840\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.417187452316284\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 4.107821464538574\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015424171462655067\n",
      "          model: {}\n",
      "          policy_loss: -0.08833794295787811\n",
      "          total_loss: 9.96436882019043\n",
      "          vf_explained_var: -0.00022036413429304957\n",
      "          vf_loss: 10.0\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 296000\n",
      "    num_agent_steps_trained: 296000\n",
      "    num_steps_sampled: 296000\n",
      "    num_steps_trained: 296000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 74\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.3125\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.15405314090170555\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2898306175861509\n",
      "    mean_inference_ms: 0.922408506899766\n",
      "    mean_raw_obs_processing_ms: 0.130048659260834\n",
      "  time_since_restore: 409.47777700424194\n",
      "  time_this_iter_s: 5.6032044887542725\n",
      "  time_total_s: 409.47777700424194\n",
      "  timers:\n",
      "    learn_throughput: 1623.418\n",
      "    learn_time_ms: 2463.938\n",
      "    load_throughput: 13263669.855\n",
      "    load_time_ms: 0.302\n",
      "    sample_throughput: 714.672\n",
      "    sample_time_ms: 5596.973\n",
      "    update_time_ms: 2.691\n",
      "  timestamp: 1650548588\n",
      "  timesteps_since_restore: 296000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 296000\n",
      "  training_iteration: 74\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:43:08,075\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 4653.0x the scale of `vf_clip_param`. This means that it will take more than 4653.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:43:13 (running for 00:07:13.13)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    74</td><td style=\"text-align: right;\">         409.478</td><td style=\"text-align: right;\">296000</td><td style=\"text-align: right;\">-46525.7</td><td style=\"text-align: right;\">            -12236.6</td><td style=\"text-align: right;\">             -287502</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 300000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-43-13\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -16563.161371160197\n",
      "  episode_reward_mean: -48351.83956993435\n",
      "  episode_reward_min: -318220.1238245255\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 12000\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.417187452316284\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 4.055485248565674\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01270162034779787\n",
      "          model: {}\n",
      "          policy_loss: -0.0710671916604042\n",
      "          total_loss: 9.969819068908691\n",
      "          vf_explained_var: -0.000230817764531821\n",
      "          vf_loss: 9.997483253479004\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 300000\n",
      "    num_agent_steps_trained: 300000\n",
      "    num_steps_sampled: 300000\n",
      "    num_steps_trained: 300000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 75\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.7875\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.15403568494718947\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.28982720434919595\n",
      "    mean_inference_ms: 0.922470308747441\n",
      "    mean_raw_obs_processing_ms: 0.1300473388702494\n",
      "  time_since_restore: 415.00682377815247\n",
      "  time_this_iter_s: 5.5290467739105225\n",
      "  time_total_s: 415.00682377815247\n",
      "  timers:\n",
      "    learn_throughput: 1627.081\n",
      "    learn_time_ms: 2458.39\n",
      "    load_throughput: 13253192.195\n",
      "    load_time_ms: 0.302\n",
      "    sample_throughput: 713.619\n",
      "    sample_time_ms: 5605.233\n",
      "    update_time_ms: 2.686\n",
      "  timestamp: 1650548593\n",
      "  timesteps_since_restore: 300000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 300000\n",
      "  training_iteration: 75\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:43:13,643\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 4835.0x the scale of `vf_clip_param`. This means that it will take more than 4835.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:43:18 (running for 00:07:18.66)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    75</td><td style=\"text-align: right;\">         415.007</td><td style=\"text-align: right;\">300000</td><td style=\"text-align: right;\">-48351.8</td><td style=\"text-align: right;\">            -16563.2</td><td style=\"text-align: right;\">             -318220</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 304000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-43-19\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -13291.662783075022\n",
      "  episode_reward_mean: -46344.26188316111\n",
      "  episode_reward_min: -247417.99725620812\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 12160\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.417187452316284\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 4.041070461273193\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01617898792028427\n",
      "          model: {}\n",
      "          policy_loss: -0.09311521798372269\n",
      "          total_loss: 9.961689949035645\n",
      "          vf_explained_var: -0.00018763246771413833\n",
      "          vf_loss: 9.999519348144531\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 304000\n",
      "    num_agent_steps_trained: 304000\n",
      "    num_steps_sampled: 304000\n",
      "    num_steps_trained: 304000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 76\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.5125\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.15402905400696967\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2898489107984525\n",
      "    mean_inference_ms: 0.9225338056099556\n",
      "    mean_raw_obs_processing_ms: 0.13005681728402413\n",
      "  time_since_restore: 420.6366138458252\n",
      "  time_this_iter_s: 5.6297900676727295\n",
      "  time_total_s: 420.6366138458252\n",
      "  timers:\n",
      "    learn_throughput: 1630.471\n",
      "    learn_time_ms: 2453.279\n",
      "    load_throughput: 13355529.374\n",
      "    load_time_ms: 0.3\n",
      "    sample_throughput: 712.49\n",
      "    sample_time_ms: 5614.114\n",
      "    update_time_ms: 2.671\n",
      "  timestamp: 1650548599\n",
      "  timesteps_since_restore: 304000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 304000\n",
      "  training_iteration: 76\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:43:19,312\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 4634.0x the scale of `vf_clip_param`. This means that it will take more than 4634.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:43:24 (running for 00:07:24.37)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    76</td><td style=\"text-align: right;\">         420.637</td><td style=\"text-align: right;\">304000</td><td style=\"text-align: right;\">-46344.3</td><td style=\"text-align: right;\">            -13291.7</td><td style=\"text-align: right;\">             -247418</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:43:24,984\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 5173.0x the scale of `vf_clip_param`. This means that it will take more than 5173.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 308000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-43-24\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -12910.634562659907\n",
      "  episode_reward_mean: -51730.869666652274\n",
      "  episode_reward_min: -474914.4927851445\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 12320\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.417187452316284\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 3.9854013919830322\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01277609821408987\n",
      "          model: {}\n",
      "          policy_loss: -0.08009068667888641\n",
      "          total_loss: 9.963567733764648\n",
      "          vf_explained_var: -0.0001885160309029743\n",
      "          vf_loss: 10.0\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 308000\n",
      "    num_agent_steps_trained: 308000\n",
      "    num_steps_sampled: 308000\n",
      "    num_steps_trained: 308000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 77\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.4875\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1540348900306451\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.28989740483506204\n",
      "    mean_inference_ms: 0.9226785913775393\n",
      "    mean_raw_obs_processing_ms: 0.13008408947081254\n",
      "  time_since_restore: 426.2703125476837\n",
      "  time_this_iter_s: 5.6336987018585205\n",
      "  time_total_s: 426.2703125476837\n",
      "  timers:\n",
      "    learn_throughput: 1623.884\n",
      "    learn_time_ms: 2463.231\n",
      "    load_throughput: 13519110.395\n",
      "    load_time_ms: 0.296\n",
      "    sample_throughput: 713.216\n",
      "    sample_time_ms: 5608.395\n",
      "    update_time_ms: 2.689\n",
      "  timestamp: 1650548604\n",
      "  timesteps_since_restore: 308000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 308000\n",
      "  training_iteration: 77\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:43:30 (running for 00:07:30.00)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    77</td><td style=\"text-align: right;\">          426.27</td><td style=\"text-align: right;\">308000</td><td style=\"text-align: right;\">-51730.9</td><td style=\"text-align: right;\">            -12910.6</td><td style=\"text-align: right;\">             -474914</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 312000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-43-30\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -14645.125400472332\n",
      "  episode_reward_mean: -49309.98416531522\n",
      "  episode_reward_min: -569552.7783654219\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 12480\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.417187452316284\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 3.925337553024292\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010008132085204124\n",
      "          model: {}\n",
      "          policy_loss: -0.05284658074378967\n",
      "          total_loss: 9.981334686279297\n",
      "          vf_explained_var: -0.0002485456061549485\n",
      "          vf_loss: 9.999982833862305\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 312000\n",
      "    num_agent_steps_trained: 312000\n",
      "    num_steps_sampled: 312000\n",
      "    num_steps_trained: 312000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 78\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.425\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.15409687416807855\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.289921782689075\n",
      "    mean_inference_ms: 0.9227328358215793\n",
      "    mean_raw_obs_processing_ms: 0.13008865999657837\n",
      "  time_since_restore: 432.03961205482483\n",
      "  time_this_iter_s: 5.769299507141113\n",
      "  time_total_s: 432.03961205482483\n",
      "  timers:\n",
      "    learn_throughput: 1613.437\n",
      "    learn_time_ms: 2479.179\n",
      "    load_throughput: 13559537.703\n",
      "    load_time_ms: 0.295\n",
      "    sample_throughput: 712.126\n",
      "    sample_time_ms: 5616.98\n",
      "    update_time_ms: 2.698\n",
      "  timestamp: 1650548610\n",
      "  timesteps_since_restore: 312000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 312000\n",
      "  training_iteration: 78\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:43:30,791\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 4931.0x the scale of `vf_clip_param`. This means that it will take more than 4931.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:43:35 (running for 00:07:35.85)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    78</td><td style=\"text-align: right;\">          432.04</td><td style=\"text-align: right;\">312000</td><td style=\"text-align: right;\">  -49310</td><td style=\"text-align: right;\">            -14645.1</td><td style=\"text-align: right;\">             -569553</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 316000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-43-36\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -14654.95278899734\n",
      "  episode_reward_mean: -43185.71899063949\n",
      "  episode_reward_min: -496995.62368958053\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 12640\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.417187452316284\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 3.893648147583008\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01269781868904829\n",
      "          model: {}\n",
      "          policy_loss: -0.05700063332915306\n",
      "          total_loss: 9.986223220825195\n",
      "          vf_explained_var: -0.0003566441882867366\n",
      "          vf_loss: 9.999833106994629\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 316000\n",
      "    num_agent_steps_trained: 316000\n",
      "    num_steps_sampled: 316000\n",
      "    num_steps_trained: 316000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 79\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.699999999999999\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.15407428298663273\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.28992773453637516\n",
      "    mean_inference_ms: 0.9224341383928041\n",
      "    mean_raw_obs_processing_ms: 0.1300919046205872\n",
      "  time_since_restore: 437.44222712516785\n",
      "  time_this_iter_s: 5.402615070343018\n",
      "  time_total_s: 437.44222712516785\n",
      "  timers:\n",
      "    learn_throughput: 1616.344\n",
      "    learn_time_ms: 2474.721\n",
      "    load_throughput: 13704636.497\n",
      "    load_time_ms: 0.292\n",
      "    sample_throughput: 711.278\n",
      "    sample_time_ms: 5623.681\n",
      "    update_time_ms: 2.34\n",
      "  timestamp: 1650548616\n",
      "  timesteps_since_restore: 316000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 316000\n",
      "  training_iteration: 79\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:43:36,232\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 4319.0x the scale of `vf_clip_param`. This means that it will take more than 4319.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:43:41 (running for 00:07:41.26)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    79</td><td style=\"text-align: right;\">         437.442</td><td style=\"text-align: right;\">316000</td><td style=\"text-align: right;\">-43185.7</td><td style=\"text-align: right;\">              -14655</td><td style=\"text-align: right;\">             -496996</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 320000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-43-41\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -11027.96763532226\n",
      "  episode_reward_mean: -49115.154839966344\n",
      "  episode_reward_min: -429852.5873553044\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 12800\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.417187452316284\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 3.8785617351531982\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012234429828822613\n",
      "          model: {}\n",
      "          policy_loss: -0.06728407740592957\n",
      "          total_loss: 9.970474243164062\n",
      "          vf_explained_var: -0.00018569384701550007\n",
      "          vf_loss: 9.995950698852539\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 320000\n",
      "    num_agent_steps_trained: 320000\n",
      "    num_steps_sampled: 320000\n",
      "    num_steps_trained: 320000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 80\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.6625\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.154018597303601\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.289880320658999\n",
      "    mean_inference_ms: 0.9221604707518946\n",
      "    mean_raw_obs_processing_ms: 0.1300743847019791\n",
      "  time_since_restore: 442.87008786201477\n",
      "  time_this_iter_s: 5.427860736846924\n",
      "  time_total_s: 442.87008786201477\n",
      "  timers:\n",
      "    learn_throughput: 1621.745\n",
      "    learn_time_ms: 2466.479\n",
      "    load_throughput: 13733804.846\n",
      "    load_time_ms: 0.291\n",
      "    sample_throughput: 712.818\n",
      "    sample_time_ms: 5611.534\n",
      "    update_time_ms: 2.331\n",
      "  timestamp: 1650548621\n",
      "  timesteps_since_restore: 320000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 320000\n",
      "  training_iteration: 80\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:43:41,702\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 4912.0x the scale of `vf_clip_param`. This means that it will take more than 4912.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:43:46 (running for 00:07:46.77)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    80</td><td style=\"text-align: right;\">          442.87</td><td style=\"text-align: right;\">320000</td><td style=\"text-align: right;\">-49115.2</td><td style=\"text-align: right;\">              -11028</td><td style=\"text-align: right;\">             -429853</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 324000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-43-47\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -13155.630690384554\n",
      "  episode_reward_mean: -43312.21906908367\n",
      "  episode_reward_min: -345357.10263054434\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 12960\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.417187452316284\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 3.784223794937134\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014882581308484077\n",
      "          model: {}\n",
      "          policy_loss: -0.07457292824983597\n",
      "          total_loss: 9.96766185760498\n",
      "          vf_explained_var: -0.00014021435345057398\n",
      "          vf_loss: 9.991378784179688\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 324000\n",
      "    num_agent_steps_trained: 324000\n",
      "    num_steps_sampled: 324000\n",
      "    num_steps_trained: 324000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 81\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.2875\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1540215802585193\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2899196971914715\n",
      "    mean_inference_ms: 0.9221321451773686\n",
      "    mean_raw_obs_processing_ms: 0.13009207977817297\n",
      "  time_since_restore: 448.3422348499298\n",
      "  time_this_iter_s: 5.472146987915039\n",
      "  time_total_s: 448.3422348499298\n",
      "  timers:\n",
      "    learn_throughput: 1626.192\n",
      "    learn_time_ms: 2459.734\n",
      "    load_throughput: 13790248.233\n",
      "    load_time_ms: 0.29\n",
      "    sample_throughput: 712.897\n",
      "    sample_time_ms: 5610.911\n",
      "    update_time_ms: 2.373\n",
      "  timestamp: 1650548627\n",
      "  timesteps_since_restore: 324000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 324000\n",
      "  training_iteration: 81\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:43:47,216\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 4331.0x the scale of `vf_clip_param`. This means that it will take more than 4331.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:43:52 (running for 00:07:52.24)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    81</td><td style=\"text-align: right;\">         448.342</td><td style=\"text-align: right;\">324000</td><td style=\"text-align: right;\">-43312.2</td><td style=\"text-align: right;\">            -13155.6</td><td style=\"text-align: right;\">             -345357</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 328000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-43-52\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -16854.70294158523\n",
      "  episode_reward_mean: -40838.79837698479\n",
      "  episode_reward_min: -320983.3044605738\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 13120\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.417187452316284\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 3.6545493602752686\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014271275140345097\n",
      "          model: {}\n",
      "          policy_loss: -0.06990684568881989\n",
      "          total_loss: 9.973884582519531\n",
      "          vf_explained_var: -0.00014543149154633284\n",
      "          vf_loss: 9.995023727416992\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 328000\n",
      "    num_agent_steps_trained: 328000\n",
      "    num_steps_sampled: 328000\n",
      "    num_steps_trained: 328000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 82\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.9\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.15404424055556792\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.289939971818599\n",
      "    mean_inference_ms: 0.9222454360669394\n",
      "    mean_raw_obs_processing_ms: 0.13010263924188703\n",
      "  time_since_restore: 453.9108393192291\n",
      "  time_this_iter_s: 5.568604469299316\n",
      "  time_total_s: 453.9108393192291\n",
      "  timers:\n",
      "    learn_throughput: 1624.513\n",
      "    learn_time_ms: 2462.276\n",
      "    load_throughput: 13865467.769\n",
      "    load_time_ms: 0.288\n",
      "    sample_throughput: 714.022\n",
      "    sample_time_ms: 5602.069\n",
      "    update_time_ms: 2.377\n",
      "  timestamp: 1650548632\n",
      "  timesteps_since_restore: 328000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 328000\n",
      "  training_iteration: 82\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:43:52,825\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 4084.0x the scale of `vf_clip_param`. This means that it will take more than 4084.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:43:57 (running for 00:07:57.88)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    82</td><td style=\"text-align: right;\">         453.911</td><td style=\"text-align: right;\">328000</td><td style=\"text-align: right;\">-40838.8</td><td style=\"text-align: right;\">            -16854.7</td><td style=\"text-align: right;\">             -320983</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 332000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-43-58\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -11587.207965183901\n",
      "  episode_reward_mean: -47850.95469705899\n",
      "  episode_reward_min: -547208.6347033269\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 13280\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.417187452316284\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 3.8183553218841553\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010760404169559479\n",
      "          model: {}\n",
      "          policy_loss: -0.05852772295475006\n",
      "          total_loss: 9.978242874145508\n",
      "          vf_explained_var: -0.00014447384455706924\n",
      "          vf_loss: 10.0\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 332000\n",
      "    num_agent_steps_trained: 332000\n",
      "    num_steps_sampled: 332000\n",
      "    num_steps_trained: 332000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 83\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.4125\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1540800817634639\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2899819335702185\n",
      "    mean_inference_ms: 0.922489354839108\n",
      "    mean_raw_obs_processing_ms: 0.13011402575685505\n",
      "  time_since_restore: 459.48039388656616\n",
      "  time_this_iter_s: 5.569554567337036\n",
      "  time_total_s: 459.48039388656616\n",
      "  timers:\n",
      "    learn_throughput: 1625.951\n",
      "    learn_time_ms: 2460.099\n",
      "    load_throughput: 13799322.257\n",
      "    load_time_ms: 0.29\n",
      "    sample_throughput: 713.998\n",
      "    sample_time_ms: 5602.26\n",
      "    update_time_ms: 2.377\n",
      "  timestamp: 1650548638\n",
      "  timesteps_since_restore: 332000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 332000\n",
      "  training_iteration: 83\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:43:58,433\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 4785.0x the scale of `vf_clip_param`. This means that it will take more than 4785.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:44:03 (running for 00:08:03.45)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    83</td><td style=\"text-align: right;\">          459.48</td><td style=\"text-align: right;\">332000</td><td style=\"text-align: right;\">  -47851</td><td style=\"text-align: right;\">            -11587.2</td><td style=\"text-align: right;\">             -547209</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 336000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-44-04\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -12348.340346503901\n",
      "  episode_reward_mean: -39251.8610317557\n",
      "  episode_reward_min: -324967.1731184012\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 13440\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.417187452316284\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 3.7350423336029053\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014190027490258217\n",
      "          model: {}\n",
      "          policy_loss: -0.07293514162302017\n",
      "          total_loss: 9.975555419921875\n",
      "          vf_explained_var: -0.00017671046953182667\n",
      "          vf_loss: 10.0\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 336000\n",
      "    num_agent_steps_trained: 336000\n",
      "    num_steps_sampled: 336000\n",
      "    num_steps_trained: 336000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 84\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.7125\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.15409024797544105\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.28998143506860025\n",
      "    mean_inference_ms: 0.922581827230896\n",
      "    mean_raw_obs_processing_ms: 0.1301089976278243\n",
      "  time_since_restore: 465.05233550071716\n",
      "  time_this_iter_s: 5.571941614151001\n",
      "  time_total_s: 465.05233550071716\n",
      "  timers:\n",
      "    learn_throughput: 1627.344\n",
      "    learn_time_ms: 2457.993\n",
      "    load_throughput: 13833456.464\n",
      "    load_time_ms: 0.289\n",
      "    sample_throughput: 714.403\n",
      "    sample_time_ms: 5599.077\n",
      "    update_time_ms: 2.399\n",
      "  timestamp: 1650548644\n",
      "  timesteps_since_restore: 336000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 336000\n",
      "  training_iteration: 84\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:44:04,045\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 3925.0x the scale of `vf_clip_param`. This means that it will take more than 3925.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:44:09 (running for 00:08:09.11)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    84</td><td style=\"text-align: right;\">         465.052</td><td style=\"text-align: right;\">336000</td><td style=\"text-align: right;\">-39251.9</td><td style=\"text-align: right;\">            -12348.3</td><td style=\"text-align: right;\">             -324967</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 340000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-44-09\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -7439.763668704676\n",
      "  episode_reward_mean: -43940.123874494326\n",
      "  episode_reward_min: -429789.77528326574\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 13600\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.417187452316284\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 3.808532476425171\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013489354401826859\n",
      "          model: {}\n",
      "          policy_loss: -0.06316941231489182\n",
      "          total_loss: 9.980583190917969\n",
      "          vf_explained_var: -0.00015079423610586673\n",
      "          vf_loss: 9.997657775878906\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 340000\n",
      "    num_agent_steps_trained: 340000\n",
      "    num_steps_sampled: 340000\n",
      "    num_steps_trained: 340000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 85\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.8\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.15411652549356666\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2899855714859087\n",
      "    mean_inference_ms: 0.9228267057983406\n",
      "    mean_raw_obs_processing_ms: 0.1301047602836995\n",
      "  time_since_restore: 470.6264052391052\n",
      "  time_this_iter_s: 5.5740697383880615\n",
      "  time_total_s: 470.6264052391052\n",
      "  timers:\n",
      "    learn_throughput: 1624.795\n",
      "    learn_time_ms: 2461.849\n",
      "    load_throughput: 13699041.398\n",
      "    load_time_ms: 0.292\n",
      "    sample_throughput: 714.578\n",
      "    sample_time_ms: 5597.71\n",
      "    update_time_ms: 2.421\n",
      "  timestamp: 1650548649\n",
      "  timesteps_since_restore: 340000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 340000\n",
      "  training_iteration: 85\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:44:09,659\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 4394.0x the scale of `vf_clip_param`. This means that it will take more than 4394.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:44:14 (running for 00:08:14.68)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    85</td><td style=\"text-align: right;\">         470.626</td><td style=\"text-align: right;\">340000</td><td style=\"text-align: right;\">-43940.1</td><td style=\"text-align: right;\">            -7439.76</td><td style=\"text-align: right;\">             -429790</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 344000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-44-15\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -14143.360523629834\n",
      "  episode_reward_mean: -43890.882648347215\n",
      "  episode_reward_min: -432496.28958003584\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 13760\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.417187452316284\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 3.843073606491089\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012711768969893456\n",
      "          model: {}\n",
      "          policy_loss: -0.060547713190317154\n",
      "          total_loss: 9.980545997619629\n",
      "          vf_explained_var: -0.00016648462042212486\n",
      "          vf_loss: 9.997655868530273\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 344000\n",
      "    num_agent_steps_trained: 344000\n",
      "    num_steps_sampled: 344000\n",
      "    num_steps_trained: 344000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 86\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.171428571428572\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.154142343693211\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2899761757902849\n",
      "    mean_inference_ms: 0.9225844995922031\n",
      "    mean_raw_obs_processing_ms: 0.13008792368714384\n",
      "  time_since_restore: 476.12110590934753\n",
      "  time_this_iter_s: 5.49470067024231\n",
      "  time_total_s: 476.12110590934753\n",
      "  timers:\n",
      "    learn_throughput: 1625.668\n",
      "    learn_time_ms: 2460.527\n",
      "    load_throughput: 13651111.473\n",
      "    load_time_ms: 0.293\n",
      "    sample_throughput: 715.627\n",
      "    sample_time_ms: 5589.503\n",
      "    update_time_ms: 2.402\n",
      "  timestamp: 1650548655\n",
      "  timesteps_since_restore: 344000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 344000\n",
      "  training_iteration: 86\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:44:15,194\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 4389.0x the scale of `vf_clip_param`. This means that it will take more than 4389.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:44:20 (running for 00:08:20.25)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    86</td><td style=\"text-align: right;\">         476.121</td><td style=\"text-align: right;\">344000</td><td style=\"text-align: right;\">-43890.9</td><td style=\"text-align: right;\">            -14143.4</td><td style=\"text-align: right;\">             -432496</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 348000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-44-20\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -10278.986513543774\n",
      "  episode_reward_mean: -41242.42425832515\n",
      "  episode_reward_min: -625611.417121101\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 13920\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 3.417187452316284\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 3.934582233428955\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.24071958661079407\n",
      "          model: {}\n",
      "          policy_loss: 0.13015782833099365\n",
      "          total_loss: 10.952742576599121\n",
      "          vf_explained_var: -0.0001664346200414002\n",
      "          vf_loss: 10.0\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 348000\n",
      "    num_agent_steps_trained: 348000\n",
      "    num_steps_sampled: 348000\n",
      "    num_steps_trained: 348000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 87\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.65\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.15410474681010744\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.28994709784097744\n",
      "    mean_inference_ms: 0.9225230260251334\n",
      "    mean_raw_obs_processing_ms: 0.1300742138791419\n",
      "  time_since_restore: 481.6335036754608\n",
      "  time_this_iter_s: 5.512397766113281\n",
      "  time_total_s: 481.6335036754608\n",
      "  timers:\n",
      "    learn_throughput: 1626.208\n",
      "    learn_time_ms: 2459.709\n",
      "    load_throughput: 13625611.955\n",
      "    load_time_ms: 0.294\n",
      "    sample_throughput: 717.243\n",
      "    sample_time_ms: 5576.912\n",
      "    update_time_ms: 2.387\n",
      "  timestamp: 1650548660\n",
      "  timesteps_since_restore: 348000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 348000\n",
      "  training_iteration: 87\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:44:20,745\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 4124.0x the scale of `vf_clip_param`. This means that it will take more than 4124.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:44:25 (running for 00:08:25.77)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    87</td><td style=\"text-align: right;\">         481.634</td><td style=\"text-align: right;\">348000</td><td style=\"text-align: right;\">-41242.4</td><td style=\"text-align: right;\">              -10279</td><td style=\"text-align: right;\">             -625611</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 352000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-44-26\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -16000.358213830637\n",
      "  episode_reward_mean: -42253.01927551691\n",
      "  episode_reward_min: -298675.6392735726\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 14080\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.125781059265137\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 3.821791410446167\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006881957873702049\n",
      "          model: {}\n",
      "          policy_loss: -0.05760546401143074\n",
      "          total_loss: 9.975282669067383\n",
      "          vf_explained_var: -0.00013150335871614516\n",
      "          vf_loss: 9.997611999511719\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 352000\n",
      "    num_agent_steps_trained: 352000\n",
      "    num_steps_sampled: 352000\n",
      "    num_steps_trained: 352000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 88\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.637499999999999\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.15411939617774725\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2899734523664671\n",
      "    mean_inference_ms: 0.9228998891024476\n",
      "    mean_raw_obs_processing_ms: 0.13008492221113016\n",
      "  time_since_restore: 487.2463264465332\n",
      "  time_this_iter_s: 5.612822771072388\n",
      "  time_total_s: 487.2463264465332\n",
      "  timers:\n",
      "    learn_throughput: 1638.068\n",
      "    learn_time_ms: 2441.901\n",
      "    load_throughput: 13459459.286\n",
      "    load_time_ms: 0.297\n",
      "    sample_throughput: 717.006\n",
      "    sample_time_ms: 5578.755\n",
      "    update_time_ms: 2.391\n",
      "  timestamp: 1650548666\n",
      "  timesteps_since_restore: 352000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 352000\n",
      "  training_iteration: 88\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:44:26,401\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 4225.0x the scale of `vf_clip_param`. This means that it will take more than 4225.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:44:31 (running for 00:08:31.46)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    88</td><td style=\"text-align: right;\">         487.246</td><td style=\"text-align: right;\">352000</td><td style=\"text-align: right;\">  -42253</td><td style=\"text-align: right;\">            -16000.4</td><td style=\"text-align: right;\">             -298676</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 356000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-44-32\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -14872.025209832836\n",
      "  episode_reward_mean: -40875.05558044155\n",
      "  episode_reward_min: -241222.84911565846\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 14240\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.125781059265137\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 3.829503297805786\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008893364109098911\n",
      "          model: {}\n",
      "          policy_loss: -0.06423062831163406\n",
      "          total_loss: 9.981354713439941\n",
      "          vf_explained_var: -0.00016943486116360873\n",
      "          vf_loss: 10.0\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 356000\n",
      "    num_agent_steps_trained: 356000\n",
      "    num_steps_sampled: 356000\n",
      "    num_steps_trained: 356000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 89\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.725\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.15413472560296415\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.29002105193666095\n",
      "    mean_inference_ms: 0.9230309390320406\n",
      "    mean_raw_obs_processing_ms: 0.13009612056287642\n",
      "  time_since_restore: 492.8247346878052\n",
      "  time_this_iter_s: 5.578408241271973\n",
      "  time_total_s: 492.8247346878052\n",
      "  timers:\n",
      "    learn_throughput: 1632.384\n",
      "    learn_time_ms: 2450.404\n",
      "    load_throughput: 13392844.256\n",
      "    load_time_ms: 0.299\n",
      "    sample_throughput: 718.109\n",
      "    sample_time_ms: 5570.185\n",
      "    update_time_ms: 2.409\n",
      "  timestamp: 1650548672\n",
      "  timesteps_since_restore: 356000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 356000\n",
      "  training_iteration: 89\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:44:32,019\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 4088.0x the scale of `vf_clip_param`. This means that it will take more than 4088.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:44:37 (running for 00:08:37.04)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    89</td><td style=\"text-align: right;\">         492.825</td><td style=\"text-align: right;\">356000</td><td style=\"text-align: right;\">-40875.1</td><td style=\"text-align: right;\">              -14872</td><td style=\"text-align: right;\">             -241223</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 360000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-44-37\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -8556.856427717852\n",
      "  episode_reward_mean: -46162.51701885153\n",
      "  episode_reward_min: -606909.2774078137\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 14400\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.125781059265137\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 3.7699286937713623\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006695527117699385\n",
      "          model: {}\n",
      "          policy_loss: -0.06853356957435608\n",
      "          total_loss: 9.965786933898926\n",
      "          vf_explained_var: -0.0001238815311808139\n",
      "          vf_loss: 10.0\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 360000\n",
      "    num_agent_steps_trained: 360000\n",
      "    num_steps_sampled: 360000\n",
      "    num_steps_trained: 360000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 90\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.387499999999999\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.15417778535882387\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.28999676858768664\n",
      "    mean_inference_ms: 0.9234643183945492\n",
      "    mean_raw_obs_processing_ms: 0.13007706988814863\n",
      "  time_since_restore: 498.32158374786377\n",
      "  time_this_iter_s: 5.496849060058594\n",
      "  time_total_s: 498.32158374786377\n",
      "  timers:\n",
      "    learn_throughput: 1636.056\n",
      "    learn_time_ms: 2444.905\n",
      "    load_throughput: 13263669.855\n",
      "    load_time_ms: 0.302\n",
      "    sample_throughput: 715.453\n",
      "    sample_time_ms: 5590.861\n",
      "    update_time_ms: 2.439\n",
      "  timestamp: 1650548677\n",
      "  timesteps_since_restore: 360000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 360000\n",
      "  training_iteration: 90\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:44:37,556\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 4616.0x the scale of `vf_clip_param`. This means that it will take more than 4616.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:44:42 (running for 00:08:42.62)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    90</td><td style=\"text-align: right;\">         498.322</td><td style=\"text-align: right;\">360000</td><td style=\"text-align: right;\">-46162.5</td><td style=\"text-align: right;\">            -8556.86</td><td style=\"text-align: right;\">             -606909</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 364000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-44-43\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -7935.726533938097\n",
      "  episode_reward_mean: -45450.45774730954\n",
      "  episode_reward_min: -361487.36378376547\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 14560\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.125781059265137\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 3.9975385665893555\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010391429997980595\n",
      "          model: {}\n",
      "          policy_loss: -0.053946636617183685\n",
      "          total_loss: 9.999317169189453\n",
      "          vf_explained_var: -0.0001110223020077683\n",
      "          vf_loss: 10.0\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 364000\n",
      "    num_agent_steps_trained: 364000\n",
      "    num_steps_sampled: 364000\n",
      "    num_steps_trained: 364000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 91\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.85\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.15417677666555252\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.29001379614868633\n",
      "    mean_inference_ms: 0.9232048042506917\n",
      "    mean_raw_obs_processing_ms: 0.1300904880001595\n",
      "  time_since_restore: 503.81933403015137\n",
      "  time_this_iter_s: 5.497750282287598\n",
      "  time_total_s: 503.81933403015137\n",
      "  timers:\n",
      "    learn_throughput: 1631.92\n",
      "    learn_time_ms: 2451.101\n",
      "    load_throughput: 13211446.571\n",
      "    load_time_ms: 0.303\n",
      "    sample_throughput: 716.586\n",
      "    sample_time_ms: 5582.021\n",
      "    update_time_ms: 2.408\n",
      "  timestamp: 1650548683\n",
      "  timesteps_since_restore: 364000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 364000\n",
      "  training_iteration: 91\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:44:43,098\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 4545.0x the scale of `vf_clip_param`. This means that it will take more than 4545.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:44:48 (running for 00:08:48.12)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    91</td><td style=\"text-align: right;\">         503.819</td><td style=\"text-align: right;\">364000</td><td style=\"text-align: right;\">-45450.5</td><td style=\"text-align: right;\">            -7935.73</td><td style=\"text-align: right;\">             -361487</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 368000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-44-48\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -15654.436573792147\n",
      "  episode_reward_mean: -55516.92808841383\n",
      "  episode_reward_min: -523922.41714041296\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 14720\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.125781059265137\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 4.175181865692139\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011495090089738369\n",
      "          model: {}\n",
      "          policy_loss: -0.055144283920526505\n",
      "          total_loss: 10.003702163696289\n",
      "          vf_explained_var: -4.350344170234166e-05\n",
      "          vf_loss: 9.99992561340332\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 368000\n",
      "    num_agent_steps_trained: 368000\n",
      "    num_steps_sampled: 368000\n",
      "    num_steps_trained: 368000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 92\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.275\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1542038279023738\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2899886955375961\n",
      "    mean_inference_ms: 0.9229719700696339\n",
      "    mean_raw_obs_processing_ms: 0.13007304906079306\n",
      "  time_since_restore: 509.34403252601624\n",
      "  time_this_iter_s: 5.524698495864868\n",
      "  time_total_s: 509.34403252601624\n",
      "  timers:\n",
      "    learn_throughput: 1632.374\n",
      "    learn_time_ms: 2450.418\n",
      "    load_throughput: 13206246.851\n",
      "    load_time_ms: 0.303\n",
      "    sample_throughput: 716.263\n",
      "    sample_time_ms: 5584.539\n",
      "    update_time_ms: 2.4\n",
      "  timestamp: 1650548688\n",
      "  timesteps_since_restore: 368000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 368000\n",
      "  training_iteration: 92\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:44:48,664\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 5552.0x the scale of `vf_clip_param`. This means that it will take more than 5552.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:44:53 (running for 00:08:53.73)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    92</td><td style=\"text-align: right;\">         509.344</td><td style=\"text-align: right;\">368000</td><td style=\"text-align: right;\">-55516.9</td><td style=\"text-align: right;\">            -15654.4</td><td style=\"text-align: right;\">             -523922</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 372000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-44-54\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -11681.57115094726\n",
      "  episode_reward_mean: -45102.96226327483\n",
      "  episode_reward_min: -500209.60361521307\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 14880\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.125781059265137\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 3.9179506301879883\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010753112845122814\n",
      "          model: {}\n",
      "          policy_loss: -0.05147284269332886\n",
      "          total_loss: 9.996381759643555\n",
      "          vf_explained_var: -3.0399074603337795e-05\n",
      "          vf_loss: 9.992735862731934\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 372000\n",
      "    num_agent_steps_trained: 372000\n",
      "    num_steps_sampled: 372000\n",
      "    num_steps_trained: 372000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 93\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.6875\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.15417951771550556\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.28995534276340085\n",
      "    mean_inference_ms: 0.9226195424515232\n",
      "    mean_raw_obs_processing_ms: 0.13005992864937038\n",
      "  time_since_restore: 514.7858090400696\n",
      "  time_this_iter_s: 5.441776514053345\n",
      "  time_total_s: 514.7858090400696\n",
      "  timers:\n",
      "    learn_throughput: 1630.42\n",
      "    learn_time_ms: 2453.355\n",
      "    load_throughput: 13207286.468\n",
      "    load_time_ms: 0.303\n",
      "    sample_throughput: 718.35\n",
      "    sample_time_ms: 5568.317\n",
      "    update_time_ms: 2.401\n",
      "  timestamp: 1650548694\n",
      "  timesteps_since_restore: 372000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 372000\n",
      "  training_iteration: 93\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:44:54,146\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 4510.0x the scale of `vf_clip_param`. This means that it will take more than 4510.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:44:59 (running for 00:08:59.17)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    93</td><td style=\"text-align: right;\">         514.786</td><td style=\"text-align: right;\">372000</td><td style=\"text-align: right;\">  -45103</td><td style=\"text-align: right;\">            -11681.6</td><td style=\"text-align: right;\">             -500210</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 376000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-44-59\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -12799.80907814567\n",
      "  episode_reward_mean: -43977.72783713479\n",
      "  episode_reward_min: -504551.0146688706\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 15040\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.125781059265137\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 3.99295973777771\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01207373570650816\n",
      "          model: {}\n",
      "          policy_loss: -0.04744469001889229\n",
      "          total_loss: 10.014442443847656\n",
      "          vf_explained_var: -3.8239006244111806e-05\n",
      "          vf_loss: 10.0\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 376000\n",
      "    num_agent_steps_trained: 376000\n",
      "    num_steps_sampled: 376000\n",
      "    num_steps_trained: 376000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 94\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 7.2749999999999995\n",
      "    ram_util_percent: 2.4375\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.15418375718650726\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.28997921086249956\n",
      "    mean_inference_ms: 0.922943565939474\n",
      "    mean_raw_obs_processing_ms: 0.13007166657027872\n",
      "  time_since_restore: 520.4014563560486\n",
      "  time_this_iter_s: 5.615647315979004\n",
      "  time_total_s: 520.4014563560486\n",
      "  timers:\n",
      "    learn_throughput: 1632.397\n",
      "    learn_time_ms: 2450.384\n",
      "    load_throughput: 13106176.08\n",
      "    load_time_ms: 0.305\n",
      "    sample_throughput: 717.034\n",
      "    sample_time_ms: 5578.539\n",
      "    update_time_ms: 2.387\n",
      "  timestamp: 1650548699\n",
      "  timesteps_since_restore: 376000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 376000\n",
      "  training_iteration: 94\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:44:59,801\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 4398.0x the scale of `vf_clip_param`. This means that it will take more than 4398.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:45:04 (running for 00:09:04.86)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    94</td><td style=\"text-align: right;\">         520.401</td><td style=\"text-align: right;\">376000</td><td style=\"text-align: right;\">-43977.7</td><td style=\"text-align: right;\">            -12799.8</td><td style=\"text-align: right;\">             -504551</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 380000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-45-05\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -15120.506271768261\n",
      "  episode_reward_mean: -48056.84032474298\n",
      "  episode_reward_min: -511132.46217124525\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 15200\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.125781059265137\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 4.002501487731934\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01086338609457016\n",
      "          model: {}\n",
      "          policy_loss: -0.04930447041988373\n",
      "          total_loss: 10.00143814086914\n",
      "          vf_explained_var: -2.1326925434550503e-06\n",
      "          vf_loss: 9.995060920715332\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 380000\n",
      "    num_agent_steps_trained: 380000\n",
      "    num_steps_sampled: 380000\n",
      "    num_steps_trained: 380000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 95\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.4125000000000005\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.15419979313698673\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.29002551473419724\n",
      "    mean_inference_ms: 0.9228569862180649\n",
      "    mean_raw_obs_processing_ms: 0.13008401819038662\n",
      "  time_since_restore: 526.0215027332306\n",
      "  time_this_iter_s: 5.620046377182007\n",
      "  time_total_s: 526.0215027332306\n",
      "  timers:\n",
      "    learn_throughput: 1630.927\n",
      "    learn_time_ms: 2452.593\n",
      "    load_throughput: 13151380.419\n",
      "    load_time_ms: 0.304\n",
      "    sample_throughput: 717.122\n",
      "    sample_time_ms: 5577.855\n",
      "    update_time_ms: 2.418\n",
      "  timestamp: 1650548705\n",
      "  timesteps_since_restore: 380000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 380000\n",
      "  training_iteration: 95\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:45:05,461\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 4806.0x the scale of `vf_clip_param`. This means that it will take more than 4806.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:45:10 (running for 00:09:10.48)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    95</td><td style=\"text-align: right;\">         526.022</td><td style=\"text-align: right;\">380000</td><td style=\"text-align: right;\">-48056.8</td><td style=\"text-align: right;\">            -15120.5</td><td style=\"text-align: right;\">             -511132</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 384000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-45-11\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -8816.437212873148\n",
      "  episode_reward_mean: -39513.75999161069\n",
      "  episode_reward_min: -295197.69154684606\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 15360\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.125781059265137\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 3.808952808380127\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011392820626497269\n",
      "          model: {}\n",
      "          policy_loss: -0.05692817270755768\n",
      "          total_loss: 9.991920471191406\n",
      "          vf_explained_var: 6.16882425674703e-06\n",
      "          vf_loss: 9.990452766418457\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 384000\n",
      "    num_agent_steps_trained: 384000\n",
      "    num_steps_sampled: 384000\n",
      "    num_steps_trained: 384000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 96\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.4375\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1542439480051298\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.29008289961036776\n",
      "    mean_inference_ms: 0.9227614410941756\n",
      "    mean_raw_obs_processing_ms: 0.13010350094035966\n",
      "  time_since_restore: 531.6078107357025\n",
      "  time_this_iter_s: 5.586308002471924\n",
      "  time_total_s: 531.6078107357025\n",
      "  timers:\n",
      "    learn_throughput: 1627.401\n",
      "    learn_time_ms: 2457.907\n",
      "    load_throughput: 13168929.356\n",
      "    load_time_ms: 0.304\n",
      "    sample_throughput: 716.403\n",
      "    sample_time_ms: 5583.449\n",
      "    update_time_ms: 2.433\n",
      "  timestamp: 1650548711\n",
      "  timesteps_since_restore: 384000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 384000\n",
      "  training_iteration: 96\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:45:11,087\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 3951.0x the scale of `vf_clip_param`. This means that it will take more than 3951.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:45:16 (running for 00:09:16.15)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    96</td><td style=\"text-align: right;\">         531.608</td><td style=\"text-align: right;\">384000</td><td style=\"text-align: right;\">-39513.8</td><td style=\"text-align: right;\">            -8816.44</td><td style=\"text-align: right;\">             -295198</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 388000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-45-16\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -11648.085985708882\n",
      "  episode_reward_mean: -57599.53796571928\n",
      "  episode_reward_min: -602249.3561987167\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 15520\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.125781059265137\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 4.177586555480957\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012753279879689217\n",
      "          model: {}\n",
      "          policy_loss: -0.04976975917816162\n",
      "          total_loss: 10.01560115814209\n",
      "          vf_explained_var: 5.1336905926291365e-06\n",
      "          vf_loss: 10.0\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 388000\n",
      "    num_agent_steps_trained: 388000\n",
      "    num_steps_sampled: 388000\n",
      "    num_steps_trained: 388000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 97\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.525\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.15429687110666923\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2901580425746188\n",
      "    mean_inference_ms: 0.9227631106216808\n",
      "    mean_raw_obs_processing_ms: 0.1301343842595282\n",
      "  time_since_restore: 537.1503465175629\n",
      "  time_this_iter_s: 5.542535781860352\n",
      "  time_total_s: 537.1503465175629\n",
      "  timers:\n",
      "    learn_throughput: 1629.958\n",
      "    learn_time_ms: 2454.052\n",
      "    load_throughput: 13058231.631\n",
      "    load_time_ms: 0.306\n",
      "    sample_throughput: 714.761\n",
      "    sample_time_ms: 5596.28\n",
      "    update_time_ms: 2.433\n",
      "  timestamp: 1650548716\n",
      "  timesteps_since_restore: 388000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 388000\n",
      "  training_iteration: 97\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:45:16,669\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 5760.0x the scale of `vf_clip_param`. This means that it will take more than 5760.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:45:21 (running for 00:09:21.69)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    97</td><td style=\"text-align: right;\">          537.15</td><td style=\"text-align: right;\">388000</td><td style=\"text-align: right;\">-57599.5</td><td style=\"text-align: right;\">            -11648.1</td><td style=\"text-align: right;\">             -602249</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 392000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-45-22\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -11899.64163796966\n",
      "  episode_reward_mean: -46439.00902555425\n",
      "  episode_reward_min: -531106.572120834\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 15680\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.125781059265137\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 3.785126209259033\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011544781737029552\n",
      "          model: {}\n",
      "          policy_loss: -0.045577749609947205\n",
      "          total_loss: 10.0060453414917\n",
      "          vf_explained_var: -0.0001081809532479383\n",
      "          vf_loss: 9.992446899414062\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 392000\n",
      "    num_agent_steps_trained: 392000\n",
      "    num_steps_sampled: 392000\n",
      "    num_steps_trained: 392000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 98\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.699999999999999\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.15431338154876295\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2901487490692201\n",
      "    mean_inference_ms: 0.9225834248073866\n",
      "    mean_raw_obs_processing_ms: 0.1301240187765476\n",
      "  time_since_restore: 542.6605043411255\n",
      "  time_this_iter_s: 5.510157823562622\n",
      "  time_total_s: 542.6605043411255\n",
      "  timers:\n",
      "    learn_throughput: 1629.923\n",
      "    learn_time_ms: 2454.103\n",
      "    load_throughput: 13152411.414\n",
      "    load_time_ms: 0.304\n",
      "    sample_throughput: 716.615\n",
      "    sample_time_ms: 5581.796\n",
      "    update_time_ms: 2.437\n",
      "  timestamp: 1650548722\n",
      "  timesteps_since_restore: 392000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 392000\n",
      "  training_iteration: 98\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:45:22,219\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 4644.0x the scale of `vf_clip_param`. This means that it will take more than 4644.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:45:27 (running for 00:09:27.28)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    98</td><td style=\"text-align: right;\">         542.661</td><td style=\"text-align: right;\">392000</td><td style=\"text-align: right;\">  -46439</td><td style=\"text-align: right;\">            -11899.6</td><td style=\"text-align: right;\">             -531107</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 396000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-45-27\n",
      "  done: false\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -6797.449105668711\n",
      "  episode_reward_mean: -34278.172864495114\n",
      "  episode_reward_min: -192525.96780340737\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 15840\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.125781059265137\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 3.6305787563323975\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016384409740567207\n",
      "          model: {}\n",
      "          policy_loss: -0.07377805560827255\n",
      "          total_loss: 10.002551078796387\n",
      "          vf_explained_var: -0.0002024428831646219\n",
      "          vf_loss: 9.992345809936523\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 396000\n",
      "    num_agent_steps_trained: 396000\n",
      "    num_steps_sampled: 396000\n",
      "    num_steps_trained: 396000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 99\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.237500000000001\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1543450989262434\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2902127826567429\n",
      "    mean_inference_ms: 0.922526221256762\n",
      "    mean_raw_obs_processing_ms: 0.13014591538360207\n",
      "  time_since_restore: 548.1698820590973\n",
      "  time_this_iter_s: 5.509377717971802\n",
      "  time_total_s: 548.1698820590973\n",
      "  timers:\n",
      "    learn_throughput: 1632.679\n",
      "    learn_time_ms: 2449.961\n",
      "    load_throughput: 13172031.091\n",
      "    load_time_ms: 0.304\n",
      "    sample_throughput: 716.953\n",
      "    sample_time_ms: 5579.163\n",
      "    update_time_ms: 2.399\n",
      "  timestamp: 1650548727\n",
      "  timesteps_since_restore: 396000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 396000\n",
      "  training_iteration: 99\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:45:27,769\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 3428.0x the scale of `vf_clip_param`. This means that it will take more than 3428.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:45:32 (running for 00:09:32.79)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>RUNNING </td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">          548.17</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\">-34278.2</td><td style=\"text-align: right;\">            -6797.45</td><td style=\"text-align: right;\">             -192526</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_SimpleSupplyChain_faf0d_00000:\n",
      "  agent_timesteps_total: 400000\n",
      "  custom_metrics: {}\n",
      "  date: 2022-04-21_13-45-33\n",
      "  done: true\n",
      "  episode_len_mean: 25.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: -12503.038885641743\n",
      "  episode_reward_mean: -44196.193679968164\n",
      "  episode_reward_min: -491211.9846777207\n",
      "  episodes_this_iter: 160\n",
      "  episodes_total: 16000\n",
      "  experiment_id: 276646b0283b4bc29b7505c158740e0f\n",
      "  hostname: ip-172-16-30-231\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 5.125781059265137\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 3.878472089767456\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00904359482228756\n",
      "          model: {}\n",
      "          policy_loss: -0.051091354340314865\n",
      "          total_loss: 9.98776912689209\n",
      "          vf_explained_var: -0.00020302803022786975\n",
      "          vf_loss: 9.99250316619873\n",
      "        num_agent_steps_trained: 128.0\n",
      "    num_agent_steps_sampled: 400000\n",
      "    num_agent_steps_trained: 400000\n",
      "    num_steps_sampled: 400000\n",
      "    num_steps_trained: 400000\n",
      "    num_steps_trained_this_iter: 4000\n",
      "  iterations_since_restore: 100\n",
      "  node_ip: 172.16.30.231\n",
      "  num_healthy_workers: 2\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 4.637499999999999\n",
      "    ram_util_percent: 2.4\n",
      "  pid: 86710\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1543465562148355\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.2902068970013273\n",
      "    mean_inference_ms: 0.9228418842799586\n",
      "    mean_raw_obs_processing_ms: 0.13014705136263593\n",
      "  time_since_restore: 553.7134416103363\n",
      "  time_this_iter_s: 5.543559551239014\n",
      "  time_total_s: 553.7134416103363\n",
      "  timers:\n",
      "    learn_throughput: 1630.072\n",
      "    learn_time_ms: 2453.88\n",
      "    load_throughput: 13189635.22\n",
      "    load_time_ms: 0.303\n",
      "    sample_throughput: 717.386\n",
      "    sample_time_ms: 5575.801\n",
      "    update_time_ms: 2.373\n",
      "  timestamp: 1650548733\n",
      "  timesteps_since_restore: 400000\n",
      "  timesteps_this_iter: 4000\n",
      "  timesteps_total: 400000\n",
      "  training_iteration: 100\n",
      "  trial_id: faf0d_00000\n",
      "  warmup_time: 9.186723470687866\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(PPOTrainer pid=86710)\u001b[0m 2022-04-21 13:45:33,352\tWARNING ppo.py:174 -- The magnitude of your environment rewards are more than 4420.0x the scale of `vf_clip_param`. This means that it will take more than 4420.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-04-21 13:45:33 (running for 00:09:33.42)<br>Memory usage on this node: 17.6/720.5 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/64 CPUs, 0/16 GPUs, 0.0/516.34 GiB heap, 0.0/186.26 GiB objects (0.0/1.0 accelerator_type:K80)<br>Result logdir: /home/ec2-user/ray_results/PPO<br>Number of trials: 1/1 (1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                       </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">   lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_SimpleSupplyChain_faf0d_00000</td><td>TERMINATED</td><td>172.16.30.231:86710</td><td style=\"text-align: right;\">0.001</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">         553.713</td><td style=\"text-align: right;\">400000</td><td style=\"text-align: right;\">-44196.2</td><td style=\"text-align: right;\">              -12503</td><td style=\"text-align: right;\">             -491212</td><td style=\"text-align: right;\">                25</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-21 13:45:33,954\tINFO tune.py:702 -- Total run time: 574.26 seconds (573.38 seconds for the tuning loop).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7fdfc850d048>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alg = 'PPO'\n",
    "tune.run(alg,\n",
    "    stop={\"training_iteration\": 100},\n",
    "    config={\n",
    "        'env':SimpleSupplyChain,\n",
    "        'num_gpus':0,\n",
    "        'num_workers':2,\n",
    "        'lr':tune.grid_search([.001,])     \n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b20017",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
