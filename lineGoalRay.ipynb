{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/mingjunwang/opt/miniconda3/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from gym.utils import seeding\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.agents import ppo\n",
    "from ray.tune.registry import register_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Space Attributes\n",
    "    Most environments have two special attributes: action_space, observation_space\n",
    "\n",
    "    These contain instances of gym.spaces classes\n",
    "    Makes it easy to find out what are valid states and actions I\n",
    "    There is a convenient sample method to generate uniform random samples in the space.\n",
    "#### gym.spaces\n",
    "    Action spaces and State spaces are defined by instances of classes of the gym.spaces modules\n",
    "\n",
    "    Included types are:\n",
    "      gym.spaces.Discrete\n",
    "\n",
    "      gym.spaces.MultiDiscrete\n",
    "\n",
    "      gym.spaces.Box\n",
    "\n",
    "      gym.spaces.Tuple\n",
    "    All instances have a sample method which will sample random instances within the space\n",
    "#### gym.spaces.Discrete\n",
    "    The homework environments will use this type of space Specifies a space containing n discrete points\n",
    "    Each point is mapped to an integer from [0 ,n−1]\n",
    "    Discrete(10) A space containing 10 items mapped to integers in [0,9] sample will return integers such as 0, 3, and 9.\n",
    "#### gym.spaces.MultiDiscrete\n",
    "    You will use this to implement an environment in the homework\n",
    "    Species a space containing k dimensions each with a separate number of discrete points.\n",
    "    Each point in the space is represented by a vector of integers of length k\n",
    "    MultiDiscrete([(1, 3), (0, 5)]) A space with k= 2 dimensions First dimension has 4 points mapped to integers in [1,3] Second dimension has 6 points mapped to integers in [0,5] sample will return a vector such as [2,5] and [1,3]\n",
    "#### gym.spaces.Box\n",
    "    Used for multidimensional continuous spaces with bounds\n",
    "    You will see environments with these types of state and action spaces in future homeworks\n",
    "    Box(np.array((-1.0, -2.0)), np.array((1.0, 2.0))) A 2D continous state spaceI First dimension has values in range [−1.0,1.0) Second dimension has values in range [−2.0,2.0) sample will return a vector such as [−.55,2.] and [.768,−1.55]\n",
    "\n",
    "### Creating an Environment\n",
    "#### gym.Env Class\n",
    "    All environments should inherit from gym.Env\n",
    "    At a minimum you must override a handful of methods:\n",
    "    step()\n",
    "    reset()\n",
    "    At a minimum you must provide the following attributes action_space, observation_space\n",
    "#### Subclass Methods\n",
    "    _step is the same api as the step function used in the example\n",
    "    _reset is the same api as the reset function in the example\n",
    "    You may also provide the following methods for additionalfunctionality:\n",
    "\n",
    "    _render\n",
    "    _close\n",
    "    _configure\n",
    "    _seed\n",
    "\n",
    "#### Attributes\n",
    "    observation_space represents the state space\n",
    "    action_space represents the action space\n",
    "    Both are instances of gym.spaces classes\n",
    "    You can also provide a reward_range , but this defaults to (−∞,∞)\n",
    "#### Registration\n",
    "    How do you get your environment to work with gym.make()? You must register it\n",
    "#### Registration Example\n",
    "    from gym.envs.registration import register\n",
    "\n",
    "    register(\n",
    "      id='Deterministic-4x4-FrozenLake-v0',\n",
    "      entry_point='gym.envs.toy_text.frozen_lake:FrozenLakeEnv',\n",
    "      kwargs={'map_name': '4x4',\n",
    "      'is_slippery': False})\n",
    "    id: the environment name used with gym.make\n",
    "    entry_point: module path and class name of environment\n",
    "    kwargs: dictionary of keyword arguments to environment constructor\n",
    "#### Discrete Environment Class\n",
    "    A subclass of the gym.Env which provides the followingattributes\n",
    "    nS: number of states\n",
    "    nA: number of actions\n",
    "    P: model of environment\n",
    "    isd: initial state distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEnv (gym.Env):\n",
    "    # possible actions\n",
    "    MOVE_LF = 0\n",
    "    MOVE_RT = 1\n",
    "\n",
    "    # possible positions\n",
    "    LF_MIN = 1\n",
    "    RT_MAX = 10\n",
    "\n",
    "    # land on the GOAL position within MAX_STEPS steps\n",
    "    MAX_STEPS = 10\n",
    "\n",
    "    # possible rewards\n",
    "    REWARD_AWAY = -2\n",
    "    REWARD_STEP = -1\n",
    "    REWARD_GOAL = MAX_STEPS\n",
    "\n",
    "    metadata = {\n",
    "        \"render.modes\": [\"human\"]\n",
    "        }\n",
    "\n",
    "\n",
    "    def __init__ (self, config):\n",
    "        # the action space ranges [0, 1] where:\n",
    "        #  `0` move left\n",
    "        #  `1` move right\n",
    "        self.action_space = gym.spaces.Discrete(2)\n",
    "\n",
    "        # NB: Ray throws exceptions for any `0` value Discrete\n",
    "        # observations so we'll make position a 1's based value\n",
    "        self.observation_space = gym.spaces.Discrete(self.RT_MAX + 1)\n",
    "\n",
    "        # possible positions to chose on `reset()`\n",
    "        self.goal = int((self.LF_MIN + self.RT_MAX - 1) / 2)\n",
    "\n",
    "        self.init_positions = list(range(self.LF_MIN, self.RT_MAX))\n",
    "        self.init_positions.remove(self.goal)\n",
    "\n",
    "        # NB: change to guarantee the sequence of pseudorandom numbers\n",
    "        # (e.g., for debugging)\n",
    "        self.seed()\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "\n",
    "    def reset (self):\n",
    "        \"\"\"\n",
    "        Reset the state of the environment and returns an initial observation.\n",
    "        Returns\n",
    "        -------\n",
    "        observation (object): the initial observation of the space.\n",
    "        \"\"\"\n",
    "        self.position = self.np_random.choice(self.init_positions)\n",
    "        self.count = 0\n",
    "\n",
    "        # for this environment, state is simply the position\n",
    "        self.state = self.position\n",
    "        self.reward = 0\n",
    "        self.done = False\n",
    "        self.info = {}\n",
    "\n",
    "        return self.state\n",
    "\n",
    "\n",
    "    def step (self, action):\n",
    "        \"\"\"\n",
    "        The agent takes a step in the environment.\n",
    "        Parameters\n",
    "        ----------\n",
    "        action : Discrete\n",
    "        Returns\n",
    "        -------\n",
    "        observation, reward, done, info : tuple\n",
    "            observation (object) :\n",
    "                an environment-specific object representing your observation of\n",
    "                the environment.\n",
    "            reward (float) :\n",
    "                amount of reward achieved by the previous action. The scale\n",
    "                varies between environments, but the goal is always to increase\n",
    "                your total reward.\n",
    "            done (bool) :\n",
    "                whether it's time to reset the environment again. Most (but not\n",
    "                all) tasks are divided up into well-defined episodes, and done\n",
    "                being True indicates the episode has terminated. (For example,\n",
    "                perhaps the pole tipped too far, or you lost your last life.)\n",
    "            info (dict) :\n",
    "                 diagnostic information useful for debugging. It can sometimes\n",
    "                 be useful for learning (for example, it might contain the raw\n",
    "                 probabilities behind the environment's last state change).\n",
    "                 However, official evaluations of your agent are not allowed to\n",
    "                 use this for learning.\n",
    "        \"\"\"\n",
    "        if self.done:\n",
    "            # code should never reach this point\n",
    "            print(\"EPISODE DONE!!!\")\n",
    "\n",
    "        elif self.count == self.MAX_STEPS:\n",
    "            self.done = True;\n",
    "\n",
    "        else:\n",
    "            assert self.action_space.contains(action)\n",
    "            self.count += 1\n",
    "\n",
    "            if action == self.MOVE_LF:\n",
    "                if self.position == self.LF_MIN:\n",
    "                    # invalid\n",
    "                    self.reward = self.REWARD_AWAY\n",
    "                else:\n",
    "                    self.position -= 1\n",
    "\n",
    "                    if self.position == self.goal:\n",
    "                        # on goal now\n",
    "                        self.reward = self.REWARD_GOAL\n",
    "                        self.done = 1\n",
    "                    elif self.position < self.goal:\n",
    "                        # moving away from goal\n",
    "                        self.reward = self.REWARD_AWAY\n",
    "                    else:\n",
    "                        # moving toward goal\n",
    "                        self.reward = self.REWARD_STEP\n",
    "\n",
    "            elif action == self.MOVE_RT:\n",
    "                if self.position == self.RT_MAX:\n",
    "                    # invalid\n",
    "                    self.reward = self.REWARD_AWAY\n",
    "                else:\n",
    "                    self.position += 1\n",
    "\n",
    "                    if self.position == self.goal:\n",
    "                        # on goal now\n",
    "                        self.reward = self.REWARD_GOAL\n",
    "                        self.done = 1\n",
    "                    elif self.position > self.goal:\n",
    "                        # moving away from goal\n",
    "                        self.reward = self.REWARD_AWAY\n",
    "                    else:\n",
    "                        # moving toward goal\n",
    "                        self.reward = self.REWARD_STEP\n",
    "\n",
    "            self.state = self.position\n",
    "            self.info[\"dist\"] = self.goal - self.position\n",
    "\n",
    "        try:\n",
    "            assert self.observation_space.contains(self.state)\n",
    "        except AssertionError:\n",
    "            print(\"INVALID STATE\", self.state)\n",
    "\n",
    "        return [self.state, self.reward, self.done, self.info]\n",
    "\n",
    "\n",
    "    def render (self, mode=\"human\"):\n",
    "        \"\"\"Renders the environment.\n",
    "        The set of supported modes varies per environment. (And some\n",
    "        environments do not support rendering at all.) By convention,\n",
    "        if mode is:\n",
    "        - human: render to the current display or terminal and\n",
    "          return nothing. Usually for human consumption.\n",
    "        - rgb_array: Return an numpy.ndarray with shape (x, y, 3),\n",
    "          representing RGB values for an x-by-y pixel image, suitable\n",
    "          for turning into a video.\n",
    "        - ansi: Return a string (str) or StringIO.StringIO containing a\n",
    "          terminal-style text representation. The text can include newlines\n",
    "          and ANSI escape sequences (e.g. for colors).\n",
    "        Note:\n",
    "            Make sure that your class's metadata 'render.modes' key includes\n",
    "              the list of supported modes. It's recommended to call super()\n",
    "              in implementations to use the functionality of this method.\n",
    "        Args:\n",
    "            mode (str): the mode to render with\n",
    "        \"\"\"\n",
    "        s = \"position: {:2d}  reward: {:2d}  info: {}\"\n",
    "        print(s.format(self.state, self.reward, self.info))\n",
    "\n",
    "\n",
    "    def seed (self, seed=None):\n",
    "        \"\"\"Sets the seed for this env's random number generator(s).\n",
    "        Note:\n",
    "            Some environments use multiple pseudorandom number generators.\n",
    "            We want to capture all such seeds used in order to ensure that\n",
    "            there aren't accidental correlations between multiple generators.\n",
    "        Returns:\n",
    "            list<bigint>: Returns the list of seeds used in this env's random\n",
    "              number generators. The first value in the list should be the\n",
    "              \"main\" seed, or the value which a reproducer should pass to\n",
    "              'seed'. Often, the main seed equals the provided 'seed', but\n",
    "              this won't be true if seed=None, for example.\n",
    "        \"\"\"\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "\n",
    "    def close (self):\n",
    "        \"\"\"Override close in your subclass to perform any necessary cleanup.\n",
    "        Environments will automatically close() themselves when\n",
    "        garbage collected or when the program exits.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "File descriptor limit 256 is too low for production servers and may result in connection errors. At least 8192 is recommended. --- Fix with 'ulimit -n 8192'\n",
      "2021-02-16 14:15:12,225\tINFO services.py:1171 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8266\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '192.168.0.23',\n",
       " 'raylet_ip_address': '192.168.0.23',\n",
       " 'redis_address': '192.168.0.23:39480',\n",
       " 'object_store_address': '/tmp/ray/session_2021-02-16_14-15-11_633148_71934/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2021-02-16_14-15-11_633148_71934/sockets/raylet',\n",
       " 'webui_url': '127.0.0.1:8266',\n",
       " 'session_dir': '/tmp/ray/session_2021-02-16_14-15-11_633148_71934',\n",
       " 'metrics_export_port': 60362,\n",
       " 'node_id': 'effed84ac8f18d25be89ff088750e3aac2388404'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using train: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(iters =20, path='checkpoint'):\n",
    "    config = {\n",
    "        \"env\": CustomEnv,  # or \"corridor\" if registered above\n",
    "        # Use GPUs iff `RLLIB_NUM_GPUS` env var set to > 0.\n",
    "        \"num_gpus\": 0,\n",
    "        \"num_workers\": 5  # parallelism\n",
    "    }\n",
    "    \n",
    "    # register the custom environment\n",
    "    #register_env(\"customEnv\", CustomEnv())\n",
    "    #trainer = ppo.PPOTrainer(env=\"customEnv\")\n",
    "    \n",
    "    #trainer = ppo.PPOTrainer(config=config, env=CustomEnv)\n",
    "    trainer = ppo.PPOTrainer(env=CustomEnv, config=config)\n",
    "    n=0\n",
    "    while True:\n",
    "        if n>=iters:\n",
    "            break\n",
    "        trainer.train()\n",
    "        chkpt = trainer.save(path)\n",
    "        \n",
    "        n=+1\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    pass\n",
    "    #main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Tune: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2/12 CPUs, 0/0 GPUs, 0.0/4.3 GiB heap, 0.0/1.46 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/PPO<br>Number of trials: 1/2 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CustomEnv_b3308_00000</td><td>RUNNING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=79384)\u001b[0m WARNING:tensorflow:From /Users/mingjunwang/opt/miniconda3/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=79384)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=79384)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=79383)\u001b[0m WARNING:tensorflow:From /Users/mingjunwang/opt/miniconda3/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=79383)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=79383)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=79384)\u001b[0m 2021-02-16 14:15:27,395\tINFO trainer.py:591 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=79384)\u001b[0m 2021-02-16 14:15:27,395\tINFO trainer.py:616 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=79383)\u001b[0m 2021-02-16 14:15:27,404\tINFO trainer.py:591 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=79383)\u001b[0m 2021-02-16 14:15:27,404\tINFO trainer.py:616 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=79381)\u001b[0m WARNING:tensorflow:From /Users/mingjunwang/opt/miniconda3/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=79381)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=79381)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=79385)\u001b[0m WARNING:tensorflow:From /Users/mingjunwang/opt/miniconda3/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=79385)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=79385)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=79384)\u001b[0m 2021-02-16 14:15:33,978\tWARNING util.py:43 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=79383)\u001b[0m 2021-02-16 14:15:33,984\tWARNING util.py:43 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=79381)\u001b[0m 2021-02-16 14:15:33,988\tWARNING deprecation.py:29 -- DeprecationWarning: `env_index` has been deprecated. Use `episode.env_id` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(pid=79385)\u001b[0m 2021-02-16 14:15:33,983\tWARNING deprecation.py:29 -- DeprecationWarning: `env_index` has been deprecated. Use `episode.env_id` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CustomEnv_b3308_00001:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-16_14-15-38\n",
      "  done: false\n",
      "  episode_len_mean: 7.487804878048781\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: -5.50844277673546\n",
      "  episode_reward_min: -21.0\n",
      "  episodes_this_iter: 533\n",
      "  episodes_total: 533\n",
      "  experiment_id: 0bd8aa1204fb4f70a63ae424f19a06ba\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6498297452926636\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.044635090976953506\n",
      "        model: {}\n",
      "        policy_loss: -0.1090070977807045\n",
      "        total_loss: 58.26771545410156\n",
      "        vf_explained_var: 0.16147367656230927\n",
      "        vf_loss: 58.36779022216797\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.75\n",
      "    ram_util_percent: 65.2\n",
      "  pid: 79383\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03167409594134669\n",
      "    mean_env_wait_ms: 0.025529260785542133\n",
      "    mean_inference_ms: 0.3805544876331271\n",
      "    mean_raw_obs_processing_ms: 0.07314760665302425\n",
      "  time_since_restore: 4.177304029464722\n",
      "  time_this_iter_s: 4.177304029464722\n",
      "  time_total_s: 4.177304029464722\n",
      "  timers:\n",
      "    learn_throughput: 2091.738\n",
      "    learn_time_ms: 1912.285\n",
      "    load_throughput: 157698.387\n",
      "    load_time_ms: 25.365\n",
      "    sample_throughput: 1923.437\n",
      "    sample_time_ms: 2079.611\n",
      "    update_time_ms: 2.272\n",
      "  timestamp: 1613506538\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: b3308_00001\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=79381)\u001b[0m WARNING:tensorflow:From /Users/mingjunwang/opt/miniconda3/lib/python3.8/site-packages/ray/rllib/policy/tf_policy.py:850: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=79381)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=79381)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "\u001b[2m\u001b[36m(pid=79385)\u001b[0m WARNING:tensorflow:From /Users/mingjunwang/opt/miniconda3/lib/python3.8/site-packages/ray/rllib/policy/tf_policy.py:850: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=79385)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=79385)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "\u001b[2m\u001b[36m(pid=79384)\u001b[0m WARNING:tensorflow:From /Users/mingjunwang/opt/miniconda3/lib/python3.8/site-packages/ray/rllib/policy/tf_policy.py:850: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=79384)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=79384)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "\u001b[2m\u001b[36m(pid=79383)\u001b[0m WARNING:tensorflow:From /Users/mingjunwang/opt/miniconda3/lib/python3.8/site-packages/ray/rllib/policy/tf_policy.py:850: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=79383)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=79383)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/12 CPUs, 0/0 GPUs, 0.0/4.3 GiB heap, 0.0/1.46 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/PPO<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CustomEnv_b3308_00000</td><td>RUNNING </td><td>                  </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_CustomEnv_b3308_00001</td><td>RUNNING </td><td>192.168.0.23:79383</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">          4.1773</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\">-5.50844</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                 -21</td><td style=\"text-align: right;\">            7.4878</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CustomEnv_b3308_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-16_14-15-38\n",
      "  done: false\n",
      "  episode_len_mean: 7.541509433962264\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: -5.943396226415095\n",
      "  episode_reward_min: -21.0\n",
      "  episodes_this_iter: 530\n",
      "  episodes_total: 530\n",
      "  experiment_id: 36699cfc061e4ba9a8db597236c9313f\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6474537253379822\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.04711364209651947\n",
      "        model: {}\n",
      "        policy_loss: -0.11257157474756241\n",
      "        total_loss: 56.84780502319336\n",
      "        vf_explained_var: 0.1571224331855774\n",
      "        vf_loss: 56.95095443725586\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.71666666666667\n",
      "    ram_util_percent: 65.2\n",
      "  pid: 79384\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03212304509779538\n",
      "    mean_env_wait_ms: 0.02559331976393109\n",
      "    mean_inference_ms: 0.3885766381890855\n",
      "    mean_raw_obs_processing_ms: 0.07331141886845793\n",
      "  time_since_restore: 4.204255104064941\n",
      "  time_this_iter_s: 4.204255104064941\n",
      "  time_total_s: 4.204255104064941\n",
      "  timers:\n",
      "    learn_throughput: 2089.35\n",
      "    learn_time_ms: 1914.471\n",
      "    load_throughput: 142170.158\n",
      "    load_time_ms: 28.135\n",
      "    sample_throughput: 1892.6\n",
      "    sample_time_ms: 2113.495\n",
      "    update_time_ms: 1.676\n",
      "  timestamp: 1613506538\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: b3308_00000\n",
      "  \n",
      "Result for PPO_CustomEnv_b3308_00001:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-16_14-15-46\n",
      "  done: false\n",
      "  episode_len_mean: 3.902439024390244\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 5.812682926829268\n",
      "  episode_reward_min: -20.0\n",
      "  episodes_this_iter: 1025\n",
      "  episodes_total: 2266\n",
      "  experiment_id: 0bd8aa1204fb4f70a63ae424f19a06ba\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.38610732555389404\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.021078266203403473\n",
      "        model: {}\n",
      "        policy_loss: -0.07332336157560349\n",
      "        total_loss: 26.522127151489258\n",
      "        vf_explained_var: 0.30455079674720764\n",
      "        vf_loss: 26.585969924926758\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.03333333333333\n",
      "    ram_util_percent: 64.88333333333333\n",
      "  pid: 79383\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03171938576804391\n",
      "    mean_env_wait_ms: 0.025679792784500215\n",
      "    mean_inference_ms: 0.3745924601027611\n",
      "    mean_raw_obs_processing_ms: 0.08343567541068557\n",
      "  time_since_restore: 12.104650974273682\n",
      "  time_this_iter_s: 3.9247968196868896\n",
      "  time_total_s: 12.104650974273682\n",
      "  timers:\n",
      "    learn_throughput: 2242.94\n",
      "    learn_time_ms: 1783.374\n",
      "    load_throughput: 446995.098\n",
      "    load_time_ms: 8.949\n",
      "    sample_throughput: 1907.166\n",
      "    sample_time_ms: 2097.353\n",
      "    update_time_ms: 2.135\n",
      "  timestamp: 1613506546\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: b3308_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/12 CPUs, 0/0 GPUs, 0.0/4.3 GiB heap, 0.0/1.46 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/PPO<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CustomEnv_b3308_00000</td><td>RUNNING </td><td>192.168.0.23:79384</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         8.23408</td><td style=\"text-align: right;\"> 8000</td><td style=\"text-align: right;\"> 1.05991</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                 -20</td><td style=\"text-align: right;\">           5.69757</td></tr>\n",
       "<tr><td>PPO_CustomEnv_b3308_00001</td><td>RUNNING </td><td>192.168.0.23:79383</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">        12.1047 </td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\"> 5.81268</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                 -20</td><td style=\"text-align: right;\">           3.90244</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CustomEnv_b3308_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-16_14-15-46\n",
      "  done: false\n",
      "  episode_len_mean: 3.9332679097154073\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 5.768400392541707\n",
      "  episode_reward_min: -19.0\n",
      "  episodes_this_iter: 1019\n",
      "  episodes_total: 2250\n",
      "  experiment_id: 36699cfc061e4ba9a8db597236c9313f\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.3933897614479065\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.019531626254320145\n",
      "        model: {}\n",
      "        policy_loss: -0.07263380289077759\n",
      "        total_loss: 26.433692932128906\n",
      "        vf_explained_var: 0.27956098318099976\n",
      "        vf_loss: 26.497535705566406\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 44.083333333333336\n",
      "    ram_util_percent: 64.88333333333333\n",
      "  pid: 79384\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03211310124180732\n",
      "    mean_env_wait_ms: 0.025810157147142024\n",
      "    mean_inference_ms: 0.38436615172053124\n",
      "    mean_raw_obs_processing_ms: 0.08356421204986456\n",
      "  time_since_restore: 12.241100072860718\n",
      "  time_this_iter_s: 4.007015943527222\n",
      "  time_total_s: 12.241100072860718\n",
      "  timers:\n",
      "    learn_throughput: 2220.243\n",
      "    learn_time_ms: 1801.605\n",
      "    load_throughput: 401923.291\n",
      "    load_time_ms: 9.952\n",
      "    sample_throughput: 1869.979\n",
      "    sample_time_ms: 2139.061\n",
      "    update_time_ms: 1.385\n",
      "  timestamp: 1613506546\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: b3308_00000\n",
      "  \n",
      "Result for PPO_CustomEnv_b3308_00001:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-16_14-15-54\n",
      "  done: false\n",
      "  episode_len_mean: 2.9703043801039346\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 7.755011135857461\n",
      "  episode_reward_min: -16.0\n",
      "  episodes_this_iter: 1347\n",
      "  episodes_total: 4838\n",
      "  experiment_id: 0bd8aa1204fb4f70a63ae424f19a06ba\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.21575067937374115\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007834591902792454\n",
      "        model: {}\n",
      "        policy_loss: -0.04929664731025696\n",
      "        total_loss: 3.9568631649017334\n",
      "        vf_explained_var: 0.3932795822620392\n",
      "        vf_loss: 4.000871181488037\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.65\n",
      "    ram_util_percent: 65.15\n",
      "  pid: 79383\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03304272185301259\n",
      "    mean_env_wait_ms: 0.026909507910529772\n",
      "    mean_inference_ms: 0.39050850973839246\n",
      "    mean_raw_obs_processing_ms: 0.0984269281332686\n",
      "  time_since_restore: 20.41377902030945\n",
      "  time_this_iter_s: 3.9637181758880615\n",
      "  time_total_s: 20.41377902030945\n",
      "  timers:\n",
      "    learn_throughput: 2349.185\n",
      "    learn_time_ms: 1702.718\n",
      "    load_throughput: 703181.86\n",
      "    load_time_ms: 5.688\n",
      "    sample_throughput: 1790.483\n",
      "    sample_time_ms: 2234.034\n",
      "    update_time_ms: 2.048\n",
      "  timestamp: 1613506554\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: b3308_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/12 CPUs, 0/0 GPUs, 0.0/4.3 GiB heap, 0.0/1.46 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/PPO<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CustomEnv_b3308_00000</td><td>RUNNING </td><td>192.168.0.23:79384</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         16.6038</td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\"> 7.23342</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                 -17</td><td style=\"text-align: right;\">           3.27518</td></tr>\n",
       "<tr><td>PPO_CustomEnv_b3308_00001</td><td>RUNNING </td><td>192.168.0.23:79383</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         20.4138</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\"> 7.75501</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                 -16</td><td style=\"text-align: right;\">           2.9703 </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CustomEnv_b3308_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-16_14-15-54\n",
      "  done: false\n",
      "  episode_len_mean: 3.0\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 7.706896551724138\n",
      "  episode_reward_min: -16.0\n",
      "  episodes_this_iter: 1334\n",
      "  episodes_total: 4805\n",
      "  experiment_id: 36699cfc061e4ba9a8db597236c9313f\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.20682156085968018\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008061332628130913\n",
      "        model: {}\n",
      "        policy_loss: -0.05003109574317932\n",
      "        total_loss: 4.269073963165283\n",
      "        vf_explained_var: 0.3641972839832306\n",
      "        vf_loss: 4.31547737121582\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.25\n",
      "    ram_util_percent: 65.15\n",
      "  pid: 79384\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.03353773751894681\n",
      "    mean_env_wait_ms: 0.027109995990221243\n",
      "    mean_inference_ms: 0.4023642106316316\n",
      "    mean_raw_obs_processing_ms: 0.09923192013693381\n",
      "  time_since_restore: 20.613960027694702\n",
      "  time_this_iter_s: 4.010150909423828\n",
      "  time_total_s: 20.613960027694702\n",
      "  timers:\n",
      "    learn_throughput: 2340.773\n",
      "    learn_time_ms: 1708.837\n",
      "    load_throughput: 630750.861\n",
      "    load_time_ms: 6.342\n",
      "    sample_throughput: 1748.576\n",
      "    sample_time_ms: 2287.576\n",
      "    update_time_ms: 1.325\n",
      "  timestamp: 1613506554\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: b3308_00000\n",
      "  \n",
      "Result for PPO_CustomEnv_b3308_00001:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-16_14-16-02\n",
      "  done: false\n",
      "  episode_len_mean: 2.711864406779661\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 8.163389830508475\n",
      "  episode_reward_min: -15.0\n",
      "  episodes_this_iter: 1475\n",
      "  episodes_total: 7736\n",
      "  experiment_id: 0bd8aa1204fb4f70a63ae424f19a06ba\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.11866942793130875\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.003002533223479986\n",
      "        model: {}\n",
      "        policy_loss: -0.03551822155714035\n",
      "        total_loss: 1.805690050125122\n",
      "        vf_explained_var: 0.50164395570755\n",
      "        vf_loss: 1.8401949405670166\n",
      "    num_steps_sampled: 28000\n",
      "    num_steps_trained: 28000\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.63333333333333\n",
      "    ram_util_percent: 64.58333333333333\n",
      "  pid: 79383\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.032559202711972414\n",
      "    mean_env_wait_ms: 0.02648064862072099\n",
      "    mean_inference_ms: 0.3844384768022724\n",
      "    mean_raw_obs_processing_ms: 0.10321371121336907\n",
      "  time_since_restore: 28.244672775268555\n",
      "  time_this_iter_s: 3.887619733810425\n",
      "  time_total_s: 28.244672775268555\n",
      "  timers:\n",
      "    learn_throughput: 2399.416\n",
      "    learn_time_ms: 1667.072\n",
      "    load_throughput: 933794.334\n",
      "    load_time_ms: 4.284\n",
      "    sample_throughput: 1797.299\n",
      "    sample_time_ms: 2225.562\n",
      "    update_time_ms: 1.91\n",
      "  timestamp: 1613506562\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 7\n",
      "  trial_id: b3308_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/12 CPUs, 0/0 GPUs, 0.0/4.3 GiB heap, 0.0/1.46 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/PPO<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CustomEnv_b3308_00000</td><td>RUNNING </td><td>192.168.0.23:79384</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         24.6146</td><td style=\"text-align: right;\">24000</td><td style=\"text-align: right;\"> 8.01697</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                  -1</td><td style=\"text-align: right;\">           2.82815</td></tr>\n",
       "<tr><td>PPO_CustomEnv_b3308_00001</td><td>RUNNING </td><td>192.168.0.23:79383</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         28.2447</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\"> 8.16339</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                 -15</td><td style=\"text-align: right;\">           2.71186</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CustomEnv_b3308_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-16_14-16-02\n",
      "  done: false\n",
      "  episode_len_mean: 2.6852348993288593\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 8.206711409395973\n",
      "  episode_reward_min: -2.0\n",
      "  episodes_this_iter: 1490\n",
      "  episodes_total: 7709\n",
      "  experiment_id: 36699cfc061e4ba9a8db597236c9313f\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.11157828569412231\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0022592744790017605\n",
      "        model: {}\n",
      "        policy_loss: -0.038646992295980453\n",
      "        total_loss: 1.0654664039611816\n",
      "        vf_explained_var: 0.5826941728591919\n",
      "        vf_loss: 1.103096604347229\n",
      "    num_steps_sampled: 28000\n",
      "    num_steps_trained: 28000\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.3\n",
      "    ram_util_percent: 64.56666666666666\n",
      "  pid: 79384\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0331724104270446\n",
      "    mean_env_wait_ms: 0.026801812521479246\n",
      "    mean_inference_ms: 0.3965851971176811\n",
      "    mean_raw_obs_processing_ms: 0.10473188095989198\n",
      "  time_since_restore: 28.662264108657837\n",
      "  time_this_iter_s: 4.04761528968811\n",
      "  time_total_s: 28.662264108657837\n",
      "  timers:\n",
      "    learn_throughput: 2371.389\n",
      "    learn_time_ms: 1686.775\n",
      "    load_throughput: 834693.296\n",
      "    load_time_ms: 4.792\n",
      "    sample_throughput: 1751.219\n",
      "    sample_time_ms: 2284.123\n",
      "    update_time_ms: 1.358\n",
      "  timestamp: 1613506562\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 7\n",
      "  trial_id: b3308_00000\n",
      "  \n",
      "Result for PPO_CustomEnv_b3308_00001:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-16_14-16-10\n",
      "  done: false\n",
      "  episode_len_mean: 2.5141420490257698\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 8.450659962287869\n",
      "  episode_reward_min: 3.0\n",
      "  episodes_this_iter: 1591\n",
      "  episodes_total: 10853\n",
      "  experiment_id: 0bd8aa1204fb4f70a63ae424f19a06ba\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.08437500149011612\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.04338650777935982\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0034784788731485605\n",
      "        model: {}\n",
      "        policy_loss: -0.02687232382595539\n",
      "        total_loss: 0.23930752277374268\n",
      "        vf_explained_var: 0.8385013937950134\n",
      "        vf_loss: 0.2658863365650177\n",
      "    num_steps_sampled: 36000\n",
      "    num_steps_trained: 36000\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.35\n",
      "    ram_util_percent: 65.13333333333333\n",
      "  pid: 79383\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.032748935229314505\n",
      "    mean_env_wait_ms: 0.026627741357736545\n",
      "    mean_inference_ms: 0.3864471655627602\n",
      "    mean_raw_obs_processing_ms: 0.10888403195321059\n",
      "  time_since_restore: 36.76871395111084\n",
      "  time_this_iter_s: 4.5674519538879395\n",
      "  time_total_s: 36.76871395111084\n",
      "  timers:\n",
      "    learn_throughput: 2385.568\n",
      "    learn_time_ms: 1676.75\n",
      "    load_throughput: 1141402.112\n",
      "    load_time_ms: 3.504\n",
      "    sample_throughput: 1771.06\n",
      "    sample_time_ms: 2258.534\n",
      "    update_time_ms: 1.867\n",
      "  timestamp: 1613506570\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 9\n",
      "  trial_id: b3308_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4/12 CPUs, 0/0 GPUs, 0.0/4.3 GiB heap, 0.0/1.46 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/PPO<br>Number of trials: 2/2 (2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CustomEnv_b3308_00000</td><td>RUNNING </td><td>192.168.0.23:79384</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         32.6744</td><td style=\"text-align: right;\">32000</td><td style=\"text-align: right;\"> 8.34521</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">           2.59003</td></tr>\n",
       "<tr><td>PPO_CustomEnv_b3308_00001</td><td>RUNNING </td><td>192.168.0.23:79383</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         36.7687</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\"> 8.45066</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   3</td><td style=\"text-align: right;\">           2.51414</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_CustomEnv_b3308_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-16_14-16-11\n",
      "  done: false\n",
      "  episode_len_mean: 2.5673940949935816\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 8.398587933247754\n",
      "  episode_reward_min: 2.0\n",
      "  episodes_this_iter: 1558\n",
      "  episodes_total: 10811\n",
      "  experiment_id: 36699cfc061e4ba9a8db597236c9313f\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.11249999701976776\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.04854472726583481\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0020619549322873354\n",
      "        model: {}\n",
      "        policy_loss: -0.022537557408213615\n",
      "        total_loss: 0.2924669682979584\n",
      "        vf_explained_var: 0.8185659646987915\n",
      "        vf_loss: 0.3147725462913513\n",
      "    num_steps_sampled: 36000\n",
      "    num_steps_trained: 36000\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.65714285714285\n",
      "    ram_util_percent: 65.14285714285714\n",
      "  pid: 79384\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.033868504532151525\n",
      "    mean_env_wait_ms: 0.02739136160944857\n",
      "    mean_inference_ms: 0.40436151282581567\n",
      "    mean_raw_obs_processing_ms: 0.1125578476201792\n",
      "  time_since_restore: 37.35816740989685\n",
      "  time_this_iter_s: 4.683783292770386\n",
      "  time_total_s: 37.35816740989685\n",
      "  timers:\n",
      "    learn_throughput: 2396.602\n",
      "    learn_time_ms: 1669.029\n",
      "    load_throughput: 1003535.381\n",
      "    load_time_ms: 3.986\n",
      "    sample_throughput: 1699.987\n",
      "    sample_time_ms: 2352.959\n",
      "    update_time_ms: 1.337\n",
      "  timestamp: 1613506571\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 9\n",
      "  trial_id: b3308_00000\n",
      "  \n",
      "Result for PPO_CustomEnv_b3308_00001:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-16_14-16-15\n",
      "  done: true\n",
      "  episode_len_mean: 2.5374365482233503\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 8.440989847715736\n",
      "  episode_reward_min: 4.0\n",
      "  episodes_this_iter: 1576\n",
      "  episodes_total: 12429\n",
      "  experiment_id: 0bd8aa1204fb4f70a63ae424f19a06ba\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.04218750074505806\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.021767331287264824\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0021667105611413717\n",
      "        model: {}\n",
      "        policy_loss: -0.01825685240328312\n",
      "        total_loss: 0.1665392816066742\n",
      "        vf_explained_var: 0.874019980430603\n",
      "        vf_loss: 0.1847047358751297\n",
      "    num_steps_sampled: 40000\n",
      "    num_steps_trained: 40000\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.166666666666664\n",
      "    ram_util_percent: 65.10000000000001\n",
      "  pid: 79383\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.033028670952995456\n",
      "    mean_env_wait_ms: 0.026849230445416844\n",
      "    mean_inference_ms: 0.3893186335521817\n",
      "    mean_raw_obs_processing_ms: 0.11172609559768638\n",
      "  time_since_restore: 41.13960409164429\n",
      "  time_this_iter_s: 4.370890140533447\n",
      "  time_total_s: 41.13960409164429\n",
      "  timers:\n",
      "    learn_throughput: 2378.231\n",
      "    learn_time_ms: 1681.923\n",
      "    load_throughput: 1235108.219\n",
      "    load_time_ms: 3.239\n",
      "    sample_throughput: 1751.271\n",
      "    sample_time_ms: 2284.055\n",
      "    update_time_ms: 1.909\n",
      "  timestamp: 1613506575\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 10\n",
      "  trial_id: b3308_00001\n",
      "  \n",
      "Result for PPO_CustomEnv_b3308_00000:\n",
      "  custom_metrics: {}\n",
      "  date: 2021-02-16_14-16-15\n",
      "  done: true\n",
      "  episode_len_mean: 2.56145966709347\n",
      "  episode_reward_max: 10.0\n",
      "  episode_reward_mean: 8.413572343149808\n",
      "  episode_reward_min: 4.0\n",
      "  episodes_this_iter: 1562\n",
      "  episodes_total: 12373\n",
      "  experiment_id: 36699cfc061e4ba9a8db597236c9313f\n",
      "  hostname: Mingjuns-MacBook-Pro.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.05624999850988388\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.024238578975200653\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0026789545081555843\n",
      "        model: {}\n",
      "        policy_loss: -0.021335527300834656\n",
      "        total_loss: 0.17988741397857666\n",
      "        vf_explained_var: 0.8673672080039978\n",
      "        vf_loss: 0.20107227563858032\n",
      "    num_steps_sampled: 40000\n",
      "    num_steps_trained: 40000\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 192.168.0.23\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 41.416666666666664\n",
      "    ram_util_percent: 65.01666666666667\n",
      "  pid: 79384\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.034093010446726746\n",
      "    mean_env_wait_ms: 0.02759404934745101\n",
      "    mean_inference_ms: 0.4069721700847096\n",
      "    mean_raw_obs_processing_ms: 0.11515346176251601\n",
      "  time_since_restore: 41.60544967651367\n",
      "  time_this_iter_s: 4.247282266616821\n",
      "  time_total_s: 41.60544967651367\n",
      "  timers:\n",
      "    learn_throughput: 2414.088\n",
      "    learn_time_ms: 1656.941\n",
      "    load_throughput: 1073556.313\n",
      "    load_time_ms: 3.726\n",
      "    sample_throughput: 1683.491\n",
      "    sample_time_ms: 2376.016\n",
      "    update_time_ms: 1.324\n",
      "  timestamp: 1613506575\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 10\n",
      "  trial_id: b3308_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/12 CPUs, 0/0 GPUs, 0.0/4.3 GiB heap, 0.0/1.46 GiB objects<br>Result logdir: /Users/mingjunwang/ray_results/PPO<br>Number of trials: 2/2 (2 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_CustomEnv_b3308_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         41.6054</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\"> 8.41357</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">           2.56146</td></tr>\n",
       "<tr><td>PPO_CustomEnv_b3308_00001</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         41.1396</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\"> 8.44099</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">                   4</td><td style=\"text-align: right;\">           2.53744</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-16 14:16:16,088\tINFO tune.py:448 -- Total run time: 52.72 seconds (52.41 seconds for the tuning loop).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7f8bb27b7b50>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop = {\n",
    "        \"training_iteration\": 10,\n",
    "        \"episode_reward_mean\": 100,\n",
    "    }\n",
    "\n",
    "config = {\n",
    "        \"env\": CustomEnv, \n",
    "        # Use GPUs iff `RLLIB_NUM_GPUS` env var set to > 0.\n",
    "        \"num_gpus\": 0,\n",
    "        \"num_workers\": 1  # parallelism\n",
    "    }\n",
    "    \n",
    "tune.run('PPO',num_samples=2,\n",
    "    stop=stop,\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
